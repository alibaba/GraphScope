apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: gthinker-mpijob
spec:
  slotsPerWorker: ${SLOTS_PER_WORKER}          
  runPolicy:
    cleanPodPolicy: None
    ttlSecondsAfterFinished: 120
  sshAuthMountPath: /home/mpiuser/.ssh

  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        spec:
          restartPolicy: OnFailure
          volumes:
            - name: hadoop-config
              configMap:
                name: my-hadoop-cluster-hadoop   
            - name: scratch
              emptyDir: {}
            - name: gthinkerdata
              hostPath:
                path: ${HOST_PATH}
                type: Directory
            - name: shared-mpi-data
              emptyDir: {}

            
          containers:
            - name: mpi-launcher
              image: gthinker-mpi:v0.1      
              imagePullPolicy: IfNotPresent
              securityContext:
                runAsUser: 1000
              env:
                - name: HADOOP_CONF_DIR
                  value: "/etc/hadoop"

                - name: HDFS_NN_SERVICE
                  value: my-hadoop-cluster-hadoop-hdfs-nn
                - name: HDFS_NN_PORT
                  value: "9000"                   

                - name: CLEAN_OUTPUT
                  value: "1"

                - name: JAVA_HOME
                  value: /usr/lib/jvm/java-17-openjdk-amd64
                - name: PATH
                  value: "$(JAVA_HOME)/bin:/usr/local/hadoop/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"


              volumeMounts:
                - name: hadoop-config
                  mountPath: /etc/hadoop
                - name: scratch
                  mountPath: /opt/scratch
                - name: gthinkerdata
                  mountPath: /opt/data/
                - name: shared-mpi-data
                  mountPath: /mnt/mpi

              command: ["/bin/bash"]
              args:
                - "-c"
                - |
                    # 开启调试模式，如果任何命令失败则立即退出
                    # set -euxo pipefail # <- 修正了拼写

                    echo "[INFO] Waiting 10s for network/DNS to fully ready..."
                    sleep 10

                    # 1. 准备 HDFS 数据
                    #    这里的 ${DATASET} 将由 envsubst 替换
                    echo "[INFO] Preparing HDFS data for dataset: ${DATASET}"
                    DATASET_HDFS_PATH="/user/mpiuser/${DATASET}"

                    # hdfs dfs -rm -r -f -skipTrash "${DATASET_HDFS_PATH}" || true

                    if hdfs dfs -test -e "${DATASET_HDFS_PATH}"; then
                       echo "[INFO] Dataset already exists on HDFS."
                    else
                        
                        echo "[INFO] Dataset not found on HDFS, uploading..."
                        HADOOP_USER_NAME=root hdfs dfs -mkdir -p /user/mpiuser
                        HADOOP_USER_NAME=root hdfs dfs -chown mpiuser:supergroup /user/mpiuser
                        HADOOP_USER_NAME=root hdfs dfs -chmod 775 /user/mpiuser
                        hdfs dfs -put "/opt/data/${DATASET}" "${DATASET_HDFS_PATH}"
                        
                    fi
                    echo "[INFO] HDFS data is ready."

                    hdfs dfs -ls /user/mpiuser/

                    cat /etc/hadoop/core-site.xml

                    HADOOP_CP=$(cat /etc/hadoop_classpath)

                    CONVERTED_HOSTFILE="/mnt/mpi/hostfile" 
                    sed 's/ slots=/:/' /etc/mpi/hostfile > "${CONVERTED_HOSTFILE}"
                    
                    time mpiexec \
                      -np ${REPLICAS} \
                      --hostfile "${CONVERTED_HOSTFILE}" \
                      -verbose \
                      -env CLASSPATH "${HADOOP_CP}" \
                      -env LD_LIBRARY_PATH "/usr/lib/jvm/java-17-openjdk-amd64/lib/server:/usr/local/hadoop/lib/native:/usr/local/bin:/usr/local/lib:/usr/local/openmpi/bin:/usr/local/openmpi/lib" \
                      -env JAVA_HOME "/usr/lib/jvm/java-17-openjdk-amd64" \
                      -env HADOOP_CONF_DIR "/etc/hadoop" \
                      /opt/gthinker/${ALGORITHM}/run "${DATASET_HDFS_PATH}" ${SLOTS_PER_WORKER}

              resources:
                limits:
                  cpu: ${CPU}
                  memory: ${MEMORY}
                requests:
                  cpu: ${CPU}
                  memory: ${MEMORY}

    Worker:
      replicas: ${REPLICAS}
      template:
        spec:
          restartPolicy: OnFailure
          volumes:
            - name: hadoop-config
              configMap:
                name: my-hadoop-cluster-hadoop
            - name: scratch
              emptyDir: {}
            - name: gthinkerdata
              hostPath:
                path: ${HOST_PATH}
                type: Directory
            - name: shared-mpi-data
              emptyDir: {}
          containers:
            - name: mpi-worker
              image: gthinker-mpi:v0.1    
              imagePullPolicy: IfNotPresent
              securityContext:
                runAsUser: 1000
              env:
                - name: JAVA_HOME
                  value: /usr/lib/jvm/java-17-openjdk-amd64
                - name: PATH
                  value: "$(JAVA_HOME)/bin:/usr/local/hadoop/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
                - name: HADOOP_CONF_DIR
                  value: "/etc/hadoop"
                - name: HDFS_NN_SERVICE
                  value: my-hadoop-cluster-hadoop-hdfs-nn
                - name: HDFS_NN_PORT
                  value: "9000"  


              volumeMounts:
                - name: hadoop-config
                  mountPath: /etc/hadoop
                - name: scratch
                  mountPath: /opt/scratch
                - name: gthinkerdata
                  mountPath: /opt/data/
                - name: shared-mpi-data
                  mountPath: /mnt/mpi
              command: ["/usr/sbin/sshd"]
              args: ["-De","-f","/home/mpiuser/.sshd_config"]
              resources:
                limits:
                  cpu: ${CPU}
                  memory: ${MEMORY}
                requests:
                  cpu: ${CPU}
                  memory: ${MEMORY}
