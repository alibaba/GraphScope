id|published|year|month|title|authors|summary|journal_ref|doi|primary_category|categories|problem_def|keywords
be358770-c0be-534c-aced-17cf753d2396|2019-03-28T16:44:33+00:00|2019|3|BENU: Distributed Subgraph Enumeration with Backtracking-Based Framework|||||||The problem definition addressed in this research is subgraph enumeration, which involves finding all subgraph instances in a large data graph that are isomorphic to a given pattern graph. The context that makes this problem important is its wide applicability in various fields, including network motif mining, graphlet-based network comparison, network evolution analysis, and social network recommendation. The rapid growth of graph-structured data in these fields has created a need for efficient and scalable subgraph enumeration methods. The key objectives or goals set by the authors to address this problem are: Efficiency: Develop a method that can enumerate subgraphs quickly, even for large data graphs and complex pattern graphs. Scalability: Design a method that can handle massive data graphs and pattern graphs, making it suitable for real-world applications. Correctness: Ensure that the method finds all valid subgraph instances without missing or duplicating any matches. To achieve these objectives, the authors propose a novel framework called BENU (Batch Enumeration of Network Units), which leverages a distributed computing platform and a specially designed execution plan to efficiently and scalably enumerate subgraphs.|Subgraph,Enumeration,Distributed,Backtracking,Framework,Pattern,Graph,Matching,Algorithm,Scalability
9726b841-c12d-5529-9825-62dfb85e0ad6|2016-11-10T08:02:08|2016|11|DFA-G: A Unified Programming Model for Vertex-Centric Parallel Graph Processing|||||||The problem definition addressed in this research revolves around the challenges of processing large-scale graphs in parallel computing environments. The context that makes this problem important is the increasing prevalence of graph-based data in various applications, such as social networks and biological networks, which require efficient and scalable processing to extract valuable insights. The key objective of this research is to develop a novel approach to graph processing that can overcome the limitations of traditional Bulk Synchronous Parallel (BSP) models. The authors aim to design a framework that can efficiently process large-scale graphs by leveraging the strengths of message-driven computation and automaton-based modeling. Specifically, the authors set out to address the following goals: 1. Develop a graph processing framework that can handle massive graphs efficiently and scalably. 2. Design a novel approach that can simplify the complexity of graph processing algorithms. 3. Create a framework that can be easily programmed and managed, even for non-experts in parallel computing. By achieving these objectives, the authors hope to provide a more efficient, scalable, and user-friendly solution for graph processing, which can have a significant impact on various applications that rely on graph-based data.|Graph,BSP,DFA,Vertex,Parallel,Processing,Model,Programming,Synchronous,Asynchronous
82b616b6-0733-5aea-85e1-5dd4e1f4db7d|2021-11-16T14:24:49+00:00|2021|11|Ethan R. ElenbergThe University of TexasAustin, Texas 78712, USAelenberg@utexas.edu|||||||The problem addressed in this research is the estimation of 3-profiles of large graphs. A 3-profile is a vector that counts the number of times each distinct 3-node graph appears as an induced subgraph of a larger graph. This problem is important in the context of graph analytics, where 3-profiles are used to characterize the structure of large graphs. The authors aim to develop a distributed algorithm for estimating 3-profiles on massive graphs, with the goal of enabling efficient computation of these profiles on large-scale graphs. Specifically, the authors focus on two key objectives: (1) estimating the global 3-profile of a graph, and (2) calculating the local 3-profile for each vertex in the graph. The authors also aim to develop an algorithm that can estimate ego 3-profiles, which are 3-profiles of the ego graphs of individual vertices. Overall, the authors seek to provide a scalable and efficient solution for estimating 3-profiles of large graphs, which is essential for various applications in graph analytics.|3 pro les,Graph Sparsi ers,Motifs,Graph Engines,GraphLab,Distributed Systems,Graph Analytics,Subgraph Counting,Edge Sampling,Triangle Counting
733cbb7b-b4e0-5ab1-bcc3-7e0a9fc03e2d|2023-04-12T13:34:26|2023|4|Shogun: A Task Scheduling Framework for Graph Mining Accelerators|Yibo Wu||||||The problem definition addressed in this research revolves around the inefficient task scheduling in graph processing accelerators, particularly in the context of subgraph isomorphism search. The background that makes this problem important is the increasing demand for efficient graph processing in various applications, such as social network analysis, recommendation systems, and bioinformatics. However, existing graph processing accelerators suffer from low parallelism and poor memory locality, leading to underutilization of processing elements (PEs) and high memory access latency. The key objectives or goals the authors set to address this problem are: Improve parallelism: Increase the number of tasks that can be executed concurrently to maximize PE utilization. Enhance memory locality: Minimize memory access latency by scheduling tasks that access similar memory regions together. To achieve these objectives, the authors propose a novel task scheduling framework called Shogun, which decouples task generation and execution, uses a task tree structure to store and schedule tasks, and prioritizes sibling tasks to improve memory locality. The ultimate goal is to develop an efficient task scheduling strategy that can fully utilize the processing power of graph processing accelerators and accelerate subgraph isomorphism search applications.|Graph mining,Task scheduling,Accelerators,Pattern aware,Search trees,Graph algorithms,Parallelism,Locality,Out-of-order execution,PE utilization
3306aca4-d957-5366-9e95-0ef3effb3e83|2021-07-22T06:25:10|2021|7|A Block-Based Triangle Counting Algorithm on Heterogeneous Environments|||||||The problem definition addressed in this research is the efficient counting of triangles in large-scale graphs, which is a fundamental problem in graph analysis. The context that makes this problem important is the increasing size and complexity of real-world graphs, such as social networks, web graphs, and biological networks, which require scalable and efficient algorithms to extract valuable insights. The key objective of this research is to develop a scalable and efficient algorithm for triangle counting that can handle massive graphs and minimize data movement, which is a major bottleneck in distributed and heterogeneous environments. The authors aim to achieve this by proposing a novel task decomposition approach that partitions the graph into smaller tasks, allowing for parallel processing and minimizing communication costs. In summary, the problem definition is to develop an efficient algorithm for triangle counting in large-scale graphs, with the key objectives being scalability, efficiency, and minimization of data movement. The authors' approach focuses on task decomposition and parallel processing to achieve these goals.|Triangle counting,Graph processing,Heterogeneous architectures,Parallel algorithms,Task-based parallelism,Block-based partitioning,Subgraph processing,Multi-core processing,Multi-GPU processing,Graph kernels
67cb5a7a-360c-521a-ace8-eefe0f4b9b0e|2014-07-10T08:41:42|2014|7|GraphGen: An FPGA Framework for Vertex-Centric Graph Computation|||||||The problem definition addressed in this research revolves around the efficient execution of graph-based applications on Field-Programmable Gate Arrays (FPGAs). The context that makes this problem important is the increasing significance of graph-based applications in various domains, such as computer vision, machine learning, and data analytics, which require high performance and energy efficiency. However, traditional computing platforms, including CPUs and GPGPUs, struggle to efficiently execute these applications due to their irregular memory access patterns and complex data dependencies. The key objective of this research is to develop a design framework, called GraphGen, that can automatically compile high-level vertex-centric graph specifications onto FPGA platforms, enabling efficient execution of graph-based applications. The authors aim to achieve this goal by providing a vertex-centric graph specification that allows application developers to describe their graph-based applications in a platform-agnostic manner, and then automatically generating a customized FPGA implementation that can efficiently execute the application. In essence, the problem definition involves bridging the gap between the high-level description of graph-based applications and their efficient execution on FPGA platforms, which requires overcoming the challenges of irregular memory access patterns, complex data dependencies, and the need for customized hardware accelerators.|Graph,Vertex,Centric,Computation,FPGA,Framework,GraphGen,Compilation,Acceleration,Specification
96ef8575-3987-555a-bfea-36c5b330135f|2024-08-14T14:46:21.842828|2024|8|Distributed Pagerank for P2P Systems|||||||The problem addressed in this research is the need for an effective document ranking system in peer-to-peer (P2P) networks, particularly for keyword search applications. The context is that P2P networks are becoming increasingly popular for information and content management, but existing keyword search methods return all relevant documents, causing large network traffic. The authors aim to develop a distributed implementation of Google's PageRank algorithm, which is a widely used ranking system for web pages, to provide a uniform ranking scheme for documents in P2P systems. The key objective is to enable incremental computation of PageRank as new documents are entered or deleted from the network, allowing for continuously accurate rankings and reducing network traffic.|Distributed,Pagerank,P2P,Systems,Algorithm,Keyword,Search,Network,Documents,Computation
38fe640a-109b-5a6b-86d4-d7a399e3f44e|2018-05-29T00:48:16+00:00|2018|5|Distributed Treewidth Computation|||||||The problem addressed in this research is the design of efficient distributed algorithms for solving various graph problems on networks with bounded treewidth. The context that makes this problem important is the increasing need for distributed computing in large-scale networks, where traditional centralized algorithms are no longer feasible. The authors aim to develop algorithms that can solve problems such as maximum independent set, minimum vertex cover, chromatic number, and minimum dominating set in near-optimal time, specifically O(D) rounds, where D is the diameter of the network graph. The key objective is to leverage the concept of treewidth, which has proven useful in classical algorithms, to design efficient distributed algorithms for these problems. The authors also aim to extend their results to solve other optimization problems that are tractable on bounded treewidth graphs.|Distributed,Treewidth,Computation,Algorithms,Graphs,Network,Optimization,Complexity,Approximation,Parallel
4d4eb858-d2a9-530f-91f6-14e546a96e54|2020-08-14T12:59:37|2020|8|Improved Communication Cost in Distributed PageRank Computation – A Theoretical Study|Siqiang Luo||||||The problem addressed in this research is the efficient computation of PageRank values in a distributed environment. PageRank is a widely used approach for measuring the importance of a node in a graph, and its computation has become increasingly important due to the rapid growth of graph sizes in real-world applications. The authors aim to design a distributed algorithm that can approximate PageRank values within a relative error of 1/log(d*n) for any node with a probability of at least 1 - 1/n, where d is a constant and n is the number of nodes in the graph. The key objective is to achieve this goal while minimizing the communication cost, specifically the bandwidth, between nodes in the distributed system. The authors also aim to improve upon the state-of-the-art approach, Radar Push, which has a high bandwidth requirement of O(log^2(d)*n^3) bits.|PageRank,Distributed,Computation,Congested,Clique,Model,Algorithm,Communication,Complexity,Bandwidth
e9910f58-6438-556a-ba86-818ea31fb2a9|2019-11-13T22:23:58+00:00|2019|11|iPregel: Strategies to Deal with an Extreme Form of Irregularity in Vertex-Centric Graph Processing|||||||The problem definition addressed in this research revolves around optimizing vertex-centric graph processing frameworks, which are crucial for handling large-scale graph data in various applications. The context that makes this problem important is the increasing need to process massive graphs efficiently, particularly in social network analysis, graph databases, and other domains. Vertex-centric frameworks, such as Pregel, offer programmability and ease of use but struggle with performance due to irregular memory access patterns, load imbalance, and data race protection issues. These challenges lead to poor performance, making it difficult to scale graph processing to large datasets. The key objectives or goals the authors set to address this problem are: 1. To develop optimizations that can improve the performance of vertex-centric graph processing frameworks without sacrificing their programmability and ease of use. 2. To design solutions that can efficiently handle the irregularities inherent in graph data, such as unpredictable memory access patterns and load imbalance. 3. To preserve the vertex-centric programming model, ensuring that users can continue to write graph algorithms in a simple and intuitive way. By achieving these objectives, the authors aim to enable efficient and scalable graph processing, unlocking the potential of large-scale graph data analysis in various domains.|Vertex-centric,Graph processing,Optimizations,Irregular applications,Edge-centric,Programmability,Performance improvement,Cache efficiency,Load balancing,Graph algorithms
24829cd1-53eb-5acb-9945-2df76e90574c|2015-01-03T18:49:01|2015|1|DREAM: Distributed RDF Engine with Adaptive QueryPlanner and Minimal Communication|||||||The problem addressed in this research is the efficient management of large-scale RDF data in distributed systems. The context is that RDF data is becoming increasingly large and complex, making it challenging to manage and query efficiently. The authors aim to address this problem by developing a system that can process RDF queries in a distributed manner while minimizing communication overhead and maximizing parallelism. The key objectives are to design a system that can adapt to different query workloads, reduce intermediate data shuffling, and provide a cost-effective solution for large-scale RDF data management. The authors also aim to overcome the limitations of existing RDF systems, which either store data unsliced at each machine or partition data and queries, resulting in high communication overhead or limited scalability.|RDF,Distributed,Query,System,Paradigm,Partitioning,Communication,Adaptive,Minimal,Quadrant
6658781b-834e-5897-89b2-6ac85a16ed96|2019-11-11T14:25:56|2019|11|A survey of current challenges in partitioning and processing of graph-structured data in parallel and distributed systems|Hamilton Wilfried Yves Adoni||||||The problem definition addressed in this research is the graph partitioning problem, which involves dividing a large graph into smaller subgraphs or partitions to facilitate efficient processing, storage, and analysis. The context that makes this problem important is the rapid growth of big graph data in various fields, such as social networks, web graphs, and biological networks, which poses significant challenges in terms of scalability, complexity, and computational resources. The key objectives or goals of the authors are to: Minimize the cut edges between partitions, which reduces communication overhead and improves processing efficiency. Balance the partition sizes to ensure even distribution of workload and resources. Develop efficient and scalable graph partitioning algorithms that can handle large graphs with millions of vertices and edges. Overall, the authors aim to address the graph partitioning problem by developing effective solutions that can efficiently process and analyze large graph data, which is critical in various applications, such as social network analysis, recommendation systems, and data mining.|Graph,Partitioning,Large-scale,Data,Processing,Systems,Algorithms,Complexity,Big,Networks
11c51c5a-3ab4-506d-963c-480e81199b0d|2018-10-31T08:39:05|2018|10|PruneJuice: Pruning Trillion-edgeGraphsto a PrecisePattern-Matching Solution|||||||The problem addressed in this research is the pattern matching problem in graphs, which involves finding subgraphs that match a small template graph within a large background graph. This problem is fundamental to graph analysis and has applications in multiple areas such as social network analysis, bioinformatics, and information mining. The authors aim to develop a technique that can efficiently and accurately identify all matches between the template and background graphs, with a focus on scalability and precision. The key objectives are to: (1) develop a systematic approach to eliminate all vertices and edges that do not participate in any match, and (2) provide a distributed algorithm that can efficiently verify non-local constraints to guarantee no false positives. The authors also aim to explore trade-offs between precision and time-to-solution, and to evaluate the impact of strategic design choices and optimizations on the performance of the algorithm.|Pattern,Matching,Graph,Pruning,Scalable,Distributed,Algorithm,Trillion,Edge,Solution
e1aecd81-90fa-525e-8fc5-49d9cdf2103e|2022-05-06T09:17:41|2022|5|Wake up and join me! An energy-efficient algorithm for maximal matching in radio networks|Varsha Dani||||||The problem addressed in this research is the energy-efficient communication in wireless radio networks, particularly in the context of sensor networks. The background that makes this problem important is the limited energy resources of sensor nodes, which necessitates energy conservation to prolong their lifespan. In such networks, communication is a significant energy drain, and optimizing energy usage is crucial. The key objective of this research is to design distributed algorithms that minimize energy consumption while ensuring reliable communication in wireless radio networks. Specifically, the authors aim to develop algorithms that can efficiently assign neighbors to backup data in case of node failure, thereby maintaining network connectivity and data integrity. The primary goal is to minimize the maximum load (i.e., the number of nodes assigned to a single node) while ensuring that the energy consumption per node remains low. In summary, the problem definition revolves around developing energy-efficient communication protocols for wireless radio networks, with a focus on minimizing energy consumption, maintaining network reliability, and ensuring data integrity in the face of node failures.|Energy,Radio,Networks,Distributed,Algorithm,Matching,Maximal,Polylog,Neighbor,Assignment
89076307-1a7d-5a89-806d-b8e4f0992933|2017-10-12T09:06:03|2017|10|Simple and Near-Optimal Distributed Coloring forSparse Graphs|||||||The problem addressed in this research is the distributed graph coloring problem, specifically for sparse graphs with low arboricity. The context that makes this problem important is its wide range of applications in networks and distributed systems, such as scheduling conflicting tasks, resource allocation, and network optimization. The current state of the art in distributed graph coloring algorithms is unsatisfactory, as they often require a large number of colors, which can lead to inefficient use of resources. The key objective of this research is to develop simple and near-optimal distributed algorithms for graph coloring that can efficiently color sparse graphs using a small number of colors. The authors aim to achieve this goal by designing algorithms that can color the graph in a short number of rounds, using a minimal number of colors, and with high probability of success. The authors also aim to improve upon the current state of the art in distributed graph coloring algorithms, which often have high complexities and require a large number of colors. Overall, the goal of this research is to provide efficient and scalable solutions for distributed graph coloring in sparse graphs, which can have a significant impact on various applications in networks and distributed systems.|Distributed,Graph,Coloring,Algorithms,Arboricity,Logarithmic,Rounds,Complexity,Deterministic,Chromatic
a257e631-8655-5cb4-9a54-d897e9c2dc1d|2017-06-13T06:45:08|2017|6|A Distributed (2 + ε)-Approximation for Vertex Cover in O(log Δ / ε log log Δ) Rounds|||||||The problem addressed in this research is the Minimum Weighted Vertex Cover (MWVC) problem in distributed networks. The context that makes this problem important is that MWVC is a fundamental problem in computer science and operations research, and it has numerous applications in various fields, including network optimization, data mining, and machine learning. Moreover, with the increasing scale and complexity of modern networks, there is a growing need for efficient distributed algorithms to solve this problem. The key objective of this research is to develop a distributed algorithm that can approximate the MWVC problem within a factor of 2 in a polylogarithmic number of rounds. The authors aim to achieve this goal by designing a local ratio algorithm that can be executed in a distributed manner, where each node in the network makes decisions based on local information and communicates with its neighbors. In particular, the authors focus on developing a 2-approximation algorithm that can be completed in O(log log log n) rounds, where n is the number of nodes in the network. This is a significant improvement over previous algorithms, which require a much larger number of rounds to achieve a similar approximation ratio. The authors' goal is to provide a fast and efficient distributed algorithm for solving the MWVC problem, which can be applied to large-scale networks and has potential applications in various fields.|Vertex Cover,Distributed Algorithm,Approximation Algorithm,Graph Algorithm,Local Ratio,Weighted Vertex Cover,Minimum Weighted Vertex Cover,Distributed Computing,Graph Theory,Approximation Ratio
5af0c511-cc44-57ef-aaa4-89f255fd3f59|2023-11-26T05:49:38|2023|11|Object-Oriented Frameworks for Distributed Systems: A Survey|Wai Ming Ho,Jean-Marc Jézéquel||||||The problem definition addressed in this research revolves around the development of distributed parallel computing frameworks, which are essential for building complex distributed applications. The context that makes this problem important is the increasing need for distributed collaborative applications in various domains, such as business, engineering, and science. These applications require efficient management of distributed resources, scalability, and adaptability, which can be achieved through the use of frameworks. The key objectives or goals set by the authors to address this problem are: 1. To provide a clear understanding of the concept of frameworks and their role in building distributed parallel computing applications. 2. To identify the challenges and difficulties associated with developing and using frameworks for distributed parallel computing. 3. To explore ways to improve the development and adaptation of frameworks for distributed parallel computing applications. Overall, the authors aim to contribute to the development of more effective and efficient distributed parallel computing frameworks, which can facilitate the creation of complex distributed applications and ultimately benefit various domains and industries.|Frameworks,Object-Oriented,Documentation,Development,Domain-Specific,Integration,Interoperability,Standards,Reuse,Middleware
36cfc05b-506b-5b5d-9097-b89fecf163d6|2023-11-20T01:40:41+00:00|2023|11|Fused Breadth-First Probabilistic Traversals on Distributed GPU Systems|||||||The problem addressed in this research is the efficient parallelization of probabilistic breadth-first traversals (BPTs) in graph analytics, specifically in the context of stochastic diffusion-based graph problems like influence maximization. The context is that modern supercomputers, such as the OLCF Frontier, leverage a large number of general-purpose GPUs to achieve high parallelism and computational density. However, the large number of BPT traversals and irregular, skewed access of memory edges due to probabilistic traversals and graph topology limit scaling. The key objective is to develop an algorithm that can efficiently fuse massive numbers of concurrently executing BPTs with random starts on the input graph, reducing the net number of visits per edge and thereby reducing time-to-solution. The authors aim to achieve this by presenting a new algorithm, the fused BPT algorithm, and demonstrating its efficacy through experiments on real-world inputs.|Fused,Breadth,First,Probabilistic,Traversals,Distributed,GPU,Systems,Influence,Maximization
fcdac258-b8af-5136-802c-97711256a474|2020-04-07T16:34:42|2020|4|Eﬃcient Deterministic Distributed Coloringwith Small Bandwidth|||||||The problem addressed in this research is the distributed graph coloring problem, specifically the degree 1 list coloring problem, in the context of distributed computing models such as CONGEST, CONGESTED CLIQUE, and MPC. The problem is important because it is a fundamental problem in distributed computing, and solving it efficiently is crucial for various applications. The authors aim to design deterministic algorithms that can solve the problem in a polylogarithmic number of rounds, which is a significant improvement over previous results. The key objective is to develop efficient algorithms that can handle large graphs with high diameters, which is a challenging task in distributed computing. The authors also aim to adapt their algorithms to different distributed computing models, including CONGESTED CLIQUE and MPC, to achieve faster solutions.|Coloring,Deterministic,Distributed,Graph,Algorithm,CONGEST,Model,Network,Decomposition,Polylogarithmic
f03adb9b-7af2-562f-9039-a002a5636462|2024-08-07T13:49:15.948138|2024|8|Graph Preprocessing for In-Memory Vertex-Centric Graph |||||||The problem addressed in this research is the scalability of graph processing, particularly when dealing with large graphs that do not fit into DRAM. The context is that graph processing is a fundamental task in many applications, such as social network analysis, optimization, and machine learning. However, as graphs grow in size, processing them becomes increasingly challenging due to memory constraints. The authors aim to address this problem by exploring graph reordering techniques to improve the performance of in-memory vertex-centric graph computation on the Intel Optane DC Persistent Memory Module. The key objective is to develop techniques that can efficiently process large graphs by optimizing data placement and reducing memory accesses. The authors also aim to provide a comprehensive evaluation of different graph reordering techniques and their impact on graph processing performance.|Graph,Partitioning,Reordering,Processing,Algorithms,Memory,Performance,Optimization,Techniques,Analysis
ba8d8dd3-0bb6-5de0-ae3d-33e89bf9d067|2020-10-01T16:42:16+00:00|2020|10|A Locality-Aware Energy-Efficient Accelerator for Graph Mining Applications|||||||The problem definition addressed in this research is the acceleration of graph mining applications, which is crucial due to the increasing importance of graph analytics in various domains, such as social networks, advertisement, and bioinformatics. The context that makes this problem important is the massive explosion in data volume, leading to severe random access problems in graph analytics, making it difficult to accelerate on traditional architectures. The key objectives or goals the authors set to address this problem are: To develop a specialized architecture that can efficiently handle the complex, irregular memory accesses inherent in graph mining applications. To identify and optimize the memory access patterns in graph mining, which are different from those in graph processing applications. To design a cost-effective and efficient accelerator that can accelerate graph mining applications, such as clique finding, frequent subgraph mining, and motif counting. Overall, the authors aim to address the problem of accelerating graph mining applications by developing a novel architecture that can efficiently handle the unique memory access patterns and computational requirements of these applications.|Graph mining,Accelerator,Extension locality,Memory hierarchy,Graph processing,Pattern matching,Subgraph isomorphism,Graph acceleration,Specialized hardware,Energy efficiency
2e4c12ab-5f42-5f82-a72e-f53a0e7db647|2014-03-15T07:16:59|2014|3|A survey on techniques for improving the energy efficiency of large-scale distributed systems|||||||The problem definition addressed in this research is the energy inefficiency of large-scale distributed systems, particularly in the context of computing and networking. The background that makes this problem important is the significant and growing energy consumption of these systems, which contributes to environmental concerns, increases operational costs, and affects the overall sustainability of the systems. The key objectives or goals set by the authors to address this problem are: To reduce the energy consumption of large-scale distributed systems while maintaining their performance and quality of service (QoS). To identify and develop techniques that can improve the energy efficiency of these systems, including power management, resource allocation, scheduling, and network traffic management. To provide a comprehensive survey of existing techniques and solutions that can help achieve energy efficiency in large-scale distributed systems. Overall, the authors aim to contribute to the development of sustainable and energy-efficient large-scale distributed systems that can support the growing demands of modern computing and networking applications.|Energy Efficiency,Distributed Systems,Power Management,Computing Resources,Data Centers,Grid Computing,Cloud Computing,Virtualization,Performance Optimization,Sustainability
52fb65a3-7784-5619-a706-d81a22fde108|2010-03-18T11:47:22|2010|3|Pregel: a system for large-scale graph processing|Grzegorz Malewicz,Matthew H Austern,Aart JC Bik,James C Dehnert,Ilan Horn,Naty Leiser,Grzegorz Czajkowski||||||The problem definition addressed in this research is the efficient processing of large-scale graphs, which has become increasingly important due to the growing size and complexity of graph-structured data in various domains, such as social networks, web graphs, and transportation networks. The context that makes this problem important is the need to analyze and extract insights from these massive graphs, which is challenging due to their scale, complexity, and poor locality of memory access. The key objectives or goals the authors set to address this problem are: To design a system that can efficiently process large-scale graphs on distributed computing architectures. To provide a simple and expressive programming model that allows users to easily implement graph algorithms. To develop a system that can scale to thousands of commodity computers, while ensuring fault tolerance and high performance. Overall, the authors aim to develop a system that can efficiently process large-scale graphs, making it possible to extract insights and knowledge from these complex data structures.|Graph,Processing,Large-scale,Distributed,Computing,Algorithm,Pregel,System,Vertex,Message
0371c4b7-c22e-5ee1-8a84-8ad51283e21b|2015-01-25T12:43:17|2015|1|A scalable graph pattern matchingengine on top of Apache Giraph|||||||The problem definition addressed in this research revolves around the scalability of graph pattern matching algorithms, specifically in the context of large-scale graph data processing. The background that makes this problem important is the increasing amount of graph-structured data being generated, which necessitates efficient and scalable algorithms to process and analyze this data. The authors aim to address this problem by exploring the feasibility of mapping Cypher, a query language for graph databases, into Giraph, a distributed graph processing system. The key objectives of this research include: (1) investigating whether a NP-complete problem can benefit from a distributed environment and scale, (2) exploring different options for mapping Cypher into Giraph and evaluating their advantages and disadvantages, and (3) determining whether all Cypher functionality can be translated into Giraph and whether the resulting system would be more scalable than Neo4j.|graph,pattern,matching,Giraph,queries,scalable,engine,Apache,NP,complete,Cypher,analytics,latency,computation,distributed,environment,data,sets,entities,connections,representation,databases,applications,project,system,model,computation,parallel,bulk,synchronous,open,source,design,research,questions,introduction,background,motivation,related,work,language,architecture,algebra,operators,local,global,models,algorithms,systems,thesis,science,computer,faculty,universiteit,amsterdam,sciences,department,student,supervisor,reader,centrum,wiskunde,informatica,amsterdam,december,abstract,advantage,presence,largedata,overcome,need,fast,processing,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,
c084ee8a-70e9-55e6-8575-70787ed41d77|2020-06-17T09:07:16|2020|6|Lower Bounds for Distributed Sketching of Maximal Matchings and Maximal Independent Sets|Sepehr Assadi,Gillat Kol,Rotem Oshman||||||Here is a concise summary of the problem definition addressed in this research: Context and Background: The problem of approximating maximum matching in graphs is a fundamental problem in computer science and has numerous applications in various fields, including computer networks, social networks, and data mining. In many real-world scenarios, the graph data is distributed across multiple machines or nodes, making it challenging to solve the maximum matching problem efficiently. This has led to the development of distributed algorithms, which, however, often rely on a high amount of communication between nodes. Problem Definition: The research addresses the problem of approximating maximum matching in graphs in a distributed setting, where the input graph is partitioned across multiple players, and each player can only communicate a limited amount of information to a central referee. The goal is to design a communication-efficient protocol that allows the players to collectively compute an approximate maximum matching in the graph. Key Objectives: The authors aim to develop a protocol that minimizes the communication cost, measured in terms of the number of bits sent by each player, while still achieving a good approximation of the maximum matching. Specifically, they seek to establish a lower bound on the communication complexity of approximating maximum matching in this distributed setting, which would provide a fundamental limit on the efficiency of any protocol designed to solve this problem.|Distributed Sketching,Maximal Matching,Maximal Independent Set,Communication Complexity,Lower Bounds,Graph Algorithms,Broadcast Congested Clique,Distributed Computing,Graph Problems,Algorithmic Complexity
9decf71d-ff1f-5bc4-b7af-b68a66db7f30|2019-02-28T09:26:46|2019|2|Potential Game Theoretic Learning for the Minimal Weighted Vertex Cover in Distributed Networking Systems|||||||The problem addressed in this research is the Minimal Weighted Vertex Cover (MWVC) problem in distributed networking systems. The context that makes this problem important is the need for efficient and decentralized solutions in modern distributed systems, where a central authority may not be available or reliable. In such systems, the MWVC problem arises when selecting a subset of nodes to monitor or control the entire network, with the goal of minimizing the total weight or cost of the selected nodes. The key objective of this research is to develop a distributed algorithm that can solve the MWVC problem in a decentralized manner, without relying on global information or a central authority. The authors aim to design a spatial potential game that can be played by nodes in the network, where each node makes decisions based on local information and interactions with its neighbors. The goal is to converge to a near-optimal solution that minimizes the total weight of the selected nodes, while ensuring that the entire network is covered. In summary, the problem definition addressed in this research is the MWVC problem in distributed networking systems, with the key objective of developing a decentralized algorithm that can efficiently solve the problem without relying on global information or a central authority.|Minimal Weighted Vertex Cover (MWVC),Distributed Optimization,Game Theoretic Approach,Nash Equilibrium,Vertex Cover,Graph Theory,Combinatorial Optimization,Distributed Algorithm,Relaxation Method,Convergence Analysis
c229bdbd-e157-581b-a314-c77bc8d50f49|2020-04-21T23:21:06+00:00|2020|4|Distributed Processing of k Shortest Path Queries over Dynamic Road Networks|Ziqiang Yu,Xiaohui Yu,Nick Koudas,Yang Liu,Yifan Li,,Yueting Chen,Dingyu Yang||||||The problem definition addressed in this research is the identification of K-Shortest Paths (KSPs) in dynamic undirected graphs, particularly in the context of road networks. The background that makes this problem important is the need for efficient and scalable solutions to process numerous KSP queries over large road networks in real-time, which is crucial for various location-based services such as route planning and traffic management. The key objectives or goals the authors set to address this problem are: 1. To develop a distributed solution that can efficiently identify KSPs in dynamic graphs, considering the constantly changing edge weights that represent evolving traffic conditions. 2. To design an approach that can process multiple KSP queries simultaneously, making it suitable for large-scale road networks with numerous users. 3. To provide a scalable solution that can adapt to changing graph structures and weights, ensuring that the identified KSPs remain optimal and up-to-date. Overall, the authors aim to develop a novel approach that can efficiently and effectively identify KSPs in dynamic graphs, enabling real-time route planning and traffic management in large-scale road networks.|KSP (K-Shortest Paths),Dynamic Road Networks,Graph Query Processing,Distributed Query Evaluation,Shortest Path Queries,Road Networks,Graphs,Query Processing,Dynamic Graphs,Navigation Services
d0bc6ccd-619e-5e9c-9c64-0152b5e4a4d6|2019-01-25T09:37:11|2019|1|A Round-Efficient Distributed Betweenness Centrality Algorithm|Loc Hoang,Matteo Pontecorvi,Roshan Dathathri,Gurbinder Gill,Bozhi You,Keshav Pingali,Vijaya Ramachandran||||||The problem addressed in this research is the efficient computation of Betweenness Centrality (BC) in large-scale graphs. BC is a metric used to measure the importance of nodes in a network by calculating the fraction of shortest paths between all pairs of nodes that pass through a given node. The context that makes this problem important is the increasing size and complexity of real-world networks, such as social networks, transportation systems, and biological networks, which require efficient algorithms to analyze and understand their structure and behavior. The key objective of this research is to develop a distributed algorithm that can efficiently compute BC in large-scale graphs, with a focus on minimizing the number of rounds and messages required for the computation. The authors aim to improve upon existing algorithms, such as Brandes' algorithm, which has a high computational complexity and is not suitable for large-scale graphs. The research goals include developing an algorithm that can handle directed and undirected graphs, and that can be implemented in a distributed computing environment.|Betweenness Centrality,Distributed Algorithm,CONGEST Model,Unweighted Directed Graph,All Pairs Shortest Paths,Brandes Algorithm,Min Rounds BC,Distributed Memory,Graph Analytics,Parallel Computing
f7761cb4-452e-545f-aeaf-237ee2fdc318|2021-05-05T14:28:59|2021|5|Near-optimal Distributed Triangle Enumeration via Expander Decompositions|||||||The problem definition addressed in this research revolves around graph optimization problems in the CONGEST model, a distributed computing framework where vertices in a graph represent computational devices, and edges represent bi-directional communication links. The context that makes this problem important is the increasing need for efficient distributed algorithms to process large-scale graph-structured data, which is common in modern applications such as social networks, web graphs, and biological networks. The key objectives or goals the authors set to address this problem are: To develop efficient distributed algorithms for solving graph optimization problems, such as triangle enumeration, sparse cut, and routing, in the CONGEST model. To minimize the round complexity, which is the number of communication rounds required to solve the problem, while ensuring that the algorithms are scalable and can handle large graphs. To bridge the gap between the CONGEST and CONGESTED CLIQUE models, which are two different distributed computing frameworks, by developing algorithms that can be efficiently implemented in both models. Overall, the authors aim to contribute to the development of efficient distributed algorithms for graph optimization problems, which is crucial for processing large-scale graph-structured data in modern applications.|Distributed,Graph,Algorithm,Routing,Congest,Model,Network,Communication,Complexity,Time
4b9633fd-1a45-5ef2-b79a-a6d8f04585ea|2021-11-29T00:17:42|2021|11|A Deterministic Distributed Algorithm for Exact WeightedAll-Pairs Shortest Paths in ˜O(n3/2) Rounds|||||||The problem addressed in this research is the design of distributed algorithms for various network or graph problems, specifically the weighted All-Pairs Shortest Paths (APSP) problem in the Congest model. The context that makes this problem important is the need for efficient distributed algorithms that can handle large-scale networks with limited bandwidth and computational power. The key objective of this research is to develop a deterministic distributed algorithm for weighted APSP that minimizes the round complexity, which is the worst-case number of rounds of distributed communication. The authors aim to achieve this goal by developing a blocker set algorithm that can efficiently compute a blocker set, which is a set of vertices that cover all root-to-leaf paths of length h in a collection of h-hop trees. The authors also aim to extend their algorithm to handle directed graphs and arbitrary edge weights, including negative weights, as long as there are no negative weight cycles.|APSP,Congest,Algorithm,Distributed,Weighted,Graph,Rounds,Deterministic,Shortest,Paths
8f8af34d-8dae-54e0-be9b-5baef36fcf89|2016-07-07T00:35:23+00:00|2016|7|Fast Distributed Algorithms for Connectivity and MST in LargeGraphs|||||||The problem addressed in this research is the study of fundamental graph problems in a distributed computing model, specifically the k-machine model. This model is relevant in the context of large-scale graph processing systems, where the input graph is distributed across multiple machines, and the goal is to minimize the time complexity of solving graph problems. The authors aim to investigate the amount of speedup possible with the number of available machines and determine which problems admit linear or super-linear scaling. The key objectives are to develop efficient algorithms for solving graph problems, such as connectivity, minimum spanning tree, and graph verification problems, and to establish lower bounds on the time complexity of these problems in the k-machine model.|Distributed,Graph,Algorithms,Connectivity,Machine,Model,Computation,Large,Scale,Processing
e173b150-1a70-5706-995e-2e8912a9a06a|2022-11-02T00:32:37+00:00|2022|11|Distributed Graph Neural Network Training: A Survey|||||||The problem definition addressed in this research revolves around the challenges of distributed Graph Neural Network (GNN) training. The context that makes this problem important is the increasing demand for processing large-scale graph data, which is becoming a bottleneck in various applications such as social networks, recommendation systems, and computer vision. GNNs have emerged as a powerful tool for graph data analysis, but their training is computationally expensive and memory-intensive, making it difficult to scale. The key objectives or goals the authors set to address this problem are: 1. To develop efficient distributed GNN training techniques that can handle large-scale graph data. 2. To minimize the communication overhead and workload imbalance among workers in distributed GNN training. 3. To ensure the convergence guarantee and model accuracy of distributed GNN training, which is essential for real-world applications. Overall, the authors aim to overcome the limitations of existing distributed GNN training methods and provide a scalable and efficient solution for large-scale graph data analysis.|Graph Neural Networks (GNNs),Distributed Training,Optimization Techniques,Large-scale Graphs,Scalability,Deep Learning,Graph Processing,Model Accuracy,Workload Imbalance,Communication Efficiency
a952998a-f358-5070-a0fd-118112b60df3|2016-12-08T02:45:58|2016|12|Graphicionado: A high-performance and energy-efficient accelerator for graph analytics|||||||The problem definition addressed in this research revolves around the inefficiencies in software graph processing frameworks, which are crucial for analyzing and processing large-scale graph data. The context that makes this problem important is the increasing significance of graph data in various domains, such as social networks, natural language processing, and system biology, which has led to a surge in the development of graph analytics applications. However, these applications are often hindered by the limitations of traditional computing architectures, which are not optimized for graph processing. The key objectives or goals the authors set to address this problem are: To improve the programmability of graph algorithms, making it easier for developers to create and optimize graph analytics applications. To enhance the performance and scalability of graph processing, allowing for faster and more efficient analysis of large-scale graph data. To reduce the energy consumption associated with graph processing, making it more sustainable and environmentally friendly. To achieve these objectives, the authors propose a novel graph processing framework, GraphMat, which leverages a hybrid approach combining the strengths of both software and hardware-based graph processing. By optimizing the graph processing pipeline and minimizing off-chip memory accesses, GraphMat aims to provide a more efficient, scalable, and energy-friendly solution for graph analytics applications.|GraphMat,Graph,Algorithms,Programming,Model,Vertex,Edge,Processing,Framework,Optimization
434e498d-4d3a-52cb-b3a7-f49d963d97ea|2020-06-06T22:44:46+00:00|2020|6|Scaph: Scalable GPU-Accelerated Graph Processing with Value-Driven Differential Scheduling|||||||The problem definition addressed in this research revolves around optimizing memory usage in graph processing systems, particularly in the context of vertex-centric graph processing on Graphics Processing Units (GPUs). The background that makes this problem important is the increasing significance of graph processing in various real-world applications, such as path navigation, social network analysis, and financial fraud detection. However, graph processing is often memory-bound, and existing CPU-based and GPU-accelerated graph systems suffer from inefficient memory utilization, leading to performance bottlenecks. The key objective of this research is to develop a novel approach that can effectively reduce the amount of redundant data transferred between the host and GPU, thereby improving the effective utilization of the host-GPU bandwidth. The authors aim to achieve this by adaptively classifying subgraphs into high-value and low-value categories based on their potential usefulness in current and future iterations, and then applying value-driven differential scheduling to optimize memory usage. The ultimate goal is to boost the performance of graph processing systems while minimizing memory overheads.|Graph,Processing,GPU,Memory,Optimization,Subgraph,Iteration,Active,Vertices,Acceleration
8f6b5d57-8fbb-51d0-bfee-803b7255ac3f|2020-03-13T03:25:51|2020|3|A Survey on Distributed Machine Learning|||||||The problem definition addressed in this research revolves around the increasing importance of machine learning (ML) solutions in various applications, which is hindered by the complexity of ML algorithms and the need for efficient systems to support their execution. The context that makes this problem important is the growing demand for ML solutions in areas like self-driving cars, speech recognition, and consumer behavior prediction, which require large amounts of data and computational resources. The key objective of this research is to provide an overview of the ML landscape, discussing the different types of ML problems, algorithms, and systems aspects, with the goal of enabling designers of ML solutions to make informed decisions about the choice of algorithms, hyperparameters, and system architectures that meet the performance and efficiency requirements of their applications.|Machine Learning,Algorithms,Hyperparameter,Optimization,Ensemble Methods,Latent Semantic Analysis,Topic Modeling,Matrix Factorization,Bayesian Optimization,Genetic Algorithms
373cbd66-b962-5387-aa60-bb6ecce6c96a|2018-02-12T01:29:05+00:00|2018|2|Distributed Spanner Approximation|||||||The problem addressed in this research is the hardness of approximation for the minimum k-spanner problem in the distributed setting, specifically in the Congest model. The context that makes this problem important is the need for efficient algorithms to solve graph problems in distributed networks, where communication is limited. The authors aim to show that there are no efficient approximation algorithms for the directed k-spanner problem for k ≥ 5 in the Congest model, and to provide lower bounds for the time complexity of any distributed algorithm that approximates the problem. The key objective is to prove that obtaining a good approximation for the minimum k-spanner problem requires a large number of rounds, even when using randomized algorithms. This has implications for the design of efficient distributed algorithms for various graph problems.|Spanner,Distributed,Algorithm,Approximation,Graph,Model,Complexity,Problem,Network,Communication
5bf73003-ef64-5a6a-860d-4614fe34dab8|2019-12-28T10:57:01+00:00|2019|12|Distributed Triangle Counting Algorithms in Simple Graph Stream|Mengdi Yu University of Electronic Science,Technology of China,Chao Song University of Electronic Science,Technology of China,Jiqing Gu University of Electronic Science,Technology of China,Ming Liu University of Electronic Science,Technology of China||||||The problem definition addressed in this research is the distributed triangle counting in graph streams. The context that makes this problem important is the increasing scale and complexity of graph-structured data, such as social networks, road traffic networks, and protein structures, where triangles are a fundamental structure. The rapid growth of these networks and the need for real-time analysis make it challenging to process and analyze graph streams efficiently. The key objective of this research is to design a distributed streaming algorithm that can accurately estimate the number of triangles in a graph stream while minimizing the communication cost and variance of estimates. The authors aim to develop an algorithm that can quickly distribute edges to workers, reduce the number of shared edge triangles in each worker, and provide an unbiased estimate of the global number of triangles in the graph stream. In summary, the problem of distributed triangle counting in graph streams is crucial due to the increasing scale and complexity of graph-structured data, and the authors' goal is to develop an efficient and accurate distributed algorithm that can handle large-scale graph streams in real-time.|Triangle counting,Distributed streaming algorithm,Graph stream,Approximate algorithm,Graph algorithm,Streaming algorithm,Distributed algorithm,Graph processing,Online algorithm,Unbiased estimation
84cf2b27-3459-5a7b-9984-0dfd6b2c1bdf|2022-12-27T07:19:25|2022|12|Graph Algorithms With Partition Transparency|||||||The problem addressed in this research is the challenge of efficiently processing large-scale graph computations in a distributed environment. The context that makes this problem important is the increasing size and complexity of real-world graphs, which necessitates parallel computations on partitioned graphs to handle them. However, existing graph processing systems often lack partition transparency, leading to significant overhead and complexity in developing parallel graph algorithms. The key objective of this research is to design a graph processing system that provides partition transparency, allowing users to write graph algorithms without worrying about the underlying graph partitioning. The authors aim to achieve this by developing a programming model that abstracts away the partitioning details, enabling users to focus on the graph computation logic. The goal is to simplify the development of parallel graph algorithms, improve their performance, and increase their scalability.|Partition Transparency,Graph Algorithms,Graph Partition,Vertex Centric,Graph Centric,Parallel Computation,Distributed Graph Processing,Incremental Algorithms,Graph Computation,Query Evaluation
7679c89c-8853-5cd5-86eb-0a7b04ce080d|2023-04-05T08:10:41|2023|4|Efficient and Scalable Distributed Graph Structural Clustering at Billion Scale|Kongzhang Hao||||||The problem definition addressed in this research is the distributed Structural Clustering Algorithm for Network (SCAN) on large-scale graphs. The context that makes this problem important is the increasing need to analyze and understand complex networks, which are ubiquitous in various domains such as social media, biology, and epidemiology. The SCAN algorithm is essential in identifying different roles of vertices in a graph, including hubs, outliers, and clusters, which is crucial in many applications such as viral marketing, epidemiology, and search engines. The key objectives or goals the authors set to address this problem are: 1. To develop an efficient and scalable distributed algorithm for SCAN that can handle billion-scale graphs. 2. To reduce the communication overhead and memory consumption, which are major challenges in existing distributed solutions. 3. To maintain the correctness of clustering results across different batches and handle skewed workloads in distributed systems. Overall, the authors aim to provide a distributed SCAN algorithm that is both efficient and scalable, enabling the analysis of large-scale graphs in a distributed computing environment.|Distributed,Graph,Clustering,SCAN,Structural,Algorithm,Scalable,Ecient,Billion-scale,Graph analysis
329bab3a-4867-5e0f-b9a6-8fdc0972e095|2011-06-09T07:36:09|2011|6|Parallel Gibbs Sampling: From Colored Fields to Thin Junction Trees|||||||The problem addressed in this research is the slow mixing of Gibbs samplers in large probabilistic models, which can lead to slow convergence and poor exploration of high-likelihood states. The context is that Gibbs sampling is a widely used Markov Chain Monte Carlo (MCMC) algorithm in statistics and machine learning, but its performance can be hindered by complex dependencies between variables. The authors aim to design efficient parallel Gibbs samplers that can improve mixing and exploration of high-likelihood states, while ensuring ergodicity. The key objectives are to develop methods that can accelerate the Gibbs sampler using parallel resources, and to provide strong guarantees on the parallel reduction in mixing time.|Gibbs,Sampling,Parallel,Ergodic,Markov,Chain,Distribution,Variables,Models,Inference
27f67358-83be-5303-b8bb-d1061e1c8d7a|2020-12-18T03:55:53|2020|12|Distributed Optimization for Weighted Vertex Cover via Heuristic Game Theoretic Learning|||||||The problem definition addressed in this research is the Minimum Weighted Vertex Cover (MWVC) problem, which is a classic NP-hard problem in graph theory. The context that makes this problem important is its wide range of practical applications, such as dynamic map labeling, gene sequence alignment, and mission planning for distributed satellites. The MWVC problem aims to find the optimal vertex cover in a weighted and undirected graph, which is a subset of vertices that touches at least one endpoint of all the edges in the graph, with the minimum weight sum. The key objectives or goals the authors set to address the problem are: 1. To develop a distributed algorithm that can provide closer-to-optimal solutions by introducing problem-specific information into distributed coordination. 2. To improve solution efficiency and computation speed compared to existing state-of-the-art algorithms. Overall, the authors aim to develop a more effective and efficient distributed algorithm to solve the MWVC problem, which has important implications for various real-world applications.|Minimum Weighted Vertex Cover (MWVC),Distributed Algorithm,Game Theory,Nash Equilibrium,Optimization,Graph Theory,Vertex Cover,Weighted Graph,Convergence,Potential Game
1ffa5786-41c7-5035-84be-cfe1e6817ac0|2018-01-03T04:22:13|2018|1|Graph Processing on GPUs: A Survey|||||||The problem definition addressed in this research is the efficient processing of large-scale graph data on Graphics Processing Units (GPUs). The context that makes this problem important is the increasing amount of graph-structured data in various fields, such as social networks, web graphs, and biological networks, which requires efficient processing to extract valuable insights. The key objective of this research is to develop a graph processing system that can efficiently utilize the massive parallelism of GPUs to process large-scale graphs, while minimizing the number of synchronization points and optimizing memory access patterns. The authors aim to achieve this goal by exploring various graph processing models, including the Bulk Synchronous Parallel (BSP) model and the Gather-Apply-Scatter (GAS) model, and optimizing their implementation on GPUs.|Graph,Processing,GPU,Algorithms,Parallel,Performance,Optimization,Memory,Architecture,Scalability
beb6ee34-bc87-587f-bb50-af13808badb0|2022-04-12T11:14:15|2022|4|IncGraph: An Improved Distributed Incremental Graph Computing Model and Framework Based on Spark GraphX|||||||The problem definition addressed in this research revolves around the efficient processing of iterative graph algorithms on dynamic graphs, which are graphs with temporal information. The context that makes this problem important is the increasing prevalence of dynamic graphs in various applications, such as social networks, traffic networks, and biological networks, where the graph structure and vertex states change over time. Traditional iterative graph algorithms are not designed to handle these dynamic changes, leading to inefficiencies and inaccuracies. The key objectives or goals the authors set to address this problem are: 1. To develop an incremental iterative computation model that can efficiently process iterative graph algorithms on dynamic graphs. 2. To minimize the overhead of re-computation and reduce the number of iterations required to converge. 3. To ensure the consistency and accuracy of the results obtained from the incremental iterative computation model. Overall, the authors aim to provide a scalable and efficient solution for processing iterative graph algorithms on dynamic graphs, which is essential for various applications that rely on graph data processing.|Incremental,Graph,Algorithm,PageRank,Update,Iterative,Method,Efficiency,Distributed,Computing
f756e97c-7e25-595a-bb8b-d84d607f0355|2016-02-26T04:12:00|2016|2|A Distributed Hybrid Algorithm for the GraphColoring Problem|||||||The problem addressed in this research is the Graph Coloring Problem (GCP), which is a well-known NP-hard problem in computer science and operations research. The context that makes this problem important is its numerous applications in various fields, such as scheduling, timetabling, and resource allocation. The GCP involves assigning colors to vertices in a graph such that no two adjacent vertices have the same color, with the goal of minimizing the number of colors used. The key objective of this research is to develop an efficient algorithm to solve the GCP, particularly for large and complex graphs. The authors aim to design a hybrid algorithm that combines the strengths of different optimization techniques, including multi-agent systems, reinforcement learning, and tabu search, to effectively explore the search space and find high-quality solutions. The ultimate goal is to find a legal k-coloring of the graph, where k is the minimum number of colors required, and to do so in a computationally efficient manner.|Graph Coloring Problem (GCP),Distributed Hybrid (DH),Multi-Agent System (MAS),Tabu Search,Crossover,Perturbation,Mediator Agent,Intensification,Diversification,Optimization
d1768946-ce98-5523-95b4-e3796a85ba96|2018-12-03T09:21:27|2018|12|VColor: A practical vertex-cut based approach for coloring large graphs|||||||The problem definition addressed in this research is the graph coloring problem, which is a fundamental NP-hard problem in graph theory. The context that makes this problem important is its wide range of real-world applications, including Operations Research, Communication Network, Computational Biology, and Compiler Optimization. In these applications, graph coloring is used to model various optimization problems, such as scheduling, frequency assignment, and register allocation. The key objective of this research is to develop an efficient algorithm for graph coloring that can minimize the number of colors used. The authors aim to achieve this goal by proposing a vertex cut-based coloring technique, which involves partitioning the graph into smaller subgraphs, coloring each subgraph, and then combining the local colorings to obtain a global coloring. The authors also focus on optimizing the coloring of a set of graphs, which is a common scenario in many real-world applications. Overall, the authors' goal is to develop a scalable and efficient graph coloring algorithm that can be applied to various real-world problems, with the ultimate objective of minimizing the number of colors used and improving the overall performance of the system.|Graph coloring,Vertex cut,Large graphs,Approximation algorithms,Optimization techniques,Graph database,Multi-query optimization,Coloring algorithms,Independent sets,Computational complexity
f7fdccef-9e9c-58ac-9ee1-e0204907cefd|2018-01-27T04:25:57|2018|1|Parallel algorithms for flexible pattern matching on big graphs|Hongzhi Wang||||||The problem addressed in this research revolves around efficiently identifying a specific group of people with diverse expertise from a large social network, which is represented as a graph. This problem is important in the context of project management, where a team with diverse skills and expertise is required to accomplish a project. The key objective is to find a subgraph in the social network that matches a given pattern, which represents the required team structure and expertise. The authors aim to address this problem by developing an efficient algorithm that can identify the desired team from a large social network, while minimizing data shipment and time complexity. The key goals are to: Develop a scalable algorithm that can handle large social networks. Minimize data shipment between machines to reduce communication overhead. Optimize the time complexity of the algorithm to ensure efficient processing. By achieving these goals, the authors aim to provide an efficient solution for identifying diverse teams from large social networks, which is crucial in various applications such as project management, team formation, and social network analysis.|Graph,Pattern,Matching,Partial,Retrieval,Memory,Semantics,Simulation,Isomorphism,Subgraph
fcc42a33-2b7a-5ecb-aab0-535b45851f29|2016-05-31T06:18:53+00:00|2016|5|Distributed Algorithms for Computation ofCentrality Measures in Complex Networks|||||||The problem addressed in this research is the distributed computation of centrality measures in complex networks, particularly in the context of the Internet. The background that makes this problem important is the increasing size and complexity of networks, making it challenging to compute centrality measures using traditional centralized methods. The authors aim to develop distributed algorithms that can efficiently compute centrality measures, such as degree, closeness, betweenness, and PageRank, using local interactions between nodes. The key objectives are to design algorithms that can handle large-scale networks, provide accurate estimates of centrality measures, and adapt to time-varying networks. The authors also aim to address the challenge of estimating the network size, which is crucial for computing PageRank, and to develop algorithms that can handle spamming pages in temporal networks. Overall, the goal is to provide a scalable and efficient solution for computing centrality measures in complex networks, which is essential for various applications, including webpage ranking, social network analysis, and epidemic disease spread.|Centrality,Distributed,Computation,Complex,Networks,PageRank,Algorithms,Randomized,Temporal,Graphs
38edc217-c7df-544c-ac80-f467e0b77ea7|2021-06-10T00:32:50+00:00|2021|6|Controlling Memory Footprint of Stateful Streaming Graph Processing|||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graphs in the context of streaming graph processing systems. The background that makes this problem important is the increasing prevalence of large-scale graphs in various domains, such as social networks, web graphs, and knowledge graphs, which require continuous processing and updating to reflect changes in the graph structure. The key challenge in this context is the high computational cost and memory requirements associated with re-computing graph algorithms from scratch upon each graph mutation. This leads to significant performance degradation and limits the applicability of graph processing systems. The authors' objective is to develop an efficient incremental processing approach that can effectively reduce the computational cost and memory requirements by selectively tracking and updating intermediate vertex states. The goal is to minimize the number of vertices that need to be re-computed from scratch, thereby improving the overall performance and scalability of streaming graph processing systems. Specifically, the authors aim to address the following key objectives: 1. Develop a selective stateful incremental processing model that can efficiently track and update intermediate vertex states. 2. Optimize the graph layout to minimize memory consumption and reduce the number of vertices that need to be tracked. 3. Propagate changes resulting from graph mutations in an incremental manner, reducing the computational cost and memory requirements. By addressing these objectives, the authors aim to provide a scalable and efficient solution for streaming graph processing systems, enabling them to handle large-scale graphs and continuous updates in real-time.|Graph,Streaming,Stateful,Iterative,Memory,Footprint,Processing,Algorithms,Intermediate,Incremental
538b8b34-d925-588b-bafd-53546b72f8e1|2015-11-12T02:50:37|2015|11|An Improved Distributed Algorithm for Maximal Independent Set|Mohsen Ghaffari||||||"The problem definition addressed in this research is the local complexity of the Maximal Independent Set (MIS) problem in distributed networks. The context that makes this problem important is that many distributed algorithms, including MIS, have been traditionally analyzed with a global mentality, focusing on the time until all nodes terminate, rather than the time until each individual node terminates. This global approach can lead to a gap in understanding the locality of the problem. The key objective of this research is to question whether this global mentality is necessary and to investigate the local complexity of the MIS problem. Specifically, the authors aim to answer the ""Local Complexity Question"": How long does it take for each particular node to terminate and know whether it is in the eventual MIS or not, with a high probability? The authors seek to provide a tight analysis of the local complexity of the MIS problem, focusing on the time until each node terminates, rather than the global time complexity."|MIS (Maximal Independent Set),Local Complexity,Distributed Algorithm,Graph Shattering,Randomized Algorithm,Luby's Algorithm,CONGESTED CLIQUE,Arboricity,Graph Theory,Distributed Computing
d33778dd-6b6a-5e6f-ac6a-c86f52877538|2018-11-05T16:24:17|2018|11|Distributed Maximal Independent Set using Small Messages|Mohsen Ghaffari||||||The problem addressed in this research is the design of efficient distributed algorithms for computing a Maximal Independent Set (MIS) in a graph, specifically in the LOCAL and CONGEST models of distributed computing. The context that makes this problem important is that MIS is a fundamental problem in graph theory, and its solution has numerous applications in various fields, including computer networks, social networks, and distributed systems. In the LOCAL model, the communication network is abstracted as an n-node graph, where each node has a processor that initially knows only its own O(log n) bit identifier. The goal is to design an algorithm that can compute an MIS in a small number of rounds, where each round allows each node to send one message to each of its neighbors. The key objectives of this research are to: 1. Develop an MIS algorithm that works in the CONGEST model, where each message has a size of at most O(log n) bits. 2. Achieve a round complexity that is polylogarithmic in the size of the graph. 3. Overcome the limitations of previous algorithms that relied on large messages and inherently needed large identifiers. By addressing these objectives, the authors aim to provide a more efficient and scalable solution for computing MIS in distributed systems, which can have a significant impact on various applications that rely on graph theory.|Distributed,Algorithm,Graph,Complexity,CONGEST,Model,Network,Decomposition,Shattering,Maximality
3dcb232d-933c-5252-95f0-db2371644e9c|2021-02-09T08:16:19|2021|2|A Survey on Distributed Graph Pattern Matchingin Massive Graphs|||||||The problem definition addressed in this research revolves around the efficient processing of graph simulation queries in large-scale graph data, particularly in the context of emerging applications such as social networks, knowledge graphs, and biological networks. The background that makes this problem important is the increasing size and complexity of graph data, which poses significant challenges to traditional query processing techniques. The key objective of this research is to develop scalable and efficient algorithms for graph simulation queries that can handle large graph data. Specifically, the authors aim to design parallel algorithms that can take advantage of distributed computing architectures to speed up the query processing time. The authors also focus on addressing the challenges of load balancing, communication overhead, and synchronization in distributed graph processing. In summary, the problem definition involves developing efficient and scalable algorithms for graph simulation queries in large-scale graph data, with a focus on parallel processing and distributed computing architectures. The key objectives are to improve query processing time, address load balancing and communication overhead challenges, and develop efficient synchronization mechanisms for distributed graph processing.|Subgraph,Isomorphism,Graph,Pattern,Matching,Algorithms,Optimization,Query,Graphs,Embeddings
9450bf6c-14cd-5d6c-ab91-bc335d5632d3|2018-06-18T21:23:34+00:00|2018|6|GraphU: A Unified Vertex-Centric Parallel Graph Processing Platform|||||||The problem definition addressed in this research revolves around the efficient execution of asynchronous graph algorithms on distributed systems. The context that makes this problem important is the increasing need to process large-scale graph data in various domains, such as social networks, web graphs, and biological networks. However, traditional synchronous graph processing approaches are often inefficient and can lead to high communication costs, making them unsuitable for large-scale graph processing. The key objective of this research is to develop a framework that can efficiently execute asynchronous graph algorithms on distributed systems, minimizing communication costs and improving scalability. Specifically, the authors aim to: Develop a sound and efficient framework for executing asynchronous graph algorithms on distributed systems. Analyze the computational complexity of asynchronous graph algorithms and provide a theoretical foundation for their execution. Optimize the execution of asynchronous graph algorithms by minimizing communication costs and improving scalability. To achieve these objectives, the authors propose a novel framework called GraphU, which is designed to efficiently execute asynchronous graph algorithms on distributed systems. GraphU provides a programming interface based on distributed finite automata (DFA) and incorporates various optimization techniques to minimize communication costs and improve scalability.|Graph,Processing,Parallel,Automaton,DFA,Vertex,Centric,Asynchronous,Complexity,Optimization
27366fad-9d47-5b1a-bfa9-1b80ec201b04|2014-05-13T07:26:28|2014|5|Continuous pattern detection over billion-edge graph using distributed framework|||||||The problem definition addressed in this research is the continuous pattern detection over evolving graphs. The context that makes this problem important is the increasing need for real-time monitoring and timely response in various applications, such as social networks, cybersecurity, and recommender systems, where graphs are highly dynamic with frequent additions and removals of vertices and edges. The key objective of this research is to develop an efficient and scalable method for detecting patterns in evolving graphs, which can facilitate timely responses to emerging patterns. The authors aim to address this problem by designing a query evaluation framework that can handle continuous pattern detection over evolving graphs, with a focus on minimizing the response time and memory consumption. The ultimate goal is to provide a timely and efficient solution for monitoring related systems, enabling end-users to respond promptly to emerging patterns in the underlying graphs.|Graph,Pattern,Query,Exploration,Matching,Subgraph,Isomorphism,Join,Incremental,Evaluation
145d47e0-ea89-541d-83bc-7bda4657cd55|2017-05-01T16:34:09|2017|5|Distributed Exact Shortest Paths in Sublinear Time|||||||The problem addressed in this research is the distributed single source shortest paths (SSSP) problem in the CONGEST model of distributed computing. The context that makes this problem important is the need for efficient algorithms in distributed networks, where the diameter of the network is relatively small compared to the number of vertices. The authors aim to develop an exact algorithm for solving the SSSP problem in sublinear time, which is a major open problem in the field. The key objective is to design an algorithm that can compute the shortest paths from a designated source vertex to all other vertices in the network in time that is independent of the aspect ratio of the network. The authors also extend their result to the s-sources shortest paths problem and the streaming model.|Distributed,Shortest,Paths,Sublinear,Time,Algorithm,Graph,Hopset,CONGEST,Model
56079ea5-c4ec-5140-a97f-9f1c7cfee321|2019-11-26T15:48:35|2019|11|Optimal Lower Bounds for Distributed and Streaming SpanningForest Computation|||||||The problem addressed in this research is the incremental spanning forest data structure problem, where edges are inserted into an initially empty undirected graph G on n vertices, and the data structure must output a spanning forest of G when queried. The context that makes this problem important is the need for efficient algorithms to process large graphs in various applications. The authors aim to prove a lower bound on the space complexity of this problem, specifically showing that any data structure for this problem must use at least Ω(n log^3 n) bits of space. The authors also aim to extend this result to the distributed sketching model, where the goal is to minimize the communication cost between two parties, Alice and Bob, who hold different parts of the graph. The key objective is to prove a lower bound on the communication cost in this model, specifically showing that any protocol for this problem must have a communication cost of at least Ω(log^3 n) bits.|Spanning,Forest,Lower,Bounds,Distributed,Streaming,Computation,Dynamic,Data,Structure
2791ce86-3c33-5b5f-8618-5a466a5a6ad0|2014-07-03T17:22:37|2014|7|Minyang Han, Khuzaima Daudjee, Khaled Ammar, M. Tamer ¨Ozsu,Xingfang Wang, Tianqi Jin|||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph data in distributed computing environments. The context that makes this problem important is the increasing prevalence of graph data in various applications, such as social networks, web graphs, and biological networks, which require processing and analysis to extract valuable insights. The sheer size and complexity of these graphs pose significant challenges to traditional computing architectures, necessitating the development of distributed graph processing systems. The key objectives or goals of this research are to: Evaluate the performance of various open-source Pregel-like systems, which are designed to process large-scale graph data in a distributed manner. Identify the impact of built-in and optional system optimizations on the performance of these systems. Provide insights into the strengths and weaknesses of each system, highlighting areas for improvement. By addressing these objectives, the authors aim to contribute to the development of more efficient and scalable distributed graph processing systems, ultimately enabling faster and more accurate analysis of large-scale graph data.|Graph,Processing,Pregel,Systems,Comparison,Experimental,Performance,Optimization,BSP,Distributed
5acd9cab-7ca9-5a9a-bac1-6193de6bd1de|2014-03-28T16:18:18|2014|3|Efﬁcient Cohesive Subgraphs Detection in Parallel|simon||||||The problem definition addressed in this research is the k-truss detection problem in massive graphs. The context that makes this problem important is the need to analyze and understand the structure of large-scale graphs, which is crucial in various applications such as social network analysis, recommendation systems, and data mining. However, traditional cohesive subgraph definitions, such as cliques, are too strict and often do not exist in real-world graphs. The k-truss detection problem is a relaxed form of cohesive subgraph detection, which aims to find subgraphs where each edge is involved in at least k-2 triangles. The key objective of this research is to develop an efficient parallel algorithm to detect the maximal k-truss in a graph, which is a fundamental problem in graph analysis. The authors' goal is to design a scalable and efficient algorithm that can handle massive graphs and minimize the number of iterations required to detect the maximal k-truss. They aim to achieve this by exploiting the power-law distribution of edge supports in real-world graphs and developing a parallel algorithm that can take advantage of distributed computing architectures. Overall, the authors' objective is to provide a practical solution for k-truss detection in large-scale graphs, enabling faster and more accurate graph analysis in various applications.|k-truss,Cohesive subgraph,Graph algorithm,Parallel computing,Graph detection,Massive graph analysis,Social cohesion,Graph processing,Subgraph detection,Polynomial time
2bcb7b9a-c93b-55f4-ba89-edbd20ba3608|2020-05-29T10:37:06|2020|5|Future Generation Computer Systems|||||||The problem definition addressed in this research is the efficient processing of large-scale graphs on Graphics Processing Units (GPUs). The context that makes this problem important is the increasing size and complexity of graphs in various applications, such as social networks, web graphs, and biological networks, which require fast and scalable processing to extract valuable insights. The traditional Central Processing Unit (CPU) architecture is no longer sufficient to handle these large graphs, leading to a significant bottleneck in graph processing. The key objectives or goals of this research are to: Develop an efficient graph processing framework that can handle large-scale graphs on GPUs. Minimize the pre-processing time, which includes reading the graph from disk, constructing the necessary data structures, and allocating memory. Optimize the graph processing algorithm to reduce the processing time and improve scalability. To achieve these objectives, the authors propose a novel edge-centric processing model that leverages the parallel processing capabilities of GPUs to accelerate graph processing. The model is designed to reduce memory allocation and pre-processing time, while also improving the scalability and performance of graph processing on GPUs.|Graph,Processing,GPU,Edge,Centric,WolfGraph,Parallel,Memory,Algorithm,Iterative
9d0c0739-6c12-5284-bf33-bba8163430c6|2018-04-09T03:58:36+00:00|2018|4|TurboGraph++: A Scalable and Fast Graph Analytics System|Seongyun Ko,Wook-Shin Han||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph analytics, particularly neighborhood-centric analytics, on distributed memory systems. The context that makes this problem important is the increasing need to analyze massive graphs, which are ubiquitous in various domains such as social networks, web graphs, and biological networks. Existing vertex-centric systems are inadequate for handling these large graphs due to their limited scalability, high memory requirements, and poor performance. The key objectives or goals set by the authors to address this problem are threefold: 1. **Balanced workload distribution**: To balance the workloads across machines to maximize overall efficiency, avoiding imbalanced workloads that can lead to poor performance. 2. **Efficient graph partitioning**: To develop an efficient graph partitioning strategy that minimizes the number of machines required to process the graph, reducing the overall processing time and memory requirements. 3. **Scalable and efficient query processing**: To design a scalable and efficient query processing framework that can handle large-scale graph analytics, ensuring that the query processing time is nearly determined by the most bottlenecked hardware resource among CPU, disk, and network. By achieving these objectives, the authors aim to develop a scalable and efficient distributed graph processing system that can handle massive graphs and support various neighborhood-centric analytics queries.|Graph,Neighborhood,Query,Processing,Model,Streaming,Vertex,Adjacency,Walk,PageRank
570fb8ea-0f41-5981-9d50-255e07dad8a6|2020-08-31T04:13:14|2020|8|TriC: Distributed-memory Triangle Counting by Exploiting the Graph Structure|||||||The problem definition addressed in this research is the efficient counting of triangles in large-scale graphs, particularly in distributed memory architectures. The context that makes this problem important is the increasing need to analyze and understand complex networks, such as social networks, web graphs, and biological networks, which can be represented as massive graphs. Counting triangles in these graphs is a fundamental problem with numerous applications, including identifying clusters, communities, and anomalies. The key objective of this research is to develop an efficient and scalable algorithm for triangle counting in distributed memory architectures, which can handle massive graphs with billions of edges. The authors aim to minimize communication overhead, reduce memory usage, and optimize computation to achieve high performance and scalability. Specifically, they focus on reducing the communication pressure by aggregating outgoing data and implementing a policy that trades off computation with communication. Overall, the goal is to enable fast and efficient triangle counting in large-scale graphs, which is essential for various applications in data mining, machine learning, and network analysis.|Triangle Counting,Distributed Memory,Graph Analytics,Parallel Systems,Graph Structure,Communication Patterns,Memory Accesses,Graph Algorithms,Scalability,Graph Processing
91b5a4b1-5f95-5e8e-adf2-fb776491eef9|2016-05-02T06:53:09|2016|5|Distributed Incremental Pattern Matchingon Streaming Graphs|||||||The problem definition addressed in this research is the pattern matching problem in a streaming graph, where a data graph is continuously updated by an unbounded sequence of updates. The context that makes this problem important is the increasing amount of graph-structured data generated from various sources, such as social networks, IoT devices, and mobile networks, which requires efficient and real-time processing to extract valuable insights. The key objective of this research is to develop an efficient and scalable algorithm to match a pattern graph against a streaming data graph, ensuring that the matching results are up-to-date and accurate despite the continuous updates. The authors aim to achieve this goal by designing an incremental algorithm that can handle graph updates in real-time, minimizing the recomputation of matching results and reducing the computational overhead. In particular, the authors focus on the graph simulation model, which is a widely used model for pattern matching in graph-structured data. They aim to develop an algorithm that can efficiently detect changes in the matching results caused by graph updates, ensuring that the matching results are always consistent with the latest graph state. Overall, the authors' goal is to provide a scalable and efficient solution for pattern matching in streaming graphs, enabling real-time insights and decision-making in various applications.|Graph,Pattern,Matching,Streaming,Distributed,Incremental,Algorithm,Processing,Data,Big
32675cd7-fd88-56fe-af99-df6ccc2419cf|2024-01-08T02:11:21+00:00|2024|1|Communication-Efficient Triangle Counting under Local Differential Privacy|||||||The problem addressed in this research is the private estimation of subgraph counts, specifically triangle counts, in a social graph while ensuring edge Local Differential Privacy (LDP). The context is the increasing importance of analyzing connection patterns in graph data, such as social networks, while protecting user privacy. The key objective is to develop communication-efficient algorithms that can accurately estimate triangle counts while minimizing the download cost, which is a significant bottleneck in previous work. The authors aim to achieve this goal by proposing new algorithms that reduce the download cost while maintaining edge LDP, making them suitable for large-scale graphs and decentralized social networks.|Triangle,Counting,Local,Differential,Privacy,Graph,Algorithm,Communication,Efficiency,Clustering
36f81dac-f33e-5ad7-9b21-f831176db6dc|2014-12-22T18:14:48|2014|12|Distributed Maximum Matching in Bounded Degree Graphs|||||||The problem definition addressed in this research revolves around designing efficient algorithms for solving graph problems, specifically maximal independent set and maximum matching, in a distributed setting. The context that makes this problem important is the increasing need for scalable and efficient solutions for large-scale graph processing, which is crucial in various applications such as social networks, web graphs, and biological networks. The key objectives or goals the authors set to address this problem are: To develop CentLocal algorithms, which are centralized algorithms that answer queries regarding global solutions to computational problems by performing local sublinear time computations on the input. To transform these CentLocal algorithms into DistLocal algorithms, which are distributed algorithms that can be executed in a local manner, with a limited number of communication rounds. To achieve a trade-off between the number of communication rounds and the quality of the solution, with the goal of obtaining approximate solutions that are close to optimal. Overall, the authors aim to provide efficient and scalable solutions for graph problems in a distributed setting, which is essential for handling large-scale graph data.|CentLocal,Algorithms,Maximum Matching,Distributed,Local,Graph,Approximation,Deterministic,Online,Complexity
cec41564-ffd6-5f13-918c-81b05b071186|2015-09-21T05:28:13|2015|9|Distributed-Memory Algorithms for Maximal Cardinality Matching Using Matrix Algebra|||||||The problem definition addressed in this research is the computation of a maximum cardinality matching in a bipartite graph, which is a fundamental problem in combinatorial optimization with applications in scientific computing, such as permuting a matrix to its block triangular form and computing minimum weight matchings used by sparse direct solvers. The context that makes this problem important is that the existing algorithms for computing maximum cardinality matchings are not scalable and do not perform well on large-scale graphs, leading to a significant decrease in the quality of the matching as the concurrency increases. This limitation hinders the performance of various applications that rely on maximum cardinality matchings. The key objectives or goals of this research are to develop a scalable and efficient algorithm for computing maximum cardinality matchings in bipartite graphs, which can maintain a high-quality matching even on large-scale graphs and with increased concurrency. The authors aim to achieve this by exploiting the properties of sparse matrices and developing a novel algorithm that can efficiently compute maximum cardinality matchings on massively parallel architectures.|Maximal matching,Bipartite graph,Distributed algorithm,Sparse matrix,Graph processing,High-performance computing,Parallel algorithm,Matching algorithm,Graph matching,Scalability
f4e88ee7-4961-5dad-ae32-6f5976edfc5e|2015-10-09T09:17:40|2015|10|Thinking Like a Vertex|||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graphs in distributed systems. The context that makes this problem important is the increasing availability of massive graph-structured data, which poses significant challenges in terms of processing, storage, and scalability. The authors highlight that traditional sequential algorithms are inadequate for handling such large graphs, and distributed graph processing frameworks are needed to tackle this issue. The key objectives or goals set by the authors to address this problem are: To provide a comprehensive survey of vertex-centric frameworks for large-scale distributed graph processing, highlighting their design principles, strengths, and limitations. To identify the key pillars of a distributed graph processing framework, including timing, communication, execution model, and partitioning, and discuss how different frameworks address these aspects. To facilitate the development of more efficient and scalable distributed graph processing systems by providing insights into the design trade-offs and challenges associated with these frameworks. Overall, the authors aim to contribute to the development of more effective distributed graph processing systems, which are essential for handling the increasingly large and complex graph-structured data in various domains.|Vertex-centric,Graph processing,Distributed systems,Big Data,TLAV (Thinking Like a Vertex),Pregel,Graph algorithms,Scalability,Frameworks,Iterative algorithms
72c152f4-e15c-5ac2-a216-0329667d218c|2020-02-20T07:59:14|2020|2|Quantum Distributed Algorithm for Triangle Finding in the CONGEST Model|Taisuke Izumi,François Le Gall,Frédéric Magniez||||||The problem definition addressed in this research is the triangle finding problem in distributed networks. The context that makes this problem important is the increasing need for efficient algorithms in distributed systems, where communication is limited and scalability is crucial. In particular, the triangle finding problem is a fundamental problem in graph theory, and its solution has numerous applications in social network analysis, data mining, and network optimization. The key objective of this research is to develop an efficient distributed algorithm for solving the triangle finding problem in a communication-restricted model, known as the CONGEST model. The authors aim to design an algorithm that can detect triangles in a distributed network with a minimal number of communication rounds, while ensuring the correctness of the solution. Specifically, the authors focus on reducing the round complexity of the triangle finding problem, which is currently unresolved. They aim to achieve this by developing a novel reduction from the triangle finding problem to a related problem, called FindTriangleInSubnetwork, which can be solved more efficiently. The ultimate goal is to design a fast and scalable distributed algorithm for triangle finding that can be applied to large-scale networks.|Triangle,Listing,CONGEST,Quantum,Algorithm,Distributed,Graph,Complexity,Model,Network
16b3a910-0eb4-5df9-9a8e-f8248552848d|2019-08-31T12:26:36|2019|8|Scalable Triangle Counting on Distributed-Memory Systems|||||||The problem definition addressed in this research is the efficient parallelization of the triangle counting algorithm in large-scale graphs. The context that makes this problem important is the increasing need to analyze and process massive graph datasets in various fields, such as social networks, web graphs, and biological networks. Triangle counting is a fundamental graph analysis task that has numerous applications, including clustering, community detection, and network topology analysis. However, the computational complexity of triangle counting algorithms grows rapidly with the size of the graph, making it a significant challenge for large-scale graphs. The key objective of this research is to develop a scalable and efficient parallel algorithm for triangle counting that can take advantage of modern distributed computing architectures. The authors aim to achieve this goal by designing a parallel algorithm that minimizes communication overhead, balances computational load, and optimizes memory usage. The ultimate goal is to enable fast and accurate triangle counting in massive graphs, facilitating insights and discoveries in various domains.|Triangle,Counting,Distributed,Memory,Parallel,Algorithm,Graph,Partitioning,MPI,Cilk
bd0f9f9d-79e7-58b0-8684-272ced9d62ae|2017-12-21T05:38:37|2017|12|High-Level Programming Abstractions for Distributed Graph Processing|||||||"The problem definition addressed in this research revolves around the challenges of processing large-scale graph data in distributed systems. The context that makes this problem important is the increasing availability of massive graph-structured data, which poses significant scalability and performance issues when processed using traditional computing architectures. The authors highlight that existing graph processing systems often require users to have extensive knowledge of distributed computing, which can be a barrier to adoption. The key objective of this research is to identify and analyze the programming models and abstractions used in distributed graph processing systems. The authors aim to provide a comprehensive understanding of the strengths and weaknesses of different programming models, including vertex-centric, scatter-gather, and linear algebra-based models. By doing so, they hope to guide the development of more efficient, scalable, and user-friendly graph processing systems that can effectively handle large-scale graph data. In essence, the problem definition can be summarized as: ""How can we design and develop efficient, scalable, and user-friendly distributed graph processing systems that can effectively handle large-scale graph data, and what are the key programming models and abstractions that can help achieve this goal?"""|Graph,Algorithms,Programming,Models,Abstractions,Vertex,Centric,PageRank,Scatter,Gather
674448d0-b2bb-55da-8ebc-fd57a4bedaef|2019-06-29T15:46:37|2019|6|Distributed PageRank Computation: An Improved Theoretical Study|Siqiang Luo||||||The problem addressed in this research is the efficient computation of PageRank in a distributed setting, specifically in the congested clique model. PageRank is a measure of node importance in large graphs, widely used in various applications such as data mining, web algorithms, recommendation systems, load balancing, and search. The congested clique model is a distributed computing model where nodes communicate with each other through message passing, with a limited bandwidth. The authors aim to improve the state-of-the-art algorithm for computing PageRank in this model, which currently requires O(log n) rounds with an edge bandwidth of O(log^3 n) bits. The key objectives of this research are to reduce the round complexity and edge bandwidth, making the algorithm more efficient and scalable for large graphs. The authors also aim to adapt their algorithm to compute another variant of PageRank, called batch one-hop Personalized PageRanks (BPPR), which has found applications in practice.|PageRank,Distributed,Computation,Congested,Clique,Model,Algorithm,Random,Walks,Graphs
ff894488-f94f-5bbd-9e4d-00e23974ccf1|2019-07-20T10:02:37|2019|7|A survey of community search over big graphs|Yixiang Fang||||||The problem definition addressed in this research is Community Search (CS) in graphs, which involves finding densely connected subgraphs or communities in a graph that satisfy certain constraints or properties. The context that makes this problem important is the increasing availability of large-scale graph data, such as social networks, biological networks, and knowledge graphs, which require effective methods to identify meaningful communities or clusters. The key objectives or goals of this research are to: Identify densely connected subgraphs or communities in a graph that satisfy certain constraints or properties, such as spatial proximity, social relationships, and attribute similarity. Develop efficient algorithms and indexing techniques to support CS queries, which can be computationally expensive and time-consuming. Provide a comprehensive review of existing CS solutions, categorize them, and analyze their strengths and limitations. The authors aim to address the problem of CS by providing a thorough review of existing solutions, identifying the limitations and challenges of current approaches, and outlining future research directions to advance the field of CS.|Community Search,Attributed Graphs,Keyword-based,Location-based,Weight-based,Graph Queries,Online Queries,Big Graph,Community Retrieval,Cohesiveness Metrics
fefdbdbf-f313-57da-bde8-657ae7d821d1|2014-12-15T15:15:00|2014|12|SYNC or ASYNC: Time to Fuse forDistributed Graph-Parallel Computation|Rong||||||The problem definition addressed in this research revolves around the efficient execution of graph-structured computations in large-scale distributed systems. The context that makes this problem important is the increasing prevalence of machine learning and data mining (MLDM) problems in various areas, such as social computation, web search, and recommendation systems, which rely heavily on graph-structured data. As the size of these graphs grows, traditional serial processing becomes inefficient, and distributed parallel processing is necessary to accommodate the increasing data size and problem complexity. The key objective of this research is to develop an efficient execution mode for graph-structured computations that can adapt to the dynamic nature of these computations. The authors aim to address the trade-offs between computation, communication, and synchronization in distributed graph processing, which is critical for achieving high performance and scalability in large-scale distributed systems. Specifically, the authors focus on developing a hybrid approach that can dynamically switch between synchronous and asynchronous execution modes, depending on the characteristics of the graph and the computation, to optimize performance and resource utilization.|Graph parallel computation,Execution modes,Synchronous mode,Asynchronous mode,Distributed graph processing,Graph algorithms,Convergence speed,Machine learning,Data mining,Large-scale graph processing
1fa7c5d3-113e-5195-aa84-38a5aed75444|2015-01-05T07:46:00|2015|1|A survey of general-purpose experiment management tools for distributed systems|Tomasz Buchert||||||The problem definition addressed in this research is the efficient processing of large-scale graphs on Graphics Processing Units (GPUs). The context that makes this problem important is the increasing size and complexity of graphs in various applications, such as social networks, web graphs, and biological networks, which require fast and scalable processing to extract valuable insights. The traditional Central Processing Unit (CPU) architecture is no longer sufficient to handle these large graphs, leading to a significant bottleneck in graph processing. The key objectives or goals of this research are to: Develop an efficient graph processing framework that can handle large-scale graphs on GPUs. Minimize the pre-processing time, which includes reading the graph from disk, constructing the necessary data structures, and allocating memory. Optimize the graph processing algorithm to reduce the processing time and improve scalability. To achieve these objectives, the authors propose a novel edge-centric processing model that leverages the parallel processing capabilities of GPUs to accelerate graph processing. The model is designed to reduce memory allocation and pre-processing time, while also improving the scalability and performance of graph processing on GPUs.|Graph,Processing,GPU,Edge,Centric,WolfGraph,Parallel,Memory,Iterative,Algorithm
79866fc5-3bca-55ff-bd95-90a9c8e678b2|2023-01-27T06:17:34|2023|1|Exploring Truss Maintenance in Fully Dynamic Graphs: A Mixed Structure-Based Approach|||||||The problem definition addressed in this research revolves around the efficient maintenance of trussness in fully dynamic graphs. The context that makes this problem important is the widespread use of graphs in modeling complex systems, VLSI design, and social networks, where identifying cohesive subgraphs is crucial for deriving useful network information. However, maintaining trussness in dynamic graphs is challenging due to the computational intractability of cohesive subgraph computation. The key objectives or goals the authors set to address this problem are: 1. To develop an efficient algorithm for maintaining trussness in fully dynamic graphs, which can handle edge and vertex insertions and deletions. 2. To improve the scalability and efficiency of truss maintenance in large-scale graphs. By achieving these objectives, the authors aim to provide a practical solution for maintaining trussness in dynamic graphs, enabling the efficient analysis of complex systems and networks.|Truss,Graph,Maintenance,Dynamic,Cohesive,Subgraphs,Algorithm,Efficiency,Scalability,Decomposition
8bafb810-6b1e-5a25-b75d-d1da4d045c9e|2013-12-17T17:39:09|2013|12|A distributed vertex-centric approach for pattern matching in massive graphs|||||||The problem definition addressed in this research is the development of efficient distributed algorithms for graph pattern matching, specifically in the context of social network analysis. The background that makes this problem important is the increasing need for scalable and efficient methods to analyze large-scale social networks, which are often represented as massive graphs. The key objectives of this research are to: (1) explore how well graph simulation and its variants fit into the vertex-centric distributed processing paradigm, (2) identify the major bottlenecks of certain simulation models, and (3) develop new simulation models that are conceptually similar to existing ones but better suited to the vertex-centric paradigm. The authors aim to address these objectives by proposing distributed algorithms for graph simulation, dual simulation, strong simulation, and strict simulation, and evaluating their performance and scalability using experiments.|Graph,Pattern,Matching,Simulation,Distributed,Algorithm,Vertex,Centric,Scalable,Framework
4303d0b6-a27d-50d0-8328-73ff56e3e689|2011-08-12T09:46:52|2011|8|Parallel breadth-first search on distributed memory systems|Aydin Bulu,Kamesh Madduri||||||The problem definition addressed in this research is the efficient parallelization of graph algorithms, specifically Breadth-First Search (BFS), on large-scale distributed memory systems. The context that makes this problem important is the increasing significance of graph abstractions in analyzing and understanding complex systems, such as social networks, biological systems, and engineered systems like the power grid and the Internet. The sheer scale of these graphs, with billions of vertices and edges, necessitates the development of scalable parallel algorithms to process them efficiently. The key objective of this research is to design and implement a hybrid parallel BFS algorithm that can effectively utilize the computing resources of large-scale distributed memory systems, while minimizing communication overhead and achieving good scalability. The authors aim to overcome the challenges posed by skewed degree distributions, irregular graph structures, and the need for efficient data partitioning and communication strategies. By achieving these goals, the authors hope to enable the analysis of massive graphs, which is critical in various domains, including social network analysis, network topology analysis, and anomaly detection.|Graph,Traversal,Parallel,BFS (Breadth-First Search),Algorithm,Memory,Performance,Optimization,Multicore,Distributed
4ebd0208-8328-5d69-8c44-ec50939c0967|2014-02-14T04:42:25|2014|2||zhanglj||||||The problem definition addressed in this research is graph pattern matching, which involves finding subgraphs of a data graph that are similar to a given query graph. The context that makes this problem important is the massive scales of modern application domains such as social networks and the World Wide Web, which have reignited the need for efficient and scalable graph pattern matching algorithms. The key objectives or goals of the authors are to develop new simulation models that are conceptually similar to existing ones but better suited to the vertex-centric distributed processing paradigm. Specifically, they aim to address the limitations of existing simulation models, such as graph simulation and strong simulation, which are not scalable or efficient for large-scale graph data. The authors' primary goal is to design a pattern matching model that is competitive, scalable, and suitable for vertex-centric graph processing. They aim to achieve this by relaxing some restrictions on matches while maintaining a good quality of result, and by developing new simulation models that can handle large-scale graph data efficiently.|Graph,Pattern,Matching,Scalable,Distributed,Algorithm,Vertex-centric,Simulation,Subgraph,Isomorphism
02441855-c966-55b1-81ec-90304764011b|2021-04-05T01:01:34+00:00|2021|4|Fast Parallel Algorithms for Euclidean Minimum Spanning Tree and Hierarchical Spatial Clustering|Yiqiu Wang,Shangdi Yu,Yan Gu,Julian Shun||||||The problem definition addressed in this research is the Hierarchical Density-Based Spatial Clustering with Added Noise (HDBSCAN) problem. The context that makes this problem important is the need for efficient and scalable clustering algorithms that can handle large datasets with varying densities and noise. Traditional clustering algorithms, such as DBSCAN, are limited in their ability to handle these complexities, leading to a need for more advanced and robust clustering methods. The key objective of this research is to develop an efficient algorithm for solving the HDBSCAN problem, which involves computing a hierarchy of clusters for a given dataset. The authors aim to achieve this by leveraging the concept of mutual reachability distance, which is a measure of the distance between two points in a dataset. The goal is to develop an algorithm that can efficiently compute the mutual reachability distance and use it to construct a hierarchy of clusters that accurately reflects the underlying structure of the data. Specifically, the authors set out to address the following key objectives: * Develop an efficient algorithm for computing the mutual reachability distance between points in a dataset * Use the mutual reachability distance to construct a hierarchy of clusters that accurately reflects the underlying structure of the data * Improve the scalability and efficiency of the algorithm to handle large datasets with varying densities and noise Overall, the authors aim to provide a robust and efficient solution to the HDBSCAN problem, which can be used to analyze and understand complex datasets in various fields, such as data mining, machine learning, and bioinformatics.|HDBSCAN,EMST,Parallel,Clustering,Algorithm,Dendrogram,MST,Reachability,Core distance,Mutual reachability
fa394308-e7f7-5d9f-96e7-27cd8828a8c2|2018-05-03T16:27:41|2018|5|On Smart Query Routing: For Distributed Graph Querying with Decoupled Storage|||||||The problem definition addressed in this research revolves around efficient query routing in distributed graph processing systems. The context that makes this problem important is the increasing scale and complexity of graph-structured data, which has led to a growing need for distributed processing systems to handle large-scale graph queries. However, existing query routing strategies in these systems often result in poor performance, high latency, and inefficient resource utilization. The key objectives or goals the authors set to address this problem are: 1. **Topology-aware locality**: The authors aim to develop a query routing strategy that takes into account the graph topology and cache locality to minimize query latency and improve system performance. 2. **Load balancing and fault tolerance**: The authors seek to design a routing strategy that balances the workload across processors, minimizes the impact of processor failures, and ensures the system remains operational even in the presence of faults. 3. **Scalability and efficiency**: The authors aim to develop a routing strategy that can handle large-scale graph queries efficiently, without incurring significant overhead in terms of storage, computation, or communication. Overall, the authors' goal is to develop a smart query routing strategy that can efficiently handle large-scale graph queries in distributed processing systems, while ensuring good performance, scalability, and fault tolerance.|Query,Graph,Distributed,Routing,Storage,Decoupled,Heterogeneous,Directed,Partitioning,Scalability
e5d11fce-292a-56e7-a7ed-9f238b68332f|2014-08-09T03:53:26|2014|8|IFIP AICT 437 - CSMR: A Scalable Algorithm for Text Clustering with Cosine Similarity and MapReduce|Giannakouris-Salalidis Victor,Plerou Antonia,Sioutas Spyros||||||The problem definition addressed in this research revolves around the challenge of processing and analyzing large volumes of text data, particularly in the context of big data. The rapid growth of data in various domains, such as business intelligence and bioinformatics, has created a need for efficient and scalable methods to handle text data. The authors identify the importance of text clustering, which involves grouping similar text documents together, as a crucial task in text mining. However, traditional methods for text clustering are limited by their inability to handle large datasets, leading to issues with processing speed and scalability. The key objectives of this research are to develop a scalable algorithm for text clustering that can efficiently handle large datasets and to improve processing speed and scalability. The authors aim to achieve this by leveraging the MapReduce programming model and Hadoop framework, which are designed for distributed computing and can handle massive data processing. Specifically, the authors focus on using the Cosine Similarity measure and tf-idf technique to improve the efficiency and effectiveness of text clustering. Overall, the goal of this research is to provide a scalable and efficient solution for text clustering that can meet the demands of big data.|MapReduce,Cosine Similarity,TF-IDF,Text Similarity,Hadoop,Distributed Algorithm,Text Clustering,Big Data,Scalability,Efficiency
7b2311ce-ec54-5cf8-9d90-665d54bb83bf|2017-09-06T01:25:27+00:00|2017|9|Distributed Triangle Counting in the Graphulo Matrix Math Library|Dylan Hutchison||||||The problem addressed in this research is the efficient counting of triangles in large graphs that exceed main memory. The context is the increasing importance of graph analysis in various fields, such as social network mining, cybersecurity, and functional biology, where triangle counting is a key algorithm. The authors aim to adapt two algorithms for counting triangles to the Graphulo library, which provides a framework for implementing graph algorithms on the Apache Accumulo distributed database. The key objective is to achieve high performance and scalability in triangle counting, while also considering the limitations and challenges of the Accumulo database. The authors aim to provide a solution that can handle large graphs and achieve good performance, making it a valuable contribution to the field of graph analysis.|Graphulo,Triangle,Accumulo,Algorithm,Matrix,Database,Graph,Counting,Distributed,Library
5829b432-9662-5411-a837-e336427be89f|2011-11-06T12:13:32|2011|11|Survey of local algorithms|Jukka Suomela||||||The problem definition addressed in this research revolves around the development of efficient distributed algorithms, specifically local algorithms, that can solve complex computational problems in a constant number of communication rounds, regardless of the size of the input. The context that makes this problem important is the increasing need for scalable and fault-tolerant solutions in modern distributed systems, such as communication networks, where the size of the input (i.e., the number of nodes) can be very large. The key objective of this research is to design local algorithms that can solve various computational problems, such as graph problems, in a highly scalable and efficient manner. The authors aim to achieve this by developing algorithms that can run in a constant number of communication rounds, independent of the size of the input, and can recover efficiently from faults. The ultimate goal is to provide practical solutions for real-world applications, such as monitoring and operating communication networks, where local algorithms can offer significant advantages in terms of scalability, fault tolerance, and robustness.|Local algorithms,Distributed algorithms,Graphs,Computational complexity,Approximation algorithms,Constant time,Network algorithms,Geometric graphs,Randomized algorithms,Distributed computing
a7ce7cbd-1a1d-57a5-97a0-4f1504eb533f|2015-10-24T08:09:44|2015|10|Improved Distributed Approximate Matching|||||||The problem addressed in this research is the computation of a set of disjoint edges in a graph, also known as a matching, in a distributed system. The context that makes this problem important is its fundamental role in combinatorial optimization and distributed systems, with applications in various fields such as communication networks, job scheduling, and resource allocation. The authors aim to develop efficient and effective algorithms for maximum matching in bipartite and general graphs, both weighted and unweighted. The key objectives are to achieve a good approximation ratio, specifically 1/2 or 1/3, and to minimize the running time, with a focus on achieving O(log n) time complexity. The authors also aim to reduce the message size to O(log n) bits, making the algorithms more efficient in distributed systems.|Matching,Distributed,Approximate,Algorithm,Graphs,Network,CONGEST,Model,Optimization,Computation
3354de83-a569-5532-ac1a-daf72cc258ae|2018-08-07T18:45:42+00:00|2018|8|Near-Optimal Random Walk Sampling in Distributed Networks|||||||The problem addressed in this research is the efficient computation of random walks in distributed networks. The context that makes this problem important is the wide range of applications in distributed computing and communication networks that use random walks as an integral subroutine, such as token management, load balancing, small world routing, search, and information dissemination. The key objective of this research is to develop algorithms that can perform random walks with both optimal round complexity and message complexity, which is crucial for efficient communication in distributed networks. The authors aim to improve upon existing algorithms that either have high round complexity or high message complexity, and to support continuous random walk requests in a distributed network.|Random,Walks,Distributed,Networks,Sampling,Algorithm,Continuous,Processing,Message,Complexity
9f32d05f-dbaf-5ea5-a93e-1b436088c93b|2022-02-10T05:39:38+00:00|2022|2|Scaling Graph Traversal to 281 Trillion Edges with 40 Million Cores|Huanqi Cao Yuanwei Wang Haojie Wang Heng Lin Zixuan Ma Wanwang Yin Wenguang Chen||||||The problem addressed in this research is the efficient processing of large-scale graph traversal, specifically Breadth-First Search (BFS), on supercomputers. The context that makes this problem important is the increasing scale and complexity of graph data, which is critical in various applications such as financial risk management, epidemic trajectory analysis, and protein sequence prediction. The current graph processing methods are limited by load imbalance, lack of locality, and inherent load imbalance, making it challenging to scale up graph processing. The key objectives or goals of this research are to: Develop an efficient graph partitioning method that can handle massive graphs with hundreds of trillions of edges. Design a parallel BFS algorithm that can scale up to thousands of processors and millions of cores. Achieve high performance and efficiency in graph traversal on new-generation supercomputers, such as the Sunway architecture. Overall, the authors aim to push the boundaries of graph processing and enable the analysis of massive graphs on supercomputers, which is essential for various applications and domains.|Graph Traversal,Breadth-First Search (BFS),Massively Parallel Algorithm,Heterogeneous Architecture,Graph 500 Benchmark,Supercomputing,Distributed Graph Processing,Scalability,Parallelization,Optimization
62d01184-34eb-5026-91df-23ad60610137|2018-12-20T02:31:02+00:00|2018|12|DISTRIBUTED COLORING IN SPARSE GRAPHS WITH FEWER COLORS|||||||The problem addressed in this research is the efficient coloring of sparse graphs in the distributed setting with as few colors as possible. The context that makes this problem important is the need for efficient distributed algorithms for coloring sparse graphs, which is a fundamental problem in computer science and has applications in various fields. The authors aim to improve upon existing results by providing a deterministic distributed algorithm that can color sparse graphs with fewer colors and in a polylogarithmic number of rounds. The key objectives of this research are to develop an algorithm that can color graphs of maximum average degree d with d+1 colors, and to achieve a round complexity that is sublinear in the number of vertices. The authors also aim to extend their results to the list coloring setting, where each vertex has a list of available colors.|Distributed,Coloring,Algorithm,Graph,List,LOCAL,Model,Complexity,Round,Deterministic
aeb988a1-8112-5b02-b87c-1dba977e8aa8|2021-10-14T13:58:35|2021|10|A Parallel Algorithm Template for Updating Single-Source Shortest Paths in Large-Scale Dynamic Networks|||||||The problem definition addressed in this research is the efficient update of Single Source Shortest Paths (SSSP) in large-scale dynamic networks. The context that makes this problem important is the increasing size and complexity of real-world networks, such as transportation, communication, social, and cyber-physical systems, which require fast and scalable algorithms to analyze and respond to changes in the network. The key objective of this research is to develop a parallel algorithm that can efficiently update the SSSP in dynamic networks, where edges or vertices are added or removed, without recomputing the entire SSSP from scratch. The authors aim to achieve this goal by designing an algorithm that can identify the affected subgraphs, update the SSSP tree, and minimize redundant computations, thereby reducing the execution time and improving scalability. In essence, the problem definition involves developing a fast, scalable, and efficient algorithm to update the SSSP in dynamic networks, which is critical for various applications, including network analysis, routing, and decision-making.|Single Source Shortest Path (SSSP),Dynamic Networks,Parallel Algorithm,Graph Theory,Network Analysis,Shortest Path Problem,Graph Updates,Scalable Algorithm,GPU Implementation,Large-Scale Networks
0b9ff581-bcdb-5d1d-8454-7d1c07a771a9|2021-01-25T19:28:10|2021|1|A distributed large graph coloring algorithm on Giraph|||||||The problem definition addressed in this research is the Vertex Graph Coloring (VGC) problem, which is a well-known problem in graph theory. The context that makes this problem important is its numerous applications in various domains such as telecommunications, bioinformatics, and the Internet. The VGC problem is NP-hard, meaning that finding the chromatic number (the smallest number of colors required to color a graph) is computationally difficult. The key objective of this research is to develop an efficient algorithm to solve the VGC problem, particularly for large graphs. The authors aim to design a distributed algorithm that can take advantage of parallel architectures to achieve significant performance improvements. The goal is to minimize the number of colors used to color the graph while ensuring that adjacent vertices have different colors. The authors propose a new Giraph graph coloring algorithm that is designed for the vertex-centric model and can effectively handle large graphs.|Graph coloring,Giraph,Distributed algorithm,Vertex centric model,Large graph processing,Parallel computation,Graph theory,Chromatic number,NP-hard problem,Scalability
295af3dc-f033-58b7-9e75-c67e2d2803ec|2021-01-04T17:27:41|2021|1|On Distributed Algorithms for Minimum Dominating Set problem, from theory to application|Sharareh Alipour,Ehsan Futuhi,Shayan Karimi||||||The problem addressed in this research is the Minimum Dominating Set (MDS) problem, which is a fundamental problem in graph theory with applications in various fields, including social networks, communication networks, and distributed systems. In the context of social networks, the MDS problem is important because it can be used to identify a small set of influential individuals who can spread information or recommendations to the entire network. The authors aim to develop a distributed algorithm that can efficiently solve the MDS problem in large-scale networks, with a focus on achieving a good approximation factor and fast running time. The key objectives of this research are to propose a local approximation algorithm for the MDS problem, prove its approximation factor for certain types of graphs, and experimentally evaluate its performance on large-scale social networks.|Dominating,Set,Problem,Algorithm,Distributed,Model,Graphs,Planar,Triangle,Free
c6e79c8f-5af5-5485-9f3e-c70403b38716|2018-10-21T16:38:09|2018|10|New and Simpliﬁed Distributed Algorithms for Weighted AllPairs Shortest Paths|||||||The problem addressed in this research is the computation of All-Pairs Shortest Paths (APSP) and related problems in the Congest model, a distributed computing model where nodes have limited topological knowledge and can send O(log n) bit messages along edges in each round. The context that makes this problem important is the need for efficient algorithms in distributed networks, where nodes have limited computational power and knowledge. The key objectives of this research are to improve the round complexity of APSP algorithms, specifically for non-negative, moderate integer weights, and to develop deterministic algorithms for the weighted k-Shortest Paths (k-SSP) problem. The authors aim to achieve these objectives by combining existing algorithms, such as the pipelined algorithm, with new techniques and improvements to existing blocker set algorithms.|APSP,Congest,Algorithm,Graph,Shortest,Paths,Distributed,Weighted,Model,Rounds
df996b89-bf39-5f62-948e-e0a99977ecfd|2015-03-21T19:40:44|2015|3|Performance comparison of parallel graph coloring algorithms on BSP model using hadoop|||||||The problem definition addressed in this research is the graph coloring problem, which is a well-known NP-hard problem with many practical applications such as frequency assignment and scheduling. The context that makes this problem important is the need for efficient graph processing systems to handle large-scale graph data, which is becoming increasingly common in big data analytics. The traditional MapReduce model is not efficient for graph processing, and therefore, there is a need for alternative solutions. The key objective of this research is to develop an efficient graph coloring algorithm that can be used in in-memory Pregel-like graph processing systems. The authors aim to design an algorithm that can color a large undirected graph in a distributed manner, using a greedy approach that does not guarantee an optimal solution but can provide a good approximation. The goal is to develop an algorithm that can complete the graph coloring task in a reasonable amount of time, using a limited number of supersteps, and with minimal communication overhead between nodes.|Graph Coloring,Parallel Algorithm,Hadoop,BSP Model,Pregel,Graph Processing,Distributed Algorithm,Big Data Analytics,In-Memory Processing,Graph Algorithm
25a5dfef-2a70-5629-b1fb-1bc3ae26c09f|2019-06-08T09:19:08|2019|6|A 2D Parallel Triangle Counting Algorithm for Distributed-Memory Architectures|Ancy Sarah Tom,George Karypis||||||The problem definition addressed in this research is the efficient parallelization of triangle counting in large-scale graphs. The context that makes this problem important is the increasing need to analyze and process massive graphs in various domains, such as social networks, web graphs, and biological networks. Triangle counting is a fundamental graph operation that has numerous applications, including graph clustering, community detection, and network analysis. The key objective of this research is to develop a scalable and efficient parallel algorithm for triangle counting that can handle large graphs and minimize communication overheads. The authors aim to achieve this by distributing the graph data and tasks across multiple processors, optimizing the computation and communication patterns, and reducing load imbalance and redundant work. In summary, the problem definition is to develop an efficient parallel triangle counting algorithm that can scale to large graphs, minimize communication overheads, and optimize computation and communication patterns to achieve high performance and scalability.|Triangle Counting,Distributed Memory,Graph Algorithms,Scalability,Parallel Processing,MPI,Graph Processing,Large-Scale Graphs,Optimization,Performance
70cb230d-59c6-5808-9723-ed3f3ca61ecf|2020-10-01T16:42:16+00:00|2020|10|GraphPulse: An Event-Driven Hardware Accelerator for Asynchronous Graph Processing|||||||The problem definition addressed in this research revolves around the inefficiencies of processing large graphs, which is a crucial computational workload in various domains such as social networks, web graphs, and brain networks. The context that makes this problem important is the increasing volume of connected data, which necessitates efficient graph analytics to uncover insights. However, traditional computation models and software frameworks are hindered by irregular memory access patterns, synchronization requirements, and poor locality, leading to underutilization of compute resources. The key objectives or goals the authors set to address this problem are: 1. To develop an efficient computation model that can handle large graphs effectively. 2. To overcome the limitations of traditional software frameworks and computation models. 3. To design a hardware accelerator that can optimize graph processing by leveraging the inherent parallelism and locality of graph computations. Overall, the authors aim to create a novel computation model and hardware accelerator that can efficiently process large graphs, enabling faster and more effective graph analytics in various domains.|Graph,Processing,Hardware,Accelerator,Asynchronous,Analytics,Memory,Event-driven,Vertex-centric,Iterative
d2a50756-adbd-543f-9c40-7cdd0227a046|2015-03-05T05:17:33|2015|3|Big Graph Processing Systems: State-of-the-Art and Open Challenges|||||||The problem definition addressed in this research revolves around the challenges of developing scalable graph processing systems. The context that makes this problem important is the increasing interconnectedness of people, devices, processes, and other entities, resulting in massive graph-structured data that needs to be processed efficiently. The authors highlight that traditional data processing systems are not designed to handle the complexities and scale of graph data, leading to performance bottlenecks and inefficiencies. The key objectives or goals the authors set to address this problem are: 1. To provide a comprehensive overview of the state-of-the-art in graph processing systems, highlighting their strengths and weaknesses. 2. To identify the current open research challenges in developing scalable graph processing systems. 3. To discuss promising directions for future research in this area. By addressing these objectives, the authors aim to contribute to the development of more efficient and scalable graph processing systems that can handle the complexities of large-scale graph data.|Graph,Processing,Scalable,Platforms,Distributed,Systems,Large-scale,MapReduce,Iterative,Analytics
b96f45f9-f570-5137-a6e8-87f09274ebb3|2015-05-22T07:03:35|2015|5|Mining maximal cliques from a large graph using MapReduce: Tackling highly uneven subproblem sizes|Michael Svendsen||||||The problem definition addressed in this research is the Maximal Clique Enumeration (MCE) problem, which involves enumerating all maximal cliques in a large graph. The context that makes this problem important is the increasing size and complexity of graphs in various applications, such as social networks, web analysis, and bioinformatics, which require efficient and scalable methods for graph analysis. The key objectives or goals of the authors are to: Develop a scalable method for enumerating maximal cliques in a graph using MapReduce, a popular framework for processing large data sets in parallel. Achieve effective load balancing to ensure that the parallel resources are utilized efficiently. Evaluate the performance of the proposed solution on large real-world graphs and demonstrate its superiority over previous MapReduce solutions. Overall, the authors aim to provide a fast and efficient solution for the MCE problem, which is a fundamental problem in graph analysis, and has numerous applications in various fields.|Maximal Clique Enumeration (MCE),Graph Mining,MapReduce,Parallel Algorithm,Load Balancing,Large Graphs,Dense Substructures,Clique Enumeration,Graph Analysis,Scalability
1e26986c-136c-577e-89f4-948133f5e822|2017-06-23T00:31:26+00:00|2017|6|GraphHP: A Hybrid Platform for Iterative Graph Processing|Qun Chen,Song Bai,Zhanhuai Li,Zhiying Gou,Bo Suo,Wei Pan||||||The problem definition addressed in this research revolves around the inefficiencies of Bulk Synchronous Parallel (BSP) platforms in processing large-scale graph data. The context that makes this problem important is the increasing scale of graph data applications, such as web and social network analysis, graph mining, and machine learning, which require iterative computations over data dependencies. However, BSP platforms, which are designed to process these computations, are plagued by high synchronization and communication overhead, leading to slow convergence rates and inefficient processing. The key objectives or goals the authors set to address this problem are: 1. To develop a new platform that can efficiently process large-scale graph data by reducing synchronization and communication overhead. 2. To design a hybrid execution model that combines the benefits of BSP platforms with the efficiency of sequential algorithms. 3. To provide a simple and intuitive programming interface that allows users to implement graph algorithms without requiring specific optimization instructions. Overall, the authors aim to develop a more efficient and scalable platform for processing large-scale graph data, which is critical for many applications in data mining, machine learning, and social network analysis.|Graph,BSP,Platform,Iterative,Processing,Hybrid,Execution,Model,Synchronization,Communication
ba2a98eb-3490-5117-804e-5551604d2320|2013-09-17T01:53:38|2013|9|X-Stream: edge-centric graph processing using streaming partitions|Amitabha Roy,Ivo Mihailovic,Willy Zwaenepoel||||||The problem definition addressed in this research is the efficient processing of large-scale graph computations, particularly in the context of graph algorithms that require iterative scatter-gather operations. The context that makes this problem important is the increasing prevalence of graph-structured data in various domains, such as social networks, web graphs, and biological networks, which necessitates the development of scalable and efficient graph processing techniques. The key objectives or goals the authors set to address this problem are: To develop a scalable and efficient graph processing approach that can handle large-scale graphs with billions of edges and vertices. To minimize the memory requirements and I/O overhead associated with graph processing, which are major bottlenecks in existing approaches. To provide a flexible and programmable framework for expressing graph computations, allowing users to easily implement various graph algorithms. To achieve these objectives, the authors propose an edge-centric scatter-gather processing model that streams edges and updates from disk, eliminating the need for random access to the graph structure. This approach enables efficient processing of large-scale graphs using limited memory resources, making it suitable for modern computing architectures.|Graph,Processing,Streaming,Edge-centric,Scatter-gather,Algorithm,Performance,Memory,Partition,Computation
cd0a9de0-6cf7-5c38-a884-1cbcd64ae751|2018-10-09T23:36:03|2018|10|Optimal Dynamic Distributed MIS|||||||The problem addressed in this research is maintaining a maximal independent set (MIS) in a dynamic distributed setting, where nodes may occasionally join and leave the network, and communication links may fail and be restored. This problem is important because it is a fundamental task in distributed computing, and solving it efficiently is crucial for various applications. The authors aim to develop an algorithm that can maintain an MIS in a dynamic network with optimal complexity guarantees, specifically in terms of the number of nodes that need to change their output in response to a topology change, the number of rounds required for the system to become stable, and the total number of broadcasts. The authors also aim to strongly separate the static and dynamic distributed models, as well as deterministic and randomized solutions.|Algorithm,Distributed,Dynamic,Graph,Greedy,Independent,Maximal,Network,Random,Sequential
1527aba5-af6c-5d70-aff2-cafd5e0d9ffb|2019-05-21T01:40:21+00:00|2019|5|Distributed Algorithms for Subgraph-Centric GraphPlatforms|||||||The problem definition addressed in this research revolves around processing large-scale graphs in a distributed manner, which has become increasingly important due to the growing size of data. The context that makes this problem important is the need to handle massive datasets that do not fit in a single machine, making it challenging to perform graph processing tasks efficiently. The key objective of this research is to develop and evaluate subgraph-centric algorithms for graph processing tasks, such as triangle counting, clustering, and minimum spanning forest. The authors aim to design algorithms that can take advantage of the subgraph-centric model, which partitions the graph into smaller subgraphs and processes them in parallel, to achieve better performance and scalability. Specifically, the authors focus on adapting existing algorithms to the subgraph-centric model, which has not been explored previously for these specific graph processing tasks. By doing so, they aim to provide a broader evaluation of the subgraph-centric model and its potential benefits in terms of performance and scalability, compared to traditional vertex-centric and shared-memory models.|Subgraph,Centric,Distributed,Graph,Algorithms,Processing,Frameworks,GoFFish,Component,Analytics
85503c78-2638-5081-9e38-b3d6bf9793dc|2015-01-09T09:30:35|2015|1|Subgraph Rank: PageRank for Subgraph-CentricDistributed Graph Processing|||||||The problem definition addressed in this research revolves around the efficient computation of graph centrality measures, specifically PageRank, on large-scale graphs. The context that makes this problem important is the increasing size and complexity of real-world networks, which necessitates the development of scalable and efficient algorithms to analyze these networks. The traditional approach of computing PageRank on a single machine is no longer feasible due to memory and computational constraints. The key objectives or goals the authors set to address this problem are: 1. To develop a subgraph-centric programming abstraction that can efficiently compute graph centrality measures on large-scale graphs. 2. To adapt the PageRank algorithm to this new abstraction, ensuring that it can scale to massive graphs while maintaining accuracy. 3. To explore the potential of BlockRank, a variant of PageRank, in the context of subgraph-centric computing and evaluate its performance. Overall, the authors aim to provide a scalable and efficient solution for computing graph centrality measures on large-scale graphs, enabling the analysis of complex networks in various domains.|PageRank,BlockRank,Subgraph,Centrality,Graph,Algorithm,Distributed,Platform,Scalability,Convergence
38182372-23ae-576e-ad5c-27f1471c1906|2010-06-17T07:51:11|2010|6|Fast distributed approximation algorithms for vertex cover and set cover in anonymous networks|Matti Åstrand,Jukka Suomela||||||The problem addressed in this research is the development of efficient distributed approximation algorithms for the minimum weight vertex cover and set cover problems in anonymous networks. The context that makes this problem important is the need for fast and scalable algorithms in distributed systems, where the number of nodes can be very large. The authors aim to design algorithms that are deterministic, have a running time independent of the number of nodes, and provide a good approximation ratio. Specifically, the authors focus on finding a maximal edge packing, which is a key component in approximating the minimum weight vertex cover, and also address the set cover problem by reducing it to a fractional packing problem. The key objectives are to develop algorithms that are fast, scalable, and provide a good approximation ratio, while also being deterministic and suitable for anonymous networks.|Distributed,Algorithms,Vertex,Cover,Approximation,Weighted,Graph,Set,Packing,Fractional
27b3cf95-fda1-5b13-b95c-0e2d9fb42f31|2019-02-18T01:19:52+00:00|2019|2|Accelerating Partial Evaluation in DistributedSPARQL Query Evaluation|||||||The problem addressed in this research is the evaluation of SPARQL queries over a large distributed RDF graph in a distributed environment. The context is that RDF data is increasingly being published on the Web, and it is necessary to design a distributed database system to process SPARQL queries. The key objective is to improve the efficiency of the partial evaluation and assembly framework for answering SPARQL queries over a distributed RDF graph, while providing performance guarantees. The authors aim to explore the intrinsic structural characteristics of partial results to compress them into a compact data structure, prune some irrelevant partial evaluation results, and assemble them efficiently to form the final results.|SPARQL,Distributed,RDF,Query,Evaluation,Partial,Framework,Optimization,Assembly,Graph
839b9eb1-0318-5868-8454-b6d4860fa4c5|2016-08-11T11:09:57|2016|8|A brief introduction to distributed systems|Maarten Steen||||||The problem definition addressed in this research revolves around the scalability and openness of distributed systems. The context that makes this problem important is the increasing demand for distributed systems to support a large number of users and resources, while maintaining performance, consistency, and fault tolerance. The authors highlight that centralized services often hit scalability limits due to computational capacity, storage capacity, and network bandwidth constraints. The key objectives or goals the authors set to address this problem are: To achieve size scalability, allowing distributed systems to support a large number of users and resources without performance degradation. To ensure openness, enabling components from different systems to be easily integrated and used together. To provide transparency, making it easy for users to access and utilize distributed systems without being aware of the underlying complexities. Overall, the authors aim to develop distributed systems that can efficiently and effectively support a large number of users and resources, while maintaining performance, consistency, and fault tolerance, and providing openness and transparency.|Distributed Systems,Computer Systems,Middleware,Networked Computers,Coherent System,Nodes,Communication,Message Passing,Overlays,Peer-to-Peer Networks
267b5ba5-8e30-57ad-84e9-df09338dc6a0|2021-12-15T14:58:29|2021|12|Jyoti Prakash Sahoo · Asis Kumar Tripathy · Manoranjan Mohanty · Kuan-Ching Li · Ajit Kumar Nayak   EditorsAdvances in Distributed Computing and Machine Learning|||||||The problem definition addressed in this research is the design of a smart autonomous collision avoidance system based on IoT modules and CAN serial communications protocol to avoid road accidents due to careless driving and negligence. The context that makes this problem important is the increasing number of road accidents caused by human error, and the need for a system that can detect obstacles and prevent collisions. The key objective of this research is to develop a system that can evaluate the position of an obstacle and control the time of impact of collision, thereby reducing the risk of accidents. The authors aim to achieve this goal by designing a system that uses Arduino UNO, ultrasonic sensors, and CAN serial communications protocol to detect obstacles and prevent collisions.|Text,Summarization,Odia,Language,Document,Automatic,Natural,Processing,Extractive,Indian
3c838db5-92ce-5198-8ed7-f2b3eb56e639|2021-09-14T01:22:18+00:00|2021|9|Distributed recoloring of interval and chordal graphs|Nicolas Bousquet,Laurent Feuilloley,Marc Heinrich,Mikaël Rabie||||||The problem addressed in this research is the distributed recoloring of interval and chordal graphs in the LOCAL model of distributed computing. The context that makes this problem important is the need for efficient frequency assignment in wireless networks, where a proper coloring of the network is required to avoid conflicts. The authors aim to find a schedule of colorings that transforms an input coloring into a target coloring, such that at each step, the coloring is safe and does not conflict with the previous coloring. The key objectives are to minimize the number of additional colors needed, the number of rounds of communication required to produce the schedule, and the length of the schedule itself. The authors also aim to improve the existing algorithms for coloring and recoloring interval and chordal graphs, which are important graph classes in the context of wireless networks.|Distributed,Recoloring,Interval,Graphs,Chordal,Coloring,LOCAL,Model,Algorithm,Complexity
e3d3010c-d623-5d7a-9a6f-35323ab904c9|2014-02-14T04:42:25|2014|2|Distributed and Scalable Graph Pattern Matching: Models and Algorithms|zhanglj||||||The problem addressed in this research is graph pattern matching, which seeks to find subgraphs of a data graph that are similar to a given query graph. This problem is important due to the massive scales of modern application domains such as social networks and the World Wide Web, which require processing massive graphs in a timely manner. The authors aim to develop new simulation models that are conceptually similar to existing ones but better suited to the vertex-centric programming paradigm, and to design distributed algorithms for these models that can mitigate performance bottlenecks and provide high scalability. The key objectives are to explore how well graph simulation models fit into the vertex-centric distributed processing paradigm, identify major bottlenecks, and develop new simulation models and algorithms that are competitive and scalable.|Graph pattern matching,Distributed algorithms,Graph simulation,Big data,Vertex centric programming,BSP model,Scalability,Pattern matching models,Subgraph isomorphism,Graph processing frameworks
e9a7161d-1181-5d18-ac60-7c6474b08b8c|2019-12-03T07:43:19|2019|12|FastSV: A Distributed-Memory Connected Component Algorithmwith Fast Convergence|||||||The problem definition addressed in this research is the efficient computation of connected components (CC) in massive graphs, which is a fundamental problem in graph processing. The context that makes this problem important is the increasing scale and complexity of modern graph datasets, which has led to a significant gap between the capabilities of existing algorithms and the needs of real-world applications. The key objective of this research is to develop a scalable and efficient algorithm for computing CC in massive graphs, with a focus on parallelization and optimization techniques to minimize computational time and resources. The authors aim to address the limitations of existing algorithms, such as the SV algorithm, which are not designed to handle massive graphs and are often slow and inefficient. Specifically, the authors set out to achieve the following goals: * Develop a parallel algorithm that can efficiently compute CC in massive graphs * Optimize the algorithm to minimize computational time and resources * Improve the scalability of the algorithm to handle large-scale graph datasets * Evaluate the performance of the algorithm on real-world graph datasets. Overall, the problem definition addressed in this research is critical for many applications, including social network analysis, web graph analysis, and data mining, where efficient CC computation is essential for extracting insights and knowledge from massive graph datasets.|Graph,Algorithm,Connected,Components,Parallel,Scalable,Distributed,Memory,GraphBLAS,SV
7d813974-b532-5e8e-9958-f16d014b7394|2022-01-07T04:49:15|2022|1|Distributed Triangle Approximately Counting Algorithms in Simple Graph Stream|||||||The problem definition addressed in this research is the distributed triangle counting in a graph stream. The context that makes this problem important is the increasing scale and complexity of graph data, which necessitates efficient and scalable algorithms for processing and analyzing graph streams. Graph streams are sequences of edges that arrive continuously, and triangle counting is a fundamental graph analysis task that has numerous applications in social network analysis, recommendation systems, and anomaly detection. The key objectives or goals the authors set to address this problem are: 1. **Accuracy**: To estimate the global triangle count and local triangle counts in the graph stream with high accuracy. 2. **Efficiency**: To design a distributed algorithm that can process the graph stream in real-time, with low communication overhead and efficient use of computational resources. 3. **Scalability**: To develop an algorithm that can handle large-scale graph streams and scale to a large number of workers. To achieve these objectives, the authors propose a distributed triangle counting algorithm that uses a novel edge distribution strategy to distribute the graph stream across multiple workers, and a reservoir sampling-based approach to estimate the triangle counts in each worker. The algorithm aims to provide a trade-off between accuracy, efficiency, and scalability, making it suitable for real-world graph stream processing applications.|Federated,Learning,Graph,Neural,Networks,Distributed,Edge,Aggregation,Decentralized,Variance
1b09fe6d-87f2-56b5-8e62-592071956b7e|2017-11-07T07:45:31|2017|11|GraphD: Distributed Vertex-Centric Graph Processing Beyond the Memory Limit|||||||The problem definition addressed in this research is the scalability and memory limitations of distributed graph processing systems, particularly in the context of large-scale graph computations. The background that makes this problem important is the increasing size and complexity of graph-structured data, which is becoming a bottleneck for many applications, including social network analysis, web graph analysis, and knowledge graph processing. The key objectives or goals the authors set to address this problem are: To develop a distributed graph processing system that can handle large-scale graphs without running out of memory. To minimize the memory requirements of the system while maintaining its scalability and performance. To enable the system to process graphs that are too large to fit in memory, by leveraging disk storage and efficient data streaming techniques. Overall, the authors aim to design a system that can efficiently process large-scale graphs in a distributed manner, without being limited by memory constraints, and thereby enable a wide range of applications that rely on graph processing.|GraphD,Out-of-core,Pregel,Graph,Vertex-centric,Distributed,Memory,Computation,Processing,Scalability
3903e312-7d07-539a-b005-7ec5cecb18f7|2017-01-17T07:17:07|2017|1|Path-based holistic detection plan for multiple patterns in distributed graph frameworks|Jun Gao||||||Here is a concise summary of the problem definition addressed in this research: Context and Background: The increasing need for multiple pattern detection over large graphs is observed in various applications, such as disease detection in gene regulatory networks and program analysis. Handling these graph analysis tasks one by one results in resource waste and inefficiency. Problem Definition: The problem addressed in this research is to efficiently evaluate multiple graph pattern queries over a large graph in a distributed environment. The queries are represented as patterns, and the goal is to detect these patterns in the graph. Key Objectives: The authors aim to design a holistic evaluation plan that can efficiently evaluate multiple graph pattern queries in a single pass, minimizing redundant computation and communication. The plan should be suitable for distributed environments, allowing each graph vertex to take actions independently. The authors also aim to optimize the plan by capturing and reusing shared subparts among different queries. Overall, the research addresses the important problem of efficient multiple pattern detection in large graphs, which has significant implications for various applications.|Pattern detection,Multiple queries,Holistic evaluation plan,Distributed graph framework,Optimization,Graph pattern query,Evaluation plan,Parallelization,Graph analysis,Query optimization
a18624ce-3d32-507f-8a96-4cd35c6cdc94|2019-01-25T03:09:38+00:00|2019|1|DISTRIBUTED COLORING OF GRAPHSWITH AN OPTIMAL NUMBER OF COLORS|||||||The problem addressed in this research is the distributed coloring of graphs with an optimal number of colors. The context that makes this problem important is the need for efficient algorithms in distributed computing systems, where a graph represents a network of processors or computers that need to communicate with each other. In this setting, graph coloring is a fundamental problem that has many applications, such as scheduling, resource allocation, and network optimization. The key objective of this research is to develop a distributed algorithm that can color a graph with the minimum number of colors, which is known as the chromatic number of the graph. The authors aim to achieve this goal in a limited number of communication rounds, which is a critical factor in distributed computing systems. Specifically, the authors focus on graphs with a chromatic number close to the maximum degree of the graph, which is a challenging scenario. They set out to design a distributed algorithm that can color such graphs with an optimal number of colors, while minimizing the number of communication rounds required to achieve this goal. The authors' approach involves combining techniques from distributed computing, probability, and graph theory to develop an efficient algorithm that can solve this problem in a scalable and efficient manner.|Distributed,Graph,Coloring,Algorithm,Randomized,Maximum,Degree,Chromatic,Number,Complexity
a12cb4ac-0dfb-5f9b-9f11-9a0e881949ae|2022-09-01T02:10:42+00:00|2022|9|Improved Distributed-memory Triangle Counting by Exploiting the Graph Structure|||||||The problem definition addressed in this research is the efficient distributed memory triangle counting in large-scale graphs. The context that makes this problem important is the increasing size and complexity of graph-structured data, which is becoming a bottleneck in various applications, such as social network analysis, web graph analysis, and bioinformatics. The authors note that traditional serial algorithms are no longer feasible for processing these massive graphs, and distributed memory algorithms are necessary to scale up the computation. The key objective of this research is to develop a scalable and efficient distributed memory algorithm for triangle counting that can handle massive graphs with billions of edges. The authors aim to achieve strong scalability, which means that the algorithm should be able to efficiently utilize an increasing number of processing nodes to solve larger problems. Additionally, the authors aim to minimize memory usage and synchronization overhead, which are critical factors in achieving scalability. Overall, the problem definition is important because it addresses a critical bottleneck in graph processing, and the authors' objectives are focused on developing a scalable and efficient solution that can handle massive graphs.|Distributed,Memory,Graph,Triangle,Counting,Edge,Query,Bloom,Filter,Optimization
c66cb2fe-7102-5fac-af1f-56e6c79c3857|2018-02-13T02:14:30+00:00|2018|2|Distributed Evaluation of Subgraph Queries UsingWorst-case Optimal Low-Memory Dataﬂows|||||||The problem addressed in this research is the evaluation of subgraph queries in a distributed setting, specifically in the context of graph processing systems. The context that makes this problem important is the increasing need for efficient and scalable graph processing systems to support various applications, such as social network analysis, recommendation systems, and web search. The authors aim to design distributed algorithms that can efficiently evaluate subgraph queries while minimizing communication and computation costs. The key objectives of this research are to develop algorithms that are worst-case optimal in terms of computation and communication costs, require memory that is linear in the size of the input graphs, and can balance the workload of machines in the cluster to achieve scalability and efficiency.|Subgraph,Queries,Distributed,Evaluation,Worst-case,Optimal,Join,Algorithms,Graphs,Parallelism
7dd1957b-503b-5fa8-80f4-1e1b6d4c6e47|2014-01-10T07:21:58|2014|1|Optimizing Graph Algorithms on Pregel-like Systems∗|||||||The problem definition addressed in this research revolves around optimizing graph processing algorithms in distributed systems, specifically in the context of large-scale graph processing. The background that makes this problem important is the increasing need to process massive graphs in various domains, such as social networks, web graphs, and biological networks, which poses significant challenges in terms of scalability, performance, and communication costs. The key objectives or goals the authors set to address this problem are: To develop optimization techniques that can reduce the communication costs and improve the performance of graph processing algorithms in distributed systems. To design algorithms that can efficiently process large-scale graphs, minimizing the number of supersteps and communication rounds required. To explore novel approaches that can take advantage of the distributed nature of the graph data, reducing the computational overhead and improving the overall efficiency of graph processing. In essence, the authors aim to develop innovative solutions that can efficiently process massive graphs in distributed systems, overcoming the limitations of traditional graph processing algorithms and enabling faster, more scalable, and more efficient graph analysis.|Graph algorithms,Pregel-like systems,Optimization techniques,Graph processing,Distributed systems,Scalability,Efficiency,Graph clustering,Minimum spanning forest,Strongly connected components
19aa7194-2413-59f6-b5a9-539316786f6f|2017-01-24T02:39:29|2017|1|GA-LP: A genetic algorithm based on Label Propagation to detect communities in directed networks|Rodrigo Francisquini||||||The problem definition addressed in this research is the community detection problem in directed networks. The context that makes this problem important is the increasing complexity of real-world networks, such as social networks, biological networks, and web graphs, which are often represented as directed graphs. The ability to identify communities or clusters in these networks is crucial for understanding their structure, behavior, and evolution.  The key objective of this research is to develop an efficient and effective algorithm for community detection in directed networks. The authors aim to address the limitations of existing methods, which are often designed for undirected networks or are computationally expensive. Specifically, the authors seek to develop a genetic algorithm (GA) that can optimize the modularity measure, a widely used metric for evaluating community structure, in directed networks.  The authors' goal is to design a GA that can efficiently search for high-quality community partitions in large-scale directed networks, while also considering the unique challenges posed by directed networks, such as link reciprocity and asymmetry. By achieving this goal, the authors hope to provide a valuable tool for researchers and practitioners working with directed networks, enabling them to gain insights into the structure and behavior of these complex systems.|Community detection,Directed networks,Genetic algorithm,Label Propagation,Modularity,Network analysis,Clustering,Graph optimization,Heuristics,Complex networks
4a974772-4ef2-5213-abe8-1e33b27db5bc|2014-09-27T05:19:53|2014|9|Large-scale distributed graph computing systems|||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph data in distributed systems. The context that makes this problem important is the increasing prevalence of massive graphs in various domains, such as social networks, web graphs, and biological networks, which pose significant challenges to traditional computing architectures. The sheer size and complexity of these graphs necessitate the development of scalable and efficient distributed graph processing systems. The key objectives or goals the authors set to address this problem are: 1. To design and implement distributed graph processing systems that can efficiently handle large-scale graphs. 2. To identify and evaluate the performance of various techniques and systems for distributed graph processing. 3. To analyze the strengths and limitations of each system and technique, and provide insights for improving their performance. Overall, the authors aim to contribute to the development of efficient and scalable distributed graph processing systems that can effectively handle the challenges posed by massive graphs, and provide a comprehensive understanding of the techniques and systems used in this domain.|Graph,Distributed,Computing,Systems,Performance,Evaluation,Vertex-centric,Algorithms,Scalability,Optimization
c5b84731-6c84-5e05-b620-6669e49b1e87|2019-11-29T20:30:35+00:00|2019|11|Approximate Pattern Matching in Massive Graphs with Precision and Recall Guarantees|Tahsin Reza ,Matei Ripeanu,Geoffrey Sanders,Roger Pearce||||||The problem addressed in this research is the efficient identification of approximate matches in large graphs, specifically in the context of exact pattern matching. The background that makes this problem important is the increasing need for graph-based data analysis in various domains, such as social networks, biology, and chemistry, where exact pattern matching is a fundamental operation. However, exact pattern matching is computationally intractable, making it impractical for large graphs. The key objectives or goals the authors set to address this problem are: To develop a solution that can efficiently identify approximate matches in large graphs, with a focus on reducing the computational complexity of exact pattern matching. To achieve full precision and recall, ensuring that all matching vertices and edges are identified without any false positives. To provide a solution that can handle a range of problem scenarios, including those with strict definitions of match similarity and those requiring full precision and recall. Overall, the authors aim to develop an efficient and effective solution for approximate pattern matching in large graphs, which can enable various applications in graph-based data analysis.|Graph,Matching,Approximate,Template,Edit,Distance,Search,Subgraph,Pattern,Algorithm
564e118b-f337-59ff-8713-5ddeb749c1c6|2019-02-27T09:01:53|2019|2|SPFC: An Effective Optimization for Vertex-Centric Graph Processing Systems|||||||The problem definition addressed in this research revolves around the inefficiency of graph processing systems, particularly in large-scale graph processing. The context that makes this problem important is the increasing demand for efficient graph processing in various applications, such as social network analysis, recommendation systems, and data mining. However, traditional graph processing systems are often bottlenecked by the high overhead of message passing, leading to slow computation times and high memory usage. The key objective of this research is to develop an efficient graph processing system that can reduce the overhead of message passing while maintaining acceptable computation times. Specifically, the authors aim to: 1. Reduce the total amount of message traffic in graph processing systems. 2. Develop a scheduling policy that prioritizes message passing based on the importance of vertices in the graph. 3. Estimate the optimal threshold value for message passing to balance computation time and accuracy. By addressing these objectives, the authors aim to develop a more efficient and scalable graph processing system that can handle large-scale graph processing tasks with reduced computation times and memory usage.|Graph,Processing,Large-scale,Distributed,Message,Passing,Synchronization,Approximation,Computation,Efficiency
45818540-6674-51ce-bc2f-8adaf6f0630f|2018-10-03T01:16:26|2018|10|A Vertex-Centric Graph Simulation Algorithm for Large Graphs|Jingdong Li||||||The problem definition addressed in this research revolves around the efficient querying of large-scale data graphs, which is a crucial task in various applications such as social network analysis, recommendation systems, and knowledge graphs. The context that makes this problem important is the rapid growth of data graphs, which has led to a significant increase in query complexity and processing time. The key objective of this research is to develop an efficient algorithm for querying large-scale data graphs, specifically focusing on cyclic query graphs. The authors aim to address the problem of high query complexity and processing time by proposing a novel algorithm that can efficiently match cyclic query graphs with large-scale data graphs. The specific goals of this research include: 1. Developing an efficient algorithm for querying large-scale data graphs with cyclic query graphs. 2. Reducing the query complexity and processing time for large-scale data graphs. 3. Improving the scalability and efficiency of data graph querying algorithms. Overall, the authors aim to provide a solution that can efficiently query large-scale data graphs, enabling faster and more accurate analysis of complex data structures.|Graph simulation,Vertex centric,Graph pattern matching,Large graphs,Distributed computation,Optimization,Query graph,Data graph,Subgraph isomorphism,BSP (Bulk Synchronous Parallel)
ed8055e1-e343-54e5-b6a9-56d058dad194|2018-01-22T12:09:24|2018|1|Parallel Algorithm for Incremental Betweenness Centrality on Large Graphs|||||||The problem definition addressed in this research is the efficient computation of betweenness centrality in dynamic graphs, which is a fundamental metric in graph analysis. The context that makes this problem important is the increasing size and complexity of real-world graphs, such as social networks, web graphs, and biological networks, which require efficient algorithms to analyze and understand their structure and behavior. The key objective of this research is to develop an incremental algorithm, called iCENTRAL, that can efficiently update betweenness centrality values in a graph after an edge insertion or deletion, without recomputing the entire graph from scratch. The authors aim to achieve this goal by exploiting the structural properties of biconnected components in graphs, which allows them to focus on the affected components and reduce the computational complexity of the algorithm. In summary, the problem definition is to develop an efficient algorithm for updating betweenness centrality in dynamic graphs, which is crucial for analyzing and understanding complex networks. The key objective is to design an incremental algorithm that can update centrality values quickly and accurately, without requiring a full recomputation of the graph.|Betweenness Centrality,Incremental Algorithm,Evolving Graphs,Graph Analysis,Dynamic Graphs,Centrality Metrics,Graph Updates,Scalability,Algorithmic Efficiency,Graph Processing
9f3cf554-3c41-5bf7-95bb-8c4bc4a4ea83|2022-03-03T04:32:49|2022|3|ANCHU RAJENDRAN and V. KRISHNA NANDIVADA, Indian Institute of Technology Madras|||||||The problem definition addressed in this research revolves around translating Green Marl, a high-level parallel programming language, to MPI (Message Passing Interface), a low-level parallel programming model. The context that makes this problem important is the need for efficient and scalable parallel programming, which is crucial in various domains such as scientific computing, data analytics, and machine learning. Green Marl, with its high-level abstractions, provides an easier way to write parallel programs, but it lacks a direct implementation on distributed memory architectures. On the other hand, MPI is a widely-used, low-level parallel programming model that provides direct access to distributed memory architectures, but it requires manual memory management and synchronization, making it error-prone and difficult to use. The key objective of this research is to develop a translation framework that can automatically convert Green Marl programs to MPI, thereby bridging the gap between the two parallel programming models. The authors aim to achieve this goal by addressing the challenges that arise from the differences in the design philosophies of Green Marl and MPI, such as handling parallelism, synchronization, and memory management. By developing such a translation framework, the authors hope to enable the execution of Green Marl programs on distributed memory architectures, making parallel programming more accessible and efficient.|Graph,Compilation,Optimization,Distributed,Graph Algorithms,Code Generation,GPU,Parallelization,Performance,Scalability
6672f549-c916-5632-bcf1-7945725dbfe9|2020-04-07T00:44:06+00:00|2020|4|Peregrine: A Pattern-Aware Graph Mining System |Kasra Jamshidi,Rakesh Mahadasa,Keval Vora||||||The problem addressed in this research is the inefficiency of graph mining systems in exploring and analyzing subgraph structures within large graphs. Graph mining is a crucial task in various domains, including bioinformatics, computer vision, and social network analysis, as it helps extract structural properties of graphs. However, existing graph mining systems are not pattern-aware, leading to unnecessary explorations, high computation demands, and large memory requirements. The authors aim to develop a pattern-aware graph mining system that can efficiently mine relevant subgraphs by analyzing the patterns to be mined and generating an exploration plan to guide the search. The key objectives are to reduce unnecessary computations, improve performance, and enable users to easily express complex mining queries.|Graph,Mining,Pattern,Exploration,System,Subgraph,Matches,Canonicality,Isomorphism,PEREGRINE
7336d57f-43a5-5f6b-a5bc-57b290145cda|2023-08-21T01:52:39+00:00|2023|8|Maximum Length-Constrained Flows and Disjoint Paths:Distributed, Deterministic and Fast|||||||The problem addressed in this research is the optimization of length-constrained flows in networks, which is a fundamental problem in network optimization. The context that makes this problem important is that it has numerous applications in distributed computing, network coding, and communication networks, where efficient routing and flow management are crucial. In particular, the authors highlight that the maximum length-constrained flow gives the minimum makespan of multiple unicasts in a network, even when network coding is allowed. The key objective of this research is to develop efficient algorithms for computing length-constrained flows and moving cuts in networks. The authors aim to achieve this goal by proposing new technical contributions, including batched multiplicative weights, a new framework for length-constrained flows, and a novel approach for moving cuts. The ultimate goal is to provide a more efficient and scalable solution for optimizing length-constrained flows in networks, which can have a significant impact on the performance of distributed systems and communication networks.|Distributed algorithms,Network optimization,Congest model,Cycle cover,Disjoint paths,Flow decomposition,Graph algorithms,Hop constraint,Parallel algorithms,Approximation ratio
c5ab0417-7445-501a-8756-efac81019599|2015-09-05T02:32:09|2015|9|Arabesque: A System for Distributed Graph Mining|||||||The problem definition addressed in this research revolves around scalable graph mining, which involves enumerating subgraphs that satisfy certain interestingness criteria in large graphs. The context that makes this problem important is the ubiquity of graph data in various fields, such as the Web, advertising, and biology, and the increasing need for graph analytics. However, designing scalable graph mining algorithms is challenging, especially when dealing with large graphs. The key objectives or goals the authors set to address this problem are: 1. To develop a system that can automatically and systematically explore all embeddings of a pattern in a graph, which is essential for graph mining. 2. To design a distributed graph mining system that can scale to large graphs, overcoming the limitations of centralized approaches. 3. To provide a high-level filter process computational model that allows users to specify their own interestingness criteria and algorithms for graph mining. Overall, the authors aim to create a system that can efficiently and effectively mine large graphs, enabling users to extract valuable insights and patterns from graph data.|Graph mining,Arabesque,Distributed graph processing,Subgraph enumeration,Graph analytics,Scalability,Pattern matching,Graph computation,Graph algorithms,Data mining
8edbdcdf-4274-5d2e-b32b-d8c2b7f8522d|2016-06-09T11:21:55|2016|6|Distributed k–core Decomposition andMaintenance in Large Dynamic Graphs|||||||The problem definition addressed in this research is the distributed k-core decomposition and maintenance in large dynamic graphs. The context that makes this problem important is the increasing size and dynamism of graph data, which makes it difficult to process and analyze using traditional centralized methods. The authors aim to address this problem by developing distributed algorithms that can efficiently compute and maintain the k-core decomposition of large dynamic graphs, which is crucial for understanding and analyzing the structure and behavior of complex networks. The key objectives of this research are to propose a distributed and streaming k-core decomposition algorithm for very large graphs, to develop a maintenance strategy that deals with incremental changes on the graph, and to experimentally evaluate the performance of the proposed approach on both real and synthetic datasets.|Distributed,k-core,decomposition,maintenance,dynamic,graphs,large,incremental,algorithm,streaming
6b801da8-897e-5b38-b3c0-62b4ab6077e1|2017-05-17T04:49:11|2017|5|Distributively Computing Random Walk Betweenness Centrality in Linear Time|Qiang-Sheng Hua,Ming Ai,Hai Jin,Dongxiao Yu,Xuanhua Shi||||||The problem definition addressed in this research is the distributed computation of random walk betweenness centrality in networks. The context that makes this problem important is the increasing need to analyze and understand the behavior of complex networks, such as social networks, transportation networks, and biological networks. Betweenness centrality is a key measure of a node's influence over the spread of information in a network, and random walk betweenness centrality is a more realistic and nuanced measure that considers not only shortest paths but also other paths. The key objective of this research is to develop a distributed algorithm that can efficiently compute random walk betweenness centrality in large-scale networks. The authors aim to design an algorithm that can achieve a linear time complexity, which is a significant improvement over existing centralized algorithms that have a time complexity of O(nm), where n is the number of nodes and m is the number of edges. The authors' goal is to enable fast and scalable computation of random walk betweenness centrality in distributed networks, which is essential for many applications, such as identifying influential nodes, predicting information diffusion, and optimizing network topology.|Random walk betweenness,Distributed algorithms,Centrality measures,Network analysis,Betweenness centrality,Random walks,Graph algorithms,Distributed computing,Network flow,Pagerank
fde5343b-4323-5a25-9876-0da8bc5b3cde|2012-12-02T05:16:34|2012|12|Asynchronous Large-Scale Graph Processing Made Easy|||||||The problem definition addressed in this research revolves around the efficient processing of graph-based algorithms, particularly Belief Propagation (BP), in large-scale distributed systems. The context that makes this problem important is the increasing need for scalable and efficient graph processing in various applications, such as machine learning, computer vision, and social network analysis. The key objective of this research is to develop an efficient and scalable approach to process graph-based algorithms like BP, which is a widely used algorithm for inference in graphical models. The authors aim to achieve this by relaxing the constraints imposed by the Bulk Synchronous Parallel (BSP) model, which is a common parallel processing model used in distributed systems. The BSP model can lead to inefficiencies in graph processing due to its synchronous nature, which can result in idle processors and slow convergence. The authors' goal is to design an asynchronous graph processing approach that can accelerate convergence and improve performance while maintaining simplicity and scalability. They aim to achieve this by allowing vertices to process messages as soon as they are received, rather than waiting for a global synchronization point, and by intelligently ordering the processing sequence of vertices to minimize communication overhead. Overall, the research aims to develop a more efficient and scalable approach to graph processing that can handle large-scale graphs and accelerate the convergence of graph-based algorithms like BP.|Graph,Processing,Iterative,Belief,Propagation,Asynchronous,Algorithm,Parallel,Computation,Convergence
94c4754c-9b1f-5b1b-81d2-b6d4e0ab30a0|2009-10-28T04:12:12|2009|10|A Distributed Algorithm to Enumerate All Maximal Cliques in MapReduce|||||||The problem addressed in this research is the enumeration of all maximal cliques in a graph, which is a fundamental problem in graph mining. The context that makes this problem important is the increasing volume and complexity of real-world data, which makes it challenging to process and analyze large graphs using traditional single-chip computational capacity. The authors aim to propose a distributed algorithm to enumerate all maximal cliques in a graph, which can handle large-scale graphs and achieve better performance and scalability. The key objectives of this research are to develop an efficient and scalable algorithm that can handle large graphs, and to evaluate the performance of the proposed algorithm using real-world datasets.|maximal cliques,MapReduce,graph,algorithm,enumeration,distributed,clique,subgraphs,parallel,mining
ebb21d36-ade1-557f-b334-5ecefe57e4c0|2011-03-30T01:23:44+00:00|2011|3|Distributed k-Core Decomposition|||||||The problem addressed in this research is the computation of the k-core decomposition of a network in a distributed setting. The context that makes this problem important is the increasing size and complexity of real-world networks, which makes it challenging to analyze them using traditional centralized algorithms. The authors aim to develop a distributed algorithm that can efficiently compute the k-core decomposition of a network, enabling the analysis of large networks in a decentralized manner. The key objectives of this research are to propose a novel distributed algorithm for computing the k-core decomposition, to prove the correctness and efficiency of the algorithm, and to evaluate its performance on real-world networks.|k core decomposition,distributed algorithm,complex networks,graph analysis,centrality measures,network structure,node importance,graph visualization,distributed systems,network analysis
8b5ce57c-b966-5025-98aa-d95ecaf77bc9|2022-12-27T10:51:29|2022|12|ScaleG: A Distributed Disk-Based System for Vertex-Centric Graph Processing|||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph data in distributed computing environments. The context that makes this problem important is the rapid growth of graph-structured data in various domains, such as social networks, web graphs, and biological networks, which poses significant challenges in terms of storage, processing, and analysis. The key objectives or goals the authors set to address this problem are: Scalability: To develop a system that can efficiently process large-scale graph data in a distributed computing environment, handling massive graphs with billions of vertices and edges. Efficiency: To minimize the communication cost and memory usage during graph processing, ensuring fast processing times and reducing the overall computational overhead. Flexibility: To design a system that can support a wide range of graph algorithms and applications, without being limited to specific use cases or graph types. To achieve these objectives, the authors propose a novel distributed graph processing system, SCALEG, which leverages a combination of in-memory processing, vertex-centric programming, and optimized communication strategies to efficiently process large-scale graph data.|Graph,Distributed,System,Vertex,Centric,Processing,Scalability,Disk,Memory,Optimization
852cfead-950f-5ce7-8e9f-7b6b53f1b596|2017-05-24T03:05:05|2017|5|Scalable Single Source Shortest Path Algorithms for Massively Parallel Systems|||||||The problem definition addressed in this research is the Single-Source Shortest Paths (SSSP) problem in large-scale graphs, which is a fundamental problem in graph algorithms. The context that makes this problem important is the increasing size and complexity of real-world graphs, such as social networks, web graphs, and traffic networks, which require efficient algorithms to compute shortest paths. The key objective of this research is to develop an efficient parallel algorithm for solving the SSSP problem in large-scale graphs, with a focus on minimizing the number of relaxations, reducing communication overhead, and achieving good load balancing. The authors aim to overcome the limitations of existing algorithms, such as Dijkstra's algorithm and the Bellman-Ford algorithm, which are not scalable for large graphs. Specifically, the authors set out to achieve the following goals: * Develop an algorithm that can efficiently handle large-scale graphs with billions of vertices and edges. * Minimize the number of relaxations, which is a major contributor to the processing time and communication overhead. * Reduce the communication overhead by avoiding redundant relaxations and minimizing the number of phases. * Achieve good load balancing to ensure that the algorithm can be efficiently parallelized. Overall, the authors aim to develop a scalable and efficient algorithm for solving the SSSP problem in large-scale graphs, which is essential for many applications in computer science and other fields.|SSSP (Single Source Shortest Path),Graph,Algorithm,Scalability,Parallelism,Performance,BFS (Breadth First Search),Graph 500,Dijkstra,Relaxation
cec09e6a-a58c-5bd8-a95d-6de7fc18b91e|2005-06-13T09:47:15|2005|6|Finding strongly connected components in distributed graphs|||||||The problem addressed in this research is the efficient identification of strongly connected components (SCCs) in distributed graphs, particularly in the context of radiation transport applications. The background that makes this problem important is the need for scalable and efficient algorithms to solve large-scale graph problems, which are increasingly common in various scientific domains. In radiation transport, SCCs play a crucial role in solving problems using sweeping methods, and their efficient identification is essential for performance. The key objective of this research is to develop a parallel algorithm that can efficiently identify SCCs in distributed graphs, with a focus on minimizing runtime and improving scalability. The authors aim to achieve this by proposing a modified version of the DCSC algorithm, which is designed to reduce the size of the problem before invoking the DCSC algorithm, thereby improving overall performance. The goal is to develop an algorithm that can efficiently handle large graphs with many SCCs, which is critical for solving complex radiation transport problems.|Parallel,Strongly,Connected,Components,Graph,Algorithm,Radiation,Transport,Sweep,Dependence
149f1916-5e75-548b-8e23-fcaab82ef748|2017-06-08T09:10:20|2017|6|Distributed MST and Routing in Almost Mixing Time|Mohsen Ghaffari,Fabian Kuhn,Hsin-Hao Su||||||The problem addressed in this research is the distributed computation of a minimum spanning tree (MST) in a network graph. The context that makes this problem important is the need for efficient distributed algorithms in various applications, such as overlay networks, where the topology is dynamic and changes with time. The authors aim to develop a distributed algorithm that can compute an MST in a time complexity close to the network's mixing time, which is a measure of how quickly information can spread through the network. The key objective is to achieve a time complexity that is sublinear in the number of nodes in the network, specifically mix 2O log n log log n rounds, where mix is the mixing time of the network. This is important because previous algorithms had a time complexity of O(n) or higher, which is not efficient for large networks. The authors also aim to develop a hierarchical routing structure that can be used to solve other graph problems efficiently.|Distributed,Graph,Algorithm,Minimum,Spanning,Tree,Routing,Random,Walk,Mixing
be8bf615-6f44-563f-b05a-764179768494|2019-05-16T18:22:20+00:00|2019|5|LACC: A Linear-Algebraic Algorithm for Finding Connected Components in Distributed Memory|||||||The problem addressed in this research is the efficient computation of connected components in large-scale graphs, which is a fundamental problem in graph algorithms. The context that makes this problem important is the increasing size and complexity of graphs in various domains, such as social networks, web graphs, and biological networks, which necessitates the development of scalable and efficient algorithms to process them. The key objective of this research is to design and implement a parallel algorithm for computing connected components that can efficiently utilize modern distributed-memory architectures. Specifically, the authors aim to develop an algorithm that can achieve good load balance, minimize communication overhead, and scale well with the number of processors. The authors focus on the Awerbuch-Shiloach (AS) algorithm, which is a well-known algorithm for computing connected components, and explore its implementation using the GraphBLAS matrix algebra framework. The goal is to develop a scalable and efficient parallel algorithm that can handle large-scale graphs and provide a competitive solution to existing methods.|Connected,Components,Graph,Algorithm,Distributed,Memory,Linear,Algebraic,Parallel,GraphBLAS
d8bbaaf0-1e77-5a1f-8c64-e1f242d8ec51|2021-11-10T14:12:58+00:00|2021|11|Scalable Top-k Query on Information Networkswith Hierarchical Inheritance Relations|||||||The problem definition addressed in this research is the graph query problem with hierarchical inheritance relations in large-scale heterogeneous information networks. The context that makes this problem important is the increasing complexity and heterogeneity of large networks, which makes it challenging to design e ective and e cient algorithms for querying these networks. The authors aim to improve the quality of query answers by considering the power of hierarchical inheritances, where a subclass inherits properties and constraints of its parents. The key objectives of this research are to model graph query with a new matching score function that incorporates hierarchical inheritance relations and to propose a bound-based technique for efficient query processing. The authors also aim to develop an algorithm that can e ectively and efficiently select the top matching candidate sets from star query results and provide accurate query answers.|Hierarchical,Inheritance,Relations,Information,Networks,Query,Graph,Heterogeneous,Matching,Algorithm
f048a647-c4d7-585b-a735-753dce7531df|2015-05-18T18:05:46|2015|5|A Fast and Scalable Graph Coloring Algorithmfor Multi-core and Many-core Architectures|gr||||||The problem addressed in this research is the need for a fast and scalable graph coloring algorithm for multi-core and many-core architectures. The context is that many modern applications operate on irregular data structures, such as graphs, and graph coloring is an important preprocessing step for ensuring safe parallel execution and enforcing neighborhood heuristics. The key objective is to develop a coloring algorithm that is not only fast and scalable but also uses as few colors as possible, as the number of colors directly affects the exposed parallelism of an irregular algorithm. The authors aim to improve upon existing algorithms, such as the greedy coloring algorithm, to achieve better performance and scalability on modern multi-core and many-core hardware.|Graph Coloring,Greedy Coloring,First Fit Coloring,Irregular Data,Parallel Graph Algorithms,Shared Memory Parallelism,Optimistic Execution,Many core Architectures,Intel Xeon Phi,Multicore Systems
7464cc70-abca-5122-bdf8-5e9e47d3bd41|2018-10-31T18:08:24|2018|10|Optimal Distributed Coloring Algorithms for Planar Graphs in the LOCAL model|Shiri Chechik,Doron Mukhtar||||||The problem definition addressed in this research is the k-coloring problem in the LOCAL model of distributed computing. The context that makes this problem important is the need for efficient algorithms in distributed systems, where communication networks are represented by graphs, and each vertex corresponds to a processor, and each edge to a communication line. The k-coloring problem is a fundamental problem in graph theory, which asks to assign colors (integers from 1 to k) to the vertices of a graph such that no two adjacent vertices have the same color. The key objective of this research is to develop an efficient algorithm that can solve the k-coloring problem in the LOCAL model, with a focus on minimizing the number of communication rounds required. The authors aim to achieve this goal by designing an algorithm that can properly k-color a triangle-free planar graph in O(log n) communication rounds, where n is the number of vertices in the graph. This is an important problem because it has applications in various areas, such as scheduling, resource allocation, and network optimization, and an efficient solution can lead to significant improvements in the performance of distributed systems.|Distributed,Coloring,Planar,Graphs,Algorithm,LOCAL,Model,Time,Optimal,Complexity
c0f44530-9ce9-5c90-8d10-4dec8fd277aa|2017-10-27T05:08:35|2017|10|Combining Vertex-Centric Graph Processing with|||||||The problem addressed in this research is the efficient evaluation of graph queries in a distributed setting. The context that makes this problem important is the increasing use of large-scale graph data in various applications, such as social networks, knowledge graphs, and recommender systems. The evaluation of graph queries is a critical operation in these applications, but it is challenging due to the massive size of the graphs and the complexity of the queries. The key objective of this research is to develop an efficient query evaluation algorithm that can scale to large graphs and minimize the communication overhead between distributed nodes. The authors aim to achieve this by optimizing the query plan to reduce the number of messages exchanged between nodes and by leveraging the partitioning of the graph data to minimize the communication overhead. In particular, the authors focus on evaluating graph queries that involve multiple edges and vertices, which is a common scenario in many applications. They propose a novel query optimization algorithm that takes into account the selectivity of the edges and the partitioning of the graph data to generate an efficient query plan. The goal is to minimize the total cost of the query evaluation, which includes the cost of message exchange, computation, and memory access. Overall, the problem addressed in this research is critical in enabling efficient and scalable graph query evaluation in distributed settings, which is essential for many applications that rely on large-scale graph data.|Graph,RDF,SPARQL,Query,Algorithm,Vertex,Centrality,PageRank,Exploration,Framework
8ee4127e-01b6-56f5-a020-eb6deeeb15b9|2019-08-20T11:44:35|2019|8|Improving Efficiency of Parallel Vertex-Centric Algorithms for Irregular Graphs|||||||The problem definition addressed in this research is the efficient parallelization of graph algorithms, specifically the PageRank algorithm, on modern multi-core architectures. The context that makes this problem important is the increasing scale and complexity of graph-structured data, which is becoming a bottleneck in various applications, such as social network analysis, web search, and recommendation systems. The authors note that traditional parallelization approaches are often limited by memory access patterns, leading to poor scalability and inefficient use of computing resources. The key objectives or goals the authors set to address this problem are: 1. To develop a parallelization strategy that can efficiently utilize the available computing resources, minimizing memory access overhead and maximizing parallelism. 2. To design a concurrent execution model that can effectively handle the irregular memory access patterns inherent in graph algorithms. 3. To achieve high performance and scalability on modern multi-core architectures, while ensuring the correctness and accuracy of the PageRank algorithm. Overall, the authors aim to develop a novel parallelization approach that can efficiently process large-scale graph-structured data, enabling faster and more accurate analysis and decision-making in various applications.|Graph,PageRank,Parallel,Memory,Propagation,Blocking,Algorithm,Irregular,Vertex,Efciency
257e68db-3eca-5c2b-a8dd-0670daf02e12|2023-12-18T16:50:57|2023|12|Distributed subgraph counting|||||||The problem definition addressed in this research is the efficient counting of induced subgraphs in large graphs, specifically focusing on local subgraph counting. The context that makes this problem important is the increasing significance of graph data in various domains, such as social networks, bioinformatics, and recommender systems, where characterizing high-order local structures is crucial. Local subgraph counting plays a key role in this characterization, enabling the computation of metrics that quantify node clustering and community structure. The key objective of this research is to develop an efficient approach for local subgraph counting, which involves counting the occurrences of a given pattern graph around every node in a large data graph. The authors aim to address the challenges of scalability, flexibility, and accuracy in existing approaches, which are limited by their ability to handle large graphs, various pattern graphs, and different orbit types. The authors' goal is to design a novel approach that can efficiently count induced subgraphs, support flexible pattern graphs, and handle different orbit types, ultimately enabling the analysis of complex graph structures in large-scale graph data.|Subgraph,Counting,Local,Graph,Isomorphism,Homomorphism,Query,Pattern,Matching,Enumeration
b6fd1cf3-f5aa-5a2a-81b5-7363ea2c44aa|2018-05-21T03:11:23|2018|5|A Parallel Complex Coloring Algorithm for Scheduling of Input-Queued Switches|||||||The problem definition addressed in this research revolves around the scheduling of packets in an Input-Queued (IQ) switch, a critical component in high-speed networks. The context that makes this problem important is the increasing demand for high-speed networks with high throughput and low latency, particularly in data centers and cloud computing environments. In such networks, IQ switches play a crucial role in forwarding packets efficiently. The key challenge in IQ switch scheduling is to determine the optimal connection pattern between inputs and outputs in each time slot, ensuring that packets are transmitted efficiently while minimizing congestion and packet loss. The authors formulate this problem as an edge coloring problem in a bipartite graph, where each edge represents a packet to be transmitted, and the goal is to find a coloring scheme that minimizes the number of colors (time slots) required. The authors' key objectives are to develop a scheduling algorithm that achieves: 1. **Optimality**: Minimizes the number of colors (time slots) required to transmit all packets. 2. **Parallelizability**: Enables fast computation of the scheduling algorithm in a distributed manner. 3. **Rearrangeability**: Allows for efficient updates to the scheduling algorithm when the traffic pattern changes. By addressing these objectives, the authors aim to develop a scheduling algorithm that can efficiently manage packet transmission in IQ switches, ultimately leading to improved network performance and reliability.|Scheduling,Bipartite graph,Edge coloring,Complex coloring,IQ switch,Frame-based,Parallel processing,Deadlocks,Stopping rule,Throughput
3208fad9-9041-5db1-a4ec-132b6d5f1f77|2023-04-11T21:05:50|2023|4|EGraph: Efficient Concurrent GPU-Based Dynamic Graph Processing|||||||The problem addressed in this research is the efficient processing of multiple Timing iterative Graph Processing (TGP) jobs on dynamic graphs, which is crucial in various applications such as social network analysis and graph-based machine learning. The context that makes this problem important is the rapid growth of dynamic graphs, which generates massive amounts of data, and the need to process this data in a timely manner to capture evolving properties. The key objectives or goals the authors set to address this problem are: To minimize the data access cost, which is a major bottleneck in TGP job processing, by reducing the frequency of data transfer between the CPU and GPU. To optimize the concurrent processing of multiple TGP jobs on the same dynamic graph, leveraging the spatial and temporal similarities between jobs to improve processing efficiency. To develop an efficient execution model that can balance the load between different jobs and ensure timely processing of graph snapshots. Overall, the authors aim to design an efficient solution that can process multiple TGP jobs on dynamic graphs in a timely and efficient manner, enabling applications to capture evolving graph properties and make timely decisions.|Dynamic Graph Processing,GPU-based,Concurrent,Timing Iterative Graph Processing (TGP),Efficient,Graph Algorithm,Dynamic Graph Analysis,Graph Partitions,Memory Optimization,Parallelism
2d60f894-2aa3-5060-950d-4e1f46cb5c4c|2015-12-14T07:33:59|2015|12|A Stable and Distributed Community Detection Algorithm Based on Maximal Cliques|||||||The problem definition addressed in this research is the community detection in large-scale networks. The context that makes this problem important is the increasing availability of large-scale network data from various sources, such as social media platforms, online forums, and government institutions. This data contains valuable information about user behavior, preferences, and relationships, which can be leveraged to identify communities or groups of users with similar characteristics. The key objective of this research is to develop an efficient and effective method for community detection in large-scale networks. The authors aim to address the limitations of existing methods, which are often computationally expensive, require prior knowledge of the number of communities, or are prone to oscillation. The authors' goal is to propose a novel approach that can handle large-scale networks, detect high-quality communities, and overcome the limitations of existing methods. Specifically, the authors focus on developing a distributed community detection algorithm that can efficiently process large-scale networks and detect communities by assembling maximal cliques. The authors' objective is to design an algorithm that can scale to large networks, detect communities with high modularity, and provide a good balance between computational efficiency and community quality.|Community Detection,Maximal Clique,Label Propagation Algorithm (LPA),Distributed Label Propagation Algorithm (DLPA),MapReduce,Graph Processing,Network Analysis,Clustering,Parallel Computing,Modularity
7d5b2c6b-f4a1-5d43-8c0b-d27a110664b7|2018-08-08T17:16:51|2018|8|Faster Distributed Shortest Path Approximationsvia Shortcuts|||||||The problem addressed in this research is the computation of approximate shortest path distances and transshipment flows in a distributed network. The context that makes this problem important is the need for efficient algorithms that can handle large-scale networks with varying topologies. The authors aim to develop algorithms that can adapt to the topology of the network, achieving significantly faster running times on many networks of interest. The key objectives are to design distributed algorithms that can approximate shortest path distances and transshipment flows with a tunable trade-off between running time and approximation ratio, and to achieve near-instance-optimal running times for many topologies of interest.|Distributed,Shortest,Path,Approximations,Shortcuts,Algorithms,Graph,Network,Topology,Optimization
6f31ca78-b485-52ba-ba3b-e750f8a19fc6|2019-05-14T05:19:30|2019|5|Vertex Coloring with Communication Constraints|||||||The problem addressed in this research is the allocation of time slots in a multi-channel Time Division Medium Access (TDMA) wireless network, where nodes have a single transceiver and can only receive messages from up to g neighbors simultaneously. The context that makes this problem important is the need for efficient and reliable communication in wireless networks, where collisions and conflicts can occur when multiple nodes try to broadcast simultaneously. The key objectives of this research are to: Develop a deterministic distributed algorithm that allocates time slots to nodes in the network, taking into account the constraints of the communication graph and the limited communication range of wireless communications. Minimize the number of time slots required to avoid conflicts and collisions, which is modeled as a frugal coloring problem in graph theory. Provide a robust solution that can tolerate conflicts and collisions, ensuring reliable communication in the network. Overall, the authors aim to provide an efficient and reliable solution for time slot allocation in multi-channel TDMA wireless networks, which is essential for enabling reliable communication in various applications, such as wireless sensor networks, ad-hoc networks, and IoT devices.|Frugal Coloring,Distributed Algorithm,Graph Coloring,Multi-Channel Networks,TDMA Slot Allocation,Collision-Free,Conflict-Free,Synchronous Systems,Message Passing,Vertex Coloring
7e54bbdf-88ce-53c0-8367-06094a5634fe|2021-01-11T14:56:54|2021|1|Efficient Distributed k-Clique Mining for Large Networks Using MapReduce|||||||The problem definition addressed in this research is the k-clique mining problem, which involves enumerating all cliques of size k in a large network. The context that makes this problem important is the increasing size and complexity of modern networks, such as social media and web graphs, which require efficient and scalable algorithms to analyze and extract valuable insights. The key objectives or goals of the authors are to: Develop an efficient algorithm for k-clique mining that can handle large networks. Design a parallel and distributed solution to take advantage of modern computing architectures. Improve the scalability and performance of k-clique mining algorithms to enable analysis of massive networks. The authors aim to address the limitations of existing solutions, which are often sequential, time-consuming, and not designed for large-scale networks. By developing a scalable and efficient k-clique mining algorithm, the authors hope to enable researchers and practitioners to extract valuable insights from large networks, such as identifying clusters, communities, and patterns.|k-clique mining,MapReduce algorithms,Distributed graph processing,Large network analysis,Clique enumeration,Parallel graph processing,Scalable algorithms,Network analysis,Graph mining,Data processing
cfaa7b1b-4126-524a-95f4-3c2822c16cee|2020-07-17T01:13:50+00:00|2020|7|Distributed Subgraph Enumeration viaBacktracking-based Framework|||||||The problem definition addressed in this research is subgraph enumeration, which involves finding all instances of a small pattern graph within a large data graph. This problem is important in various graph analytic applications, such as network motif mining, graphlet-based network comparison, and social network recommendation. The context that makes this problem important is the increasing size and complexity of real-world graphs, making it challenging to enumerate subgraph instances efficiently. The key objective of this research is to develop a novel framework for distributed subgraph enumeration that can efficiently handle large data graphs and support dynamic data graphs. The authors aim to achieve near real-time performance and reduce redundant computation as much as possible. They also propose the concept of incremental pattern graphs to support continuous subgraph enumeration in dynamic graphs.|Subgraph,Enumeration,Distributed,Backtracking,Framework,Continuous,Matching,Graph,Dynamic,Algorithm
66e0cb02-3b00-5cbc-b41f-6c5212805ea9|2019-05-30T18:27:13|2019|5|Optimal Distributed Covering Algorithms|||||||The problem addressed in this research is the Minimum Weight Hypergraph Vertex Cover (MWHVC) problem, which is a generalization of the Minimum Weight Vertex Cover (MWVC) problem in hypergraphs. The MWHVC problem is important because it is equivalent to the Minimum Weight Set Cover problem, which is a fundamental problem in computer science and operations research. The authors aim to develop a distributed approximation algorithm for the MWHVC problem in the congest model, with the objective of achieving a time-optimal deterministic algorithm that completes in O(log log log) rounds, which is optimal according to a known lower bound. The authors also aim to improve over previous results for the integral case and to show that integer covering programs can be reduced to the MWHVC problem in the distributed setting.|Distributed,Algorithm,Hypergraph,Vertex,Cover,Approximation,Primal,Dual,Congest,Model
d436e361-c342-5385-89ca-26ec4f7b2df3|2018-04-12T19:14:22|2018|4|Improved Distributed Algorithms for Exact Shortest Paths|||||||The problem addressed in this research is the Single Source Shortest Path (SSSP) problem in distributed computing, which is a fundamental problem in the field of distributed algorithms. The context that makes this problem important is the need for efficient algorithms to compute shortest paths in large-scale networks, such as computer networks, social networks, and transportation networks. The authors aim to develop efficient distributed algorithms for solving the SSSP problem, with a focus on achieving sublinear time complexity and low edge congestion. The key objectives of this research are to: (1) develop a scaling framework that reduces the problem to solving SSSP with a small s-radius, (2) design an algorithm that computes h-hop distances for a source node, and (3) extend the algorithm to compute exact shortest paths for all nodes in the network. The authors also aim to improve upon existing algorithms in terms of time complexity, edge congestion, and scalability.|Shortest,Paths,Distributed,Computing,Algorithm,Graphs,Network,Time,Complexity,Framework
76e7ed9d-2494-5e0a-850c-24332e5ee091|2020-05-22T18:21:27+00:00|2020|5|GaaS-X: Graph Analytics Accelerator Supporting Sparse Data Representation using Crossbar Architectures|||||||The problem definition addressed in this research revolves around the inefficiency of general-purpose computing systems in processing large-scale graph algorithms. The context that makes this problem important is the widespread adoption of graph algorithms in various applications, such as social networks, recommendation systems, machine learning, biology, fraud detection, and network analysis. However, the iterative and sequential nature of graph algorithms, combined with the random and scale-free connectivity of real-world graphs, leads to poor temporal and spatial locality, resulting in high memory access latency and low compute utilization. The key objectives or goals the authors set to address this problem are to design a novel computing architecture that can efficiently process large-scale graph algorithms, leveraging the inherent parallelism in graph data structures, and minimizing memory access latency and energy consumption. The authors aim to achieve this by developing a specialized computing system that can effectively handle the unique characteristics of graph data and algorithms, thereby improving the performance, energy efficiency, and scalability of graph processing.|Graph,Analytics,Accelerator,Sparse,Data,Representation,Crossbar,Architectures,Processing,Computing
4380ff2f-67d6-5496-8247-a245230181f1|2017-04-19T16:56:49|2017|4|PGX.D/Async: A Scalable Distributed Graph Pattern Matching Engine|Nicholas P Roth,Vasileios Trigonakis,Sungpack Hong,Hassan Chafi,Anthony Potter,Boris Motik,Ian Horrocks||||||The problem addressed in this research is the efficient execution of graph pattern matching queries on large-scale property graphs in a distributed setting. The context that makes this problem important is the increasing need for graph analytics in various domains, such as social networks, recommendation systems, and knowledge graphs. The authors aim to address this problem by designing a distributed graph pattern matching engine, PGX.D Async, that can efficiently execute graph pattern matching queries on large-scale property graphs while ensuring strict memory bounds and precise flow control. The key objectives of this research are to: (1) develop a distributed query plan that can handle the intricacies of querying a distributed graph, (2) design an execution plan that ensures efficient execution of the query while maintaining strict memory bounds, and (3) implement a runtime system that can efficiently execute the query plan while ensuring precise flow control and termination detection.|Graph,Pattern,Matching,Distributed,Property,Query,PGX.D,Asynchronous,Depth-first,Traversal
541de6a8-49ac-5c26-bee3-86a84fab04c6|2021-07-02T00:41:21+00:00|2021|7|Parallel Graph Coloring Algorithms for Distributed GPU Environments|Ian Bogle,George M Slota Erik G Boman,Karen D Devine,Sivasankaran Rajamanickam||||||The problem addressed in this research is the graph coloring problem, specifically variants of distance-1 and distance-2 coloring. The context that makes this problem important is its application in various fields, such as parallel scientific computations, sparse matrix preconditioning, and circuit design. The goal of the research is to develop efficient parallel algorithms for graph coloring that can minimize the number of colors used, which is an NP-hard optimization problem. The authors aim to achieve good weak scaling behavior and reduce the number of colors used, while also exploring the impact of different heuristics and techniques on the performance of their algorithms.|Graph,Coloring,Distributed,GPU,Algorithms,Parallel,Memory,Distance,Coloring,Problems
1deea2e1-8503-561c-b7c3-b63d420f4b0b|2019-03-29T00:29:16+00:00|2019|3|Distributed Algorithms for Fully Personalized PageRank on Large Graphs|Wenqing Lin||||||The problem addressed in this research is the computation of fully personalized PageRank (PPR) on large graphs, particularly those with weighted edges. The context is that PPR has numerous applications, such as link prediction and recommendation systems for social networks, which often require the fully PPR to be known. However, computing fully PPR is challenging, especially on large graphs, due to the high computational cost and memory requirements. The authors aim to devise efficient distributed algorithms to compute fully approximate PPR on large graphs, with a focus on optimizing the computation to reduce the number of iterations and improve scalability. The key objectives are to develop a distributed algorithm that can handle large graphs, reduce the computational cost, and provide an accurate estimate of PPR values.|Distributed,Algorithm,Fully,Personalized,PageRank,Large,Graphs,Monte,Carlo,Approximation
0d2ef30d-d503-5d79-9e34-b7d00e03e5e3|2017-02-02T01:27:07+00:00|2017|2|Communication-Optimal Distributed Clustering∗|||||||The problem addressed in this research is the task of clustering, a fundamental task in machine learning with widespread applications in data mining, computer vision, and social network analysis. The context that makes this problem important is the increasing need to process large amounts of data in a distributed manner, where data is split across multiple sites or machines. The key objective of this research is to develop efficient algorithms for clustering in a distributed setting, specifically in the message passing model and the blackboard model, while minimizing communication costs. The authors aim to provide nearly optimal solutions by proving almost matching communication lower bounds, highlighting the surprising power of a broadcast channel for clustering problems.|Clustering,Distributed,Spectral,Graph,Communication,Complexity,Models,Algorithms,Lower,Bounds
0430c77a-d35d-51e4-883f-521e01317313|2018-10-29T09:26:50|2018|10|Fast Clustering using MapReduce|||||||The problem addressed in this research is the development of efficient algorithms for solving the k-center and k-median clustering problems in the MapReduce model. The context that makes this problem important is the increasing need to process large-scale data sets in a distributed computing environment, where the data is too large to fit into the memory of a single machine. The key objective of the authors is to design algorithms that can efficiently solve these clustering problems in the MapReduce model, which is a widely used framework for processing large data sets in a distributed environment. The authors aim to achieve a good trade-off between the quality of the solution and the computational resources required, with a focus on minimizing the number of rounds of computation and the amount of data that needs to be communicated between machines.|MapReduce,k-center,k-median,clustering,approximation,algorithm,parallel,scalability,big data,distributed computing
cc205cef-b43e-5b2d-b8fc-9ab61b4d7461|2002-09-18T05:01:37|2002|9|Dynamic and Self-Stabilizing Distributed Matching |||||||The problem addressed in this research is finding a maximal or maximum matching in a graph, which is a well-understood problem in sequential algorithms but challenging in a distributed setting. The context that makes this problem important is the need for dynamic and self-stabilizing distributed matching in systems where processors are connected and need to be paired efficiently. The authors aim to develop algorithms that can dynamically construct a matching as the system configuration changes, without requiring initialization and able to recover from transient faults. The key objectives are to design self-stabilizing algorithms for maximal and maximum matching in a distributed setting, with a focus on bipartite graphs, and to analyze their time complexity.|Matching,Distributed,Algorithm,Graph,Bipartite,Self-stabilizing,Composite,Atomicity,Scheduler,Legitimate
408cc305-b8f9-51be-ab5f-1fd082a8a99a|2016-01-09T07:04:22|2016|1|A Survey of Task Allocation and Load Balancing in Distributed Systems|||||||The problem definition addressed in this research is the task allocation and load balancing in distributed systems. The context that makes this problem important is the increasing complexity and dynamic nature of distributed systems, which are composed of various nodes with different resources and capabilities. These systems are designed to achieve resource sharing, scalability, and flexibility, but they also introduce challenges in allocating tasks efficiently and balancing loads among nodes. The key objectives or goals set by the authors to address this problem are: To minimize the response time of tasks, which is the time elapsed between task arrival and initial execution. To minimize the makespan of tasks, which is the total time required to complete all tasks. To maximize the throughput of tasks, which is the number of tasks completed per unit time. To maximize the reliability of tasks, which is the probability of task completion without failure. The authors aim to develop a task allocation and load balancing mechanism that can optimize these objectives while considering the constraints of distributed systems, such as network structures, resource availability, and node autonomy.|Task Allocation,Distributed Systems,Load Balancing,Resource Allocation,Networks,Heterogeneous Nodes,Optimization,Performance Metrics,Network Structures,Coordination
d2774905-d542-5d0e-bdf5-141b9a9b133e|2019-10-26T07:38:43|2019|10|Survey of external memory large-scale graph processing on a multi-core system|Jianqiang Huang||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph data, which is crucial in various applications such as social networks, recommendation systems, and web search engines. The context that makes this problem important is the rapid growth of graph-structured data, which has led to scalability issues in traditional graph processing systems. The sheer size and complexity of these graphs make it challenging to achieve efficient processing, load balancing, and fault tolerance. The key objectives or goals set by the authors to address this problem are: To develop a scalable and efficient graph processing system that can handle large-scale graph data. To achieve load balancing and fault tolerance in distributed graph processing systems. To optimize graph processing algorithms to minimize computational overhead and improve performance. By addressing these objectives, the authors aim to provide a solution that can efficiently process large-scale graph data, ensuring the scalability and reliability of graph-based applications.|Graph,Processing,Large-scale,External,Memory,Algorithms,PageRank,BFS,SSSP,GraphChi
0c167b95-0992-59a6-86e6-bdf1935768e7|2016-01-02T03:30:41|2016|1|Processing SPARQL queries over distributed RDF graphs|Peng Peng||||||The problem definition addressed in this research revolves around efficiently processing SPARQL queries over distributed RDF graphs. The context that makes this problem important is the increasing scale and decentralization of RDF data, which necessitates distributed query processing to ensure scalability and performance. In this setting, the key challenge lies in dealing with crossing edges between fragments, which can lead to a significant increase in intermediate results and communication overhead. The authors aim to address this problem by developing a partition-agnostic framework that can efficiently process SPARQL queries over distributed RDF graphs, minimizing the number of involved vertices and edges in intermediate results. The key objectives or goals of this research are: To develop a method that can efficiently compute local partial matches at each site, which are the overlapping parts between a crossing match and a fragment. To minimize the number of involved vertices and edges in intermediate results, thereby reducing communication overhead and improving query performance. By achieving these objectives, the authors aim to provide a scalable and efficient solution for processing SPARQL queries over distributed RDF graphs, which is essential for various applications that rely on large-scale RDF data.|SPARQL,Distributed,RDF,Graph,Queries,Query,Evaluation,Partial,Processing,Optimization
acb19fca-17bf-5d50-8ef3-be102b4be556|2017-01-23T05:06:10|2017|1|Community Search over Big Graphs: Models,Algorithms, and Opportunities|||||||The problem definition addressed in this research is community search, which involves finding cohesive communities containing a given set of query nodes in a large graph or network. The context that makes this problem important is the prevalence of community structures in numerous real-world networks, such as social, biological, collaboration, and communication networks. These communities are essential for understanding the behavior, relationships, and interactions within these networks. The key objectives or goals of the authors are to: Identify cohesive communities containing a given set of query nodes in a large graph or network. Develop efficient and effective algorithms for community search that can handle large-scale networks. Address the challenges posed by community search, including the complexity of underlying community structures, the need to balance cohesiveness and query node coverage, and the requirement for scalability and efficiency. Overall, the authors aim to provide a comprehensive overview of community search, including its principles, methodologies, algorithms, and applications, and to offer future directions for research in this important and growing area.|Community Search,Graphs,Networks,Social Networks,Community Detection,Querying,Attributed Community Search,Geo-Social Groups,Social Circle Discovery,Big Graphs
c145a936-3487-53f4-8e79-74ef84df25ba|2019-05-31T19:02:42+00:00|2019|5|Improving Distribued Subgraph Matching Algorithm on Timely Dataflow|||||||The problem definition addressed in this research is the efficient subgraph matching in large-scale graphs, particularly in the context of graph analytics and querying. The background that makes this problem important is the increasing need to analyze and query large-scale graphs in various domains, such as social networks, biological networks, and knowledge graphs. Subgraph matching is a fundamental operation in graph analytics, but it is computationally expensive and challenging to scale. The key objectives or goals the authors set to address this problem are: To develop an efficient subgraph matching algorithm that can scale to large graphs. To minimize the computational cost and memory usage of the algorithm. To optimize the join plan for subgraph matching to reduce the number of intermediate results. The authors aim to achieve these objectives by proposing a novel algorithm called CliqueJoin, which extends the traditional join-based approach to subgraph matching by using clique and star structures as join units. The algorithm is designed to work efficiently on large-scale graphs and to minimize the computational cost and memory usage.|Subgraph,Matching,CliqueJoin,Distributed,Graph,Query,Join,Algorithm,Enumeration,Optimization
85852c3f-c175-5812-ba30-ee301cf631cc|2019-08-31T21:28:20+00:00|2019|8|GraphQ: Scalable PIM-Based Graph Processing|Youwei Zhuo,Chao Wang,Mingxing Zhang,Rui Wang,Dimin Niu,Yanzhi Wang,Xuehai Qian||||||The problem definition addressed in this research revolves around the efficient processing of graph analytics on emerging Processing-In-Memory (PIM) architectures. The context that makes this problem important is the increasing significance of graph analytics in various applications, including machine learning, natural language processing, and anomaly detection, among others. However, traditional computing architectures struggle to efficiently process large-scale graphs due to the irregularity of graph data and the high overhead of inter-node communication. The key objectives or goals the authors set to address this problem are: To design an efficient PIM-based architecture that can handle the irregularity of graph data and minimize inter-node communication overhead. To develop a programming model that can effectively utilize the PIM architecture and reduce the complexity of graph analytics. To maximize intra-cube, inter-cube, and inter-node communication throughput while minimizing energy consumption. Overall, the authors aim to develop a novel PIM-based architecture and programming model that can efficiently process large-scale graphs, enabling faster and more energy-efficient graph analytics.|Graph,Processing,PIM,Memory,Architecture,Scalable,Vertex,Program,Analytics,In-Memory
77828914-c87e-59a9-b6cd-849b128ec7ca|2020-06-16T05:21:22+00:00|2020|6|A Distributed Path Query Engine forTemporal Property Graphs *|||||||The problem definition addressed in this research revolves around the efficient execution of path queries on temporal property graphs. The context is that property graphs are increasingly used to represent and analyze linked data in various domains, and path queries are a common class of queries over these graphs. However, existing graph processing systems are not optimized for temporal property graphs, leading to inefficient query execution. The key objective of this research is to develop a cost model that can accurately estimate the execution time of different query plans and select the optimal plan, thereby reducing the actual query execution time. The authors aim to achieve this by proposing a novel cost model that takes into account the temporal properties of the graph and the query, and by developing an efficient query execution engine that can execute the selected plan.|Graph,Query,Temporal,Property,Path,Execution,Model,Plan,Time,Predicate
fafc10f1-74df-5627-a721-89fdf52b9bee|2016-06-10T09:20:48|2016|6|Distributed Greedy Approximation to Maximum Weighted Independent Set for Scheduling With Fading Channels|||||||The problem definition addressed in this research revolves around designing a distributed scheduling algorithm for wireless networks with fading channels. The context that makes this problem important is the need for efficient and adaptive scheduling in wireless networks, where channel conditions can vary rapidly. The traditional approach of solving the Maximum Weighted Independent Set (MWIS) problem to achieve throughput optimality is NP-hard and impractical for large networks. The key objectives or goals the authors set to address this problem are: 1. To design a distributed scheduling algorithm that can adapt to changing channel conditions in real-time. 2. To achieve a provable fraction of the optimal throughput, ensuring the algorithm's performance is close to optimal. 3. To develop an algorithm with low complexity, making it feasible for implementation in large-scale wireless networks. By addressing these objectives, the authors aim to provide a practical and efficient solution for scheduling in wireless networks with fading channels, enabling better utilization of network resources and improved overall performance.|Scheduling,Fading Channels,Distributed Algorithm,Maximum Weighted Independent Set,Wireless Networks,Throughput Optimality,Greedy Approximation,Opportunistic Gain,Con ict Graph,Channel State
d6a6df8f-2109-5198-b361-c9436583f457|2018-12-01T05:29:52+00:00|2018|12|Exploiting Locality in Graph Analytics through Hardware-Accelerated Traversal Scheduling|||||||Here is a clear and concise summary of the problem definition addressed in this research: Context and Background: Graph algorithms are increasingly important in various domains, such as social networks, web graphs, and recommendation systems. However, these algorithms are often memory-bound, leading to poor performance due to expensive main memory accesses. The irregular structure of graphs results in seemingly random accesses, making it challenging to optimize memory access patterns. Problem Definition: The problem addressed in this research is the lack of temporal locality in graph algorithms, which leads to poor performance. Specifically, the authors focus on the vertex-ordered schedule, a common approach used in software graph processing frameworks, which ignores the graph structure and alternates between different regions of the graph, resulting in poor temporal locality. Key Objectives and Goals: The authors aim to improve the performance of graph algorithms by addressing the lack of temporal locality. Their primary objective is to develop a technique that can exploit the community structure of real-world graphs to improve locality, without incurring the high overheads of preprocessing techniques. The authors seek to achieve this goal by designing a scheduling approach that can adapt to the graph structure at runtime, thereby improving temporal locality and reducing memory accesses.|Graph,Locality,Scheduling,Traversal,Memory,Accesses,PageRank,Algorithm,Preprocessing,Optimization
7667990f-872f-5b6e-b30d-1ad43785d7dc|2018-06-19T01:50:07+00:00|2018|6|VEBO: A Vertex- and Edge-Balanced OrderingHeuristic to Load Balance Parallel Graph Processing|||||||The problem addressed in this research is the load imbalance in parallel graph processing, which occurs when different partitions of a graph take significantly different amounts of time to process. This imbalance is particularly problematic in scale-free graphs, where a small number of high-degree vertices can dominate the processing time. The context that makes this problem important is the increasing use of graph processing in various applications, such as social network analysis, web search, and recommendation systems. The authors aim to address this problem by proposing a new heuristic, called Vertex and Edge Balanced Ordering (VEBO), which balances the number of edges and the number of unique destinations in each partition. The key objective is to minimize the processing time of each partition, thereby achieving better load balance and improving the overall performance of parallel graph processing systems.|Graph,Partitioning,Load,Balance,Parallel,Processing,VEBO,Algorithm,Heuristic,Optimization
ce8c354d-031c-545d-8dd1-2b452a725b0a|2012-05-31T01:51:02+00:00|2012|5|Efﬁcient Subgraph Matching on Billion Node Graphs|||||||The problem definition addressed in this research is the subgraph matching problem on very large graphs, specifically billion node graphs. The context that makes this problem important is the increasing number of applications that handle large scale graph data, such as social networks, web graphs, and bioinformatics applications. The authors aim to support efficient online query processing on these large graphs, which is challenging due to the lack of locality in graph accesses and the infeasibility of using sophisticated indices. The key objective is to develop a method that can perform subgraph matching on billion node graphs without relying on super linear indices, which are often infeasible for very large graphs. The authors also aim to address the locality issue and support efficient graph exploration, which is crucial for online query processing on large graphs.|Subgraph,Matching,Billion,Node,Graphs,Graph,Query,Processing,Trinity,Memory
c9a7fb50-d222-5593-8bab-94d873dcec2e|2017-09-27T12:24:35|2017|9|An Eﬃcient Silent Self-stabilizing 1-MaximalMatching Algorithm Under Distributed Daemonfor Arbitrary Networks|||||||The problem addressed in this research is the development of an efficient self-stabilizing algorithm for the 1-maximal matching problem in arbitrary networks. The context that makes this problem important is the need for distributed systems to withstand transient failures, such as memory corruption, erroneous initialization, or topology changes, and recover to a legitimate configuration. In this context, self-stabilization is a versatile technique to ensure the system's resilience. The key objective of this research is to design an algorithm that can efficiently solve the 1-maximal matching problem in arbitrary networks, which is a fundamental problem in distributed computing with applications in communication scheduling, resource allocation, and client-server systems. The authors aim to achieve this objective by proposing an algorithm that is self-stabilizing, meaning it can converge to a legitimate configuration from any initial state, and remains in a legitimate configuration thereafter. Specifically, the authors focus on developing an algorithm that can solve the 1-maximal matching problem in arbitrary networks, without relying on global identifiers, and under the unfair distributed daemon model, which allows the daemon to select any non-empty set of nodes to execute actions. The authors' goal is to design an algorithm that is efficient in terms of the number of moves required to converge to a legitimate configuration, and can operate in arbitrary networks, making it a versatile solution for various distributed systems.|Self-stabilization,Maximal matching,Distributed algorithm,Unfair daemon,Arbitrary networks,Silent algorithm,Ecient algorithm,Matching problem,Distributed systems,Fault tolerance
c56515e5-d741-50d1-8e98-0247488ea450|2017-08-02T00:19:43+00:00|2017|8|Distributed Approximation of Maximum Independent Set andMaximum Matching|||||||The problem definition addressed in this research is the Maximum Independent Set (MaxIS) problem in distributed networks. The context that makes this problem important is that many real-world networks, such as social networks, wireless sensor networks, and peer-to-peer networks, have a distributed nature, and solving the MaxIS problem in these networks has numerous applications, including clustering, resource allocation, and network optimization. The key objective of this research is to develop an efficient distributed algorithm that can approximate the maximum independent set in a weighted graph, where each node has a bounded degree. The authors aim to achieve a good approximation factor while minimizing the number of communication rounds required to solve the problem. Specifically, the authors focus on designing a distributed algorithm that can be executed in the CONGEST model, which is a standard model for distributed computing in networks. The goal is to develop an algorithm that can be implemented in a distributed manner, where each node only communicates with its neighbors, and the algorithm terminates within a reasonable number of rounds. Overall, the authors' objective is to provide an efficient and scalable solution to the MaxIS problem in distributed networks, which can be applied to various real-world scenarios and has the potential to improve the performance and efficiency of these networks.|Distributed,Approximation,Algorithms,Maximum,Independent,Set,Graphs,CONGEST,Model,Weighted
debd9eec-503a-523a-80e4-15410a2d6edb|2022-09-30T11:17:45|2022|9|Scaling Graph 500 SSSP to 140 Trillion Edges with over 40 Million Cores|||||||The problem definition addressed in this research is the scalability issue of Single-Source Shortest Path (SSSP) algorithms on large-scale graphs, particularly on power-law graphs, in the context of high-performance computing (HPC) and distributed graph processing. The background that makes this problem important is the increasing significance of large-scale data-intensive applications, which are characterized by heavy communication loads, latency sensitivity, and load imbalance. These characteristics make it challenging to scale out graph algorithms, including SSSP, on supercomputers. The key objectives or goals the authors set to address this problem are: 1. To analyze the work efficiency of existing SSSP algorithms, including Bellman-Ford and Stepping, and identify the limitations that hinder their scalability. 2. To design and develop a new SSSP algorithm that can efficiently process large-scale graphs on distributed systems, minimizing the repeated visits to edges and reducing the communication overhead. 3. To optimize the algorithm for better scalability, exploring techniques such as sparsity optimization and dynamic sliding windows to reduce the computational complexity and improve the performance. Overall, the authors aim to develop a scalable SSSP algorithm that can efficiently process large-scale graphs on distributed systems, addressing the challenges of heavy communication loads, latency sensitivity, and load imbalance.|SSSP (Single Source Shortest Path),Graph 500,Supercomputers,Scalability,Graph Computing,Benchmarking,Shortest Path Problem,Bellman Ford,Hyper Stepping,Parallel Computing
9aa4635f-0dc5-52fc-b96d-4f38d9da04d2|2022-10-18T08:29:51|2022|10|ReGraph: Scaling Graph Processing on HBM-enabled FPGAs with Heterogeneous Pipelines|||||||The problem definition addressed in this research revolves around the inefficiency of graph processing on High-Bandwidth Memory (HBM) enabled Field-Programmable Gate Arrays (FPGAs). The context that makes this problem important is the increasing significance of graph processing in various application domains, such as social networks, genomics, and machine learning, which generates massive amounts of graph data. Existing FPGA accelerators for graph processing suffer from poor resource efficiency, leading to scalability issues. The key objectives or goals the authors set to address this problem are: To improve resource efficiency in graph processing on HBM-enabled FPGAs. To develop a heterogeneous pipeline architecture that can adapt to diverse workloads in graph partitions. To design a graph-aware task scheduling method that can efficiently schedule graph partitions to the right pipeline types, generating the most efficient pipeline combination and balancing workloads. By achieving these objectives, the authors aim to overcome the scalability limitations of existing FPGA accelerators and provide a more efficient solution for graph processing on HBM-enabled FPGAs.|Graph,Processing,FPGA,Partitioning,Workload,Characterization,Optimization,Resource,Efciency,Architecture
e6914436-38aa-58ff-8928-28c94b011892|2014-09-27T05:24:29|2014|9|Large-Scale Distributed Graph Computing Systems:An Experimental Evaluation[Technical Report]|||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph data in distributed computing systems. The context that makes this problem important is the increasing need to analyze and process massive graph-structured data, which is common in various domains such as social networks, web graphs, and biological networks. The sheer size and complexity of these graphs pose significant challenges to traditional computing systems, making it essential to develop scalable and efficient distributed graph computing systems. The key objectives or goals of this research are to: Develop a distributed graph computing system that can efficiently process large-scale graph data. Improve the performance and scalability of graph computing systems by minimizing the number of messages exchanged between workers, reducing the workload imbalance, and optimizing the graph partitioning and data placement. Provide a flexible and programmable framework that can support various graph algorithms and applications. Overall, the authors aim to address the problem of efficient large-scale graph processing in distributed computing systems, with a focus on developing a scalable, efficient, and flexible system that can support a wide range of graph algorithms and applications.|Graph,Distributed,Computing,Systems,Performance,Evaluation,Vertex-centric,Algorithms,Scalability,Benchmarking
a024cf91-6b24-5f4d-bb8b-8345cbaa4726|2022-12-15T08:41:46|2022|12|DRONE: An Efficient Distributed Subgraph-Centric Framework for Processing Large-Scale Power-law Graphs|||||||The problem definition addressed in this research is the efficient processing of large-scale power-law graphs in distributed graph computing systems. The context that makes this problem important is the increasing prevalence of massive graphs in various domains, such as social networks, web graphs, and biological networks, which pose significant challenges in terms of storage, processing, and scalability. The key objectives or goals the authors set to address this problem are: To develop an efficient graph partitioning algorithm that can balance the load among workers, minimize communication overhead, and ensure fault tolerance. To design a distributed graph computing system that can handle large-scale power-law graphs and provide high performance, scalability, and reliability. The authors aim to achieve these objectives by proposing a novel graph partitioning algorithm called Edge-Based Vertex (EBV) partitioning, which takes into account the power-law degree distribution of real-world graphs. They also develop a distributed graph computing system called DRONE, which incorporates the EBV partitioning algorithm and provides a fault-tolerant mechanism to ensure reliable processing of large-scale graphs.|Graph,Distributed,Framework,Processing,Large-scale,Power-law,Subgraph,Centric,Partitioning,Algorithms
d6890cf5-a10a-5373-a4fb-f1cc2c1f6188|2021-03-29T15:45:50+00:00|2021|3|Improved Distributed Steiner Forest Construction|||||||The problem addressed in this research is the Distributed Steiner Forest (DSF) problem, a generalization of the Minimum Spanning Tree (MST) problem. The DSF problem involves finding a minimum-weight subgraph that connects all terminals in a given network, where terminals are nodes with specific connection requests. The problem is important in the context of network design, where efficient algorithms are needed to construct networks that satisfy various connectivity demands. The authors aim to develop efficient distributed algorithms for the DSF problem, with a focus on achieving a good approximation ratio and minimizing the number of communication rounds. Specifically, they seek to design algorithms that can solve the problem in a distributed manner, where nodes in the network can only communicate with their neighbors, and the goal is to minimize the total weight of the selected edges while ensuring that all terminals are connected.|Steiner Forest,Distributed Algorithm,Network Algorithm,CONGEST Model,Approximation Ratio,Spanning Tree,Minimum-Weight Spanning Tree,Distributed Network,Parallel Algorithm,Computational Complexity
ce718e2b-ebfe-5cc9-8fca-fb62157ca238|2020-07-12T16:38:07|2020|7|On Distributed Listing of Cliques|||||||The problem addressed in this research is the listing of all instances of a subgraph pattern, specifically Kp (complete graph on p nodes), in a distributed network, where each node has limited bandwidth and can only communicate with its neighbors. The context that makes this problem important is the increasing need for efficient algorithms in distributed networks, such as social networks, web graphs, and peer-to-peer networks, where subgraph listing is a fundamental problem with numerous applications. The key objective of this research is to develop an efficient algorithm that can list all instances of Kp in a distributed network, while minimizing the number of communication rounds required. The authors aim to overcome two main challenges: (1) controlling the sparsity of the problem assigned to each cluster, and (2) ensuring that the bandwidth available to each cluster is proportional to the size of the problem assigned to it. In particular, the authors focus on the CONGEST model, which is a widely used model for distributed networks, and aim to develop an algorithm that can list all instances of Kp in O(n^(3/4)) rounds, which is a significant improvement over existing algorithms. Overall, the goal of this research is to provide an efficient solution for subgraph listing in distributed networks, which can have a significant impact on various applications, such as network analysis, data mining, and graph processing.|Distributed,Listing,Cliques,CONGEST,Subgraph,Algorithm,Arboricity,Expander,Decomposition,Sparsity
3d8063b5-69f4-5979-b144-e954152d8d9b|2022-07-26T08:20:42|2022|7|Multi-Task Processing in Vertex-Centric Graph Systems: Evaluations and Insights|Siqiang Luo,Zichen Zhu,Xiaokui Xiao,Yin Yang,Chunbo Li,Ben Kao||||||"The problem addressed in this research revolves around the round-congestion tradeoff in vertex-centric (VC) systems, specifically in the context of multi-processing tasks. The background that makes this problem important is the increasing need for efficient processing of large-scale graph data, which is a fundamental component of many modern applications. VC systems, such as Pregel, are designed to handle these tasks, but they often overlook the critical tradeoff between the number of communication rounds and message congestion. The key objective of this research is to investigate and optimize this tradeoff, as it significantly impacts the performance of VC systems. The authors aim to identify the optimal balance between the number of rounds and message congestion, which is crucial for achieving efficient processing of graph data. The goal is to develop a general strategy that can determine a suitable tradeoff for various multi-processing tasks, taking into account the constraints of VC systems, such as linear space usage, linear computation cost, and logarithmic rounds. In essence, the problem definition can be summarized as follows: ""How to optimize the round-congestion tradeoff in vertex-centric systems to achieve efficient processing of large-scale graph data, while considering the constraints of these systems?"""|Multi-processing,Graph processing,Vertex-centric systems,Distributed systems,Personalized PageRank,Shortest path,Hop search,Graph algorithms,Parallel processing,Optimization
9515ca63-19f2-55bf-b45c-ba1eda31ccb8|2016-04-18T06:56:48|2016|4|Vertex-Centric Graph Processing on FPGA|||||||The problem addressed in this research is the difficulty of programming FPGAs (Field-Programmable Gate Arrays) for graph processing, which leads to impractically long development times even for simple applications. The context that makes this problem important is that FPGAs have been shown to be efficient at processing many graph algorithms, but their programming complexity hinders their adoption. The key objective of this research is to develop a vertex-centric framework for graph processing on FPGAs that can improve development speed, making it possible for even casual users to take advantage of the platform's massive parallelism and energy efficiency.|Graph,Processing,FPGA,Vertex,Centric,Framework,Architecture,Programming,Model,Algorithm
faf44597-a5fd-5c6d-82be-f7c2bae1b2a8|2018-09-26T13:40:17|2018|9|Distributed Approximate Maximum Matching inthe CONGEST Model|||||||The problem addressed in this research is the maximum fractional matching problem in the CONGEST model, a synchronous distributed computing model. The context that makes this problem important is the need for efficient distributed algorithms to solve graph problems, particularly in large-scale networks where centralized algorithms are impractical. Maximum fractional matching is a fundamental problem in graph theory, and its distributed solution has applications in various fields, such as network optimization, resource allocation, and data processing. The key objective of this research is to develop a deterministic distributed algorithm that approximates the maximum fractional matching in a graph, with a focus on minimizing the number of communication rounds required. The authors aim to achieve a nearly optimal approximation ratio while keeping the communication complexity low. Specifically, they target a polylogarithmic number of rounds, which is a significant improvement over previous algorithms that require a large number of rounds. In summary, the problem definition addressed in this research is the development of an efficient distributed algorithm for maximum fractional matching in the CONGEST model, with the goal of achieving a good approximation ratio while minimizing communication complexity.|Matching,Bipartite,Graphs,Distributed,Algorithm,Maximum,CONGEST,Model,Complexity,Approximation
13d8f711-860f-5a3b-856f-ddf4b38cede7|2015-11-26T01:39:30+00:00|2015|11|Fast Distributed PageRank Computation ∗|||||||The problem definition addressed in this research is the efficient computation of PageRank in a distributed network. The context that makes this problem important is the increasing significance of distributed networks, such as peer-to-peer (P2P) and overlay networks, where nodes need to communicate with each other to perform tasks. PageRank, a fundamental algorithm in graph theory, is crucial in these networks for tasks like ranking nodes, identifying influential nodes, and detecting spam. The key objective of this research is to design a distributed algorithm that can efficiently compute PageRank in a network with n nodes, where each node has limited knowledge and can only communicate with its neighbors. The authors aim to achieve this goal by minimizing the number of communication rounds required to compute PageRank, while ensuring that the algorithm is scalable, efficient, and accurate. Specifically, the authors focus on developing a distributed algorithm that can estimate the PageRank vector with high probability, using a limited number of communication rounds and message sizes. They also aim to improve upon existing algorithms by reducing the number of rounds required to achieve a certain level of accuracy. Overall, the research addresses a critical problem in distributed networks, with the goal of enabling efficient and accurate computation of PageRank in large-scale networks.|PageRank,Distributed Algorithm,Random Walk,Monte Carlo Method,Graphs,Networks,Distributed Computing,Computational Complexity,Scalability,Undirected Graphs
7929d6b4-bd91-5a5c-bced-2e98515fa2ed|2020-05-22T18:21:27+00:00|2020|5|GraphABCD: Scaling Out Graph Analytics with Asynchronous Block Coordinate Descent|||||||The problem definition addressed in this research revolves around optimizing graph algorithms on heterogeneous and distributed systems. The context that makes this problem important is the increasing scale and complexity of graph data, which has led to a significant performance gap between graph processing and other domains. This gap is attributed to the lack of efficient optimization techniques for graph algorithms, particularly on modern computing architectures. The key objective of this research is to bridge this performance gap by developing an optimization framework that can efficiently execute graph algorithms on heterogeneous and distributed systems. The authors aim to achieve this by formulating graph problems as optimization problems, which can be solved using Block Coordinate Descent (BCD) methods. The goal is to develop a flexible and scalable optimization framework that can be applied to various graph problems, including machine learning-based and conventional graph algorithms. In summary, the problem definition involves optimizing graph algorithms to achieve better performance on modern computing architectures, and the authors' objective is to develop a flexible and scalable optimization framework that can efficiently execute graph algorithms on heterogeneous and distributed systems.|Graph Analytics,Asynchronous,Block Coordinate Descent (BCD),Heterogeneous,Iterative Algorithms,Convergence Rate,Graph Processing,Optimization,Distributed Systems,Scalability
c299088d-2e9a-50ca-925b-e300ec8b6137|2013-10-04T09:44:45|2013|10|"From ""Think Like a Vertex"" to ""Think Like a Graph"""|||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph data using distributed computing systems. The context that makes this problem important is the rapid growth of graph-structured data in various domains, such as social networks, web graphs, and biological networks, which poses significant challenges in terms of scalability, performance, and fault tolerance. The key objective of this research is to develop a graph processing system that can efficiently handle massive graph data by leveraging distributed computing architectures. Specifically, the authors aim to design a system that can: Scale to handle large graphs with billions of vertices and edges. Provide high performance and efficiency in processing graph algorithms. Ensure fault tolerance and reliability in the presence of node failures or other errors. To achieve these objectives, the authors propose a novel graph processing system that combines the benefits of vertex-centric and graph-centric programming models. The system, called Giraph, is designed to provide a flexible and efficient framework for processing large-scale graph data using distributed computing systems.|Graph,Centric,Model,Vertex,Programming,Giraph,Pregel,GraphLab,Distributed,Processing
bd1a2704-938f-5414-8674-1aab139f7478|2016-07-03T19:33:28|2016|7|GraVF: A vertex-centric distributed graph processing framework on FPGAs|||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph data on Field-Programmable Gate Arrays (FPGAs). The context that makes this problem important is the increasing need for high-performance graph processing in various domains, such as social network analysis, recommendation systems, and machine learning. However, traditional computing architectures are often limited by their memory bandwidth and processing power, leading to slow processing times and high energy consumption. The key objectives or goals the authors set to address this problem are: To develop a high-level design framework for distributed graph processing on FPGAs, which can efficiently process large-scale graph data. To overcome the limitations of traditional computing architectures by leveraging the parallel processing capabilities of FPGAs. To provide a flexible and scalable solution that can be easily adapted to different graph processing algorithms and applications. To achieve these objectives, the authors propose a novel architecture called GraVF, which is designed to efficiently process graph data on FPGAs. GraVF is based on the vertex-centric programming model, which is well-suited for graph processing, and uses a pipelined architecture to minimize memory access and maximize processing throughput. The authors also develop a set of optimization techniques to further improve the performance and efficiency of GraVF.|Graph,FPGA,Vertex,Centric,Processing,Framework,GraVF,Algorithm,Distributed,Computation
a8405192-4506-513b-a7de-67892f45d872|2021-02-02T12:26:03|2021|2|A classification of community detection methods in social networks: a survey|||||||The problem definition addressed in this research revolves around community detection in complex networks. The context that makes this problem important is the increasing complexity of real-world networks, such as social media, biological systems, and the internet, which necessitates the development of effective methods to identify and analyze their underlying community structures. Community detection is crucial for understanding the organization, behavior, and evolution of these networks. The key objective of this research is to develop optimization-based approaches to detect communities in complex networks. The authors aim to address the problem by proposing methods that can efficiently identify high-quality community structures, which are typically characterized by dense connections within communities and sparse connections between them. The primary goal is to maximize the modularity metric, which measures the quality of a community structure, and to develop algorithms that can handle large-scale networks and multiple objectives. In essence, the problem definition involves developing efficient and effective community detection methods that can uncover meaningful patterns and structures in complex networks, ultimately contributing to a deeper understanding of these systems and their applications in various domains.|Community detection,Label propagation,Bottom-up approaches,Top-down approaches,Data structures,Optimization methods,Network analysis,Graph structured data,Heuristic algorithms,Modularity
7f212a83-25e9-534d-a751-23baedb19492|2015-04-15T04:49:20|2015|4|Efficient distributed subgraph similarity matching|Ye Yuan||||||The problem definition addressed in this research revolves around optimizing query decomposition in distributed query processing systems. The context that makes this problem important is the increasing complexity of querying large-scale distributed data, which leads to high processing costs and network overhead. In this context, query decomposition is a crucial step that involves breaking down a query into smaller sub-queries that can be executed in parallel across multiple machines. However, the decomposition process can significantly impact the overall performance of the system. The key objective of this research is to optimize query decomposition by minimizing the number of intermediate results and maximizing the sharing of computation among sub-queries. This is achieved by finding the optimal set of h-trees that cover the query, where h-trees are a data structure used to represent the decomposition of a query. The authors' goal is to develop an efficient algorithm that can solve this optimization problem, which is shown to be NP-complete. By addressing this problem, the authors aim to reduce the processing cost and network overhead associated with distributed query processing, ultimately leading to improved system performance and scalability.|Subgraph,Similarity,Matching,Distributed,Graph,Query,Relaxation,Decomposition,Parallel,Processing
ff5d78fa-d9bb-5e05-a6cf-066fb76976eb|2013-10-21T20:18:51+00:00|2013|10|CCF: Fast and Scalable Connected ComponentComputation in MapReduce|||||||The problem addressed in this research is finding connected components in a graph, which is a well-known problem in various application areas such as social network analysis, data mining, image processing, and record linkage. The context that makes this problem important is the massive scale of the data, with billions of records and edges, which requires novel types of algorithms to analyze. The authors' objective is to develop an efficient and scalable approach to find connected components in a graph using the MapReduce programming model, which is implemented using Hadoop. The goal is to design an algorithm that can handle massive graphs with billions of nodes and edges, and to evaluate its performance on a real-world graph.|Connected,Components,MapReduce,Graph,Hadoop,Scalable,Algorithm,Record,Linkage,Clustering
8ce87105-8fd2-5e0d-977e-35f3d9a65cbb|2021-06-30T13:20:07|2021|6|Distributed Graph Processing System and Processing-in-memory Architecture with Precise Loop-carried Dependency Guarantee|||||||The problem definition addressed in this research revolves around the inefficiencies in distributed graph processing systems, particularly in handling loop-carried dependencies in graph algorithms. The context that makes this problem important is the increasing need to process large-scale graphs in various applications, such as machine learning, natural language processing, and social influence analysis. Existing distributed graph processing systems struggle to efficiently handle graph algorithms with loop-carried dependencies, leading to redundant computation and communication, which results in significant performance degradation and energy waste. The key objectives or goals the authors set to address this problem are: 1. To develop a distributed graph processing system that can efficiently handle graph algorithms with loop-carried dependencies. 2. To minimize redundant computation and communication, thereby reducing energy consumption and improving performance. 3. To ensure the correctness of graph algorithms with loop-carried dependencies in a distributed environment. Overall, the authors aim to design a system that can efficiently process large-scale graphs with complex dependencies, while ensuring the correctness and performance of graph algorithms.|Graph,Processing,Distributed,System,Architecture,Memory,Algorithm,Dependency,Loop,Guarantee
2f1de64d-7a0b-52d2-a71b-b2ed36334e65|2023-07-14T13:09:07|2023|7|Distributed (&#x03B1;, &#x03B2;)-Core Decomposition over Bipartite Graphs|||||||The problem definition addressed in this research revolves around efficiently computing the Bi-index, a measure used to analyze the structure of bipartite graphs. The context that makes this problem important is the increasing prevalence of bipartite graphs in various domains, such as social networks, recommender systems, and biological networks, where understanding the graph structure is crucial for applications like community detection, link prediction, and graph clustering. The key objective of this research is to develop an efficient algorithm for computing the Bi-index, which is a challenging task due to the high computational complexity of existing methods. The authors aim to address this problem by proposing optimizations that reduce the computational cost and improve the scalability of Bi-index computation, making it feasible for large-scale bipartite graphs. Specifically, the authors focus on developing a more efficient algorithm that can handle large bipartite graphs, reduce the number of iterations required for convergence, and minimize the memory usage. By achieving these objectives, the authors aim to provide a practical solution for computing the Bi-index, enabling researchers and practitioners to better analyze and understand the structure of bipartite graphs in various domains.|Core decomposition,Bipartite graphs,Distributed algorithms,Graph processing,Cohesive subgraph,core,Bi-indexes,Distributed computing,Graph partitioning,Scalability
3efa43c4-8ca4-5525-b845-da9800c7752a|2022-10-18T08:38:42|2022|10|A Data-Centric Accelerator for High-Performance Hypergraph Processing|||||||The problem definition addressed in this research revolves around the efficient processing of hypergraphs, which are a powerful approach for analyzing complex multilateral relationships among multiple entities. The context that makes this problem important is the increasing need to model and analyze complex relationships in various domains, such as social networks, author collaboration networks, and biological networks. Conventional graph models are limited in their ability to represent these complex relationships, making hypergraph processing a crucial area of research. The key objective of this research is to design an efficient hypergraph processing system that can fully exploit the overlap-induced locality in hypergraph computations. The authors aim to achieve this by developing a data-centric approach that can effectively utilize the memory hierarchy and minimize data movement. The specific goals of this research include: 1. Developing a novel execution model that can efficiently process hypergraphs by leveraging the overlap-induced locality. 2. Designing a data-centric accelerator that can minimize data movement and maximize the utilization of the memory hierarchy. 3. Improving the performance and scalability of hypergraph processing systems to handle large-scale hypergraphs. Overall, the authors aim to address the limitations of existing hypergraph processing systems and provide a more efficient and scalable solution for analyzing complex relationships in various domains.|Hypergraph,Processing,Accelerator,Data-centric,XuLin,Hyperedge,Vertex,Scheduling,Locality,Overlapping
323f7f75-f334-5804-b031-bb8b1f6d563d|2014-03-28T15:36:57|2014|3|Parallel Subgraph Listing in a Large-Scale Graph|||||||The problem addressed in this research is the efficient execution of subgraph listing, a fundamental operation in graph computing, on large-scale undirected graphs in parallel. The context that makes this problem important is the increasing need to analyze massive graphs in various domains, such as social networks, biological networks, and web graphs, to uncover hidden patterns and relationships. Subgraph listing is a computationally challenging task, as the size of the result set can be exponential to the number of vertices in the pattern graph. The key objectives or goals the authors set to address this problem are: 1. To design a parallel framework, PSgL, that can efficiently execute subgraph listing on large-scale graphs. 2. To develop optimization techniques to reduce the computation, communication, and memory costs associated with subgraph listing. 3. To achieve a good workload balance among parallel workers to minimize the impact of workload imbalance on performance. Overall, the authors aim to provide a scalable and efficient solution for subgraph listing on large-scale graphs, enabling faster and more accurate analysis of complex networks.|Subgraph,Listing,Parallel,Graph,Large-scale,Algorithm,Enumeration,Pattern,Distributed,Scalability
3ae1dae8-54fe-5160-8d98-8fc70518ce5b|2014-06-21T10:26:58|2014|6|Distributed Graph Simulation: Impossibility and Possibility|XinWang||||||"The problem definition addressed in this research is the efficient querying of big graphs that are fragmented and distributed across multiple sites. The context that makes this problem important is the increasing prevalence of large-scale graph-structured data in various domains, such as social networks, biological networks, and knowledge graphs, which are often too large to be stored and processed on a single machine. As a result, these graphs are typically fragmented and distributed across multiple sites, making it challenging to query them efficiently. The key objective of this research is to develop algorithms that can efficiently query these distributed graphs while minimizing data shipment and response time. Specifically, the authors aim to design algorithms that are ""parallel scalable"" in response time and data shipment, meaning that the algorithm's performance should improve as the number of processing sites increases, and the amount of data shipped should be independent of the size of the graph. Overall, the problem of querying big graphs is important because it has significant implications for various applications, such as social network analysis, recommendation systems, and knowledge graph querying. The authors' goal is to develop efficient and scalable algorithms that can handle large-scale graph-structured data, enabling faster and more efficient querying and analysis of these datasets."|Graph,Simulation,Pattern,Matching,Distributed,Query,Evaluation,Fragmentation,Algorithm,Scalability
ac59b0f4-2fbe-5c56-962c-6d55550fed84|2016-07-12T00:52:05+00:00|2016|7|High-Level Programming Abstractions forDistributed Graph Processing|||||||"The problem definition addressed in this research revolves around the challenges of processing large-scale graph data in distributed systems. The context that makes this problem important is the increasing availability of massive graph-structured data, which poses significant scalability and performance issues when processed using traditional computing architectures. The authors highlight that existing graph processing systems often require users to have extensive knowledge of distributed computing, which can be a barrier to adoption. The key objective of this research is to identify and analyze the programming models and abstractions used in distributed graph processing systems. The authors aim to provide a comprehensive understanding of the strengths and weaknesses of different programming models, including vertex-centric, scatter-gather, and linear algebra-based models. By doing so, they hope to guide the development of more efficient, scalable, and user-friendly graph processing systems that can effectively handle large-scale graph data. In essence, the problem definition can be summarized as: ""How can we design and develop efficient, scalable, and user-friendly distributed graph processing systems that can effectively handle large-scale graph data, and what are the key programming models and abstractions that can help achieve this goal?"""|Graph,Algorithms,Programming,Models,Abstractions,Vertex,Centric,PageRank,Scatter,Gather
19e3d7ed-ff13-578f-9c1d-63898c64be65|2022-10-04T17:20:53|2022|10|Awake-Efficient Distributed Algorithms for Maximal Independent Set|||||||The problem definition addressed in this research is the design of distributed algorithms for the Maximal Independent Set (MIS) problem in the sleeping model, which is a fundamental problem in distributed computing with various applications. The context that makes this problem important is the need to minimize resource utilization in resource-constrained networks, such as ad hoc wireless, sensor, and IoT networks. In these networks, nodes have limited energy and computational resources, and hence, it is crucial to design algorithms that are energy-efficient and minimize the number of rounds a node needs to be awake. The key objective of this research is to design distributed MIS algorithms that have small awake complexity, which is the worst-case number of awake rounds needed by any node until it terminates. The authors aim to achieve this objective by developing algorithms that can solve the MIS problem in a small number of awake rounds, while also minimizing the traditional time complexity of the algorithm. In other words, the authors want to design algorithms that are not only energy-efficient but also have a fast running time.|Distributed,Algorithms,Maximal,Independent,Set,Awake,Complexity,Random,Graphs,Sleeping
6cae13c9-9af6-5461-8749-57dbe935f3a8|2013-12-25T01:21:17|2013|12|STREAMER: A distributed framework for incremental closeness centrality computation|||||||The problem addressed in this research is the efficient maintenance of closeness centrality (CC) scores in dynamic networks. The context that makes this problem important is the widespread use of networks to model various systems, such as social interactions, web pages, and traffic patterns. In these networks, nodes have varying levels of importance, and CC scores quantify the relative importance of each node. However, when the network changes, the CC scores need to be updated, which is a computationally expensive task. The key objective of this research is to develop an efficient method for incrementally maintaining CC scores in dynamic networks. The authors aim to achieve this by designing a distributed framework that can quickly update CC scores in response to changes in the network, without having to recompute them from scratch. This is crucial for applications that require real-time analysis of network dynamics, such as power grid contingency analysis and social network analysis. Overall, the goal is to provide a scalable and efficient solution for maintaining CC scores in dynamic networks, enabling faster and more accurate analysis of network behavior.|Closeness Centrality,Incremental Computation,Dynamic Networks,Distributed Framework,STREAMER,Network Analysis,Centrality Metrics,Graph Algorithms,Parallel Computing,Scalability
ce983682-f937-5583-9005-500bab52879c|2021-06-23T20:31:16|2021|6|Distributed Coloring for Everywhere Sparse Graphs|||||||The problem addressed in this research is graph coloring, a central problem in distributed graph algorithms with applications in networks and distributed systems. The context that makes this problem important is the need for efficient algorithms that can color graphs with a minimal number of colors, particularly in sparse graphs where not all nodes have high degrees. The key objective of this research is to develop simple randomized distributed algorithms that can color graphs with fewer colors than the maximum degree of the graph, while still achieving a fast running time of O(log n) rounds. The authors aim to make progress on an open problem in the field, which is to use significantly fewer than 2 colors and still stay within deterministic O(log n) time.|Coloring,Distributed,Graph,Arboricity,Algorithm,Randomized,Deterministic,Rounds,Complexity,Time
b5e2f9cd-9696-51b1-8b91-f6760b863ece|2021-04-13T22:06:44+00:00|2021|4|Distributed Memory Graph Coloring Algorithms for Multiple GPUs|||||||The problem definition addressed in this research is the graph coloring problem, specifically focusing on distance-1 and distance-2 coloring. The context that makes this problem important is its wide range of applications in various fields, including computer science, operations research, and engineering. Graph coloring is used to model various problems, such as scheduling, resource allocation, and circuit design, where vertices represent entities, and edges represent conflicts or constraints between them. The goal is to assign colors to vertices such that adjacent vertices have different colors, minimizing the total number of colors used. The key objectives or goals the authors set to address this problem are: 1. To develop an efficient parallel algorithm for graph coloring that can handle large-scale graphs. 2. To minimize the number of colors used in the coloring, which is an NP-hard optimization problem. 3. To achieve good weak scaling behavior on multiple GPUs, enabling the solution of massive graphs. Overall, the authors aim to provide a scalable and efficient solution to the graph coloring problem, which is essential for various applications and has significant implications for performance and resource utilization.|Graph coloring,Distributed memory,Multiple GPUs,Parallel algorithms,Coloring problem,Distance-1 coloring,Distance-2 coloring,Speculative approach,Iterative approach,Scalability
702d43dd-9884-54f3-9d6d-396b7e7daa11|2020-06-23T18:41:37+00:00|2020|6|GraphWalker: An I/O-Efficient and Resource-Friendly Graph Analytic System for Fast and Scalable Random Walks|||||||The problem definition addressed in this research revolves around the efficient processing of massive random walks on large-scale graphs, which is a fundamental operation in various graph-based applications, such as social network analysis, recommendation systems, and graph-based machine learning models. The context that makes this problem important is the rapid growth of graph data, which has led to a significant increase in the number of random walks required to analyze these graphs, resulting in high computational costs and memory requirements. The key objectives or goals the authors set to address this problem are: 1. **Scalability**: To develop a system that can efficiently process massive random walks on large-scale graphs, handling tens of billions of walks and thousands of steps per walk. 2. **Efficient I/O utilization**: To minimize the number of I/O operations required to process random walks, reducing the overhead of loading and storing graph data. 3. **High walk updating rate**: To maximize the number of walks that can be updated in each iteration, ensuring that the system can efficiently process a large number of walks. To achieve these objectives, the authors propose a novel system called GraphWalker, which adopts a state-aware graph loading approach, asynchronous walk updating, and lightweight walk management to efficiently process massive random walks on large-scale graphs.|GraphWalker,Random Walks,Graph Analytic System,I/O Efficient,Resource Friendly,Fast and Scalable,Graph Systems,Iteration-based Model,Disk I/O,Parallel Computing
257c00c0-89de-5268-8b90-131dd79205d6|2015-05-23T08:40:34|2015|5|BFS-4K: An Efficient Implementation of BFS for Kepler GPU Architectures|||||||The problem addressed in this research is the efficient parallelization of the Breadth-First Search (BFS) algorithm on Graphics Processing Units (GPUs) for large-scale graph processing. The context that makes this problem important is the increasing need to process massive graphs in various domains, such as engineering, finance, medicine, and scientific applications, which can involve millions of vertices. The traditional sequential BFS algorithm is inefficient for large graphs, and existing parallel solutions on GPUs often suffer from workload imbalance and are asymptotically less efficient than the fastest CPU implementations. The key objectives or goals of this research are to: Develop an efficient parallel BFS algorithm on GPUs that can handle large-scale graphs. Overcome the workload imbalance issue in existing parallel solutions. Achieve a computational complexity comparable to or better than the fastest CPU implementations. To address these objectives, the authors propose a novel parallel BFS algorithm that leverages the massively parallel architecture of GPUs, incorporating techniques such as dynamic virtual warps, edge discovery, and two-level exclusive prefix sum to manage the frontier propagation steps efficiently.|BFS (Breadth-First Search),GPU (Graphics Processing Unit),Parallel,Graph,Algorithm,Kepler,CUDA,Implementation,Performance,Optimization
4dacaca9-df5b-5e12-88a6-ce3bc7a6e814|2019-04-10T00:51:07+00:00|2019|4|Distributed Edge Connectivity in Sublinear Time|||||||The problem addressed in this research is the computation of the minimum cut in a distributed network, where nodes can communicate with each other in rounds, exchanging messages of size O(log n). The context is that of distributed graph algorithms, where the goal is to find the minimum cut in a network, which is a fundamental problem in graph theory and has applications in network reliability and connectivity. The authors aim to develop an efficient distributed algorithm to compute the minimum cut, with the objective of minimizing the number of rounds required to achieve this goal. Specifically, they aim to improve upon the existing O(n) bound for this problem in the CONGEST model, where n is the number of nodes in the network. The authors also consider the problem of finding a set of edges whose removal disconnects the graph, and require that every node knows which of its incident edges are in this set.|Distributed,Algorithm,Graph,Minimum,Cut,Connectivity,Network,Model,Complexity,Time
d99c679c-0bdc-5cd0-a780-b6a2387144eb|2023-07-07T14:19:39|2023|7|Engineering a Distributed-Memory Triangle Counting Algorithm|||||||The problem definition addressed in this research is triangle counting in large-scale graphs, which is a fundamental graph analysis problem. The context that makes this problem important is the increasing size and complexity of real-world graph datasets, such as social networks, web graphs, and biological networks, which require efficient algorithms to process and analyze. The key objective of this research is to develop scalable and efficient algorithms for triangle counting in distributed memory architectures, which can handle massive graph datasets. The authors aim to address the limitations of existing sequential and parallel algorithms, which are not designed to handle large-scale graphs and are often memory-bound or communication-intensive. Specifically, the authors set out to develop algorithms that can: 1. Scale to thousands of processing elements (PEs) to handle massive graph datasets. 2. Reduce memory requirements and communication overheads. 3. Provide accurate and efficient triangle counting results. By addressing these objectives, the authors aim to enable fast and efficient graph analysis on large-scale graphs, which is essential for various applications, such as social network analysis, web graph analysis, and biological network analysis.|Triangle counting,Distributed memory algorithm,Graph analysis,Clustering coefficient,MPI,Graph processing,Scalability,Parallel computing,Graph algorithms,Large-scale graphs
3d44812e-0506-5b82-b79f-32194c72258b|2017-08-10T13:05:22|2017|8|QFrag: Distributed Graph Search via Subgraph Isomorphism|Marco Serafini,Gianmarco De Francisci Morales,Georgos Siganos||||||The problem addressed in this research is the inefficiency of Bulk Synchronous Parallel (BSP) systems in handling straggler tasks, which are tasks that take significantly longer to complete than others. This problem is important because BSP systems are widely used in various applications, including graph processing, machine learning, and data analytics, where straggler tasks can lead to significant performance degradation and slow down the entire system. The key objective of this research is to develop a task scheduling technique that can efficiently handle straggler tasks and improve the overall performance of BSP systems. The authors aim to achieve this by developing a task fragmentation approach that breaks down tasks into smaller subtasks, detects and separates straggler subtasks, and redistributes them across workers to balance the load and minimize the impact of straggler tasks. In summary, the problem addressed in this research is the inefficiency of BSP systems in handling straggler tasks, and the key objective is to develop a task scheduling technique that can efficiently handle straggler tasks and improve the overall performance of BSP systems.|Graph,Search,Subgraph,Isomorphism,Distributed,Query,Algorithm,Embedding,Enumeration,Fragmentation
e48c6462-8b4a-55b3-88d9-5d12792ee968|2019-08-20T11:37:00|2019|8|HitGraph: High-throughput Graph Processing Framework on FPGA|||||||The problem addressed in this research is the optimization of graph processing on edge-centric accelerators, which are specialized hardware designed to accelerate graph processing workloads. The context that makes this problem important is the increasing significance of graph processing in various domains, such as social networks, recommendation systems, and data analytics, which demands high-performance and energy-efficient processing. The key objectives or goals the authors set to address this problem are: To develop an optimized framework for graph processing on edge-centric accelerators, which can efficiently utilize the hardware resources and minimize memory accesses. To design a scalable and flexible architecture that can accommodate various graph algorithms and sizes. To reduce the power consumption and improve the throughput of graph processing on edge-centric accelerators. To achieve these objectives, the authors propose a novel framework that incorporates algorithmic optimizations, data layout optimization, and a design automation tool to generate optimized accelerators for various graph algorithms.|Graph,Processing,FPGA,Edge-centric,Optimization,Memory,Performance,Algorithm,Analytics,Acceleration
ea65c626-5b30-5d54-a43a-0cdc383396c1|2024-01-11T17:04:23|2024|1|Sublinear-Time Quantum Computation of the Diameter inCONGEST Networks|||||||The problem definition addressed in this research revolves around the challenges of designing and optimizing the layout of a warehouse, specifically focusing on the storage and retrieval of items. The context that makes this problem important is the increasing demand for efficient warehouse operations due to the growth of e-commerce and the need for companies to reduce costs and improve customer satisfaction. The authors aim to address this problem by developing a mathematical model and solution approach that can effectively determine the optimal storage and retrieval policies for a warehouse, taking into account factors such as storage capacity, retrieval time, and labor costs. The key objectives of this research are to minimize the total cost of warehouse operations, reduce retrieval time, and improve the overall efficiency of the warehouse.|3D,3D reconstruction,3D modeling,3D scanning,3D printing,3D visualization,3D rendering,3D animation,3D graphics,3D computer vision
c0f3ea48-395b-577b-a1e3-2e846ec2f6fa|2014-03-25T01:00:05+00:00|2014|3|Scalable Big Graph Processing in MapReduce|||||||The problem addressed in this research is the development of a scalable graph processing algorithm in MapReduce, a popular framework for big data processing. The context is that graph processing is a crucial task in many applications, but existing algorithms are not scalable or efficient, leading to high communication costs and slow processing times. The authors aim to define a new class of graph algorithms, called Scalable Graph Processing Class (SGC), that can achieve high scalability, stability, and robustness in MapReduce. The key objectives are to minimize communication cost, reduce the number of MapReduce rounds, and ensure that the algorithm can be speeded up by adding more machines. The authors also aim to develop a unified graph processing system that can handle various graph operations, including node and edge tables, and support iterative algorithms. Overall, the goal is to provide a scalable and efficient solution for graph processing in MapReduce, which is essential for many big data applications.|MapReduce,Scalable,Graph,Processing,Big,Data,Cloud,Computing,Algorithm,Class
7deed2f4-ab43-58ce-a26e-70266636f14b|2022-07-29T07:44:50|2022|7|Distributed PageRank computation with improved round complexities|Siqiang Luo||||||The problem definition addressed in this research is the efficient computation of PageRank in a distributed setting, specifically in the congested clique model. The context that makes this problem important is the increasing need for distributed computation in various applications, such as spatial databases, road network systems, and graph systems. The congested clique model is a fundamental model in distributed computation, where nodes communicate with each other via message passing in synchronous rounds. The key objective of this research is to design an efficient algorithm for computing PageRank in the congested clique model, with a focus on minimizing the round complexity (i.e., the number of communication rounds) and bandwidth (i.e., the size of messages exchanged between nodes). The authors aim to achieve a better trade-off between these two efficiency measures, which is crucial for large-scale distributed systems. In particular, the authors set out to improve upon existing algorithms, such as the IPRA algorithm, which has a round complexity of O(log n) and a bandwidth of O(log n)^3 bits. The goal is to design an algorithm that can estimate PageRank values with a relative error of O(1/log n) in fewer rounds and with a smaller bandwidth, making it more suitable for real-world distributed systems.|PageRank,Distributed Computation,Congested Clique Model,Graph Algorithms,Random Walks,Distributed PageRank,Communication Rounds,Relative Error,Algorithm Design,Graph Processing
09d1720a-ea43-5e86-b562-6eabaaa9fdef|2017-05-25T13:47:59|2017|5|Distributed MIS via All-to-All Communication|Mohsen Ghaffari||||||The problem addressed in this research is the computation of a Maximal Independent Set (MIS) in the CONGESTED CLIQUE model, a distributed computing model where all-to-all communication is allowed. The context that makes this problem important is the need for efficient algorithms in distributed systems, where locality limitations are a major challenge. The authors aim to improve upon existing algorithms that have a round complexity of O(log n) and explore the possibility of going below the locality-based lower bounds. The key objective is to design an algorithm that can compute an MIS in O(log log n) rounds, which would be a significant improvement over existing algorithms.|Distributed,Algorithm,Graph,Model,Congested,Clique,Independent,Set,Maximal,Computation
f2ddd3cd-0f78-5562-b3bb-577f20a3322d|2019-04-30T12:58:15|2019|4|Vertex-Centric Graph Processing: Good, Bad, and the Ugly|||||||The problem definition addressed in this research revolves around the limitations and inefficiencies of vertex-centric graph processing algorithms, particularly in the context of distributed graph processing. The background that makes this problem important is the increasing need to process large-scale graphs in various domains, such as social networks, web graphs, and biological networks. Existing vertex-centric algorithms, which are widely used in distributed graph processing, often suffer from imbalanced workloads, high message complexity, and large numbers of iterations, leading to poor performance and scalability issues. The key objectives or goals of this research are to: 1. Investigate the limitations and inefficiencies of vertex-centric graph processing algorithms. 2. Analyze the complexity of various vertex-centric algorithms and identify those that do not meet the desired performance and scalability requirements. 3. Develop a framework to evaluate the efficiency of vertex-centric algorithms, focusing on the time-processor product as a metric to measure their performance. 4. Identify graph workloads and algorithms that are difficult to express in the vertex-centric framework, highlighting important research directions. By addressing these objectives, the authors aim to provide a deeper understanding of the limitations of vertex-centric graph processing algorithms and to guide the development of more efficient and scalable algorithms for distributed graph processing.|Vertex-centric,Graph algorithms,Distributed graph processing,Pregel,Computational complexity,Time-processor product,Balanced practical Pregel algorithms,Graph processing frameworks,Iterative algorithms,Distributed systems
5281aa7b-edb2-5d13-8753-ad91c3a408ef|2020-04-14T18:03:07+00:00|2020|4|G-thinker: A Distributed Framework for Mining Subgraphs in a Big Graph|||||||The problem definition addressed in this research is the efficient and scalable mining of subgraphs in large graphs, which is a fundamental problem in graph data analysis. The context that makes this problem important is the increasing availability of large-scale graph data in various domains, such as social networks, biological networks, and web graphs, which requires efficient algorithms to extract valuable insights and patterns. The key objectives or goals the authors set to address this problem are: To design a scalable and efficient subgraph mining algorithm that can handle large graphs with millions of vertices and edges. To minimize the input/output (IO) cost and maximize the CPU utilization, making the algorithm CPU-bound rather than IO-bound. To develop a framework that can be applied to various subgraph mining problems, such as clique, triangle, and subgraph matching. Overall, the authors aim to develop a scalable and efficient subgraph mining algorithm that can handle large graphs and extract valuable insights and patterns, which is essential for various applications in data analysis and machine learning.|Subgraph,Mining,Graph,CPU-bound,Compute-intensive,Clique,Triangle,Subgraph-centric,Distributed,Scalability
8cee156f-b943-5c40-a323-0e9378d6bb6b|2023-01-18T22:05:53+00:00|2023|1|Khuzdul: Efficient and Scalable Distributed Graph Pattern Mining Engine|Jingji Chen,Xuehai Qian||||||The problem definition addressed in this research is the efficient and scalable distributed graph pattern mining, which is a crucial task in various applications such as social network analysis, recommendation systems, and bioinformatics. The context that makes this problem important is the increasing size and complexity of graph-structured data, which poses significant challenges to existing graph mining systems in terms of scalability, performance, and memory usage. The key objectives or goals the authors set to address this problem are: 1. To design a distributed graph pattern mining engine that can efficiently process large-scale graphs and scale to thousands of machines. 2. To minimize the communication overhead and memory usage, which are major bottlenecks in existing systems. 3. To develop a flexible and adaptive scheduling strategy that can effectively utilize the computation resources and balance the workload across machines. To achieve these objectives, the authors propose a novel distributed graph pattern mining engine called Khuzdul, which employs a fine-grained task scheduling strategy, an extendable embedding abstraction, and a hierarchical caching mechanism to optimize the performance and scalability of graph pattern mining.|Graph,Pattern,Mining,Distributed,Engine,Efficient,Scalable,Embedding,Enumeration,Computation
b5ad8214-7885-571f-bb8e-d6e57eb8e8ab|2018-03-09T04:49:08|2018|3|G-Miner: An Efficient Task-Oriented Graph Mining System|Hongzhi Chen,Miao Liu,Yunjian Zhao,Xiao Yan,Da Yan,James Cheng||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph mining tasks in a distributed computing environment. The context that makes this problem important is the increasing availability of massive graph-structured data, which has led to a growing need for scalable and efficient graph mining algorithms to extract valuable insights from these datasets. However, traditional graph mining approaches are often limited by their computational complexity, memory requirements, and communication overhead, making them unsuitable for large-scale graphs. The key objectives or goals the authors set to address this problem are: 1. To design a scalable and efficient distributed graph mining framework that can handle massive graphs. 2. To minimize the communication overhead and memory requirements associated with graph mining tasks. 3. To develop a task pipeline that can effectively overlap computation and communication, ensuring optimal resource utilization. By achieving these objectives, the authors aim to enable fast and efficient graph mining on large-scale graphs, facilitating the extraction of valuable insights and patterns from these datasets.|Graph Mining,Distributed System,Large Scale Graph,Task Oriented,Graph Processing,CPU Utilization,Memory Consumption,Graph Algorithms,Subgraph Mining,Parallel Processing
d7af3f82-0e66-5783-a356-cdf34c445377|2018-05-16T21:38:55|2018|5|Possibilities and Impossibilities for Distributed Subgraph Detection|Orr Fischer,Tzlil Gonen,Fabian Kuhn,Rotem Oshman||||||The problem addressed in this research is the subgraph detection problem, also known as subgraph freeness, in the context of distributed computing. The goal is to determine whether a given network graph contains a copy of a fixed subgraph H as a subgraph or not. This problem is important because it is an extremely local problem, and solving it efficiently is crucial for various applications in distributed computing. The authors aim to better understand the fundamental questions regarding the subgraph freeness problem, including solving C2k detection in sublinear time and improving lower bounds on triangle detection. They also seek to provide a separation between the CONGEST and LOCAL models, nearly the largest possible, by showing that some graphs require nearly quadratic running time to detect in the CONGEST model.|Subgraph,Detection,Distributed,Algorithms,Lower,Bounds,Congest,Model,Network,Complexity
dcce33ff-a6a5-5563-85f3-4efa2a4863c6|2022-02-17T11:40:01|2022|2|Distributed CONGEST Approximation of Weighted Vertex Covers and Matchings|||||||The problem definition addressed in this research is the distributed approximation of weighted vertex covers and matchings in graphs. The context that makes this problem important is the increasing need for efficient distributed algorithms to solve graph optimization problems in large-scale networks, such as social networks, communication networks, and the internet. The minimum weighted vertex cover (MWVC) and maximum weighted matching (MWM) problems are fundamental optimization problems in graph theory, and their distributed solutions have numerous applications in network optimization, resource allocation, and data analysis. The key objectives or goals of the authors are to develop efficient distributed algorithms that can approximate the MWVC and MWM problems with a good approximation ratio, while minimizing the number of communication rounds required to solve the problems. Specifically, the authors aim to design algorithms that can achieve an approximation ratio below 2 in the CONGEST model, which is a widely used model for distributed graph algorithms. The authors also aim to explore the limitations of distributed algorithms for these problems, including the trade-offs between approximation ratio and communication rounds. Overall, the research aims to contribute to the development of efficient and scalable distributed algorithms for solving graph optimization problems in large-scale networks.|Distributed,Graph,Algorithms,Vertex,Cover,Minimum,Weighted,Approximation,CONGEST,Model
7334f68d-fe0e-5a8f-ad8a-26476a8a3cb7|2015-10-09T21:29:06|2015|10|PL2AP: Fast Parallel Cosine Similarity Search|||||||The problem definition addressed in this research is the All-Pairs Similarity Search (APSS) problem, which involves finding, for each object in a set, all other similar objects with a similarity value above a certain threshold. The context that makes this problem important is its widespread application in various domains, including clustering, online advertising, recommender systems, near-duplicate document detection, and query refinement. The key objective of this research is to develop efficient parallel algorithms to solve the APSS problem, particularly in the context of high-dimensional sparse data, which is common in many real-world applications. The authors aim to achieve significant speedups over existing serial and parallel methods, while ensuring the accuracy and scalability of their approach.|Similarity,Search,Parallel,Cosine,APSS,AllPairs,Sparse,Multi-core,Inverted,Index
228fc41d-0106-5ad9-a006-84c15d23f9a9|2024-08-14T14:41:14.894332|2024|8|Distributed D-core Decomposition over Large Directed Graphs|||||||The problem addressed in this research is the distributed D-core decomposition of large-scale directed graphs. The context that makes this problem important is the increasing need to analyze and understand the structure of large-scale networks, such as social networks, communication networks, and financial networks, which are often represented as directed graphs. The authors aim to develop efficient algorithms for distributed D-core decomposition, which is a fundamental problem in graph analysis. The key objectives of this research are to: (1) define a concept called anchored coreness, which is a pair of values representing the maximum in-degree and out-degree of a vertex in a D-core; (2) propose a new H-index-based algorithm for distributed D-core decomposition; and (3) develop a novel concept called skyline coreness, which is a pair of values representing the maximum in-degree and out-degree of a vertex in a D-core, and design an algorithm to compute skyline corenesses distributedly. The authors aim to achieve efficient and scalable algorithms for distributed D-core decomposition, which can handle large-scale graphs and provide insights into the structure and properties of these networks.|D-core,decomposition,distributed,graph,algorithm,coreness,skyline,anchored,computation,framework
e7e98e43-7120-532e-8a59-82d95efba671|2016-11-01T23:25:35|2016|11|Scalable distributed subgraph enumeration|||||||The problem definition addressed in this research is the subgraph enumeration problem in a distributed environment. The context that makes this problem important is the increasing need to process large-scale graph data in various applications, such as social networks, biological networks, and knowledge graphs. The traditional centralized approach is no longer efficient due to the massive size of these graphs, and distributed processing is necessary. The key objective of this research is to develop an efficient algorithm for subgraph enumeration in a distributed environment. The authors aim to address the following goals: 1. To design an optimal execution plan for solving subgraph enumeration, which minimizes the total cost of processing. 2. To develop a pattern decomposition strategy that can effectively reduce the cost of subgraph enumeration. 3. To propose a join plan that can efficiently process the decomposed patterns in a distributed environment. Overall, the authors aim to provide a scalable and efficient solution for subgraph enumeration in large-scale graph data, which is essential for various applications in data mining, machine learning, and graph analytics.|Subgraph,Enumeration,Distributed,Algorithm,Graph,Pattern,Matching,Join,Optimization,Scalability
c76abc26-664d-57ba-8506-f9e244909f3c|2018-06-27T16:07:28|2018|6|The Distributed Minimum Spanning TreeProblem|||||||The problem addressed in this research is the distributed Minimum Spanning Tree (MST) problem, a fundamental problem in graph theory and network algorithms. The context that makes this problem important is its application in communication networks, where an MST can serve as a backbone for efficient communication, optimizing network performance parameters such as transmission delays and communication costs. The key objective of this research is to design and analyze distributed algorithms that can efficiently compute an MST in a network, with the goal of achieving singular optimality, i.e., optimal time and message complexity simultaneously. The authors aim to improve upon existing algorithms that achieve either optimal time or optimal message complexity, but not both, and to provide a deeper understanding of lower bounds in distributed network algorithms.|Distributed,Algorithms,Minimum,Spanning,Tree,Graph,Network,Complexity,Time,Message
a498fff3-b5e7-57ba-a394-e166a062733e|2016-08-31T09:51:58|2016|8|NScaleSpark: Subgraph-centric Graph Analytics onApache Spark|pinetec||||||The problem definition addressed in this research revolves around the limitations of vertex-centric graph processing frameworks in handling complex graph analysis tasks, particularly those involving neighborhood-centric analysis. The context that makes this problem important is the increasing need to analyze large-scale graphs, which are ubiquitous in various domains such as social networks, web graphs, and biological networks. The key objectives or goals the authors set to address this problem are: To develop a system that can efficiently process neighborhood-centric graph analysis tasks, which are critical in many applications, such as identifying structural holes, brokerage analysis, counting motifs, and link prediction. To overcome the limitations of vertex-centric frameworks, which are ill-suited for these tasks due to their focus on individual vertices rather than neighborhoods. To design a system that can scale to handle large graphs, which are often too large to fit in memory, and require distributed processing. By addressing these objectives, the authors aim to provide a more efficient and scalable solution for neighborhood-centric graph analysis tasks, which are essential in many real-world applications.|Graph,Neighborhood,Centric,Analysis,Subgraph,Extraction,Query,PageRank,Recommendation,Framework
b47576a5-a3fa-5f9c-8ece-1953a9f6653c|2021-07-27T15:02:14|2021|7|FlexMiner: A Pattern-Aware Accelerator for Graph Pattern Mining|Xuhao Chen||||||The problem definition addressed in this research is Graph Pattern Mining (GPM), which involves finding all subgraphs in a large data graph that are isomorphic to a given pattern graph. The context that makes this problem important is the increasing availability of large-scale graph data in various domains, such as social networks, biological networks, and web graphs, which can provide valuable insights and knowledge when mined effectively. However, the massive combinatorial search space and expensive graph isomorphism tests make GPM computationally intensive, even on moderate-sized graphs. The key objectives or goals of this research are to: Develop a scalable and efficient solution for GPM problems, which can handle large graphs and patterns. Improve the performance of GPM solvers by exploiting parallelism and reducing memory latency. Design a flexible and programmable hardware architecture that can support various GPM problems and patterns. Overall, the authors aim to address the computational challenges of GPM and provide a scalable and efficient solution that can unlock the potential of graph data in various domains.|Graph Pattern Mining (GPM),Accelerator,Pattern Aware,Software-Hardware Co-Design,Graph,Pattern,Mining,Subgraph,Enumeration,Search Tree
044fa5a2-05cb-5d7c-be00-5f020191bdd4|2010-12-02T07:01:35|2010|12|dMaximalCliques: A Distributed Algorithm for Enumerating All Maximal Cliques and Maximal Clique Distribution|||||||The problem addressed in this research is the enumeration of all maximal cliques in a graph, which is a fundamental problem in graph theory. The context that makes this problem important is the increasing size of real-world graphs, such as social networks, which makes it difficult to conduct analysis using existing sequential algorithms due to computation and memory limitations. The key objective of this research is to design a distributed algorithm that can efficiently enumerate all maximal cliques in large-scale graphs, with the goal of obtaining clique information from million-node graphs within a few minutes on an 80-node computer cluster.|Maximal Cliques,Distributed Algorithm,Graph Theory,Social Graphs,Community Detection,Maximal Clique Distribution,Lognormal Distribution,Graph Partition,Distributed Computing,Clique Detection
566ec10f-f322-589e-83a4-a6330c17df51|2022-03-03T04:32:49|2022|3|DisGCo: A Compiler for Distributed Graph Analytics|||||||The problem addressed in this research is the efficient translation of Green Marl, a high-level programming language for graph analytics, to MPI (Message Passing Interface), a widely used parallel programming model. The context that makes this problem important is the increasing need for scalable and efficient graph analytics in various fields, and the limitations of existing graph processing systems in handling large-scale graphs. The authors aim to address this problem by developing a compiler, DisGCo, that can efficiently translate Green Marl programs to MPI code, while ensuring correctness, scalability, and performance. The key objectives of this research are to: (1) develop a compiler that can translate Green Marl programs to MPI code, (2) ensure the correctness of the translated code, (3) optimize the performance of the translated code, and (4) evaluate the scalability of the translated code on large-scale graphs.|DisGCo,Green Marl,MPI,Graph algorithms,Distributed graph processing,Parallel computing,Code generation,Optimization,Scalability,Performance
08f7fd69-4f54-57b2-a5b9-f84c056ab92f|2017-03-28T17:29:32|2017|3|Parallelizing Sequential Graph Computations|||||||The problem definition addressed in this research revolves around efficiently processing graph queries in a distributed computing environment. The context that makes this problem important is the increasing scale and complexity of graph-structured data, which is becoming a bottleneck for many applications. The authors identify that existing solutions are limited by their reliance on sequential algorithms, which are not optimized for distributed computing and lead to high computational costs and slow query response times. The key objectives or goals the authors set to address this problem are: 1. To develop a parallel graph query processing framework that can efficiently process graph queries in a distributed computing environment. 2. To minimize the communication overhead and iterative recomputation inherent in incremental graph query processing. 3. To support a wide range of graph queries and algorithms, while ensuring correctness and scalability. Overall, the authors aim to provide a scalable and efficient solution for processing graph queries in distributed computing environments, which is essential for many applications that rely on graph-structured data.|Graph,Parallel,GRAPE,Query,Algorithm,Partition,Incremental,Computation,Optimization,Parallelization
2d82f26e-2273-5778-bdce-8dc66e0c359c|2018-10-23T20:08:56+00:00|2018|10|Efﬁcient Distributed Random Walks with Applications|||||||The problem addressed in this research is the efficient computation of a random spanning tree (RST) in a distributed network. The context that makes this problem important is the need for decentralized algorithms that can efficiently compute global metrics of a network, such as the mixing time, spectral gap, and conductance. These metrics are crucial for understanding the network's connectivity and expansion properties. The authors' objective is to design a fast distributed algorithm for computing an RST, which can be used as a building block for computing these global metrics. The key goal is to improve upon the existing O(n) time complexity of RST algorithms, which is not efficient for large-scale networks. The authors aim to achieve a faster running time, specifically O(2^(3D/3) * log n), where D is the network diameter.|Random walks,Distributed algorithms,Decentralized computation,Network applications,Random sampling,Distributed computing,Sublinear time,Mixing time,Random spanning tree,Distributed random walk algorithms
4e2705f2-6f63-5264-9aa0-78defe46b86a|2022-03-03T04:54:58|2022|3|An Efficient Index-Based Approach to Distributed Set Reachability on Small-World Graphs|||||||The problem definition addressed in this research is the efficient processing of set reachability queries in distributed graph systems. The context that makes this problem important is the increasing scale and complexity of graph-structured data, which is common in many applications such as social networks, web graphs, and biological networks. In this context, set reachability queries, which aim to find all reachable pairs of vertices between a set of source vertices and a set of target vertices, are a fundamental operation. However, processing these queries efficiently in a distributed environment is challenging due to the need to balance computation and communication costs. The key objectives or goals of this research are to design an efficient indexing approach that can reduce both computation and communication costs, and to develop a query algorithm that can accurately answer set reachability queries in a distributed graph system. The authors aim to achieve these objectives by proposing a novel indexing structure called ML2hop, which is designed to minimize the number of message exchanges between partitions and reduce the computation cost of local queries.|Reachability,Graph,Indexing,Query,Distributed,2-Hop,Labeling,Partitioning,Scalability,Efficiency
9c4e2d87-5bce-5a63-b8b4-293f2b3ebc41|2019-05-27T00:00:44+00:00|2019|5|Improved Distributed Expander Decomposition and Nearly Optimal Triangle Enumeration|Yi-Jun Chang,Thatchaphol Saranurak||||||The problem addressed in this research is the efficient distributed computation of triangle enumeration and expander decomposition in massive graphs, particularly in the Congested Clique model. The context that makes this problem important is the increasing need to process large-scale graphs in various applications, such as social networks, web graphs, and biological networks, where the graph size exceeds the memory capacity of a single machine. Distributed computing is essential to handle such massive graphs, but it poses significant challenges due to communication constraints. The key objective of this research is to develop efficient distributed algorithms for triangle enumeration and expander decomposition, which are fundamental graph problems. Specifically, the authors aim to design algorithms that can solve these problems in a small number of rounds, ideally in O(1) or O(log n) rounds, where n is the number of vertices in the graph. This is crucial because the number of rounds directly affects the communication overhead and, consequently, the overall computation time. In the Congested Clique model, each vertex can communicate with every other vertex, but the bandwidth constraint limits the amount of information that can be exchanged in each round. The authors' goal is to develop algorithms that can efficiently utilize this communication model to solve triangle enumeration and expander decomposition problems, which are essential for various graph analytics tasks.|Expander Decomposition,Distributed Algorithms,Graph Partitioning,Triangle Enumeration,Congested Clique,Low-Diameter Decomposition,Nearly Optimal,Sparse Cut,Graph Decomposition,Distributed Computing
b5df9f40-dcf3-565e-ba7b-7128203ad3e2|2016-06-12T04:46:00|2016|6|PTE: Enumerating Trillion Triangles On DistributedSystems|||||||The problem definition addressed in this research is the enumeration of triangles in massive graphs, which is a fundamental task in graph data analysis. The context that makes this problem important is the increasing size and complexity of real-world networks, such as social networks and the web, which have millions or billions of vertices and edges. This scale poses significant challenges to existing algorithms, making it difficult to process and analyze these networks efficiently. The key objective of this research is to develop a scalable and efficient algorithm for triangle enumeration that can handle massive graphs. The authors aim to overcome the limitations of existing algorithms, which fail to process large graphs due to massive intermediate data and high computational costs. Specifically, the authors set out to design an algorithm that can enumerate triangles in a distributed manner, reducing the amount of shuffled data and improving the performance and scalability of the algorithm. Overall, the problem definition is critical in graph data analysis, and the authors' objectives are focused on developing a practical solution that can efficiently process massive graphs, enabling various applications such as identifying suspicious users, detecting web spams, and finding communities.|Triangle,Enumeration,Distributed,Algorithm,Graph,Big,Data,MapReduce,Scalable,Network
e1c4d452-15ee-5baf-9057-47d7ad0c4ac6|2020-04-24T08:43:41|2020|4|Label Propagation-Based Parallel Graph Partitioning for Large-Scale Graph Data|||||||The problem definition addressed in this research is the balanced graph partitioning problem, which is a critical issue in distributed parallel processing. The context that makes this problem important is the need to efficiently process large-scale graphs in distributed computing environments, where the graph is divided into smaller subgraphs and processed in parallel across multiple machines. However, the partitioning of the graph can significantly impact the performance of the distributed processing, and an unbalanced partitioning can lead to poor performance, increased communication overhead, and decreased scalability. The key objective of this research is to develop an efficient and scalable graph partitioning algorithm that can achieve a balanced partitioning of the graph, minimizing the edge cut while ensuring that the size of each partition is balanced. The authors aim to address this problem by proposing a novel approach that combines the benefits of heuristic and multilevel approaches, with the goal of achieving a better balance between the quality of the partitioning and the computational efficiency of the algorithm.|Graph Partitioning,Balanced Graph Partitioning,Heuristic Approaches,Multilevel Approach,Label Propagation,Local Search Algorithm,Graph Clustering,Edge Cut,Vertex Balance,Scalability
0bc5abde-656d-5874-87bc-7abb8f83cff4|2018-10-05T05:25:16|2018|10|CASS: A distributed network clustering algorithm based on structure similarity for large-scale network|Jungrim Kim,Mincheol Shin,Jeongwoo Kim,Chihyun Park,Sujin Lee,Jaemin Woo,Hyerim Kim,Dongmin Seo,Seokjong Yu,Sanghyun Park||||||The problem addressed in this research is the need for an efficient and scalable network clustering algorithm that can handle large-scale network data. The context is that network data is rapidly increasing in size, and conventional clustering algorithms are not suitable for operation in distributed systems, leading to memory problems and huge computation times. The authors aim to develop a distributed network clustering algorithm based on structure similarity that can efficiently analyze large-scale network data. The key objectives are to propose a novel algorithm that can handle large-scale network data, evaluate its performance on various network datasets, and demonstrate its effectiveness in identifying meaningful clusters.|Clustering,Network,Algorithm,Apache,Spark,Structure,Similarity,Optimization,Scalability,Biological
d5617d85-7698-5a36-a302-4a6864c67aae|2015-07-20T09:07:42|2015|7|S2X: Graph-Parallel Querying of RDF withGraphX|||||||The problem definition addressed in this research revolves around efficiently processing and querying large-scale RDF (Resource Description Framework) data, which is a standard for modeling and exchanging data on the web. The context that makes this problem important is the growing amount of RDF data, driven by initiatives like Schema.org, which requires distributed solutions to store and query it. The key objective of this research is to develop a system that can seamlessly combine the benefits of data parallel frameworks (e.g., Spark) and graph parallel computation (e.g., GraphX) to efficiently process and query large-scale RDF data. The authors aim to bridge the gap between the record-centric view of data parallel frameworks and the graph parallel computation of specialized systems, enabling new applications that can leverage the strengths of both approaches.|GraphX,SPARQL,RDF,Hadoop,Spark,Graph parallel,Data parallel,Distributed querying,Graph abstraction,In-memory cluster computing
591c1fa4-6885-574f-9ec4-4f7533346e25|2022-11-08T14:59:15|2022|11|Distributed Maximal Matching and|||||||The problem addressed in this research is the lower bound for the round complexity of locally checkable problems on hypergraphs in the port numbering model. The context that makes this problem important is the need to understand the fundamental limits of distributed computing, particularly in the context of hypergraphs, which are increasingly used to model complex systems in various domains. The key objective of this research is to develop a new approach to prove lower bounds for the round complexity of locally checkable problems on hypergraphs, which has been a challenging task due to the complexity of hypergraphs. The authors aim to overcome the limitations of previous approaches and provide a more general and powerful framework for proving lower bounds. Specifically, the authors focus on the hypergraph maximum matching (MM) problem, which is a fundamental problem in distributed computing, and aim to prove a lower bound for its round complexity in the port numbering model. The goal is to develop a sequence of problems that are increasingly relaxed versions of the original problem, and to show that each problem in the sequence is not solvable in a certain number of rounds, ultimately leading to a lower bound for the round complexity of the original problem. Overall, the authors' objective is to make a significant contribution to the understanding of the fundamental limits of distributed computing on hypergraphs, and to provide a new approach that can be applied to a wide range of locally checkable problems on hypergraphs.|Hypergraph,Lower bound,Maximal matching,Round elimination,Distributed algorithm,CONGEST model,Graph,Coloring,Maximal independent set,Complexity theory
b00ce766-2716-5cc2-98ed-13375257a479|2021-05-12T18:23:17|2021|5|<sc>Trust</sc>: Triangle Counting Reloaded on GPUs|||||||The problem definition addressed in this research is the efficient counting of triangles in massive graphs, which is a fundamental task in graph analysis with numerous applications in anomaly detection, community detection, and robustness analysis. The context that makes this problem important is the rapid growth of graph sizes, making traditional serial algorithms inefficient and unable to handle large-scale graphs. The key objective of this research is to develop a scalable and efficient triangle counting algorithm that can handle massive graphs, with a specific goal of achieving a trillion triangle enumeration per second (TEPS) rate. The authors aim to overcome the limitations of existing algorithms, which are bottlenecked by memory access patterns, workload imbalance, and collision reduction, and to design a novel algorithm that can efficiently utilize GPU architecture to accelerate triangle counting.|Graph,Triangle,Listing,Parallel,Algorithm,Optimization,Reordering,Partitioning,Hashing,CUDA
a08ae348-5898-5ec6-8042-3eeee4b67e03|2019-01-07T02:34:52+00:00|2019|1|Improved Distributed Degree Splitting and Edge Coloring|Mohsen Ghaffari,Juho Hirvonen,Fabian Kuhn,Yannic Maus,Jukka Suomela,Jara Uitto||||||The problem addressed in this research is the degree splitting problem, which involves partitioning the edges of a graph into two parts such that the difference between the number of edges incident on each node in each part is at most some small discrepancy value. This problem is important in the context of distributed graph algorithms, where the goal is to find efficient algorithms that can be executed in a distributed manner by nodes in a network. The authors aim to develop a deterministic distributed algorithm that can solve the degree splitting problem in a constant number of rounds, with a focus on minimizing the discrepancy value. The problem is challenging because it requires finding a balanced partition of the edges around each node, and the authors aim to achieve this goal in a distributed setting where nodes have limited knowledge of the global graph structure.|Distributed,Algorithm,Graph,Orientation,Degree,Splitting,Path,Decomposition,Discrepancy,Complexity
c6d4a008-94b6-5a27-b3ac-cda7bf7456ca|2021-06-12T22:20:14|2021|6|Parallel Algorithms for Counting Triangles inNetworks with Large Degrees|arif||||||The problem addressed in this research is the efficient counting of triangles in massive networks, which is a fundamental problem in the analysis of complex networks. The context that makes this problem important is the increasing size and complexity of real-world networks, such as social networks, web graphs, and biological networks, which require scalable and efficient algorithms to analyze their structure and properties. The key objective of this research is to develop a parallel algorithm that can efficiently count triangles in large networks, with a focus on reducing communication overhead and improving scalability. The authors aim to achieve this by proposing a dynamic load balancing scheme, a new estimation function for task granularity, and a surrogate approach for reducing message redundancy. The goal is to improve the performance of triangle counting algorithms, making them more suitable for large-scale network analysis.|Triangle,Counting,Parallel,Algorithm,Network,Space,Ef cient,Massive,Distributed,Scalable
479cec88-0c7e-5ce0-b0ce-bc3da9388715|2014-08-06T02:39:51|2014|8|Distributed Maximal Clique Computation|||||||The problem definition addressed in this research is the computation and update maintenance of maximal cliques in a graph. Maximal cliques are important substructures in graph analysis, and their computation is a fundamental problem in graph theory and computer science. The context that makes this problem important is the increasing size and complexity of real-world graphs, which makes it challenging to compute and update maximal cliques efficiently. The authors' key objectives are to develop efficient algorithms for computing maximal cliques and updating them when the underlying graph is updated. They aim to achieve this by proposing a new vertex ordering method, called core number ordering, which can reduce the time complexity of maximal clique computation, and by developing algorithms for incremental update maintenance of maximal cliques. The authors also aim to verify the efficiency of their algorithms through experiments on real-world graphs.|Maximal Cliques,Graphs,Algorithms,Distributed Computing,Big Data,MapReduce,Hadoop,Vertex Ordering,Degeneracy,Core Number
b82117d0-cb52-5f21-8e34-ef6b858f6616|2021-02-09T08:16:19|2021|2|A Survey on Distributed Graph Pattern Matching in Massive Graphs|||||||The problem definition addressed in this research revolves around the efficient processing of graph simulation queries in large-scale graph datasets. The context that makes this problem important is the increasing prevalence of graph-structured data in various domains, such as social networks, biological networks, and knowledge graphs, which necessitates the development of scalable and efficient query processing techniques. The key objective of this research is to design and develop distributed algorithms that can efficiently process graph simulation queries in large-scale graph datasets. The authors aim to address the challenges of scalability, efficiency, and load balancing in distributed graph processing, with a focus on graph simulation queries that involve complex constraints and patterns. Specifically, the authors seek to develop algorithms that can: Scale to large graph datasets with millions of vertices and edges. Efficiently process graph simulation queries with complex constraints and patterns. Achieve good load balancing and minimize communication overhead in distributed environments. By addressing these objectives, the authors aim to provide a scalable and efficient solution for graph simulation query processing, which can facilitate various applications, such as graph pattern matching, graph clustering, and graph-based data mining.|Graph Pattern Matching (GPM),Subgraph Isomorphism,Query Optimization,Join Ordering,Graph Exploration,Data Graph,Query Graph,Embedding Enumeration,Intermediate Results,Optimization Techniques
83dd1402-adc2-5705-9e4a-b439fab406ac|2016-05-03T00:46:23+00:00|2016|5|Relationship Queries on Large graphs using Pregel|||||||"The problem addressed in this research is the ""Relationship Query"" problem, which involves finding the top K best minimal answer trees in a graph that connect a set of given keywords. The context of this problem is important in various applications such as mining for complex graph patterns, detecting collusive frauds, and identifying relationships between entities. The key objective of this research is to develop an efficient algorithm that can solve this problem in a distributed manner, with the goal of reducing the search space and improving the scalability of the solution. The authors aim to achieve this by proposing a novel algorithm called Distributed Keyword Search (DKS), which is based on a breadth-first search traversal of the graph and uses a distributed processing framework to identify the top K answer trees. The authors also aim to provide an analytical proof for the optimality of the answers discovered by the DKS algorithm."|Keyword search,Graph data,Distributed algorithm,Relationship queries,Pregel,Fagin's algorithm,BFS exploration,Top-K answers,Linked open data,Steiner tree problem
dd0f3ec0-b60a-58d2-af8f-52de51e6ec3f|2018-09-25T01:41:43|2018|9|Distributed Strong Diameter Network Decomposition|||||||The problem addressed in this research is the computation of a strong network decomposition in a distributed setting. Network decomposition is a fundamental problem in distributed computing, where the goal is to partition the vertices of a graph into clusters of small diameter, such that the clusters are at least a certain distance apart. This problem is important because it has numerous applications in distributed computing, such as solving symmetry breaking problems, computing sparse spanners and linear size skeletons, and building universal Steiner trees. The authors aim to improve the state-of-the-art algorithms for computing strong network decompositions, specifically in the context of unweighted graphs. Their objective is to develop a distributed algorithm that can compute a strong network decomposition with a small number of colors, in a reasonable number of rounds, and with a small message size. The authors also aim to improve the tradeoff between the number of colors and the number of rounds, and to provide a more efficient algorithm than the previous state-of-the-art algorithms.|Network,Decomposition,Distributed,Algorithm,Graph,Strong,Diameter,Coloring,Partition,Clustering
ec118c62-15bd-5fa5-a1ba-5df9463936e0|2018-10-26T04:01:53+00:00|2018|10|Distributed Triangle Detection via Expander Decomposition|Yi-Jun Chang,Seth Pettie,Hengjie Zhang||||||The problem definition addressed in this research is the Triangle Detection problem in distributed networks, specifically in the CONGEST model. The context that makes this problem important is the increasing need for efficient distributed algorithms in modern networks, where bandwidth constraints and locality of information are crucial considerations. The Triangle Detection problem is a fundamental graph problem that has numerous applications in social network analysis, data mining, and network optimization. The key objective of this research is to develop an efficient distributed algorithm for Triangle Detection in the CONGEST model, which takes into account the bandwidth constraints and locality of information. The authors aim to design an algorithm that can detect triangles in a distributed network in a minimum number of rounds, while ensuring that the algorithm is scalable and can handle large networks. In particular, the authors focus on developing an algorithm that can solve the Triangle Detection problem in O(√n) rounds, which is a significant improvement over the existing algorithms. The authors also aim to provide a comprehensive analysis of the algorithm's performance, including its round complexity, message complexity, and the trade-offs between these two metrics. Overall, the goal of this research is to contribute to the development of efficient distributed algorithms for graph problems, with a specific focus on the Triangle Detection problem in the CONGEST model.|Graph Partition,CONGEST Model,Distributed Algorithm,Graph Clustering,Minimum Degree,Mixing Time,Local Graph Exploration,Polylogarithmic Time,Graph Diameter,Sublinear Time
2a561905-dfd7-5366-80f8-702fad75e107|2009-08-14T08:31:06|2009|8|Distributed Aggregation for Data-Parallel Computing:Interfaces and Implementations|||||||The problem definition addressed in this research revolves around the optimization of distributed aggregation in large-scale data processing systems. The context that makes this problem important is the increasing need for scalable and efficient data processing in various applications, such as data mining, machine learning, and graph analysis. In traditional data processing systems, aggregation operations like GroupBy and Aggregate are crucial for summarization and analysis. However, as data sizes grow, these operations become bottlenecks in distributed systems, leading to performance issues. The authors identify that the programming interface and execution plan design choices significantly impact the performance of these computations. The key objectives of this research are to: 1. Analyze the programming models for user-defined aggregation in different systems (Hadoop, DryadLINQ, and parallel databases). 2. Investigate the impact of interface design choices on optimizations for distributed aggregation. 3. Develop and evaluate optimization techniques for distributed aggregation in real-world applications. By addressing these objectives, the authors aim to improve the performance and scalability of distributed aggregation operations, ultimately enabling more efficient and effective large-scale data processing.|Distributed,Aggregation,Programming,Models,GroupBy,MapReduce,Hadoop,Cloud,Computing,Optimization
44bb3b40-41b4-534e-94f0-795dc6b9f01e|2018-11-02T17:25:36+00:00|2018|11|Graph Coloring Algorithms for Multi-core and MassivelyMultithreaded Architectures|||||||The problem addressed in this research is the graph coloring problem, specifically distance-1 coloring, which is an NP-hard problem. The context that makes this problem important is its application in various fields such as computational science and engineering, data mining, and data analysis. The authors aim to develop efficient parallel algorithms for solving this problem on multi-core architectures, with the objective of achieving good scalability and minimizing the number of colors used. The key goal is to design algorithms that can effectively utilize the available concurrency on emerging architectures, while maintaining the quality of the solution in terms of the number of colors used.|Graph,Coloring,Algorithms,Multithreaded,Architectures,Shared,Memory,Parallel,Performance,Scalability
2d2976fd-fcd8-5c87-aaff-f7e45bb2259c|2016-08-01T09:29:33|2016|8|s6raph: Vertex-Centric Graph Processing Framework with Functional Interface|Onofre Coll Ruiz,Kiminori Matsuzaki,Shigeyuki Sato||||||"The problem definition addressed in this research revolves around the challenges of processing large-scale graph data in a parallel and efficient manner. The context that makes this problem important is the increasing prevalence of graph-structured data in various domains, such as social networks, web graphs, and biological networks, which necessitates the development of scalable and efficient processing techniques. The key objective of this research is to design a functional programming framework that can simplify the development of parallel graph processing algorithms, making it easier for developers to write, understand, and reuse graph processing code. The authors aim to achieve this by providing a high-level, vertex-centric interface that abstracts away the complexities of parallel graph processing, allowing developers to focus on the logic of the algorithm rather than the underlying parallelization and communication mechanisms. In essence, the problem definition can be summarized as follows: ""How can we design a functional programming framework that simplifies the development of parallel graph processing algorithms, making it easier to process large-scale graph data efficiently and scalably?"""|Vertex-centric,Graph processing,Functional programming,Pregel,Graph algorithms,Parallel computing,Distributed systems,GraphLab,Erlang,s6raph
8ad7a29b-5e8d-553b-859d-1fd43ecb70a0|2017-02-01T03:22:53|2017|2|Fast Connected Components Computation in Large Graphs by Vertex Pruning|||||||The problem definition addressed in this research is the efficient detection of connected components (CCs) in large-scale graphs, which is a fundamental problem in graph theory and has numerous applications in various fields, including social network analysis, image clustering, and population estimation. The context that makes this problem important is the increasing size and complexity of graphs, which has led to a significant rise in the computational cost and communication overhead of traditional CC detection algorithms. This has resulted in a need for more efficient and scalable solutions that can handle large graphs. The key objectives or goals of the authors are to design a distributed algorithm that can efficiently detect CCs in large-scale graphs, with a focus on minimizing the number of iterations and messages exchanged between nodes. Specifically, the authors aim to develop an algorithm that can achieve a time complexity of O(log n), where n is the number of nodes in the graph, and reduce the communication overhead by exploiting the graph structure. Overall, the authors' goal is to provide a scalable and efficient solution for CC detection in large-scale graphs, which can be applied in various domains and has the potential to significantly impact the performance of graph-based applications.|Distributed,Graph,Algorithms,Connected,Components,Detection,Labelling,Clustering,MapReduce,Pregel
0234de85-8831-52b2-a7a6-a0457e6582d0|2020-05-28T16:51:43|2020|5|Distributed Algorithms for Covering, Packing andMaximum Weighted Matching|||||||The problem addressed in this research is the development of efficient distributed algorithms for solving covering problems, specifically the Maximum Weighted Matching (MWM) problem and the Submodular Cost Covering problem. The context that makes this problem important is the need for efficient distributed algorithms in various applications, such as network design and covering problems, where the input is too large to be processed by a single machine. The key objective of the authors is to design distributed algorithms that can achieve a good approximation ratio in a small number of communication rounds. Specifically, the authors aim to develop algorithms that can achieve a 2-approximation for MWM and a 4-approximation for Submodular Cost Covering in O(log^2 C) communication rounds, where C is the number of constraints.|Distributed,Algorithm,Approximation,Weighted,Vertex,Cover,Fractional,Packing,Hypergraphs,Matching
18311aa5-d82d-54f4-a0a7-2bfea6753af5|2021-12-04T01:56:36+00:00|2021|12|Parallel Scheduling Algorithm based on ComplexColoring for Input-Queued Switches|||||||The problem addressed in this research is the scheduling of input-queued (IQ) switches in high-performance datacenter networks. The context that makes this problem important is the rapid growth of datacenter traffic, driven by the increasing demand for cloud computing, big data, and online services. This growth necessitates the development of high-throughput switches that can efficiently manage large volumes of traffic. The key objective of this research is to design an optimal scheduling algorithm that can maximize the throughput of IQ switches while minimizing the complexity of the scheduling process. Specifically, the authors aim to develop a scheduling algorithm that can achieve 100% throughput, which is the maximum possible throughput of an IQ switch, while using a minimal number of colors (or time slots) in the scheduling process. To address this problem, the authors formulate the scheduling problem as an edge-coloring problem of a bipartite graph, where the graph represents the connections between input and output ports of the switch. The goal is to find an optimal edge-coloring scheme that can schedule the packets in the input queues to the output ports without conflicts, while minimizing the number of colors used.|Scheduling,Bipartite graph,Edge coloring,Packet switch,Frame-based,Online scheduling,Chromatic index,Complex coloring,Kempe chain,Graph theory
33f1ba5a-3fa1-527c-b159-1a9085b81cb7|2017-07-26T16:27:26|2017|7|Fast Distributed Approximation for Max-Cut|||||||The problem addressed in this research is the Max Cut problem, a fundamental task in many computational settings. The Max Cut problem involves finding a maximum cut in a given graph, which is a bipartition of the vertices that maximizes the number of edges crossing the bipartition. This problem is important in various applications, including wireless mesh networks, where finding a maximum cut can increase the network's capacity. The authors aim to develop distributed algorithms for approximating Max Cut in the classic distributed network models, where vertices communicate by synchronous message passing. The key objective is to design algorithms that achieve a good approximation ratio while minimizing the number of communication rounds required. The authors also consider the related problem of Max Dicut, which is a variant of Max Cut for directed graphs. The goal is to develop algorithms that can efficiently solve these problems in a distributed setting, where each vertex decides locally whether it joins a subset S or S, and outputs 1 or 0, respectively, to obtain a cut of largest possible size.|Max Cut,Distributed,Approximation,Algorithms,Graph,Clustering,Coloring,Submodular,Function,Optimization
c5c9deef-abd6-5de0-a219-7411629f4101|2021-11-19T11:04:19|2021|11|Towards Distributed Square Counting in Large Graphs|||||||The problem addressed in this research is the efficient counting of squares (4-cycles) in large graphs, particularly bipartite graphs. The context that makes this problem important is the need for network scientists to understand the higher-order structures within their large relational datasets. Squares are a fundamental motif in bipartite graphs, and their counts can be used to measure notions of connectivity and clustering. The authors aim to develop a distributed and asynchronous approach to counting squares, building on previous techniques such as degree-based vertex ordering. The key objective is to provide a scalable and efficient method for counting squares in massive graphs, which can facilitate the understanding of higher-order structure within decorated relational datasets.|Square,Counting,Graph,Distributed,Asynchronous,Approach,Bipartite,Motifs,Network,Scientists
ce04285b-d1bf-532e-a022-67de14d24583|2018-06-04T18:29:35|2018|6|Distributed, Shared-Memory Parallel Triangle Counting|Thejaka Amila Kanewala,Marcin Zalewski,Andrew Lumsdaine||||||The problem addressed in this research is the efficient counting of triangles in large-scale graphs, which is a fundamental problem in graph analysis with numerous applications in social network analysis, web graph analysis, and data mining. The context that makes this problem important is the increasing size and complexity of real-world graphs, which poses significant challenges to existing algorithms and data structures. The key objective of this research is to develop a scalable and efficient algorithm for triangle counting that can handle large-scale graphs with billions of edges. The authors aim to achieve this goal by proposing a novel approach that combines distributed memory and shared memory parallelism, along with optimizations such as blocking and grouping vertices, to reduce the number of messages and improve cache utilization.|Triangle,Counting,Distributed,Memory,Graph,Algorithm,Parallel,Performance,Optimization,Scalability
71940a3f-1244-598d-95f9-0838896fa0c8|2021-12-08T11:52:42|2021|12|Distributed Vertex Cover Reconfiguration|||||||The problem addressed in this research is the Distributed Vertex Cover Reconfiguration problem. In the context of network monitoring, a vertex cover is a set of nodes that monitor all communication links in a network. The problem arises when the network needs to switch from one vertex cover to another, ensuring that each communication link is always monitored during the transition process. This problem is important because it enables efficient and robust network monitoring, which is crucial in various applications such as distributed systems, social networks, and communication networks. The key objectives of this research are to develop distributed algorithms that can efficiently compute a reconfiguration schedule, which is a sequence of steps that transforms one vertex cover into another while maintaining a valid vertex cover at all times. The authors aim to minimize the number of batches (or rounds) required to complete the reconfiguration process, while ensuring that the size of the intermediate vertex covers remains small. The ultimate goal is to provide a robust and efficient solution for distributed vertex cover reconfiguration, enabling seamless transitions between different monitoring configurations in networks.|Reconfiguration,Vertex Cover,Distributed Algorithm,Graph Decomposition,Network Decomposition,Approximation Algorithm,Scheduling,Batch Scheduling,Graph Theory,Computational Complexity
c44a19b4-dbdd-5a1f-acd9-435418bf611c|2018-06-29T11:24:41+00:00|2018|6|Partout: A Distributed Engine for Efﬁcient RDF Processing|||||||The problem addressed in this research is the efficient storage and querying of large amounts of RDF (Resource Description Framework) data, which has grown rapidly due to the increasing interest in Semantic Web technologies. The context is that existing state-of-the-art systems for storing RDF and processing SPARQL queries are no longer sufficient to handle the huge amounts of data and future growth. The key objectives of this research are to develop a distributed engine for efficient RDF processing in a cluster of machines, and to propose an effective approach for fragmenting RDF data sets based on a given sample query load that finds the optimal configuration of a cluster of machines. The authors aim to improve the performance of RDF query processing and make it scalable to handle large amounts of data.|RDF,SPARQL,Distributed,Query,Processing,Partitioning,Scalability,Optimization,Clustering,Semantic
761ab75b-0aeb-560d-92f4-d46ecaba0412|2022-01-28T09:26:22|2022|1|Svelto: High-Level Synthesis of Multi-Threaded Accelerators for Graph Analytics|||||||The problem definition addressed in this research revolves around the efficient execution of tasks in a multi-threaded accelerator architecture. The context that makes this problem important is the increasing demand for high-performance computing in various domains, such as machine learning, data analytics, and scientific simulations. Traditional architectures struggle to meet these demands due to memory access bottlenecks, leading to underutilization of computing resources. The key objective of this research is to design an efficient task scheduling and memory management system that can effectively utilize the available computing resources in a multi-threaded accelerator architecture. The authors aim to achieve this by developing a novel architecture that can dynamically schedule tasks, manage memory requests, and minimize context switching overhead. The primary goal is to improve the overall system performance, increase resource utilization, and reduce memory access latency. In summary, the problem definition involves addressing the memory access bottlenecks and underutilization of computing resources in multi-threaded accelerator architectures, with the objective of designing an efficient task scheduling and memory management system that can improve system performance, resource utilization, and reduce memory access latency.|Graph,Accelerators,Synthesis,Irregular,Algorithms,Memory,Parallelism,Architecture,HLS (High-Level Synthesis),Latency
07169962-b926-5ec0-9d24-77804ce438be|2021-08-04T13:11:12|2021|8|G-thinker: a general distributed framework for finding qualified subgraphs in a big graph with load balancing|Da Yan||||||The problem definition addressed in this research revolves around efficiently processing and mining large-scale graph data to extract valuable subgraphs that satisfy certain structural or label constraints. The context that makes this problem important is the increasing availability of massive graph datasets in various domains, such as social networks, biological networks, and web graphs, which hold valuable insights and patterns waiting to be uncovered. However, the sheer scale and complexity of these graphs pose significant computational challenges, making it difficult to extract meaningful subgraphs in a timely and efficient manner. The key objectives or goals the authors set to address this problem are: 1. To develop a scalable and efficient framework for mining large-scale graph data, capable of handling massive graphs with billions of vertices and edges. 2. To enable the extraction of valuable subgraphs that satisfy specific structural or label constraints, such as maximum cliques, quasi-cliques, and triangles. 3. To minimize the computational cost and memory requirements of graph mining, while ensuring high throughput and responsiveness. Overall, the authors aim to provide a general-purpose distributed framework for graph mining that can efficiently process large-scale graph data, uncover hidden patterns and insights, and support a wide range of graph mining applications.|Subgraph,Graph,Mining,CPU-bound,Compute-intensive,Clique,Triangle,Subgraph-centric,Parallel,Scalability
3687d00c-234f-5bae-be60-066f6d5402e2|2019-02-09T09:54:56|2019|2|Fast Failure Recovery in Vertex-Centric Distributed Graph Processing Systems|||||||The problem definition addressed in this research revolves around the failure recovery mechanism in distributed graph processing systems (DGPS). The context that makes this problem important is the increasing scale and complexity of graph data, which necessitates the use of DGPS to process large-scale graphs efficiently. However, as the number of nodes in DGPS increases, the likelihood of node failures also increases, leading to significant performance degradation and even system crashes. The key objective of this research is to design an efficient failure recovery mechanism that minimizes the recovery time and ensures continuous system performance. Specifically, the authors aim to develop a recovery algorithm that can handle both single failures and cascading failures, and optimize the recovery process by taking into account both computation and communication costs. In summary, the problem definition is to design a failure recovery mechanism for DGPS that can efficiently recover from node failures, minimize recovery time, and ensure continuous system performance, while also considering the complexities of large-scale graph data processing.|Failure Recovery,Distributed Graph Processing,Checkpoint,Log Compression,Graph Processing Systems,Scalability,Fault Tolerance,Recovery Time,Partition-Based Recovery,Cascading Failures
d93a0ac2-d363-55cb-a9a7-5d3a09592d9a|2014-01-10T07:21:58|2014|1|Semih SalihogluStanford Universitysemih@cs.stanford.edu|||||||The problem addressed in this research is the challenge of implementing graph algorithms efficiently on Pregel-like systems, which can be surprisingly difficult and require careful optimizations. The context that makes this problem important is the increasing need for scalable and efficient graph processing systems to handle large-scale graph data. The authors aim to identify and address various inefficiencies that arise when executing standard graph algorithms on Pregel-like systems, with the goal of reducing communication and memory costs, and improving overall performance. The authors focus on optimizing five fundamental graph algorithms: strongly connected components, minimum spanning forest, graph coloring, approximate maximum weight matching, and weakly connected components. Their objective is to develop optimization techniques that can be applied to these algorithms to improve their efficiency on Pregel-like systems.|Pregel,Graph,Algorithms,Optimization,Distributed,Systems,Scalability,Performance,Computation,Parallelism
a2a1cf48-c344-58fc-af09-764cfb771429|2015-12-07T04:29:53|2015|12|Distributed Graph Algorithms and their Complexity: An IntroductionThis work is supported by KAKENHI No.25106507.|Taisuke IZUMI||||||The problem definition addressed in this research is the development of efficient distributed algorithms for solving graph problems in the CONGEST model. The context that makes this problem important is the increasing need for distributed computing in various modern applications, such as the Internet, mobile phones, and social networks, where data is distributed across multiple nodes and communication between nodes is limited. The key objective of this research is to design distributed algorithms that can efficiently solve graph problems, such as Breadth-First Search (BFS) trees, broadcast and aggregation, all pair shortest paths, minimum spanning trees (MST), vertex coloring, and minimum dominating set (MDS), in a distributed manner. The authors aim to develop algorithms that can achieve this goal while minimizing the number of communication rounds and the total number of messages exchanged between nodes. In particular, the authors focus on the CONGEST model, which is a standard model for distributed computing that assumes a synchronous network with bounded bandwidth and latency. The authors' goal is to design algorithms that can work efficiently in this model, taking into account the limitations of communication and computation in distributed systems. Overall, the problem definition addressed in this research is important because it has implications for the design of efficient distributed algorithms for solving graph problems in various applications, such as social network analysis, traffic optimization, and data mining. The authors' objectives are to develop algorithms that can solve these problems efficiently and scalably in distributed systems, which is crucial for many modern applications.|Distributed,Graph,Algorithms,Computing,Complexity,CONGEST,Model,Global,Local,Problems
01f3b7a4-6716-5d11-b1ae-62005abf187f|2021-04-26T17:45:25|2021|4|A Subquadratic-Time Distributed Algorithm for Exact MaximumMatching|||||||The problem addressed in this research is the maximum matching problem in the CONGEST model, a standard computational model for distributed graph algorithms. The context that makes this problem important is the need for efficient distributed algorithms to solve graph problems in large-scale networks, where the limited bandwidth and synchrony constraints of the CONGEST model pose significant challenges. The key objective of this research is to develop a subquadratic time distributed algorithm for exact maximum matching, which is a fundamental problem in graph theory. The authors aim to bridge the complexity gap of the exact maximum matching problem in the CONGEST model, where the current best known algorithm has a time complexity of O(n^2) rounds, and the lower bound is Ω(D) rounds, where D is the diameter of the input graph. The authors' goal is to design an algorithm that can compute the maximum matching in a more efficient manner, specifically in O(s^3|Matching,Maximum,Distributed,Algorithm,Graph,CONGEST,Model,Augmenting,Path,Problem
ca439333-46fe-52c7-8973-4011b9561141|2024-08-13T19:48:21|2024|8|Distributed graph pattern matching|Shuai Ma,Yang Cao,Jinpeng Huai,Tianyu Wo||||||The problem addressed in this research is the efficient evaluation of graph pattern matching queries in a distributed setting. The context is the increasing use of graph pattern matching in various applications, such as software plagiarism detection, protein interaction networks, social networks, and intelligence analysis. The authors aim to develop distributed algorithms for graph simulation that can efficiently evaluate pattern queries on large data graphs, while minimizing data shipment and maximizing parallelization. The key objectives are to design algorithms that can scale well with large data graphs, reduce the complexity of graph pattern matching, and capture the needs of novel applications. The authors also aim to identify cases where data locality can be exploited to facilitate the evaluation of graph simulation, and to develop optimization techniques to improve the performance of the algorithms.|Graph,Simulation,Pattern,Matching,Distributed,Algorithm,Data,Graphs,Social,Networks
521b04ce-540c-556b-8564-6f0fb84829f0|2022-07-02T06:39:10|2022|7|Better Approximation for Distributed Weighted Vertex Cover via Game-Theoretic Learning|||||||The problem addressed in this research is the Minimum Weighted Vertex Cover (MWVC) problem, which is a fundamental problem in computer science and graph theory. The context that makes this problem important is its wide range of applications in distributed networking systems, such as network optimization, resource allocation, and fault tolerance. In these systems, finding a minimum weighted set of nodes that covers all edges is crucial to minimize costs and optimize performance. The key objective of this research is to develop a distributed algorithm that can approximate the MWVC solution in a decentralized manner, where each node makes decisions based on local information only. The authors aim to design an algorithm that can provide high-quality solutions, preferably with a better approximation ratio than existing methods, while also being computationally efficient. In summary, the MWVC problem is important due to its applications in distributed networking systems, and the authors' goal is to develop a distributed algorithm that can efficiently approximate the MWVC solution with high quality, without relying on a central authority or global information.|Distributed,Minimum Weighted Vertex Cover (MWVC),Game Theoretic,Optimization,Nash Equilibrium,Potential Game,Local Search,Distributed Decision Making,Combinatorial Optimization,Graph Theory
177a9be5-eb78-5575-bce4-1627bac70dd3|2012-09-21T09:07:02|2012|9|PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs|||||||The problem definition addressed in this research revolves around the challenges of processing large-scale graph-structured data in machine learning and data mining (MLDM) applications. The context that makes this problem important is the increasing need to reason about massive graph datasets, which is critical in various domains such as social networks, web graphs, and recommendation systems. The key challenge lies in the fact that natural graphs, which are derived from real-world phenomena, have power-law degree distributions, making them difficult to partition and process efficiently. This leads to issues with communication, locality, and load balancing in distributed graph processing systems. The authors' key objectives are to: Develop a distributed graph processing system that can efficiently handle large-scale natural graphs. Overcome the limitations of existing graph parallel abstractions, such as Pregel and GraphLab, which struggle with natural graphs due to their skewed degree distributions. Achieve scalable and efficient processing of graph-structured data, enabling fast and accurate MLDM applications. To address these objectives, the authors propose PowerGraph, a novel distributed graph processing system designed to handle the challenges of natural graphs.|Graph,Parallel,PowerGraph,Abstraction,Computation,Large-scale,Distributed,Graph-structured,Machine Learning,Data Mining
a5459a01-0253-5d32-8025-5a6a5b08aedc|2022-12-29T03:44:44|2022|12|Toward the minimum vertex cover of complexnetworks using distributed potential games|||||||The problem definition addressed in this research is the Minimum Vertex Cover (MVC) problem in complex networks. The context that makes this problem important is the increasing complexity of real-world networks, such as social networks, biological networks, and the internet, which hinders the efficiency of network covering. The MVC problem is a fundamental problem in computer science and operations research, and its solution has significant implications for various applications, including network optimization, resource allocation, and fault tolerance. The key objective of this research is to develop a novel approach to solve the MVC problem in complex networks. The authors aim to design a decentralized algorithm that can efficiently find the MVC state in a distributed manner, without relying on a centralized controller. The goal is to minimize the number of covered vertices while ensuring that all edges in the network are covered, thereby optimizing network resources and improving overall network performance. In summary, the problem definition addressed in this research is the MVC problem in complex networks, which is important due to its implications for network optimization and resource allocation. The authors' key objective is to develop a decentralized algorithm that can efficiently find the MVC state in a distributed manner, thereby minimizing the number of covered vertices and optimizing network resources.|Vertex Cover,Complex Networks,Potential Game,Minimum Vertex Cover,Strategic Game,Nash Equilibrium,Optimization Algorithm,Distributed Algorithm,Network Science,Combinatorial Optimization
6cadb5ff-4c2d-52eb-8d8e-94fdad69ec13|2013-04-19T09:36:03|2013|4|Trinity: a distributed graph engine on a memory cloud|Bin Shao,Haixun Wang,Yatao Li||||||The problem definition addressed in this research is the challenge of efficiently processing and managing large-scale graphs in a distributed system. The context that makes this problem important is the increasing prevalence of large graphs in various domains, such as social networks, web graphs, and biological networks, which require scalable and efficient processing to extract valuable insights. The key objectives or goals the authors set to address this problem are: To design a distributed system that can efficiently store and process large-scale graphs, with a focus on both online query processing and offline graph analytics. To provide a flexible and scalable architecture that can handle diverse graph applications and workloads. To enable users to define their own graph schema, communication protocols, and computation paradigms, allowing for customization and adaptability to different use cases. Overall, the authors aim to develop a distributed graph processing system that can efficiently handle large-scale graphs, provide flexibility and customization, and support both online and offline graph processing workloads.|Graph,Trinity,Distributed,Memory,System,Computation,Analytics,Online,Query,Processing
a1f142ee-7da6-5885-be97-f48d84bd42a4|2023-10-02T16:59:10|2023|10|Distributed Deterministic Exact Minimum Weight Cycle andMulti Source Shortest Paths in Near Linear Rounds inCONGEST model|||||||The problem addressed in this research is the computation of shortest cycles in graphs, specifically in the context of distributed computing. The background that makes this problem important is the need for efficient algorithms to solve graph problems in a distributed setting, where nodes have limited computational power and can only communicate with their neighbors. The key objective of this research is to develop efficient algorithms for computing shortest cycles in both directed and undirected graphs, with the goal of achieving a significant improvement over the current best known bounds for these problems. The authors aim to achieve this by developing a new framework that involves computing a sequence of blocker sets, which can be used to compute shortest cycles without requiring an initial computation of all-pairs shortest paths (APSP). The authors also aim to obtain O(n) rounds algorithms for the shortest cycle problems, which is a significant improvement over the current best known bounds.|Deterministic,Distributed,CONGEST,Minimum,Weight,Cycle,Shortest,Paths,Algorithm,Graphs
daf8a07d-3782-5aa0-b334-ca239d276bed|2015-06-20T05:06:03|2015|6|One Trillion Edges: Graph Processing at Facebook-Scale|||||||The problem definition addressed in this research is the scalable processing of large-scale graphs, specifically at the scale of hundreds of billions of edges, in a production environment. The context that makes this problem important is the increasing prevalence of graph structures in various applications, such as social networks, recommendation systems, and data mining, which require efficient processing of massive graphs. The key objectives or goals the authors set to address this problem are: To develop a scalable graph processing infrastructure that can handle massive graphs with hundreds of billions of edges. To improve the usability and scalability of graph processing systems, particularly in a production environment. To generalize the Pregel graph processing model to create more powerful application building blocks and reusable code. To achieve high performance and scalability in graph processing, while minimizing memory usage and overcoming limitations of existing systems. Overall, the authors aim to develop a scalable and efficient graph processing system that can handle massive graphs in a production environment, while also improving usability, scalability, and performance.|Graph,Processing,Facebook,Scale,Trillion,Edges,Apache,Giraph,Pregel,Applications
71823dc4-d6a9-5723-b43b-e8a5339e7bb1|2021-04-03T11:35:09+00:00|2021|4|HUGE: An Efficient and Scalable Subgraph Enumeration System|Zhengyi Yang,Longbin Lai,Xuemin Lin,Kongzhang Hao,Wenjie Zhang||||||The problem addressed in this research is the efficient execution of graph pattern matching (GPM) queries, which are crucial in various applications such as social network analysis, recommendation systems, and knowledge graphs. The context that makes this problem important is the increasing scale and complexity of graph data, which poses significant challenges to existing GPM query execution methods. These challenges include high computational costs, large memory requirements, and inefficient communication between machines. The key objectives or goals the authors set to address this problem are: 1. To develop an efficient GPM query execution method that can handle large-scale graph data. 2. To minimize both computation and communication costs in the execution of GPM queries. 3. To design a system that can scale to handle massive graph data and support various GPM query types. To achieve these objectives, the authors propose a novel system called HUGE, which leverages a bounded-memory execution strategy and an optimized execution plan to efficiently execute GPM queries.|Subgraph,Enumeration,Graph,Query,Optimization,Execution,Plan,Join,Algorithm,Scalability
834a75e5-2f5f-5561-abb1-b417634df752|2012-04-11T12:37:11|2012|4|Challenges in very large distributed systems|||||||The problem definition addressed in this research revolves around the challenges of designing and managing large-scale distributed systems, particularly in the context of socio-technical systems that integrate people with computer systems. The background that makes this problem important is the increasing reliance on distributed systems in various aspects of modern life, such as social media, mobile computing, and the Internet of Things. These systems are becoming increasingly complex, with many components and users interacting with each other, making it difficult to ensure their performance, scalability, and reliability. The key objectives or goals the authors set to address this problem are: 1. To develop systems that can effectively integrate people and technology, taking into account the complexities of human behavior and the need for distribution transparency. 2. To design systems that can adapt to changing user behavior and context, and provide personalized experiences. 3. To develop mechanisms for analyzing user behavior and taking the right measures to optimally adapt the system. Overall, the authors aim to develop a deeper understanding of the challenges and opportunities in designing and managing large-scale distributed systems, and to identify potential solutions that can address these challenges.|Distributed systems,Socio-technical systems,Cloud computing,Large-scale systems,Collaboration,Internet of things,Users,System design,Scalability,Transparency
8ea96f7e-8462-53ad-b153-1a93f4f4cd71|2018-10-12T05:05:00|2018|10|Enumerating Maximal Bicliques from a LargeGraph using MapReduce ∗|sneha||||||The problem definition addressed in this research is the enumeration of all maximal bicliques from a graph, referred to as MBE (Maximal Biclique Enumeration). The context that makes this problem important is the increasing need to mine features from massive graphs, which are ubiquitous in applications such as online social networks, information retrieval from the web, citation networks, and physical simulation and modeling. The key objective of this research is to develop scalable methods for discovering densely connected subgraphs within large graphs, specifically maximal bicliques, which are valuable for understanding the actions of users on social networks, identifying significant substructures within graphs, and other applications. The authors aim to address the problem by developing efficient algorithms that can process large graphs and scale out with the cluster size, with the goal of enumerating all maximal bicliques from a graph.|Bicliques,Maximal,Enumeration,Graph,MapReduce,Parallel,Algorithm,Mining,Clustering,Scalable
b7c58b35-4817-5a74-b050-6795220957d6|2015-01-09T02:18:42|2015|1|PRS: Parallel Relaxation Simulation for Massive Graphs|||||||The problem definition addressed in this research revolves around graph pattern matching, specifically in the context of social network analysis. The background that makes this problem important is the rapid growth of data volumes in various domains, including social networks, biological networks, and the World Wide Web, which are often represented as graphs. Traditional notions of graph pattern matching are too restrictive to identify patterns in these emerging fields, particularly in social network analysis where some nodes may be absent or replaced by others. The key objective of this research is to develop a more flexible and realistic approach to graph pattern matching, allowing for the absence of some nodes and their replacement by others. The authors aim to design a framework that can efficiently identify patterns in massive graphs, such as those found in social networks, while considering the complexities and nuances of real-life applications. The ultimate goal is to provide a more effective and practical solution for graph pattern matching in big data analytics, particularly in the context of social network analysis.|Graph pattern matching,Distributed algorithms,Big data,Cloud computing,Social networks,Graph query,Data graphs,Pattern graphs,Bulk synchronous parallel,Emerging applications
604f5a15-d233-59cf-8477-bb36074f40a1|2018-02-15T17:06:31|2018|2|Efficient Disk-Based Directed Graph Processing: A Strongly Connected Component Approach|||||||The problem definition addressed in this research revolves around the efficient processing of iterative graph algorithms on large-scale directed graphs. The context that makes this problem important is the prevalence of directed graphs in real-world applications, such as web graphs, biological networks, and social networks, which require iterative graph algorithms to analyze their properties. However, the existing solutions suffer from high computational costs, slow convergence, and excessive I/O overhead, making it challenging to process these graphs efficiently. The key objectives or goals set by the authors to address this problem are: 1. To reduce the computational cost and I/O overhead associated with iterative graph processing on large-scale directed graphs. 2. To improve the convergence speed of iterative graph algorithms, enabling faster analysis of graph properties. 3. To develop a scalable and efficient solution that can handle evolving graphs, which are common in many real-world applications. To achieve these objectives, the authors propose a novel approach that leverages the concept of Strongly Connected Components (SCCs) to optimize the processing of iterative graph algorithms. By identifying and processing SCCs in a specific order, the authors aim to minimize redundant computations, reduce I/O overhead, and accelerate convergence, ultimately enabling efficient and scalable graph processing.|Graph,Directed,Iterative,Processing,Algorithms,Convergence,SCCs (Strongly Connected Components),DAG (Directed Acyclic Graph),I/O,Optimization
c638efb6-1911-500b-893b-d2fa4e7cd816|2017-02-18T13:59:56|2017|2|Distributed Algorithms on Exact Personalized PageRank|Tao||||||The problem definition addressed in this research is the efficient computation of Personalized PageRank (PPR) vectors in large-scale graphs. The context that makes this problem important is the increasing need for personalized recommendations and search results in various applications, such as web search, community detection, link prediction, anomaly detection, and recommendation systems. PPR is a widely used measure to capture node-to-node proximities in graphs, but its computation is time-consuming and memory-intensive, making it challenging to scale to large graphs. The key objective of this research is to develop efficient and scalable algorithms for computing PPR vectors in large-scale graphs, with a focus on reducing the computation time, memory requirements, and communication costs. The authors aim to achieve this by proposing distributed algorithms that can be parallelized on multiple machines, enabling the computation of PPR vectors in a timely and efficient manner. The ultimate goal is to enable fast and accurate personalized recommendations and search results in various applications.|Personalized PageRank,Distributed Algorithms,Exact Computation,Graph Computation,Random Walks,Teleport Probability,Hub Nodes,Partial Vectors,Skeleton Vectors,Scalability
e5c627c1-d755-53bb-9272-54a0a2e294be|2020-01-23T02:34:57+00:00|2020|1|Simple and Fast Distributed Computation ofBetweenness Centrality|||||||The problem definition addressed in this research revolves around the efficient computation of betweenness centrality in large-scale networks, particularly in the context of wireless networks and distributed systems. The background that makes this problem important is the increasing reliance on network analysis in various fields, including social network analysis, epidemiology, and computer networks. Betweenness centrality, a key metric in network analysis, measures the extent to which a node lies on the shortest paths between other nodes, making it a crucial indicator of a node's influence and importance in the network. The key objectives or goals the authors set to address this problem are: 1. To develop a decentralized algorithm that can efficiently compute betweenness centrality in large-scale networks, without relying on a centralized authority or global knowledge of the network. 2. To minimize the communication overhead and computational complexity associated with computing betweenness centrality, making the algorithm scalable and suitable for real-world networks. By achieving these objectives, the authors aim to provide a practical solution for computing betweenness centrality in large-scale networks, enabling researchers and practitioners to better understand and optimize network behavior, particularly in the context of wireless networks and distributed systems.|Betweenness Centrality,Distributed Algorithm,Network Analysis,Graph Algorithm,Centrality Measure,Shortest Paths,Communication Complexity,Congest Model,Decentralized Computation,Graph Theory
a65cf2a3-5a71-510b-b2ce-1340d07b333b|2018-06-05T10:01:18|2018|6|Parallel and Streaming Algorithms for K-Core Decomposition|||||||The problem addressed in this research is the k-core decomposition of large graphs, which is a fundamental task in graph mining and network analysis. The context that makes this problem important is the increasing size of data sets available in various fields, such as social networks, biology, and machine learning. The k-core decomposition is a hierarchical clustering method that assigns a score to every node in the network, making it useful for understanding the structure of the network and the role of nodes in different networks. The authors aim to develop efficient algorithms for computing the k-core decomposition in parallel and streaming models, with the goal of achieving a good approximation of the core labeling using small memory and a small number of rounds. Specifically, the authors aim to develop a 1-approximate core labeling algorithm that can be computed efficiently in both streaming and MapReduce models.|k core decomposition,parallel algorithms,streaming algorithms,MapReduce model,graph mining,data mining,machine learning,graph theory,densest subgraph,approximation algorithms
05e7c88e-975a-5001-96b6-d91c44554aa7|2023-07-14T16:40:43|2023|7|Distributed Near-Maximum Independent Set Maintenance over Large-scale Dynamic Graphs|||||||The problem definition addressed in this research is the computation of the Maximum Independent Set (MIS) in large-scale dynamic graphs in a distributed environment. The context that makes this problem important is the increasing scale and dynamic nature of real-world graphs, such as social networks, web graphs, and biological networks, which require efficient and scalable solutions to maintain and analyze graph data. The key objectives or goals of this research are to: Develop an efficient distributed algorithm to compute an approximate Maximum Independent Set (MIS) in large-scale dynamic graphs. Maintain a high-quality maximal near-Maximum Independent Set (MIS) in the presence of edge insertions and deletions. Address the scalability issue of computing MIS in large graphs that cannot fit in a single machine's memory. Overall, the authors aim to provide a distributed solution that can efficiently compute and maintain an approximate MIS in large-scale dynamic graphs, which is essential for various applications in social network analysis, web search, and biology.|Maximum Independent Set (MIS),Distributed Algorithm,Dynamic Graph,Graph Theory,Approximation Algorithm,Vertex Centric,Large Scale Graph,Graph Processing,Independent Set,Graph Update
e2048e62-c1d0-5e15-84ce-3a0a10a4af28|2022-01-25T19:32:21+00:00|2022|1|A localized distributed algorithm for vertex cover problem|Vahid Khalilpour Akram||||||The problem addressed in this research is the Minimum Vertex Cover (MVC) problem in graph theory, which is an NP-hard problem. The context that makes this problem important is its numerous applications in real-life scenarios, such as covering all streets of a city with a minimum number of security cameras, finding the best nodes for monitoring and controlling links in a wireless sensor network, and solving other related NP-hard problems in graph theory. The key objective of this research is to develop a localized distributed algorithm for the MVC problem, which can efficiently find a minimum vertex cover in a distributed system without requiring a single node to have the entire graph of the network. This is important because collecting the network topology of a wireless sensor or multi-hop network as a graph in a single node may impose a large amount of message passing and energy consumption in the nodes. The authors aim to design an algorithm that uses local information to decide about the minimum vertex cover, reducing the need for message passing and energy consumption in distributed systems. The goal is to develop an efficient and scalable algorithm that can be applied in various distributed applications, particularly in battery-powered networks such as wireless sensor networks or the Internet of Things.|Vertex Cover,Distributed Algorithm,Localized Algorithm,Minimum Vertex Cover,Graph Theory,Optimization,Distributed Systems,NP-Hard Problem,Approximation Algorithm,Graph Optimization
87676004-b8a5-5fc8-bcff-1b6ae8646b10|2014-07-21T08:18:55|2014|7|LNCS 8632 - <TEX>{\itshape GoFFish}</TEX>: A Sub-graph Centric Framework for Large-Scale Graph Analytics|Yogesh Simmhan,Alok Kumbhare,Charith Wickramaarachchi,Soonil Nagarkar,Santosh Ravi,Cauligi Raghavendra,Viktor Prasanna||||||The problem definition addressed in this research revolves around the limitations of vertex-centric models in large-scale graph processing. The context that makes this problem important is the increasing complexity of big data, particularly in graph datasets, which are inherent in novel applications such as the Internet of Things and Social Networks. The vertex-centric model, which is widely used in graph processing frameworks like Apache Giraph, has two major shortcomings: it is communication-bound, leading to high messaging overhead, and it requires non-trivial porting of shared memory graph algorithms. The key objectives or goals the authors set to address this problem are: 1. To develop a new graph processing framework that can efficiently handle large-scale graph analytics. 2. To reduce the communication overhead and messaging complexity associated with vertex-centric models. 3. To provide a more scalable and efficient approach to graph processing that can handle complex graph datasets. To achieve these objectives, the authors propose a sub-graph centric framework, GoFFish, which operates on connected components within a partition of an undirected graph, allowing for more efficient processing and reduced messaging overhead.|Graph,Analytics,Sub-graph,Centric,Framework,Large-scale,Distributed,Processing,Vertex,Pregel
42777dc0-4d15-5c81-ba5b-99f2bf37340b|2018-06-12T00:48:31+00:00|2018|6|Distributed Algorithms for Minimum DegreeSpanning Trees|||||||The problem addressed in this research is the Minimum Degree Spanning Tree (MDST) problem, which involves constructing a spanning tree with the smallest maximum degree among all spanning trees of a given graph. This problem is important in network communication scenarios where low-degree backbones reduce routing overhead. The authors aim to develop efficient distributed algorithms for the MDST problem, which has been primarily studied in the context of sequential algorithms. The key objectives are to refine the approximation factor and reduce the round complexity, with the goal of achieving a solution that is closer to the optimal solution for the related Minimum Spanning Tree (MST) problem.|Distributed,Algorithms,Spanning,Trees,Minimum,Degree,CONGEST,Model,Approximation,Complexity
fc35de3c-e353-5c3a-b35a-708a759e9f62|2022-03-03T05:12:52|2022|3|AsynGraph: Maximizing Data Parallelism for EfficientIterative Graph Processing on GPUs|||||||The problem addressed in this research is the inefficient processing of iterative graph algorithms on Graphics Processing Units (GPUs). The context that makes this problem important is the widespread existence of graphs in real-world applications, leading to the development of many iterative algorithms to process them. However, these algorithms are time-consuming and can be accelerated by using GPUs, which are designed for parallel processing. Despite many previous studies on GPU-based graph processing, there are still challenges in achieving efficient data parallelism, leading to low GPU utilization and long processing times. The key objectives or goals of this research are to: Maximize data parallelism for efficient iterative graph processing on GPUs. Address the challenges of low GPU utilization and long processing times in existing GPU-based graph processing systems. Develop a novel approach that can efficiently process iterative graph algorithms on GPUs, taking into account the importance of vertices in the graph and the need for asynchronous execution. Overall, the authors aim to develop a system that can efficiently process iterative graph algorithms on GPUs, achieving high data parallelism and low processing times, which is essential for many real-world applications that rely on graph processing.|Graph Processing,GPU,Data Parallelism,Iterative Graph Algorithms,Convergence Speed,State Propagation,Parallelism,Graph Sketch,AsynGraph,Acceleration
115eb991-36d6-52ae-aa0a-58952787ed43|2021-10-22T13:36:33|2021|10|Distributed Graph Realizations|||||||The problem definition addressed in this research is the graph realization problem in a fully distributed setting, specifically in the context of peer-to-peer (P2P) overlay networks. The background that makes this problem important is the need for efficient and scalable overlay network construction in P2P systems, which is crucial for various applications such as content sharing, social networks, and distributed computing. In a P2P overlay network, nodes need to work together to satisfy certain global graph properties, such as degree sequences or connectivity constraints, without a centralized authority. The key objectives of this research are to: 1. Formulate and study graph realization problems in a fully distributed setting, which has not been formally explored before. 2. Develop algorithms that can efficiently construct an overlay network that satisfies the desired graph properties, such as degree sequences or connectivity constraints. 3. Ensure that the constructed overlay network is scalable, efficient, and fault-tolerant, which is essential for P2P applications. The authors aim to address this problem by employing the node-capacitated clique (NCC) model, which captures key aspects of P2P networks, and by developing distributed algorithms that can efficiently realize graphs with desired properties. Overall, the goal is to provide a fundamental understanding of graph realization problems in distributed settings and to develop practical solutions for P2P overlay network construction.|Distributed Algorithms,Network Creation,Knowledge Graph,NCC0 Model,Sorting,Balanced Binary Search Tree,Realization Problem,Degree Realization,Graph Construction,Communication Rounds
b741a5ea-a8a2-5cfa-8bd0-def8538b7fcc|2022-12-15T08:24:12|2022|12|Distributed Approaches to Butterfly Analysis on Large Dynamic Bipartite Graphs|||||||The problem definition addressed in this research is the efficient analysis of butterfly structures in large dynamic bipartite graphs. The context that makes this problem important is the increasing prevalence of large-scale bipartite graphs in various domains, such as social networks, recommender systems, and bioinformatics, where butterfly structures play a crucial role in understanding the graph's properties and behavior. The key objective of this research is to develop distributed approaches to efficiently analyze butterfly structures in large dynamic bipartite graphs, which is challenging due to the massive scale and dynamic nature of these graphs. The authors aim to address this problem by designing algorithms that can process large-scale graphs in a distributed manner, reducing the computational time and memory requirements while maintaining accuracy. Specifically, the authors focus on developing strategies to incrementally update the butterfly analysis when the graph is updated, rather than recomputing the entire analysis from scratch. This is crucial in real-world applications where graphs are constantly evolving, and timely analysis is essential. Overall, the research aims to provide a scalable and efficient solution for butterfly analysis in large dynamic bipartite graphs, enabling better insights and decision-making in various domains.|Bipartite graph,Distributed algorithm,Butterfly counting,Tip decomposition,Dynamic graph,Large-scale graph,Graph mining,Scalability,Memory allocation,Graph processing
a5fc3e5b-c50b-558f-a371-4e7ab480df84|2018-06-16T09:01:33|2018|6|An FPGA Framework for Edge-Centric Graph Processing|Luís Pina,Anastasios Andronidis,Cristian Cadar||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph data, particularly in the context of edge-centric graph algorithms. The background that makes this problem important is the increasing significance of graph-structured data in various applications, such as social networks, web graphs, and recommendation systems. The processing of these massive graphs poses significant computational challenges, including memory access patterns, data locality, and parallelization. The key objective of this research is to develop an efficient and scalable framework for edge-centric graph processing on Field-Programmable Gate Arrays (FPGAs). The authors aim to achieve this by designing a novel architecture that can effectively handle the irregular memory access patterns and data dependencies inherent in graph algorithms. The primary goals of this research are to: 1. Develop a flexible and efficient FPGA-based framework for edge-centric graph processing. 2. Optimize the framework to minimize memory access latency and maximize parallelization. 3. Demonstrate the effectiveness of the proposed framework using real-world graph datasets and various edge-centric graph algorithms. By addressing these objectives, the authors aim to provide a solution that can efficiently process large-scale graph data, enabling faster and more accurate analysis in various applications.|FPGA,Graph,Acceleration,PageRank,SpMV,Edge-centric,Paradigm,Architecture,Optimization,Performance
e6a1a527-43ad-5341-b79e-08aa83cf99a6|2020-01-31T02:10:42+00:00|2020|1|Shared-Memory Parallel Maximal CliqueEnumeration|||||||The problem addressed in this research is Maximal Clique Enumeration (MCE) from a graph, which involves identifying all complete subgraphs in the graph that are maximal. The context that makes this problem important is its wide application in various research areas, such as clustering and community detection in social and biological networks, data mining, and inference from graphical models. The key objective of this research is to develop shared memory parallel algorithms for MCE that are theoretically efficient, scalable, and can handle large graphs with a reasonable turnaround time. The authors aim to improve upon existing sequential algorithms, which can be computationally intensive and may not be able to handle large graphs, and prior parallel algorithms, which may not be scalable or efficient.|Maximal Clique Enumeration,Shared Memory Parallel,Graph Analytics,Dense Subgraphs,Parallel Algorithm,Work-Efficient,Parallel Depth,Load Balancing,Scalability,Computational Complexity
6fce2b81-dcb8-5122-b567-783bf0c72d4f|2017-11-02T12:56:34|2017|11|On the distribution of betweenness centrality in random trees|Kevin Durant||||||The problem definition addressed in this research revolves around understanding the distribution of betweenness centrality (b.c.) in random trees, particularly in simply generated (s.g.) trees and increasing trees. The context that makes this problem important is the widespread use of betweenness centrality as a measure of vertex importance in complex networks, which is crucial in various fields such as social network analysis, epidemiology, and computer science. The key objective of this research is to characterize the distribution of b.c. in random trees, which has been lacking in the literature despite its significance. The authors aim to provide a comprehensive understanding of the b.c. distribution in s.g. trees and increasing trees, including the maximum b.c. and the behavior of the centroid (the vertex with the highest b.c.). By achieving this goal, the authors hope to contribute to a deeper understanding of the structural properties of complex networks and inform the development of algorithms and models that rely on betweenness centrality.|Betweenness centrality,Random trees,Simply generated trees,Increasing trees,Subcritical graph classes,Centroid,Network science,Graph theory,Combinatorial analysis,Random graph models
910f76e7-a566-5a06-a82b-223ea6f474c2|2019-06-23T06:41:18+00:00|2019|6|Distributed subgraph matching on timely dataflow|||||||The problem definition addressed in this research revolves around the efficient processing of subgraph matching queries in large-scale graph datasets. The context that makes this problem important is the increasing prevalence of graph-structured data in various domains, such as social networks, biological networks, and knowledge graphs, which necessitates the development of scalable and efficient query processing techniques. The key objective of this research is to design and implement efficient algorithms for subgraph matching that can handle large graphs and complex queries. The authors aim to achieve this goal by exploring various optimization strategies, such as workload-aware expanding, clique-based optimization, and distributed processing, to reduce the computational cost and improve the scalability of subgraph matching algorithms. Specifically, the authors focus on addressing the challenges of subgraph matching, including the high computational complexity, the need for efficient data access and communication, and the requirement for handling large graphs and complex queries. By developing efficient algorithms and optimization strategies, the authors aim to enable fast and scalable subgraph matching, which is essential for various applications, such as graph pattern mining, graph-based machine learning, and graph querying.|Subgraph,Matching,Distributed,Algorithm,Graph,Query,Optimization,Join,Strategy,Performance
113009e1-9d97-550e-920f-1ff8b1d675f4|2019-08-28T12:44:13|2019|8|DistTC: High Performance Distributed Triangle Counting|||||||The problem definition addressed in this research is the efficient counting of triangles in large-scale undirected graphs, which is a fundamental problem in graph analysis with applications in social network analysis, graph statistics, and k-truss identification. The context that makes this problem important is the increasing size and complexity of modern graphs, which renders traditional serial algorithms inefficient and necessitates the development of distributed and parallel solutions. The key objectives or goals set by the authors to address this problem are: 1. To develop a scalable and efficient distributed algorithm for triangle counting that can handle massive graphs. 2. To minimize communication overhead and optimize data locality in the distributed computation. 3. To design a partitioning strategy that ensures each machine in the distributed cluster can perform triangle counting independently without requiring communication with other hosts. By achieving these objectives, the authors aim to provide a practical solution for triangle counting in large-scale graphs, enabling faster and more accurate graph analysis in various domains.|Triangle counting,Distributed memory,Multi-GPUs,Graph partitioning,Clusters,Graph processing,Scalability,Performance optimization,Parallel processing,Large-scale graphs
409bb358-2494-58a7-b6e6-45e05a007239|2007-01-21T16:30:09|2007|1|Challenges in Parallel Graph Processing.|||||||The problem definition addressed in this research is the challenge of efficiently processing large-scale graph problems in parallel computing environments. The context that makes this problem important is the increasing scale and complexity of graph-based data in various fields, such as scientific computing, data analysis, and informatics, which outgrow the computation and memory capacities of single processors. The key objectives or goals the authors set to address this problem are to develop parallel graph algorithms and software frameworks that can efficiently utilize multiple processors, overcome the limitations of single processors, and scale well for large graph problems. The authors aim to achieve this by addressing the inherent challenges of graph problems, such as data-driven computations, unstructured data, and varying workloads, and by developing software solutions that are flexible, extensible, portable, and maintainable.|Graph algorithms,Parallel processing,Scalability,Graph processing,Parallel computing,Distributed memory,Load balancing,Task granularity,Coarse-grained parallelism,Massively multithreaded machines
c288fae3-38f7-50b4-8a9b-ecb63bf78abe|2021-07-06T17:02:58+00:00|2021|7|Asynchronous Graph Pattern Matching onMultiprocessor Systems|||||||The problem addressed in this research is the efficient processing of graph pattern matching queries on large-scale graphs using Non-Uniform Memory Access (NUMA) systems. The context is that modern servers have multiple memory domains, leading to issues with data processing, and graph pattern matching is a fundamental operation in graph databases. The authors aim to develop a scalable graph pattern matching approach that can efficiently utilize the resources of NUMA systems. The key objectives are to investigate the influence of routing table and partitioning strategy on query performance, to mitigate the impact of broadcasts on scalability, and to evaluate the effectiveness of redundancy in terms of partitioning. The authors also aim to develop a data-oriented architecture that can process graph pattern matching queries in parallel, using local worker threads that communicate asynchronously via a high-throughput message passing layer.|Graph,Pattern,Matching,Query,Scalability,NUMA,System,Design,Performance,Architecture
b02f8814-ee06-5c98-b083-0f6dbda3f020|2014-01-28T07:15:26|2014|1|"Distributed $(\Delta+1)$-Coloring in Linear (in $\Delta$) Time | SIAM Journal on Computing | Vol. 43, No. 1 | Society for Industrial and Applied Mathematics"|Leonid Barenboim,Michael Elkin,Fabian Kuhn||||||The problem addressed in this research is the distributed 1-coloring problem, a fundamental problem in distributed algorithms. The context is a network of processors, each with a distinct identity number, communicating synchronously in discrete rounds. The problem is important due to its applications in scheduling, resource allocation, symmetry breaking, and workload balancing. The key objective is to devise an efficient algorithm that can color the graph in linear time, specifically O(1) time, which is a significant improvement over previous algorithms. The authors aim to achieve this goal by developing a new approach that employs defective coloring, a generalized variant of coloring, and set-theoretic methods.|coloring,distributed,algorithm,graph,time,vertices,colors,maximum,degree,complexity
3aee1322-a73c-5c4f-bc31-188b3056113f|2014-10-03T10:11:13|2014|10|Faster Parallel Traversal of Scale Free Graphs at Extreme Scale with Vertex Delegates|||||||"The problem definition addressed in this research is the efficient processing and analysis of large-scale free graphs, which are commonly found in various domains such as social networks, biology, chemistry, and social sciences. The context that makes this problem important is the increasing size and complexity of these graphs, which poses significant challenges to their analysis and processing. The key objectives or goals set by the authors to address this problem are: To develop an efficient graph partitioning strategy that can handle large-scale free graphs and minimize communication overhead between processing nodes. To design a scalable and parallelizable algorithm that can perform graph analysis tasks, such as graph traversal, k-core decomposition, and PageRank calculation, on distributed memory architectures. The authors aim to achieve these objectives by proposing a novel graph partitioning strategy called ""delegate partitioning,"" which is designed to reduce communication overhead and improve load balancing among processing nodes. They also develop a parallel graph analysis framework that can efficiently execute graph analysis tasks on distributed memory architectures."|Graphs,Scale-free,Parallel,Traversal,Algorithms,Extreme scale,Vertex,Delegate,Asynchronous,Distributed
2767e0ca-e154-5653-947b-32f9874740ba|2022-05-17T07:24:59|2022|5|Distributed Multimodal Path Queries|||||||The problem addressed in this research is the Distributed Multimodal Path (DMP) query, which involves finding the shortest path in a multimodal transportation network that satisfies certain constraints. The context that makes this problem important is the increasing need for efficient and personalized route planning in modern transportation systems, which often involve multiple modes of transportation such as buses, trains, cars, bikes, and walking. The key objective of the authors is to develop an efficient algorithm that can process DMP queries in a distributed manner, taking into account the constraints and preferences of users. The authors aim to minimize the running time and network overhead of the algorithm, while ensuring that the computed path is feasible and satisfies the user's constraints. Specifically, the authors focus on solving the RegLCSP (Regular Language Constrained Shortest Path) problem, which involves finding the shortest path in a graph that satisfies a given regular language constraint. The authors propose a parallel algorithm, ALGdmp, that can efficiently process DMP queries in a distributed environment, and evaluate its performance through extensive experiments on real and synthetic datasets.|Multimodal,Graph,Distributed,Path,Query,Parallel,Computation,Regular,Language,Algorithm
93779d64-bb41-5658-8d87-84be439b76fe|2017-07-18T06:57:42|2017|7|An Adaptive Parallel Algorithm for Computing Connected Components|||||||The problem addressed in this research is the efficient parallelization of the connected component labeling (CCL) algorithm for large-scale graphs. The context that makes this problem important is the increasing need to process and analyze massive graphs in various domains, such as social networks, web graphs, and biological networks. The CCL algorithm is a fundamental graph processing task that identifies connected components in a graph, which is essential for many graph-based applications. The key objectives of this research are to develop a scalable and efficient parallel CCL algorithm that can handle large graphs and minimize the communication overhead between processors. The authors aim to achieve this by designing an algorithm that can effectively utilize the available parallel processing resources, reduce the number of iterations required for convergence, and optimize the data distribution and communication patterns. In summary, the problem addressed in this research is the development of an efficient parallel CCL algorithm for large-scale graphs, with the goal of minimizing communication overhead and maximizing scalability, to support various graph-based applications.|Connected Components,Undirected Graphs,Parallel Algorithms,Distributed Memory,Graph Analytics,Scalability,Performance,Hybrid Approach,Graph Topology,Metagenomic Graphs
fb8a90e9-616f-5e53-9202-c7c91232940b|2019-09-18T16:48:45|2019|9|On Derandomizing Local Distributed Algorithms|||||||The problem addressed in this research is the derandomization of distributed graph algorithms, specifically in the context of the LOCAL and SLOCAL models. The background that makes this problem important is the need for efficient and reliable distributed algorithms in various applications, such as network protocols and distributed systems. The authors aim to develop a general derandomization technique for the distributed message passing model, which is a long-standing open problem in the field. The key objective is to transform a randomized algorithm into a deterministic one, while maintaining its efficiency and correctness. The authors also aim to provide a better understanding of the power and limitations of randomization in distributed algorithms.|Deterministic,Distributed,Algorithms,Graph,LOCAL,Model,Complexity,Polylog,Network,Decomposition
47a7893c-e967-5a6d-839f-a1f1db35a2ca|2020-01-28T20:27:37+00:00|2020|1|Review ArticleSurvey of Methodologies, Approaches, and Challenges in ParallelProgramming Using High-Performance Computing Systems|||||||The problem definition addressed in this research revolves around the complexity and diversity of parallel programming models, APIs, and technologies in contemporary High-Performance Computing (HPC) systems. The context that makes this problem important is the increasing need for efficient utilization of modern computing architectures, which are becoming increasingly heterogeneous and complex. This complexity hinders the development of efficient parallel applications, making it challenging for developers to choose the most suitable parallel programming technology for their specific use case. The key objectives or goals the authors set to address this problem are to: Provide a comprehensive analysis of various parallel programming models, APIs, and technologies, highlighting their strengths, weaknesses, and suitability for different use cases. Identify the key aspects that distinguish these technologies, such as programming methods, supported languages, platforms, and ease of programming. Develop a framework for evaluating and comparing these technologies, enabling developers to make informed decisions when selecting a parallel programming technology for their application. By addressing these objectives, the authors aim to facilitate the development of efficient parallel applications, promote the efficient utilization of modern computing architectures, and contribute to the advancement of HPC systems.|Parallel Programming,High Performance Computing,APIs,Methodologies,Approaches,Multithreaded Processing,Distributed Memory,Shared Memory,Hybrid Systems,Synchronization
43bd599f-23e1-5403-b81f-de7b292dcea9|2008-12-26T00:24:08|2008|12|Distributed (∆+ 1)-coloring in linear (in ∆) time|||||||The problem addressed in this research is the distributed 1-coloring problem, which is a fundamental problem in Distributed Algorithms. The context that makes this problem important is the need for efficient algorithms to solve the problem in a distributed setting, where vertices in a graph need to communicate with their neighbors to achieve a common goal. The key objective of this research is to improve the state-of-the-art upper bounds on the complexity of the 1-coloring problem, specifically to devise a deterministic 1-coloring algorithm with a running time of O(1/2 log n). The authors also aim to address the problem of computing a Maximal Independent Set (MIS) on graphs with maximum degree Δ, which is closely related to the coloring problem.|Coloring,Distributed,Algorithm,Graph,Time,Defective,Independent,Set,Maximum,Degree
5b0a1f3f-ce53-5e4d-ab8f-d7c189068882|2023-12-28T17:51:35|2023|12|Universally-Optimal Distributed Algorithms for Known Topologies|||||||"The problem definition addressed in this research revolves around the concept of ""universal optimality"" in distributed network optimization problems. The context that makes this problem important is the need for efficient algorithms that can solve global optimization problems in distributed networks, where nodes have limited knowledge and communication capabilities. The authors aim to formalize the concept of universal optimality, which refers to the ability of an algorithm to achieve optimal performance on every instance of a problem, rather than just on worst-case scenarios. The key objective is to develop algorithms that can adapt to different network topologies and problem instances, and to identify the fundamental graph parameters that inherently characterize the complexity of these problems. The authors also seek to address the question of whether preprocessing can offer any benefits in solving these problems, and to provide a partial answer to this question. Overall, the goal is to advance the understanding of distributed network optimization problems and to develop more efficient and adaptive algorithms for solving them."|Distributed Algorithms,Universal Optimality,Universal Lower Bounds,Shortcuts,Shortcut Quality,CONGEST,Supported CONGEST,Network Optimization,Distributed Complexity,Graph Parameters
3cbb9abc-d3d9-56d2-8a99-faa095db0b6c|2014-05-23T02:10:21+00:00|2014|5|Distributed Approximation Algorithms for Weighted Shortest Paths|||||||The problem addressed in this research is the computation of shortest paths in a distributed network, specifically the single-source shortest paths (SSSP) and all-pairs shortest paths (APSP) problems. The context that makes this problem important is the need for efficient distributed algorithms in various applications, such as network routing, traffic management, and social network analysis. The authors aim to develop algorithms that can solve these problems in a distributed setting, where nodes have limited knowledge of the network and can only communicate with their neighbors. The key objectives are to design algorithms with small approximation guarantees and low time complexity, specifically sublinear in the number of nodes, to address the limitations of previous algorithms that either have large approximation guarantees or high time complexity. The authors also aim to provide a better understanding of the possibilities and limitations of distributed computation in solving global tasks.|Distributed,Algorithms,Shortest,Paths,Networks,Approximation,Time,Complexity,Graphs,Optimization
87bf8b8d-0a1c-535c-b8f5-1ba45c4f3a95|2018-05-28T05:31:32|2018|5|Distributed Veriﬁcation and Hardness of DistributedApproximation∗|||||||The problem definition addressed in this research revolves around the concept of distributed verification in the context of distributed network algorithms. The context that makes this problem important is the need for efficient algorithms in large and complex networks, such as the human society, the Internet, or the brain, where each node has limited global knowledge and can only communicate with its immediate neighborhood. The key objective of this research is to initiate a systematic study of distributed verification, which involves verifying whether a given subgraph satisfies certain properties, and to establish strong hardness results on the distributed approximation of many classical optimization problems. The authors aim to provide almost tight uniform lower bounds on the running time of distributed verification algorithms for many fundamental problems, and to make progress in establishing strong hardness results on the distributed approximation of many classical optimization problems.|Distributed,Verification,Hardness,Approximation,Algorithms,Lower,Bounds,Complexity,Communication,Networks
77c7c8c9-e27d-586a-946d-ecbddd29a3a8|2013-10-27T03:14:58|2013|10|Distributed Community Detectionin Web-Scale Networks|myday||||||The problem definition addressed in this research is the scalability issue in community detection algorithms for large-scale networks. The context that makes this problem important is the increasing size and complexity of real-world networks, such as social media platforms, biological networks, and the internet, which require efficient and effective community detection methods to uncover hidden patterns and structures. The key objective of this research is to develop a scalable and distributed community detection algorithm that can handle massive networks with billions of vertices and edges. The authors aim to achieve this by proposing a pre-processing algorithm that reduces the problem size, making it possible to apply traditional community detection algorithms. The goal is to develop an algorithm that can efficiently identify core groups in large networks, which can then be used as input for traditional community detection algorithms to identify cohesive communities. In summary, the problem definition is the need for scalable community detection algorithms that can handle large-scale networks, and the key objective is to develop a distributed algorithm that can efficiently identify core groups in massive networks, enabling the application of traditional community detection algorithms to uncover hidden patterns and structures.|Community Detection,Graph Clustering,Distributed Algorithms,Ensemble Learning,MapReduce,Network Analysis,Core Groups,Graph Partitioning,Scalability,Modularity
18f2a30c-56a4-5709-83d6-8d0ec7c20686|2017-11-15T15:24:30|2017|11|Deterministic Distributed Edge-Coloringwith Fewer Colors|||||||The problem addressed in this research is the edge coloring problem in distributed graph algorithms. The context is that of local distributed algorithms, where nodes in a graph need to communicate with their neighbors to solve a problem. Edge coloring is one of the four classic problems in this area, along with vertex coloring, maximal independent set (MIS), and maximal matching. The problem is important because it has been studied extensively since the 1980s, and finding efficient deterministic algorithms is still an open problem. The authors aim to provide a deterministic distributed algorithm for edge coloring that is efficient in terms of time complexity. Specifically, they aim to color the edges of a graph with a number of colors that is close to the minimum required, and to do so in a time that is polylogarithmic in the number of nodes in the graph. The authors also aim to improve upon previous results, which have been limited to randomized algorithms or algorithms with high time complexity.|Edge,Coloring,Algorithm,Graph,Distributed,Matching,Polylogarithmic,Deterministic,Complexity,Time
12300048-e5cb-5322-af6d-034ad0d186fa|2021-10-31T16:39:05|2021|10|Universally-Optimal Distributed Shortest Paths andTransshipment via Graph-Based ℓ1-Oblivious Routing∗|||||||Here is a clear and concise summary of the problem definition addressed in this research: The research addresses the problem of designing an efficient oblivious routing scheme for the transshipment problem in a distributed network setting. **Problem Definition:** The research addresses the problem of designing an efficient oblivious routing scheme for the transshipment problem in a distributed network setting. **Context and Background:** In a distributed network, the transshipment problem involves finding the minimum-cost flow that satisfies a given demand. This problem is important because it has numerous applications in various fields, such as logistics, telecommunications, and finance. The traditional approach to solving this problem involves computing the optimal solution centrally, which can be computationally expensive and may not be feasible in large-scale networks. Therefore, there is a need for an efficient distributed algorithm that can solve the transshipment problem in a scalable and efficient manner. **Key Objectives and Goals:** The authors aim to design an oblivious routing scheme that can efficiently solve the transshipment problem in a distributed network setting. The key objectives are to: 1. Develop a routing scheme that can route any demand in the network with a low competitive ratio, i.e., the cost of the routing scheme should be close to the optimal solution. 2. Design an algorithm that can be executed efficiently in a distributed manner, without requiring a centralized coordinator. 3. Ensure that the algorithm is scalable and can handle large networks with a large number of nodes and edges. Overall, the research aims to develop a practical and efficient solution for the transshipment problem in distributed networks, which can have a significant impact on various applications that rely on efficient network optimization.|Distributed algorithms,Minor aggregation,Universally optimal,Congestion,Shortcut quality,Graph algorithms,Polylogarithmic,Competitive analysis,Distributed graph processing,CONGEST model
51bc8f25-d3e9-5c99-95ef-2703bb14aa5a|2021-11-16T08:33:28+00:00|2021|11|Fast and Robust Distributed Subgraph Enumeration|||||||The problem addressed in this research is the subgraph enumeration problem under distributed settings. The context that makes this problem important is the increasing size of modern graphs, which makes it hard to load the whole graph into memory, and the fact that data graphs are often fragmented and distributed across different sites. The key objective of this research is to develop a distributed subgraph enumeration framework that can efficiently and robustly find all occurrences of a query graph in a data graph without relying on large indexes or exchanging and caching large intermediate results. The authors aim to improve the performance and robustness of existing solutions, which suffer from severe memory crisis or rely on heavy indexes, and to provide a more efficient and scalable approach for subgraph enumeration in distributed settings.|Subgraph,Enumeration,Distributed,System,Robust,Asynchronous,Memory,Control,Strategy,Framework
884b861d-4bc6-5967-8284-5cee8dc2ef80|2018-04-06T08:06:58|2018|4|Adaptive Asynchronous Parallelization of Graph Algorithms|Wenfei Fan,Ping Lu,Xiaojian Luo,Jingbo Xu,Qiang Yin,Wenyuan Yu,Ruiqi Xu||||||The problem addressed in this research is the efficient computation of graph-based iterative algorithms in a distributed setting. The context that makes this problem important is the increasing scale and complexity of graph-structured data, which necessitates the development of scalable and efficient algorithms to process such data. The key objective of this research is to design a distributed computing framework that can efficiently execute graph-based iterative algorithms, such as those used for graph connectivity, PageRank, and graph clustering. The authors aim to achieve this by developing a framework that can adapt to the dynamic nature of distributed computing environments, mitigate the straggler problem, and reduce redundant computations. Specifically, the authors focus on addressing the challenges of asynchronous communication, inconsistent updates, and redundant computations that arise in distributed graph processing. They propose a novel framework, called PIE, which integrates three key components: PEval, IncEval, and Assemble. The PIE framework is designed to provide a flexible and efficient way to execute graph-based iterative algorithms in a distributed setting, with the goal of achieving faster convergence and improved scalability.|Asynchronous,Parallel,Graph,Computation,AAP (Asynchronous Adaptive Parallel),BSP (Bulk Synchronous Parallel),AP (Asynchronous Parallel),Stragglers,PageRank,Collaborative Filtering
437b471d-c084-56e5-9409-5a6d5f3afeba|2018-10-26T05:24:32|2018|10|A Deterministic Distributed 2-Approximation for Weighted Vertex Cover in Rounds|Ran Ben-Basat||||||The problem addressed in this research is the Minimum Weight Vertex Cover (MWVC) problem, a classical NP-hard problem in computer science and graph theory. The context that makes this problem important is its wide range of applications in various fields, including network optimization, data mining, and machine learning. In MWVC, the input is a graph with non-negative vertex weights, and the goal is to find a subset of vertices (a vertex cover) that covers all edges in the graph while minimizing the total weight of the selected vertices. The key objective of this research is to develop an efficient distributed algorithm for solving the MWVC problem in the CONGEST model, a popular model for distributed computing. The authors aim to design an algorithm that achieves a 2-approximation of the optimal solution while minimizing the number of communication rounds required to solve the problem. The authors also focus on improving the round complexity of existing algorithms, particularly for the case where the maximum degree of the graph is unknown. Overall, the research aims to provide a fast and efficient distributed solution for the MWVC problem, which can be applied to various real-world applications.|Vertex Cover,Minimum Weight,Distributed Algorithm,Approximation Algorithm,CONGEST Model,Graph Algorithm,Weighted Vertex Cover,2-Approximation,Round Complexity,Logarithmic Rounds
55d3bb51-1482-5075-b68e-1da9bf1fbda5|2022-03-20T18:36:43|2022|3|Improved Massively Parallel Computation Algorithms forMIS, Matching, and Vertex Cover|||||||The problem addressed in this research is the study of fundamental graph problems, specifically maximal independent set (MIS), maximum matching, and minimum vertex cover, in the context of parallel and distributed computing models, such as Massively Parallel Computation (MPC) and CONGESTED CLIQUE. The importance of this problem lies in the fact that these graph problems are central to algorithmic graph theory and have been extensively studied in various models of computation. The authors aim to design simple randomized algorithms that construct approximate instances for these problems in the MPC model, with a focus on achieving low round complexity and memory requirements. The key objectives are to develop algorithms that can efficiently solve these problems in a parallel and distributed setting, with a goal of improving the state-of-the-art in terms of round complexity and memory requirements.|MPC,CONGESTED CLIQUE,MIS,Matching,Vertex Cover,Parallel Computation,Distributed Computing,Graph Algorithms,Approximation Algorithms,Randomized Algorithms
93750600-2f58-57b2-ae36-21f15c3c543b|2019-09-17T04:43:38|2019|9|GCache: Neighborhood-Guided Graph Caching in a Distributed Environment|||||||The problem definition addressed in this research is the Large-scale Clustered Graph Partitioning (LCGP) problem, which involves dividing a large graph into smaller clusters or partitions while minimizing the number of crossing edges between different clusters and ensuring that each cluster has a balanced size. The context that makes this problem important is the increasing scale and complexity of modern graph-structured data, which poses significant challenges to graph processing and analysis. The need for efficient graph partitioning arises in various applications, such as social network analysis, recommendation systems, and data mining, where large graphs need to be processed and analyzed in a distributed manner. The key objectives or goals the authors set to address the LCGP problem are: 1. To develop an efficient and scalable algorithm that can handle large-scale graphs with millions of nodes and edges. 2. To minimize the number of crossing edges between different clusters, which is essential for reducing communication overhead and improving the performance of distributed graph processing. 3. To ensure that each cluster has a balanced size, which is critical for achieving load balancing and scalability in distributed graph processing. Overall, the authors aim to develop a novel algorithm that can effectively address the LCGP problem and provide a scalable solution for large-scale graph partitioning.|Graph,Caching,Distributed,Large,Optimization,Algorithms,Meta,Partitioning,Connectivity,Percolation
8dbac85a-c395-5587-b150-42a42ff11c34|2021-06-07T00:48:56+00:00|2021|6|Improved Distributed Lower Bounds for MIS and Bounded (Out-)Degree Dominating Sets in Trees|Alkida Balliu,Sebastian Brandt,Fabian Kuhn,Dennis Olivetti||||||The problem addressed in this research is the computation of a k-outdegree dominating set in a graph, which is a fundamental problem in distributed graph algorithms. The context that makes this problem important is the need for efficient distributed algorithms in various applications, such as wireless networks and distributed systems. The authors aim to prove a lower bound on the time complexity of computing a k-outdegree dominating set in the deterministic port numbering model, which is a model that captures the communication constraints of distributed systems. The key objective is to show that there exists a family of graphs for which any algorithm that computes a k-outdegree dominating set requires at least Ω(log n) rounds, where n is the number of nodes in the graph. This lower bound has implications for the design of efficient distributed algorithms for various graph problems.|Lower,Bound,Distributed,Algorithm,Complexity,Graph,Problem,Model,Technique,Round
c2e8e422-7461-5a74-ae7b-744662c058b2|2013-09-27T08:09:00|2013|9|LNCS 8205 - Distributed Minimum Cut Approximation|Mohsen Ghaffari,Fabian Kuhn||||||The problem addressed in this research is the distributed minimum cut approximation in a network graph. The context that makes this problem important is that minimum cuts and their sizes (i.e., edge connectivity) are crucial in network design and optimization, as they represent the throughput capacity of the network. Decomposing a network using small cuts helps in designing efficient communication strategies and identifying communication bottlenecks. The key objective of this research is to develop efficient distributed algorithms that can approximate the minimum cut in a network graph. The authors aim to design algorithms that can be executed in a distributed manner, where each node in the network can communicate with its neighbors, and the goal is to compute an approximate minimum cut in a time-efficient manner. The authors focus on developing algorithms with low time complexity, specifically in terms of the number of rounds required to compute the approximate minimum cut, while ensuring the correctness of the solution with high probability.|Minimum Cut,Distributed Algorithm,Approximation,Graph,Edge Connectivity,Randomized Algorithm,Communication Network,Cut Size,Graph Decomposition,CONGEST Model
4dfc4653-ea25-5a02-a503-ccffbd41b74b|2021-09-15T05:58:53|2021|9|TriPoll: Computing Surveys of Triangles in Massive-ScaleTemporal Graphs with Metadata|||||||The problem addressed in this research is the efficient processing of triangles in massive scale temporal graphs with metadata. The context is that network scientists seek to understand higher-order interactions within network data, and triangles are a fundamental pattern that is commonly difficult to enumerate in massive distributed real-world networks. The key objective is to develop a system that can survey triangles in massive graphs containing metadata on their edges and vertices, allowing users to answer hypotheses regarding the relevance of metadata triangles on their datasets and leverage their discoveries for various discovery tasks. The authors aim to provide a scalable solution that can handle large-scale graphs with hundreds of billions of edges and support metadata-aware capabilities.|TriPoll,Triangle,Graph,Metadata,Distributed,Scalable,Temporal,Network,Asynchronous,Communication
172ace0f-6fd5-5170-9977-3ad8b604a923|2021-07-01T03:51:57|2021|7|Efficient Distributed Approaches to Core Maintenance on Large Dynamic Graphs|||||||The problem definition addressed in this research revolves around maintaining core decomposition in large dynamic graphs, which is a fundamental problem in graph theory and network analysis. The context that makes this problem important is the increasing scale and dynamism of real-world graphs, such as social networks, web graphs, and biological networks, which undergo frequent updates (e.g., edge insertions or deletions). This dynamism poses significant challenges to maintaining accurate core decomposition, which is essential for various applications, including network analysis, community detection, and anomaly detection. The key objectives or goals the authors set to address this problem are: Efficiently maintaining core decomposition in large dynamic graphs, which involves updating the core numbers of vertices in response to edge updates. Scalability, as the graphs can be massive, with millions of vertices and edges. Accuracy, ensuring that the updated core numbers are precise and reflect the changes in the graph structure. To achieve these objectives, the authors aim to develop distributed algorithms that can efficiently process edge updates, minimize communication overhead, and ensure accurate core decomposition in large dynamic graphs.|Core maintenance,Dynamic graphs,Distributed approaches,Core decomposition,Graph analysis,Large-scale networks,Real-time updates,Edge insertion,Edge deletion,Graph structure
883cb608-df17-5a7e-b4e4-9219a29a5165|2015-08-06T14:56:09|2015|8|PDTL: Parallel and Distributed Triangle Listing for Massive Graphs|||||||The problem addressed in this research is the efficient listing and counting of triangles in massive graphs, which is crucial for various applications such as density and connectivity metrics, clustering coefficient, and detecting fake accounts in social networks. The context that makes this problem important is the increasing size of graphs, which can reach billions of vertices and trillions of edges, making in-memory algorithms insufficient. The key objectives of this research are to create a general framework for triangle listing and counting that provides efficient and well-understood bounds on CPU, I/O, Memory, and Network utilization, and to develop a parallel and distributed algorithm that can handle massive graphs without running out of memory. The authors aim to achieve these objectives by proposing a novel algorithm that combines efficient external memory access with parallelization, and by providing theoretical guarantees and experimental evaluations to demonstrate its effectiveness.|Triangle,Distributed,Algorithm,Graphs,Massive,Parallel,Listing,Counting,Ef cient,I O
d8d0fe23-0893-5a08-a133-40f719006478|2016-08-30T02:44:59|2016|8|PathGraph: A Path Centric Graph Processing System|||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph analysis applications, which are becoming increasingly prevalent. The context that makes this problem important is the growing need for scalable and efficient graph processing systems to handle massive graphs with billions of vertices and edges. Existing graph processing systems face challenges such as poor access locality, inefficient partitioning methods, and contention among parallel tasks, leading to performance bottlenecks. The key objectives or goals the authors set to address this problem are: To develop a novel graph partitioning method that can efficiently divide the graph into smaller subgraphs, ensuring good access locality and minimizing communication overhead. To design a parallel task scheduling strategy that can effectively manage load balancing, reduce contention, and optimize resource utilization. To improve the overall performance and scalability of graph processing systems, enabling them to handle large-scale graph analysis applications efficiently. By addressing these objectives, the authors aim to provide a more efficient and scalable solution for large-scale graph analysis applications, which is critical for various domains such as social network analysis, web graph analysis, and recommendation systems.|Graph,Path,Centric,Processing,System,Iterative,Computation,Algorithm,Partitioning,Parallel
47865235-f272-5186-b89b-e08721ce2c3f|2021-02-04T04:17:02|2021|2|A Minimal Memory Game-based Distributed Algorithm to Vertex Cover of Networks|||||||The problem definition addressed in this research is the Minimum Vertex Cover (MVC) problem, which is a fundamental problem in graph theory and computer science. The context that makes this problem important is its broad applications in various fields, such as wireless sensor networks, crossroads monitoring, military surveillance, and health monitoring, where it is essential to select a minimum set of vertices to cover all edges in a graph. The key objective of this research is to develop a distributed algorithm to solve the MVC problem, which is an NP-hard problem. The authors aim to design a game-theoretic approach that can efficiently find a near-optimal solution to the MVC problem in a distributed manner, without relying on a central administrator. The goal is to overcome the contradiction between individual interests and collective benefits, which often leads to inefficient solutions in distributed algorithms. In summary, the problem definition is to develop a distributed algorithm to solve the MVC problem, which is crucial in various applications, and the key objective is to design a game-theoretic approach that can efficiently find a near-optimal solution to the MVC problem in a distributed manner.|Vertex Cover,Distributed Algorithm,Nash Equilibrium,Bounded Rational Behavioral,Memory Length,Selection Intensity,Snowdrift Game,Networks,Optimization,Game Theory
827b1116-dc30-593b-a55c-16956c39c829|2019-06-17T17:14:25+00:00|2019|6|simd-x: Programming and Processing  of Graph Algorithms on GPUs|||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph algorithms on Graphics Processing Units (GPUs). The context that makes this problem important is the increasing significance of graph algorithms in various domains, such as social networks, web graphs, and recommendation systems, which require processing massive graphs. However, traditional CPU-based approaches are limited by their sequential processing nature, leading to poor performance and scalability issues. The key objectives or goals the authors set to address this problem are: 1. To develop an efficient graph processing framework that can harness the parallel processing capabilities of GPUs. 2. To overcome the challenges of workload imbalance, memory consumption, and synchronization overheads that arise when processing large graphs on GPUs. 3. To provide a programming model that is easy to use, flexible, and scalable for various graph algorithms and datasets. By achieving these objectives, the authors aim to enable fast and efficient processing of large-scale graph algorithms on GPUs, which can have a significant impact on various applications and domains that rely on graph processing.|Graph,Algorithm,Optimization,Performance,Parallel,Processing,SIMD,Kernel,Fusion,Management
3921d623-5f68-5009-a302-e091d6672ebc|2021-03-15T03:22:42+00:00|2021|3|DISTRIBUTED ALGORITHMS FOR FRACTIONAL COLORING|||||||The problem addressed in this research is the fractional coloring of graphs in the context of distributed computing. Fractional coloring is a relaxation of the classical notion of coloring, where each vertex is assigned a set of colors, and adjacent vertices have disjoint sets of colors. The goal is to find a fractional coloring with a minimum total weight, which has applications in scheduling and other areas. The authors aim to design efficient distributed algorithms to solve this problem, with a focus on minimizing the round complexity and output size. They also investigate the complexity threshold for fractional coloring, which is the minimum number of colors required to achieve a certain level of efficiency. The authors aim to provide a better understanding of the complexity of fractional coloring and to develop efficient algorithms for solving this problem in distributed computing environments.|Distributed,Fractional,Coloring,Algorithms,Graphs,LOCAL,Model,Complexity,Threshold,Deterministic
36b4a881-40c8-5fc3-a04c-4345d6db9b33|2022-06-01T19:39:30+00:00|2022|6|Asynchronous Distributed-Memory Triangle Counting and LCC with RMA Caching|||||||The problem addressed in this research is the efficient computation of Local Clustering Coefficient (LCC) in large-scale graphs, which is a crucial metric in graph analysis with applications in community detection, link prediction, and thematic relationships. The context that makes this problem important is the rapid growth in the size of graphs, exceeding the memory and computational capacities of a single machine, making distributed memory computing necessary. The key objective of this research is to develop an efficient distributed memory algorithm for LCC computation, leveraging MPI Remote Memory Access (RMA) to minimize communication overhead and optimize data reuse. The authors aim to achieve strong scaling capabilities, reduce initialization overheads, and lower per-node memory requirements, enabling the analysis of massive graphs on distributed computing systems.|Distributed,Graph,Analysis,Local,Clustering,Coefficient,Triangle,Counting,Asynchronous,RDMA
d0eb8d85-1b55-5d81-a6ac-f3088597e843|2020-09-05T06:30:18|2020|9|Resource-efficient cyber-physical systems design: A survey|Zhao Li||||||The problem definition addressed in this research revolves around the efficient utilization of resources in Cyber-Physical Systems (CPSs), particularly in embedded control systems. The context that makes this problem important is the increasing scale and complexity of CPSs in industrial applications, which demands optimal resource allocation to ensure safety, performance, and cost-effectiveness. The key objectives or goals set by the authors to address this problem are: To optimize the computation resource, which includes processor utilization and operating frequency, to achieve better performance while reducing costs. To efficiently manage communication resources, such as shared buses, shared caches, and shared memories, to minimize interference and ensure timely data exchange. To optimize memory resource allocation, considering the non-deterministic nature of Event-Triggered Architectures (ETAs) and the need to incorporate timing analysis techniques to obtain the worst-case closed-loop delay. Overall, the authors aim to develop a co-design approach that integrates computation, communication, and memory resource management to ensure the efficient and reliable operation of CPSs in industrial applications.|Multi-core,Scheduling,Real-time,Systems,Analysis,Response Time,Cache,Contention,Optimization,Predictability
a02ffda1-3532-551a-ba06-81e64f58420e|2015-04-16T03:54:06|2015|4|Scalable Subgraph Enumeration in MapReduce|||||||The problem definition addressed in this research is the subgraph enumeration problem, which involves finding all subgraph instances of a given data graph that are isomorphic to a pattern graph. This problem is important in various applications, such as network motif computing, biochemistry, neurobiology, ecology, and bioinformatics. The context that makes this problem important is the increasing size and complexity of graph data, which makes it challenging to efficiently and effectively enumerate all subgraph instances. The authors aim to address this problem by developing a scalable and efficient solution that can handle large graph data. The key objectives or goals of this research are: 1. To develop a distributed algorithm for subgraph enumeration that can efficiently process large graph data. 2. To minimize the computational cost and memory usage of the algorithm. 3. To ensure the correctness and completeness of the subgraph enumeration results. Overall, the authors aim to provide a scalable and efficient solution for the subgraph enumeration problem, which is essential for various applications in graph analysis and data mining.|Subgraph,Enumeration,MapReduce,Graph,Pattern,Join,Algorithm,Scalability,Optimization,Distributed
7b1809e1-1c7f-537d-875c-3f12d76ea349|2017-05-28T12:53:01|2017|5|Distributed Approximation of Maximum Independent Set and Maximum Matching|Reuven Bar-Yehuda,Keren Censor-Hillel,Mohsen Ghaffari,Gregory Schwartzman||||||The problem addressed in this research is the approximation of maximum independent set (MaxIS) and maximum matching (MWM) in the CONGEST model, a classic distributed computing model. The context that makes this problem important is the need for efficient distributed algorithms for large-scale networks, where nodes have limited communication capabilities. The authors aim to develop algorithms that can approximate MaxIS and MWM in a distributed setting, with a focus on achieving a good approximation factor while minimizing the number of communication rounds. The key objectives are to design algorithms that can achieve a 2-approximation for MaxIS and MWM, and to improve the round complexity of existing algorithms. The authors also aim to develop a deterministic algorithm for MaxIS that runs in O(log n) rounds, and to extend their results to the weighted case. Overall, the goal is to provide efficient and scalable solutions for approximating MaxIS and MWM in distributed networks.|Distributed,Approximation,Algorithm,Graph,Matching,Independent,Set,CONGEST,Model,Complexity
72dbbf74-2166-5c55-8c62-37886b14605c|2017-02-20T05:12:17|2017|2|Optimizing Graph Processing on GPUs|||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph data on Graphics Processing Units (GPUs). The context that makes this problem important is the increasing significance of graph processing in various domains, such as scientific computing, data mining, and social networks, where massive graphs need to be processed quickly and efficiently. The key objective of this research is to design a novel graph processing framework that can effectively utilize the parallel processing capabilities of GPUs to accelerate graph computations. The authors aim to achieve this by developing a framework that can efficiently manage graph data, optimize memory access patterns, and minimize data transfer between the CPU and GPU. Specifically, the authors focus on addressing the limitations of existing graph processing models, such as the vertex-centric model, which can lead to suboptimal performance due to inefficient memory access patterns and data transfer overhead. They propose a new graph processing model, called the Edge-Message-Vertex (EMV) model, which separates the processing of vertices, edges, and messages to offer more flexibility in designing algorithmic optimizations. The goal is to develop a framework that can efficiently process large-scale graphs on GPUs, achieving high performance and scalability while minimizing programming complexity.|Graph,Processing,GPUs,Pregel,Programming,Model,Optimization,Parallel,Computing,Framework
34e3e49c-b758-593b-bef9-8c7f04d29d10|2017-09-05T04:33:56|2017|9|Towards Practical and Robust Labeled Pattern Matching in Trillion-Edge Graphs|Tahsin Reza,Christine Klymko,Matei Ripeanu,Geoffrey Sanders,Roger Pearce||||||The problem definition addressed in this research is the exact matching of a small template graph within a large background graph. The context that makes this problem important is the increasing need for efficient graph pattern matching algorithms in various applications, such as social network analysis, data mining, and information retrieval. The existing algorithms for exact matching have high computational complexity, limiting their scalability and robustness guarantees for modern large graph datasets. The key objectives or goals the authors set to address this problem are: 1. To develop an efficient algorithm that can exactly match a small template graph within a large background graph, with a focus on reducing the computational complexity. 2. To design a vertex-centric approach that iteratively eliminates vertices that do not meet local constraints imposed by the template graph, thereby reducing the search space and improving the algorithm's scalability. 3. To evaluate the feasibility and effectiveness of this approach in terms of its ability to prune the search space and reduce the computational cost of exact matching. Overall, the authors aim to provide a more efficient and scalable solution for exact graph pattern matching, which can be applied to various real-world applications involving large graph datasets.|Pattern Matching,Graphs,Subgraph,Labeled,Vertex Centric,Distributed Memory,Scalability,Algorithm,Semantic Graphs,Exact Matching
3b1def04-b03c-5f59-9658-434d92da9265|2019-06-04T01:58:16+00:00|2019|6|Improved Distributed Approximations for Minimum-WeightTwo-Edge-Connected Spanning Subgraph|||||||The problem addressed in this research is the 2-Edge-Connected Subgraph (2-ECSS) problem, which is a fundamental problem in network design. The context that makes this problem important is the need for efficient and reliable network design in various applications, such as communication networks, transportation systems, and social networks. The objective of the 2-ECSS problem is to find a minimum-weight subgraph that is 2-edge-connected, meaning that it remains connected even if one edge fails. The authors aim to develop a distributed algorithm that can solve this problem efficiently, with a focus on achieving a constant approximation ratio and a near-optimal round complexity. The key goal is to design an algorithm that can be executed in a distributed manner, where each node in the network can make decisions based on local information, without requiring a centralized authority. The authors also aim to improve upon existing algorithms, which have limitations in terms of approximation ratio, round complexity, or both.|Distributed,Algorithm,Network,Design,Approximation,Tree,Augmentation,Complexity,Graph,Optimization
f4925e79-b3c1-5f3a-8bf8-15dd56fd7b0a|2021-12-15T14:58:29|2021|12|Lecture Notes in Networks and Systems 302|||||||The problem definition addressed in this research revolves around optimizing the routing of vehicles in a vehicular ad-hoc network (VANET) to minimize congestion and reduce travel time. The context that makes this problem important is the increasing number of vehicles on the road, leading to congestion, accidents, and environmental pollution. The authors aim to address this problem by developing an efficient routing algorithm that can adapt to real-time road conditions, such as accidents, roadblocks, and car breakdowns, to provide optimal routes to vehicles. The key objectives of this research are to reduce travel time, minimize congestion, and improve the overall efficiency of the transportation system.|Summarization,Text,Odia,Language,Automatic,Document,Extractive,Natural,Processing,NLP
9220e9fa-aff9-5f02-b27c-bac459f345d6|2009-01-17T10:55:27|2009|1|Distributed Data Aggregation Scheduling in Wireless Sensor Networks|||||||Here is a concise summary of the problem definition addressed in this research: Context and Background: In wireless sensor networks, data aggregation is a crucial technique to reduce energy consumption and improve network efficiency. However, existing data aggregation scheduling algorithms have high latencies, and centralized approaches are inefficient due to frequent topology changes. Distributed data aggregation scheduling is essential to address these limitations. Problem Definition: The problem addressed in this research is the Distributed Aggregation Scheduling Problem (DASP), which aims to find a data aggregation schedule in a distributed manner to minimize the aggregation latency while ensuring collision-free data transmission. Key Objectives: 1. Minimize Aggregation Latency: The primary goal is to reduce the time required to aggregate data from all sensor nodes to the sink node. 2. Ensure Collision-Free Transmission: The schedule should prevent data collisions during transmission, ensuring that all data is received correctly at the sink node. 3. Distributed Approach: The algorithm should be distributed, allowing sensor nodes to collaborate and determine the aggregation schedule without relying on a centralized authority. By addressing the DASP, the authors aim to develop an efficient and reliable data aggregation scheduling algorithm that can be applied in various wireless sensor network applications.|Data Aggregation,Scheduling Algorithm,Wireless Sensor Networks,Distributed Algorithm,Collision-Free,Time Latency,Energy Consumption,Network Diameter,Node Degree,Sensor Networks
a931de24-62fc-5145-8a0f-8e6354e0a3b9|2021-09-15T05:58:47|2021|9|cuTS: Scaling Subgraph Isomorphism on Distributed Multi-GPUSystems Using Trie Based Data Structure|||||||The problem addressed in this research is the subgraph isomorphism problem, which involves finding all subgraphs in a large data graph that are isomorphic to a given query graph. This problem is important in various domains such as bioinformatics, computer vision, social network analysis, and cheminformatics. The authors aim to develop an efficient algorithm to solve this problem, with a focus on achieving high performance and scalability on modern GPUs. The key objectives are to reduce memory usage, minimize synchronization requirements, and balance workload between different nodes to achieve the best performance.|Subgraph,Isomorphism,GPU,Graph,Search,Algorithm,Query,Data,Parallel,Optimization
c022ab5b-d5c7-5ff5-b911-d5fff72f5868|2017-08-07T17:20:26|2017|8|Improved Deterministic Distributed Matching via Rounding|||||||The problem addressed in this research is the development of efficient deterministic distributed algorithms for computing approximate maximum matchings in graphs. The context that makes this problem important is the need for efficient algorithms in distributed computing, where nodes in a network can only communicate with their neighbors. The authors aim to improve upon existing algorithms by developing a deterministic distributed method for rounding fractional matchings to integral matchings, which can be applied to various graph problems. The key objectives are to develop algorithms for computing a 2-approximate maximum matching, a maximal matching, and a 2-approximate minimum edge dominating set, all within a polylogarithmic number of rounds.|Matching,Distributed,Algorithm,Graph,Deterministic,Approximation,Complexity,Rounds,Logarithmic,Local
421c598a-3238-5ffa-b0c1-aefbd0ecd126|2018-05-28T07:18:51|2018|5|Accelerating PageRank using Partition-Centric Processing|||||||The problem definition addressed in this research is the optimization of PageRank computation, a fundamental graph algorithm, on shared-memory platforms. The context that makes this problem important is the growing scale of graph-based problems in various fields, such as web and social network analysis, biology, and transportation, which demands high-performance graph analytics. The key objective of this research is to overcome the limitations of traditional Graph Algorithmic Skeleton (GAS) models, which are inherently suboptimal due to their node-centric approach. The authors aim to develop a novel approach that can efficiently utilize the shared-memory architecture, reduce memory accesses, and improve the performance of PageRank computation. Specifically, the authors set out to design a partition-centric processing model that can take advantage of the graph structure, minimize memory traffic, and optimize the computation of PageRank. The goal is to achieve significant performance gains over state-of-the-art methods, making it possible to efficiently process large-scale graphs on shared-memory platforms.|PageRank,Partition Centric Processing,Sparse Matrix-Vector Multiplication,Graph Algorithms,Graph Locality,Memory Access Patterns,Cache Miss Ratio,Memory Bandwidth,Gather-Apply-Scatter Model,Graph Computation
6bd07b97-425d-5f09-9faf-fe16f372b0d9|2016-06-07T11:21:30|2016|6|A distributed approach for graph mining in massive networks|N Talukder||||||The problem definition addressed in this research is the efficient mining of frequent patterns in a large graph distributed across multiple machines. The context that makes this problem important is the increasing availability of large-scale graph data in various domains, such as social networks, web graphs, and biological networks, which requires scalable and efficient methods for pattern discovery. The key objective of this research is to develop a distributed graph mining approach that can handle large graphs by partitioning them across multiple machines, while ensuring that no frequent patterns are missed (no false negatives) and no infrequent patterns are reported as frequent (no false positives). The authors aim to achieve this goal by minimizing communication between machines, allowing local pruning of infrequent patterns, and eliminating false negative patterns via external neighbors. In summary, the problem definition involves developing a scalable and efficient distributed graph mining approach that can handle large graphs, minimize communication, and ensure accurate pattern discovery without missing any frequent patterns or reporting infrequent ones as frequent.|Distributed,Graph,Mining,Frequent,Subgraph,Massive,Networks,Parallel,Algorithm,Scalability
08d59296-3c55-5705-ac55-06c20ee54089|2009-11-19T18:53:33|2009|11|PEGASUS: A Peta-Scale Graph Mining System - Implementation and Observations|U-U-TPFTZIBSIJBukang,S-------||||||The problem definition addressed in this research revolves around the efficient processing and analysis of large-scale graphs, which have become increasingly prevalent in various domains such as social networks, web graphs, and biological networks. The context that makes this problem important is the rapid growth of graph data, which has outpaced the development of scalable algorithms and systems to handle them. This has led to a significant bottleneck in graph processing, making it challenging to extract valuable insights and knowledge from these massive datasets. The key objectives or goals set by the authors to address this problem are: 1. To develop a unified framework that can efficiently handle various graph mining tasks, such as computing connected components, diameter, PageRank, and node proximities. 2. To design a scalable system that can process large-scale graphs with billions of nodes and edges, leveraging distributed computing architectures like HADOOP. 3. To achieve significant performance improvements over existing approaches, making graph analysis more efficient and practical for real-world applications. By addressing these objectives, the authors aim to provide a comprehensive solution for large-scale graph processing, enabling researchers and practitioners to extract valuable insights from massive graph datasets and driving advancements in various fields.|Graph mining,PEGASUS,HADOOP,MAPREDUCE,Large-scale graphs,Parallel processing,Distributed computing,Graph algorithms,Scalability,Performance optimization
fa209d88-34eb-52a0-8f51-61bf8ed2b2e6|2021-07-27T15:09:51|2021|7|Large-Scale Graph Processing on FPGAs with Caches for Thousands of Simultaneous Misses|Mikhail Asiatici||||||The problem definition addressed in this research revolves around the inefficient processing of large-scale graph algorithms on traditional computing architectures. The context that makes this problem important is the increasing prevalence of graph-based data in various domains, such as social networks, web graphs, and biological networks, which necessitates efficient processing of these massive datasets. However, traditional computing architectures, including CPUs, GPUs, and ASICs, are ill-equipped to handle the irregular memory access patterns and massive data sizes associated with graph algorithms, leading to poor performance and high energy consumption. The key objectives or goals set by the authors to address this problem are: 1. To design a novel memory system that can efficiently handle the irregular memory access patterns of graph algorithms. 2. To develop a programming model that can effectively utilize the proposed memory system to accelerate graph processing. 3. To optimize the memory system for throughput-oriented accelerators, focusing on minimizing memory access latency and maximizing bandwidth utilization. By achieving these objectives, the authors aim to develop a scalable and efficient solution for processing large-scale graph algorithms, enabling faster and more energy-efficient processing of massive graph datasets.|Graph,Accelerator,Memory,System,Miss,Optimized,FPGA,Processing,Irregular,Nonblocking
