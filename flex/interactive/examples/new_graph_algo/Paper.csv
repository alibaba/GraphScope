id|published|year|month|title|authors|summary|journal_ref|doi|primary_category|categories|problem_def|keywords
be358770-c0be-534c-aced-17cf753d2396|2019-03-28T16:44:33+00:00|2019|3|BENU: Distributed Subgraph Enumeration with Backtracking-Based Framework|||||||The problem definition addressed in this research is subgraph enumeration, which involves finding all subgraph instances in a large data graph that are isomorphic to a given pattern graph. The context that makes this problem important is its wide applicability in various fields, including network motif mining, graphlet-based network comparison, network evolution analysis, and social network recommendation. The rapid growth of graph-structured data in these fields has created a need for efficient and scalable subgraph enumeration methods. The key objectives or goals set by the authors to address this problem are: Efficiency: Develop a method that can enumerate subgraphs quickly, even for large data graphs and complex pattern graphs. Scalability: Design a method that can handle massive data graphs and pattern graphs, making it suitable for real-world applications. Correctness: Ensure that the method finds all valid subgraph instances without missing or duplicating any matches. To achieve these objectives, the authors propose a novel framework called BENU (Batch Enumeration of Network Units), which leverages a distributed computing platform and a specially designed execution plan to efficiently and scalably enumerate subgraphs.|Subgraph,Enumeration,Distributed,Backtracking,Framework,Pattern,Graph,Matching,Algorithm,Scalability
3306aca4-d957-5366-9e95-0ef3effb3e83|2021-07-22T06:25:10|2021|7|A Block-Based Triangle Counting Algorithm on Heterogeneous Environments|||||||The problem definition addressed in this research is the efficient counting of triangles in large-scale graphs, which is a fundamental problem in graph analysis. The context that makes this problem important is the increasing size and complexity of real-world graphs, such as social networks, web graphs, and biological networks, which require scalable and efficient algorithms to extract valuable insights. The key objective of this research is to develop a scalable and efficient algorithm for triangle counting that can handle massive graphs and minimize data movement, which is a major bottleneck in distributed and heterogeneous environments. The authors aim to achieve this by proposing a novel task decomposition approach that partitions the graph into smaller tasks, allowing for parallel processing and minimizing communication costs. In summary, the problem definition is to develop an efficient algorithm for triangle counting in large-scale graphs, with the key objectives being scalability, efficiency, and minimization of data movement. The authors' approach focuses on task decomposition and parallel processing to achieve these goals.|Triangle counting,Graph processing,Heterogeneous architectures,Parallel algorithms,Task-based parallelism,Block-based partitioning,Subgraph processing,Multi-core processing,Multi-GPU processing,Graph kernels
11c51c5a-3ab4-506d-963c-480e81199b0d|2018-10-31T08:39:05|2018|10|PruneJuice: Pruning Trillion-edgeGraphsto a PrecisePattern-Matching Solution|||||||The problem addressed in this research is the pattern matching problem in graphs, which involves finding subgraphs that match a small template graph within a large background graph. This problem is fundamental to graph analysis and has applications in multiple areas such as social network analysis, bioinformatics, and information mining. The authors aim to develop a technique that can efficiently and accurately identify all matches between the template and background graphs, with a focus on scalability and precision. The key objectives are to: (1) develop a systematic approach to eliminate all vertices and edges that do not participate in any match, and (2) provide a distributed algorithm that can efficiently verify non-local constraints to guarantee no false positives. The authors also aim to explore trade-offs between precision and time-to-solution, and to evaluate the impact of strategic design choices and optimizations on the performance of the algorithm.|Pattern,Matching,Graph,Pruning,Scalable,Distributed,Algorithm,Trillion,Edge,Solution
e1aecd81-90fa-525e-8fc5-49d9cdf2103e|2022-05-06T09:17:41|2022|5|Wake up and join me! An energy-efficient algorithm for maximal matching in radio networks|Varsha Dani||||||The problem addressed in this research is the energy-efficient communication in wireless radio networks, particularly in the context of sensor networks. The background that makes this problem important is the limited energy resources of sensor nodes, which necessitates energy conservation to prolong their lifespan. In such networks, communication is a significant energy drain, and optimizing energy usage is crucial. The key objective of this research is to design distributed algorithms that minimize energy consumption while ensuring reliable communication in wireless radio networks. Specifically, the authors aim to develop algorithms that can efficiently assign neighbors to backup data in case of node failure, thereby maintaining network connectivity and data integrity. The primary goal is to minimize the maximum load (i.e., the number of nodes assigned to a single node) while ensuring that the energy consumption per node remains low. In summary, the problem definition revolves around developing energy-efficient communication protocols for wireless radio networks, with a focus on minimizing energy consumption, maintaining network reliability, and ensuring data integrity in the face of node failures.|Energy,Radio,Networks,Distributed,Algorithm,Matching,Maximal,Polylog,Neighbor,Assignment
89076307-1a7d-5a89-806d-b8e4f0992933|2017-10-12T09:06:03|2017|10|Simple and Near-Optimal Distributed Coloring forSparse Graphs|||||||The problem addressed in this research is the distributed graph coloring problem, specifically for sparse graphs with low arboricity. The context that makes this problem important is its wide range of applications in networks and distributed systems, such as scheduling conflicting tasks, resource allocation, and network optimization. The current state of the art in distributed graph coloring algorithms is unsatisfactory, as they often require a large number of colors, which can lead to inefficient use of resources. The key objective of this research is to develop simple and near-optimal distributed algorithms for graph coloring that can efficiently color sparse graphs using a small number of colors. The authors aim to achieve this goal by designing algorithms that can color the graph in a short number of rounds, using a minimal number of colors, and with high probability of success. The authors also aim to improve upon the current state of the art in distributed graph coloring algorithms, which often have high complexities and require a large number of colors. Overall, the goal of this research is to provide efficient and scalable solutions for distributed graph coloring in sparse graphs, which can have a significant impact on various applications in networks and distributed systems.|Distributed,Graph,Coloring,Algorithms,Arboricity,Logarithmic,Rounds,Complexity,Deterministic,Chromatic
a257e631-8655-5cb4-9a54-d897e9c2dc1d|2017-06-13T06:45:08|2017|6|A Distributed (2 + ε)-Approximation for Vertex Cover in O(log Δ / ε log log Δ) Rounds|||||||The problem addressed in this research is the Minimum Weighted Vertex Cover (MWVC) problem in distributed networks. The context that makes this problem important is that MWVC is a fundamental problem in computer science and operations research, and it has numerous applications in various fields, including network optimization, data mining, and machine learning. Moreover, with the increasing scale and complexity of modern networks, there is a growing need for efficient distributed algorithms to solve this problem. The key objective of this research is to develop a distributed algorithm that can approximate the MWVC problem within a factor of 2 in a polylogarithmic number of rounds. The authors aim to achieve this goal by designing a local ratio algorithm that can be executed in a distributed manner, where each node in the network makes decisions based on local information and communicates with its neighbors. In particular, the authors focus on developing a 2-approximation algorithm that can be completed in O(log log log n) rounds, where n is the number of nodes in the network. This is a significant improvement over previous algorithms, which require a much larger number of rounds to achieve a similar approximation ratio. The authors' goal is to provide a fast and efficient distributed algorithm for solving the MWVC problem, which can be applied to large-scale networks and has potential applications in various fields.|Vertex Cover,Distributed Algorithm,Approximation Algorithm,Graph Algorithm,Local Ratio,Weighted Vertex Cover,Minimum Weighted Vertex Cover,Distributed Computing,Graph Theory,Approximation Ratio
5af0c511-cc44-57ef-aaa4-89f255fd3f59|2023-11-26T05:49:38|2023|11|Object-Oriented Frameworks for Distributed Systems: A Survey|Wai Ming Ho,Jean-Marc Jézéquel||||||The problem definition addressed in this research revolves around the development of distributed parallel computing frameworks, which are essential for building complex distributed applications. The context that makes this problem important is the increasing need for distributed collaborative applications in various domains, such as business, engineering, and science. These applications require efficient management of distributed resources, scalability, and adaptability, which can be achieved through the use of frameworks. The key objectives or goals set by the authors to address this problem are: 1. To provide a clear understanding of the concept of frameworks and their role in building distributed parallel computing applications. 2. To identify the challenges and difficulties associated with developing and using frameworks for distributed parallel computing. 3. To explore ways to improve the development and adaptation of frameworks for distributed parallel computing applications. Overall, the authors aim to contribute to the development of more effective and efficient distributed parallel computing frameworks, which can facilitate the creation of complex distributed applications and ultimately benefit various domains and industries.|Frameworks,Object-Oriented,Documentation,Development,Domain-Specific,Integration,Interoperability,Standards,Reuse,Middleware
ba8d8dd3-0bb6-5de0-ae3d-33e89bf9d067|2020-10-01T16:42:16+00:00|2020|10|A Locality-Aware Energy-Efficient Accelerator for Graph Mining Applications|||||||The problem definition addressed in this research is the acceleration of graph mining applications, which is crucial due to the increasing importance of graph analytics in various domains, such as social networks, advertisement, and bioinformatics. The context that makes this problem important is the massive explosion in data volume, leading to severe random access problems in graph analytics, making it difficult to accelerate on traditional architectures. The key objectives or goals the authors set to address this problem are: To develop a specialized architecture that can efficiently handle the complex, irregular memory accesses inherent in graph mining applications. To identify and optimize the memory access patterns in graph mining, which are different from those in graph processing applications. To design a cost-effective and efficient accelerator that can accelerate graph mining applications, such as clique finding, frequent subgraph mining, and motif counting. Overall, the authors aim to address the problem of accelerating graph mining applications by developing a novel architecture that can efficiently handle the unique memory access patterns and computational requirements of these applications.|Graph mining,Accelerator,Extension locality,Memory hierarchy,Graph processing,Pattern matching,Subgraph isomorphism,Graph acceleration,Specialized hardware,Energy efficiency
2e4c12ab-5f42-5f82-a72e-f53a0e7db647|2014-03-15T07:16:59|2014|3|A survey on techniques for improving the energy efficiency of large-scale distributed systems|||||||The problem definition addressed in this research is the energy inefficiency of large-scale distributed systems, particularly in the context of computing and networking. The background that makes this problem important is the significant and growing energy consumption of these systems, which contributes to environmental concerns, increases operational costs, and affects the overall sustainability of the systems. The key objectives or goals set by the authors to address this problem are: To reduce the energy consumption of large-scale distributed systems while maintaining their performance and quality of service (QoS). To identify and develop techniques that can improve the energy efficiency of these systems, including power management, resource allocation, scheduling, and network traffic management. To provide a comprehensive survey of existing techniques and solutions that can help achieve energy efficiency in large-scale distributed systems. Overall, the authors aim to contribute to the development of sustainable and energy-efficient large-scale distributed systems that can support the growing demands of modern computing and networking applications.|Energy Efficiency,Distributed Systems,Power Management,Computing Resources,Data Centers,Grid Computing,Cloud Computing,Virtualization,Performance Optimization,Sustainability
0371c4b7-c22e-5ee1-8a84-8ad51283e21b|2015-01-25T12:43:17|2015|1|A scalable graph pattern matchingengine on top of Apache Giraph|||||||The problem definition addressed in this research revolves around the scalability of graph pattern matching algorithms, specifically in the context of large-scale graph data processing. The background that makes this problem important is the increasing amount of graph-structured data being generated, which necessitates efficient and scalable algorithms to process and analyze this data. The authors aim to address this problem by exploring the feasibility of mapping Cypher, a query language for graph databases, into Giraph, a distributed graph processing system. The key objectives of this research include: (1) investigating whether a NP-complete problem can benefit from a distributed environment and scale, (2) exploring different options for mapping Cypher into Giraph and evaluating their advantages and disadvantages, and (3) determining whether all Cypher functionality can be translated into Giraph and whether the resulting system would be more scalable than Neo4j.|graph,pattern,matching,Giraph,queries,scalable,engine,Apache,NP,complete,Cypher,analytics,latency,computation,distributed,environment,data,sets,entities,connections,representation,databases,applications,project,system,model,computation,parallel,bulk,synchronous,open,source,design,research,questions,introduction,background,motivation,related,work,language,architecture,algebra,operators,local,global,models,algorithms,systems,thesis,science,computer,faculty,universiteit,amsterdam,sciences,department,student,supervisor,reader,centrum,wiskunde,informatica,amsterdam,december,abstract,advantage,presence,largedata,overcome,need,fast,processing,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,distributed,environments,relevant,problem,complete,especially,presence,largedata,fast,scalable,processing,project,leverages,open,source,built,accepts,equivalent,subset,existing,designed,provide,interactive,strive,focused,o,ine,types,operate,amounts,suited,evaluation,showed,scales,supports,data,sets,entities,large,amounts,data,better,suited,
c084ee8a-70e9-55e6-8575-70787ed41d77|2020-06-17T09:07:16|2020|6|Lower Bounds for Distributed Sketching of Maximal Matchings and Maximal Independent Sets|Sepehr Assadi,Gillat Kol,Rotem Oshman||||||Here is a concise summary of the problem definition addressed in this research: Context and Background: The problem of approximating maximum matching in graphs is a fundamental problem in computer science and has numerous applications in various fields, including computer networks, social networks, and data mining. In many real-world scenarios, the graph data is distributed across multiple machines or nodes, making it challenging to solve the maximum matching problem efficiently. This has led to the development of distributed algorithms, which, however, often rely on a high amount of communication between nodes. Problem Definition: The research addresses the problem of approximating maximum matching in graphs in a distributed setting, where the input graph is partitioned across multiple players, and each player can only communicate a limited amount of information to a central referee. The goal is to design a communication-efficient protocol that allows the players to collectively compute an approximate maximum matching in the graph. Key Objectives: The authors aim to develop a protocol that minimizes the communication cost, measured in terms of the number of bits sent by each player, while still achieving a good approximation of the maximum matching. Specifically, they seek to establish a lower bound on the communication complexity of approximating maximum matching in this distributed setting, which would provide a fundamental limit on the efficiency of any protocol designed to solve this problem.|Distributed Sketching,Maximal Matching,Maximal Independent Set,Communication Complexity,Lower Bounds,Graph Algorithms,Broadcast Congested Clique,Distributed Computing,Graph Problems,Algorithmic Complexity
9decf71d-ff1f-5bc4-b7af-b68a66db7f30|2019-02-28T09:26:46|2019|2|Potential Game Theoretic Learning for the Minimal Weighted Vertex Cover in Distributed Networking Systems|||||||The problem addressed in this research is the Minimal Weighted Vertex Cover (MWVC) problem in distributed networking systems. The context that makes this problem important is the need for efficient and decentralized solutions in modern distributed systems, where a central authority may not be available or reliable. In such systems, the MWVC problem arises when selecting a subset of nodes to monitor or control the entire network, with the goal of minimizing the total weight or cost of the selected nodes. The key objective of this research is to develop a distributed algorithm that can solve the MWVC problem in a decentralized manner, without relying on global information or a central authority. The authors aim to design a spatial potential game that can be played by nodes in the network, where each node makes decisions based on local information and interactions with its neighbors. The goal is to converge to a near-optimal solution that minimizes the total weight of the selected nodes, while ensuring that the entire network is covered. In summary, the problem definition addressed in this research is the MWVC problem in distributed networking systems, with the key objective of developing a decentralized algorithm that can efficiently solve the problem without relying on global information or a central authority.|Minimal Weighted Vertex Cover (MWVC),Distributed Optimization,Game Theoretic Approach,Nash Equilibrium,Vertex Cover,Graph Theory,Combinatorial Optimization,Distributed Algorithm,Relaxation Method,Convergence Analysis
c229bdbd-e157-581b-a314-c77bc8d50f49|2020-04-21T23:21:06+00:00|2020|4|Distributed Processing of k Shortest Path Queries over Dynamic Road Networks|Ziqiang Yu,Xiaohui Yu,Nick Koudas,Yang Liu,Yifan Li,,Yueting Chen,Dingyu Yang||||||The problem definition addressed in this research is the identification of K-Shortest Paths (KSPs) in dynamic undirected graphs, particularly in the context of road networks. The background that makes this problem important is the need for efficient and scalable solutions to process numerous KSP queries over large road networks in real-time, which is crucial for various location-based services such as route planning and traffic management. The key objectives or goals the authors set to address this problem are: 1. To develop a distributed solution that can efficiently identify KSPs in dynamic graphs, considering the constantly changing edge weights that represent evolving traffic conditions. 2. To design an approach that can process multiple KSP queries simultaneously, making it suitable for large-scale road networks with numerous users. 3. To provide a scalable solution that can adapt to changing graph structures and weights, ensuring that the identified KSPs remain optimal and up-to-date. Overall, the authors aim to develop a novel approach that can efficiently and effectively identify KSPs in dynamic graphs, enabling real-time route planning and traffic management in large-scale road networks.|KSP (K-Shortest Paths),Dynamic Road Networks,Graph Query Processing,Distributed Query Evaluation,Shortest Path Queries,Road Networks,Graphs,Query Processing,Dynamic Graphs,Navigation Services
f7761cb4-452e-545f-aeaf-237ee2fdc318|2021-05-05T14:28:59|2021|5|Near-optimal Distributed Triangle Enumeration via Expander Decompositions|||||||The problem definition addressed in this research revolves around graph optimization problems in the CONGEST model, a distributed computing framework where vertices in a graph represent computational devices, and edges represent bi-directional communication links. The context that makes this problem important is the increasing need for efficient distributed algorithms to process large-scale graph-structured data, which is common in modern applications such as social networks, web graphs, and biological networks. The key objectives or goals the authors set to address this problem are: To develop efficient distributed algorithms for solving graph optimization problems, such as triangle enumeration, sparse cut, and routing, in the CONGEST model. To minimize the round complexity, which is the number of communication rounds required to solve the problem, while ensuring that the algorithms are scalable and can handle large graphs. To bridge the gap between the CONGEST and CONGESTED CLIQUE models, which are two different distributed computing frameworks, by developing algorithms that can be efficiently implemented in both models. Overall, the authors aim to contribute to the development of efficient distributed algorithms for graph optimization problems, which is crucial for processing large-scale graph-structured data in modern applications.|Distributed,Graph,Algorithm,Routing,Congest,Model,Network,Communication,Complexity,Time
5bf73003-ef64-5a6a-860d-4614fe34dab8|2019-12-28T10:57:01+00:00|2019|12|Distributed Triangle Counting Algorithms in Simple Graph Stream|Mengdi Yu University of Electronic Science,Technology of China,Chao Song University of Electronic Science,Technology of China,Jiqing Gu University of Electronic Science,Technology of China,Ming Liu University of Electronic Science,Technology of China||||||The problem definition addressed in this research is the distributed triangle counting in graph streams. The context that makes this problem important is the increasing scale and complexity of graph-structured data, such as social networks, road traffic networks, and protein structures, where triangles are a fundamental structure. The rapid growth of these networks and the need for real-time analysis make it challenging to process and analyze graph streams efficiently. The key objective of this research is to design a distributed streaming algorithm that can accurately estimate the number of triangles in a graph stream while minimizing the communication cost and variance of estimates. The authors aim to develop an algorithm that can quickly distribute edges to workers, reduce the number of shared edge triangles in each worker, and provide an unbiased estimate of the global number of triangles in the graph stream. In summary, the problem of distributed triangle counting in graph streams is crucial due to the increasing scale and complexity of graph-structured data, and the authors' goal is to develop an efficient and accurate distributed algorithm that can handle large-scale graph streams in real-time.|Triangle counting,Distributed streaming algorithm,Graph stream,Approximate algorithm,Graph algorithm,Streaming algorithm,Distributed algorithm,Graph processing,Online algorithm,Unbiased estimation
7679c89c-8853-5cd5-86eb-0a7b04ce080d|2023-04-05T08:10:41|2023|4|Efficient and Scalable Distributed Graph Structural Clustering at Billion Scale|Kongzhang Hao||||||The problem definition addressed in this research is the distributed Structural Clustering Algorithm for Network (SCAN) on large-scale graphs. The context that makes this problem important is the increasing need to analyze and understand complex networks, which are ubiquitous in various domains such as social media, biology, and epidemiology. The SCAN algorithm is essential in identifying different roles of vertices in a graph, including hubs, outliers, and clusters, which is crucial in many applications such as viral marketing, epidemiology, and search engines. The key objectives or goals the authors set to address this problem are: 1. To develop an efficient and scalable distributed algorithm for SCAN that can handle billion-scale graphs. 2. To reduce the communication overhead and memory consumption, which are major challenges in existing distributed solutions. 3. To maintain the correctness of clustering results across different batches and handle skewed workloads in distributed systems. Overall, the authors aim to provide a distributed SCAN algorithm that is both efficient and scalable, enabling the analysis of large-scale graphs in a distributed computing environment.|Distributed,Graph,Clustering,SCAN,Structural,Algorithm,Scalable,Ecient,Billion-scale,Graph analysis
27f67358-83be-5303-b8bb-d1061e1c8d7a|2020-12-18T03:55:53|2020|12|Distributed Optimization for Weighted Vertex Cover via Heuristic Game Theoretic Learning|||||||The problem definition addressed in this research is the Minimum Weighted Vertex Cover (MWVC) problem, which is a classic NP-hard problem in graph theory. The context that makes this problem important is its wide range of practical applications, such as dynamic map labeling, gene sequence alignment, and mission planning for distributed satellites. The MWVC problem aims to find the optimal vertex cover in a weighted and undirected graph, which is a subset of vertices that touches at least one endpoint of all the edges in the graph, with the minimum weight sum. The key objectives or goals the authors set to address the problem are: 1. To develop a distributed algorithm that can provide closer-to-optimal solutions by introducing problem-specific information into distributed coordination. 2. To improve solution efficiency and computation speed compared to existing state-of-the-art algorithms. Overall, the authors aim to develop a more effective and efficient distributed algorithm to solve the MWVC problem, which has important implications for various real-world applications.|Minimum Weighted Vertex Cover (MWVC),Distributed Algorithm,Game Theory,Nash Equilibrium,Optimization,Graph Theory,Vertex Cover,Weighted Graph,Convergence,Potential Game
f756e97c-7e25-595a-bb8b-d84d607f0355|2016-02-26T04:12:00|2016|2|A Distributed Hybrid Algorithm for the GraphColoring Problem|||||||The problem addressed in this research is the Graph Coloring Problem (GCP), which is a well-known NP-hard problem in computer science and operations research. The context that makes this problem important is its numerous applications in various fields, such as scheduling, timetabling, and resource allocation. The GCP involves assigning colors to vertices in a graph such that no two adjacent vertices have the same color, with the goal of minimizing the number of colors used. The key objective of this research is to develop an efficient algorithm to solve the GCP, particularly for large and complex graphs. The authors aim to design a hybrid algorithm that combines the strengths of different optimization techniques, including multi-agent systems, reinforcement learning, and tabu search, to effectively explore the search space and find high-quality solutions. The ultimate goal is to find a legal k-coloring of the graph, where k is the minimum number of colors required, and to do so in a computationally efficient manner.|Graph Coloring Problem (GCP),Distributed Hybrid (DH),Multi-Agent System (MAS),Tabu Search,Crossover,Perturbation,Mediator Agent,Intensification,Diversification,Optimization
d1768946-ce98-5523-95b4-e3796a85ba96|2018-12-03T09:21:27|2018|12|VColor: A practical vertex-cut based approach for coloring large graphs|||||||The problem definition addressed in this research is the graph coloring problem, which is a fundamental NP-hard problem in graph theory. The context that makes this problem important is its wide range of real-world applications, including Operations Research, Communication Network, Computational Biology, and Compiler Optimization. In these applications, graph coloring is used to model various optimization problems, such as scheduling, frequency assignment, and register allocation. The key objective of this research is to develop an efficient algorithm for graph coloring that can minimize the number of colors used. The authors aim to achieve this goal by proposing a vertex cut-based coloring technique, which involves partitioning the graph into smaller subgraphs, coloring each subgraph, and then combining the local colorings to obtain a global coloring. The authors also focus on optimizing the coloring of a set of graphs, which is a common scenario in many real-world applications. Overall, the authors' goal is to develop a scalable and efficient graph coloring algorithm that can be applied to various real-world problems, with the ultimate objective of minimizing the number of colors used and improving the overall performance of the system.|Graph coloring,Vertex cut,Large graphs,Approximation algorithms,Optimization techniques,Graph database,Multi-query optimization,Coloring algorithms,Independent sets,Computational complexity
f7fdccef-9e9c-58ac-9ee1-e0204907cefd|2018-01-27T04:25:57|2018|1|Parallel algorithms for flexible pattern matching on big graphs|Hongzhi Wang||||||The problem addressed in this research revolves around efficiently identifying a specific group of people with diverse expertise from a large social network, which is represented as a graph. This problem is important in the context of project management, where a team with diverse skills and expertise is required to accomplish a project. The key objective is to find a subgraph in the social network that matches a given pattern, which represents the required team structure and expertise. The authors aim to address this problem by developing an efficient algorithm that can identify the desired team from a large social network, while minimizing data shipment and time complexity. The key goals are to: Develop a scalable algorithm that can handle large social networks. Minimize data shipment between machines to reduce communication overhead. Optimize the time complexity of the algorithm to ensure efficient processing. By achieving these goals, the authors aim to provide an efficient solution for identifying diverse teams from large social networks, which is crucial in various applications such as project management, team formation, and social network analysis.|Graph,Pattern,Matching,Partial,Retrieval,Memory,Semantics,Simulation,Isomorphism,Subgraph
538b8b34-d925-588b-bafd-53546b72f8e1|2015-11-12T02:50:37|2015|11|An Improved Distributed Algorithm for Maximal Independent Set|Mohsen Ghaffari||||||"The problem definition addressed in this research is the local complexity of the Maximal Independent Set (MIS) problem in distributed networks. The context that makes this problem important is that many distributed algorithms, including MIS, have been traditionally analyzed with a global mentality, focusing on the time until all nodes terminate, rather than the time until each individual node terminates. This global approach can lead to a gap in understanding the locality of the problem. The key objective of this research is to question whether this global mentality is necessary and to investigate the local complexity of the MIS problem. Specifically, the authors aim to answer the ""Local Complexity Question"": How long does it take for each particular node to terminate and know whether it is in the eventual MIS or not, with a high probability? The authors seek to provide a tight analysis of the local complexity of the MIS problem, focusing on the time until each node terminates, rather than the global time complexity."|MIS (Maximal Independent Set),Local Complexity,Distributed Algorithm,Graph Shattering,Randomized Algorithm,Luby's Algorithm,CONGESTED CLIQUE,Arboricity,Graph Theory,Distributed Computing
d33778dd-6b6a-5e6f-ac6a-c86f52877538|2018-11-05T16:24:17|2018|11|Distributed Maximal Independent Set using Small Messages|Mohsen Ghaffari||||||The problem addressed in this research is the design of efficient distributed algorithms for computing a Maximal Independent Set (MIS) in a graph, specifically in the LOCAL and CONGEST models of distributed computing. The context that makes this problem important is that MIS is a fundamental problem in graph theory, and its solution has numerous applications in various fields, including computer networks, social networks, and distributed systems. In the LOCAL model, the communication network is abstracted as an n-node graph, where each node has a processor that initially knows only its own O(log n) bit identifier. The goal is to design an algorithm that can compute an MIS in a small number of rounds, where each round allows each node to send one message to each of its neighbors. The key objectives of this research are to: 1. Develop an MIS algorithm that works in the CONGEST model, where each message has a size of at most O(log n) bits. 2. Achieve a round complexity that is polylogarithmic in the size of the graph. 3. Overcome the limitations of previous algorithms that relied on large messages and inherently needed large identifiers. By addressing these objectives, the authors aim to provide a more efficient and scalable solution for computing MIS in distributed systems, which can have a significant impact on various applications that rely on graph theory.|Distributed,Algorithm,Graph,Complexity,CONGEST,Model,Network,Decomposition,Shattering,Maximality
27366fad-9d47-5b1a-bfa9-1b80ec201b04|2014-05-13T07:26:28|2014|5|Continuous pattern detection over billion-edge graph using distributed framework|||||||The problem definition addressed in this research is the continuous pattern detection over evolving graphs. The context that makes this problem important is the increasing need for real-time monitoring and timely response in various applications, such as social networks, cybersecurity, and recommender systems, where graphs are highly dynamic with frequent additions and removals of vertices and edges. The key objective of this research is to develop an efficient and scalable method for detecting patterns in evolving graphs, which can facilitate timely responses to emerging patterns. The authors aim to address this problem by designing a query evaluation framework that can handle continuous pattern detection over evolving graphs, with a focus on minimizing the response time and memory consumption. The ultimate goal is to provide a timely and efficient solution for monitoring related systems, enabling end-users to respond promptly to emerging patterns in the underlying graphs.|Graph,Pattern,Query,Exploration,Matching,Subgraph,Isomorphism,Join,Incremental,Evaluation
5acd9cab-7ca9-5a9a-bac1-6193de6bd1de|2014-03-28T16:18:18|2014|3|Efﬁcient Cohesive Subgraphs Detection in Parallel|simon||||||The problem definition addressed in this research is the k-truss detection problem in massive graphs. The context that makes this problem important is the need to analyze and understand the structure of large-scale graphs, which is crucial in various applications such as social network analysis, recommendation systems, and data mining. However, traditional cohesive subgraph definitions, such as cliques, are too strict and often do not exist in real-world graphs. The k-truss detection problem is a relaxed form of cohesive subgraph detection, which aims to find subgraphs where each edge is involved in at least k-2 triangles. The key objective of this research is to develop an efficient parallel algorithm to detect the maximal k-truss in a graph, which is a fundamental problem in graph analysis. The authors' goal is to design a scalable and efficient algorithm that can handle massive graphs and minimize the number of iterations required to detect the maximal k-truss. They aim to achieve this by exploiting the power-law distribution of edge supports in real-world graphs and developing a parallel algorithm that can take advantage of distributed computing architectures. Overall, the authors' objective is to provide a practical solution for k-truss detection in large-scale graphs, enabling faster and more accurate graph analysis in various applications.|k-truss,Cohesive subgraph,Graph algorithm,Parallel computing,Graph detection,Massive graph analysis,Social cohesion,Graph processing,Subgraph detection,Polynomial time
2bcb7b9a-c93b-55f4-ba89-edbd20ba3608|2020-05-29T10:37:06|2020|5|Future Generation Computer Systems|||||||The problem definition addressed in this research is the efficient processing of large-scale graphs on Graphics Processing Units (GPUs). The context that makes this problem important is the increasing size and complexity of graphs in various applications, such as social networks, web graphs, and biological networks, which require fast and scalable processing to extract valuable insights. The traditional Central Processing Unit (CPU) architecture is no longer sufficient to handle these large graphs, leading to a significant bottleneck in graph processing. The key objectives or goals of this research are to: Develop an efficient graph processing framework that can handle large-scale graphs on GPUs. Minimize the pre-processing time, which includes reading the graph from disk, constructing the necessary data structures, and allocating memory. Optimize the graph processing algorithm to reduce the processing time and improve scalability. To achieve these objectives, the authors propose a novel edge-centric processing model that leverages the parallel processing capabilities of GPUs to accelerate graph processing. The model is designed to reduce memory allocation and pre-processing time, while also improving the scalability and performance of graph processing on GPUs.|Graph,Processing,GPU,Edge,Centric,WolfGraph,Parallel,Memory,Algorithm,Iterative
570fb8ea-0f41-5981-9d50-255e07dad8a6|2020-08-31T04:13:14|2020|8|TriC: Distributed-memory Triangle Counting by Exploiting the Graph Structure|||||||The problem definition addressed in this research is the efficient counting of triangles in large-scale graphs, particularly in distributed memory architectures. The context that makes this problem important is the increasing need to analyze and understand complex networks, such as social networks, web graphs, and biological networks, which can be represented as massive graphs. Counting triangles in these graphs is a fundamental problem with numerous applications, including identifying clusters, communities, and anomalies. The key objective of this research is to develop an efficient and scalable algorithm for triangle counting in distributed memory architectures, which can handle massive graphs with billions of edges. The authors aim to minimize communication overhead, reduce memory usage, and optimize computation to achieve high performance and scalability. Specifically, they focus on reducing the communication pressure by aggregating outgoing data and implementing a policy that trades off computation with communication. Overall, the goal is to enable fast and efficient triangle counting in large-scale graphs, which is essential for various applications in data mining, machine learning, and network analysis.|Triangle Counting,Distributed Memory,Graph Analytics,Parallel Systems,Graph Structure,Communication Patterns,Memory Accesses,Graph Algorithms,Scalability,Graph Processing
91b5a4b1-5f95-5e8e-adf2-fb776491eef9|2016-05-02T06:53:09|2016|5|Distributed Incremental Pattern Matchingon Streaming Graphs|||||||The problem definition addressed in this research is the pattern matching problem in a streaming graph, where a data graph is continuously updated by an unbounded sequence of updates. The context that makes this problem important is the increasing amount of graph-structured data generated from various sources, such as social networks, IoT devices, and mobile networks, which requires efficient and real-time processing to extract valuable insights. The key objective of this research is to develop an efficient and scalable algorithm to match a pattern graph against a streaming data graph, ensuring that the matching results are up-to-date and accurate despite the continuous updates. The authors aim to achieve this goal by designing an incremental algorithm that can handle graph updates in real-time, minimizing the recomputation of matching results and reducing the computational overhead. In particular, the authors focus on the graph simulation model, which is a widely used model for pattern matching in graph-structured data. They aim to develop an algorithm that can efficiently detect changes in the matching results caused by graph updates, ensuring that the matching results are always consistent with the latest graph state. Overall, the authors' goal is to provide a scalable and efficient solution for pattern matching in streaming graphs, enabling real-time insights and decision-making in various applications.|Graph,Pattern,Matching,Streaming,Distributed,Incremental,Algorithm,Processing,Data,Big
36f81dac-f33e-5ad7-9b21-f831176db6dc|2014-12-22T18:14:48|2014|12|Distributed Maximum Matching in Bounded Degree Graphs|||||||The problem definition addressed in this research revolves around designing efficient algorithms for solving graph problems, specifically maximal independent set and maximum matching, in a distributed setting. The context that makes this problem important is the increasing need for scalable and efficient solutions for large-scale graph processing, which is crucial in various applications such as social networks, web graphs, and biological networks. The key objectives or goals the authors set to address this problem are: To develop CentLocal algorithms, which are centralized algorithms that answer queries regarding global solutions to computational problems by performing local sublinear time computations on the input. To transform these CentLocal algorithms into DistLocal algorithms, which are distributed algorithms that can be executed in a local manner, with a limited number of communication rounds. To achieve a trade-off between the number of communication rounds and the quality of the solution, with the goal of obtaining approximate solutions that are close to optimal. Overall, the authors aim to provide efficient and scalable solutions for graph problems in a distributed setting, which is essential for handling large-scale graph data.|CentLocal,Algorithms,Maximum Matching,Distributed,Local,Graph,Approximation,Deterministic,Online,Complexity
cec41564-ffd6-5f13-918c-81b05b071186|2015-09-21T05:28:13|2015|9|Distributed-Memory Algorithms for Maximal Cardinality Matching Using Matrix Algebra|||||||The problem definition addressed in this research is the computation of a maximum cardinality matching in a bipartite graph, which is a fundamental problem in combinatorial optimization with applications in scientific computing, such as permuting a matrix to its block triangular form and computing minimum weight matchings used by sparse direct solvers. The context that makes this problem important is that the existing algorithms for computing maximum cardinality matchings are not scalable and do not perform well on large-scale graphs, leading to a significant decrease in the quality of the matching as the concurrency increases. This limitation hinders the performance of various applications that rely on maximum cardinality matchings. The key objectives or goals of this research are to develop a scalable and efficient algorithm for computing maximum cardinality matchings in bipartite graphs, which can maintain a high-quality matching even on large-scale graphs and with increased concurrency. The authors aim to achieve this by exploiting the properties of sparse matrices and developing a novel algorithm that can efficiently compute maximum cardinality matchings on massively parallel architectures.|Maximal matching,Bipartite graph,Distributed algorithm,Sparse matrix,Graph processing,High-performance computing,Parallel algorithm,Matching algorithm,Graph matching,Scalability
72c152f4-e15c-5ac2-a216-0329667d218c|2020-02-20T07:59:14|2020|2|Quantum Distributed Algorithm for Triangle Finding in the CONGEST Model|Taisuke Izumi,François Le Gall,Frédéric Magniez||||||The problem definition addressed in this research is the triangle finding problem in distributed networks. The context that makes this problem important is the increasing need for efficient algorithms in distributed systems, where communication is limited and scalability is crucial. In particular, the triangle finding problem is a fundamental problem in graph theory, and its solution has numerous applications in social network analysis, data mining, and network optimization. The key objective of this research is to develop an efficient distributed algorithm for solving the triangle finding problem in a communication-restricted model, known as the CONGEST model. The authors aim to design an algorithm that can detect triangles in a distributed network with a minimal number of communication rounds, while ensuring the correctness of the solution. Specifically, the authors focus on reducing the round complexity of the triangle finding problem, which is currently unresolved. They aim to achieve this by developing a novel reduction from the triangle finding problem to a related problem, called FindTriangleInSubnetwork, which can be solved more efficiently. The ultimate goal is to design a fast and scalable distributed algorithm for triangle finding that can be applied to large-scale networks.|Triangle,Listing,CONGEST,Quantum,Algorithm,Distributed,Graph,Complexity,Model,Network
16b3a910-0eb4-5df9-9a8e-f8248552848d|2019-08-31T12:26:36|2019|8|Scalable Triangle Counting on Distributed-Memory Systems|||||||The problem definition addressed in this research is the efficient parallelization of the triangle counting algorithm in large-scale graphs. The context that makes this problem important is the increasing need to analyze and process massive graph datasets in various fields, such as social networks, web graphs, and biological networks. Triangle counting is a fundamental graph analysis task that has numerous applications, including clustering, community detection, and network topology analysis. However, the computational complexity of triangle counting algorithms grows rapidly with the size of the graph, making it a significant challenge for large-scale graphs. The key objective of this research is to develop a scalable and efficient parallel algorithm for triangle counting that can take advantage of modern distributed computing architectures. The authors aim to achieve this goal by designing a parallel algorithm that minimizes communication overhead, balances computational load, and optimizes memory usage. The ultimate goal is to enable fast and accurate triangle counting in massive graphs, facilitating insights and discoveries in various domains.|Triangle,Counting,Distributed,Memory,Parallel,Algorithm,Graph,Partitioning,MPI,Cilk
ff894488-f94f-5bbd-9e4d-00e23974ccf1|2019-07-20T10:02:37|2019|7|A survey of community search over big graphs|Yixiang Fang||||||The problem definition addressed in this research is Community Search (CS) in graphs, which involves finding densely connected subgraphs or communities in a graph that satisfy certain constraints or properties. The context that makes this problem important is the increasing availability of large-scale graph data, such as social networks, biological networks, and knowledge graphs, which require effective methods to identify meaningful communities or clusters. The key objectives or goals of this research are to: Identify densely connected subgraphs or communities in a graph that satisfy certain constraints or properties, such as spatial proximity, social relationships, and attribute similarity. Develop efficient algorithms and indexing techniques to support CS queries, which can be computationally expensive and time-consuming. Provide a comprehensive review of existing CS solutions, categorize them, and analyze their strengths and limitations. The authors aim to address the problem of CS by providing a thorough review of existing solutions, identifying the limitations and challenges of current approaches, and outlining future research directions to advance the field of CS.|Community Search,Attributed Graphs,Keyword-based,Location-based,Weight-based,Graph Queries,Online Queries,Big Graph,Community Retrieval,Cohesiveness Metrics
1fa7c5d3-113e-5195-aa84-38a5aed75444|2015-01-05T07:46:00|2015|1|A survey of general-purpose experiment management tools for distributed systems|Tomasz Buchert||||||The problem definition addressed in this research is the efficient processing of large-scale graphs on Graphics Processing Units (GPUs). The context that makes this problem important is the increasing size and complexity of graphs in various applications, such as social networks, web graphs, and biological networks, which require fast and scalable processing to extract valuable insights. The traditional Central Processing Unit (CPU) architecture is no longer sufficient to handle these large graphs, leading to a significant bottleneck in graph processing. The key objectives or goals of this research are to: Develop an efficient graph processing framework that can handle large-scale graphs on GPUs. Minimize the pre-processing time, which includes reading the graph from disk, constructing the necessary data structures, and allocating memory. Optimize the graph processing algorithm to reduce the processing time and improve scalability. To achieve these objectives, the authors propose a novel edge-centric processing model that leverages the parallel processing capabilities of GPUs to accelerate graph processing. The model is designed to reduce memory allocation and pre-processing time, while also improving the scalability and performance of graph processing on GPUs.|Graph,Processing,GPU,Edge,Centric,WolfGraph,Parallel,Memory,Iterative,Algorithm
79866fc5-3bca-55ff-bd95-90a9c8e678b2|2023-01-27T06:17:34|2023|1|Exploring Truss Maintenance in Fully Dynamic Graphs: A Mixed Structure-Based Approach|||||||The problem definition addressed in this research revolves around the efficient maintenance of trussness in fully dynamic graphs. The context that makes this problem important is the widespread use of graphs in modeling complex systems, VLSI design, and social networks, where identifying cohesive subgraphs is crucial for deriving useful network information. However, maintaining trussness in dynamic graphs is challenging due to the computational intractability of cohesive subgraph computation. The key objectives or goals the authors set to address this problem are: 1. To develop an efficient algorithm for maintaining trussness in fully dynamic graphs, which can handle edge and vertex insertions and deletions. 2. To improve the scalability and efficiency of truss maintenance in large-scale graphs. By achieving these objectives, the authors aim to provide a practical solution for maintaining trussness in dynamic graphs, enabling the efficient analysis of complex systems and networks.|Truss,Graph,Maintenance,Dynamic,Cohesive,Subgraphs,Algorithm,Efficiency,Scalability,Decomposition
4303d0b6-a27d-50d0-8328-73ff56e3e689|2011-08-12T09:46:52|2011|8|Parallel breadth-first search on distributed memory systems|Aydin Bulu,Kamesh Madduri||||||The problem definition addressed in this research is the efficient parallelization of graph algorithms, specifically Breadth-First Search (BFS), on large-scale distributed memory systems. The context that makes this problem important is the increasing significance of graph abstractions in analyzing and understanding complex systems, such as social networks, biological systems, and engineered systems like the power grid and the Internet. The sheer scale of these graphs, with billions of vertices and edges, necessitates the development of scalable parallel algorithms to process them efficiently. The key objective of this research is to design and implement a hybrid parallel BFS algorithm that can effectively utilize the computing resources of large-scale distributed memory systems, while minimizing communication overhead and achieving good scalability. The authors aim to overcome the challenges posed by skewed degree distributions, irregular graph structures, and the need for efficient data partitioning and communication strategies. By achieving these goals, the authors hope to enable the analysis of massive graphs, which is critical in various domains, including social network analysis, network topology analysis, and anomaly detection.|Graph,Traversal,Parallel,BFS (Breadth-First Search),Algorithm,Memory,Performance,Optimization,Multicore,Distributed
02441855-c966-55b1-81ec-90304764011b|2021-04-05T01:01:34+00:00|2021|4|Fast Parallel Algorithms for Euclidean Minimum Spanning Tree and Hierarchical Spatial Clustering|Yiqiu Wang,Shangdi Yu,Yan Gu,Julian Shun||||||The problem definition addressed in this research is the Hierarchical Density-Based Spatial Clustering with Added Noise (HDBSCAN) problem. The context that makes this problem important is the need for efficient and scalable clustering algorithms that can handle large datasets with varying densities and noise. Traditional clustering algorithms, such as DBSCAN, are limited in their ability to handle these complexities, leading to a need for more advanced and robust clustering methods. The key objective of this research is to develop an efficient algorithm for solving the HDBSCAN problem, which involves computing a hierarchy of clusters for a given dataset. The authors aim to achieve this by leveraging the concept of mutual reachability distance, which is a measure of the distance between two points in a dataset. The goal is to develop an algorithm that can efficiently compute the mutual reachability distance and use it to construct a hierarchy of clusters that accurately reflects the underlying structure of the data. Specifically, the authors set out to address the following key objectives: * Develop an efficient algorithm for computing the mutual reachability distance between points in a dataset * Use the mutual reachability distance to construct a hierarchy of clusters that accurately reflects the underlying structure of the data * Improve the scalability and efficiency of the algorithm to handle large datasets with varying densities and noise Overall, the authors aim to provide a robust and efficient solution to the HDBSCAN problem, which can be used to analyze and understand complex datasets in various fields, such as data mining, machine learning, and bioinformatics.|HDBSCAN,EMST,Parallel,Clustering,Algorithm,Dendrogram,MST,Reachability,Core distance,Mutual reachability
e5d11fce-292a-56e7-a7ed-9f238b68332f|2014-08-09T03:53:26|2014|8|IFIP AICT 437 - CSMR: A Scalable Algorithm for Text Clustering with Cosine Similarity and MapReduce|Giannakouris-Salalidis Victor,Plerou Antonia,Sioutas Spyros||||||The problem definition addressed in this research revolves around the challenge of processing and analyzing large volumes of text data, particularly in the context of big data. The rapid growth of data in various domains, such as business intelligence and bioinformatics, has created a need for efficient and scalable methods to handle text data. The authors identify the importance of text clustering, which involves grouping similar text documents together, as a crucial task in text mining. However, traditional methods for text clustering are limited by their inability to handle large datasets, leading to issues with processing speed and scalability. The key objectives of this research are to develop a scalable algorithm for text clustering that can efficiently handle large datasets and to improve processing speed and scalability. The authors aim to achieve this by leveraging the MapReduce programming model and Hadoop framework, which are designed for distributed computing and can handle massive data processing. Specifically, the authors focus on using the Cosine Similarity measure and tf-idf technique to improve the efficiency and effectiveness of text clustering. Overall, the goal of this research is to provide a scalable and efficient solution for text clustering that can meet the demands of big data.|MapReduce,Cosine Similarity,TF-IDF,Text Similarity,Hadoop,Distributed Algorithm,Text Clustering,Big Data,Scalability,Efficiency
9f32d05f-dbaf-5ea5-a93e-1b436088c93b|2022-02-10T05:39:38+00:00|2022|2|Scaling Graph Traversal to 281 Trillion Edges with 40 Million Cores|Huanqi Cao Yuanwei Wang Haojie Wang Heng Lin Zixuan Ma Wanwang Yin Wenguang Chen||||||The problem addressed in this research is the efficient processing of large-scale graph traversal, specifically Breadth-First Search (BFS), on supercomputers. The context that makes this problem important is the increasing scale and complexity of graph data, which is critical in various applications such as financial risk management, epidemic trajectory analysis, and protein sequence prediction. The current graph processing methods are limited by load imbalance, lack of locality, and inherent load imbalance, making it challenging to scale up graph processing. The key objectives or goals of this research are to: Develop an efficient graph partitioning method that can handle massive graphs with hundreds of trillions of edges. Design a parallel BFS algorithm that can scale up to thousands of processors and millions of cores. Achieve high performance and efficiency in graph traversal on new-generation supercomputers, such as the Sunway architecture. Overall, the authors aim to push the boundaries of graph processing and enable the analysis of massive graphs on supercomputers, which is essential for various applications and domains.|Graph Traversal,Breadth-First Search (BFS),Massively Parallel Algorithm,Heterogeneous Architecture,Graph 500 Benchmark,Supercomputing,Distributed Graph Processing,Scalability,Parallelization,Optimization
aeb988a1-8112-5b02-b87c-1dba977e8aa8|2021-10-14T13:58:35|2021|10|A Parallel Algorithm Template for Updating Single-Source Shortest Paths in Large-Scale Dynamic Networks|||||||The problem definition addressed in this research is the efficient update of Single Source Shortest Paths (SSSP) in large-scale dynamic networks. The context that makes this problem important is the increasing size and complexity of real-world networks, such as transportation, communication, social, and cyber-physical systems, which require fast and scalable algorithms to analyze and respond to changes in the network. The key objective of this research is to develop a parallel algorithm that can efficiently update the SSSP in dynamic networks, where edges or vertices are added or removed, without recomputing the entire SSSP from scratch. The authors aim to achieve this goal by designing an algorithm that can identify the affected subgraphs, update the SSSP tree, and minimize redundant computations, thereby reducing the execution time and improving scalability. In essence, the problem definition involves developing a fast, scalable, and efficient algorithm to update the SSSP in dynamic networks, which is critical for various applications, including network analysis, routing, and decision-making.|Single Source Shortest Path (SSSP),Dynamic Networks,Parallel Algorithm,Graph Theory,Network Analysis,Shortest Path Problem,Graph Updates,Scalable Algorithm,GPU Implementation,Large-Scale Networks
0b9ff581-bcdb-5d1d-8454-7d1c07a771a9|2021-01-25T19:28:10|2021|1|A distributed large graph coloring algorithm on Giraph|||||||The problem definition addressed in this research is the Vertex Graph Coloring (VGC) problem, which is a well-known problem in graph theory. The context that makes this problem important is its numerous applications in various domains such as telecommunications, bioinformatics, and the Internet. The VGC problem is NP-hard, meaning that finding the chromatic number (the smallest number of colors required to color a graph) is computationally difficult. The key objective of this research is to develop an efficient algorithm to solve the VGC problem, particularly for large graphs. The authors aim to design a distributed algorithm that can take advantage of parallel architectures to achieve significant performance improvements. The goal is to minimize the number of colors used to color the graph while ensuring that adjacent vertices have different colors. The authors propose a new Giraph graph coloring algorithm that is designed for the vertex-centric model and can effectively handle large graphs.|Graph coloring,Giraph,Distributed algorithm,Vertex centric model,Large graph processing,Parallel computation,Graph theory,Chromatic number,NP-hard problem,Scalability
df996b89-bf39-5f62-948e-e0a99977ecfd|2015-03-21T19:40:44|2015|3|Performance comparison of parallel graph coloring algorithms on BSP model using hadoop|||||||The problem definition addressed in this research is the graph coloring problem, which is a well-known NP-hard problem with many practical applications such as frequency assignment and scheduling. The context that makes this problem important is the need for efficient graph processing systems to handle large-scale graph data, which is becoming increasingly common in big data analytics. The traditional MapReduce model is not efficient for graph processing, and therefore, there is a need for alternative solutions. The key objective of this research is to develop an efficient graph coloring algorithm that can be used in in-memory Pregel-like graph processing systems. The authors aim to design an algorithm that can color a large undirected graph in a distributed manner, using a greedy approach that does not guarantee an optimal solution but can provide a good approximation. The goal is to develop an algorithm that can complete the graph coloring task in a reasonable amount of time, using a limited number of supersteps, and with minimal communication overhead between nodes.|Graph Coloring,Parallel Algorithm,Hadoop,BSP Model,Pregel,Graph Processing,Distributed Algorithm,Big Data Analytics,In-Memory Processing,Graph Algorithm
25a5dfef-2a70-5629-b1fb-1bc3ae26c09f|2019-06-08T09:19:08|2019|6|A 2D Parallel Triangle Counting Algorithm for Distributed-Memory Architectures|Ancy Sarah Tom,George Karypis||||||The problem definition addressed in this research is the efficient parallelization of triangle counting in large-scale graphs. The context that makes this problem important is the increasing need to analyze and process massive graphs in various domains, such as social networks, web graphs, and biological networks. Triangle counting is a fundamental graph operation that has numerous applications, including graph clustering, community detection, and network analysis. The key objective of this research is to develop a scalable and efficient parallel algorithm for triangle counting that can handle large graphs and minimize communication overheads. The authors aim to achieve this by distributing the graph data and tasks across multiple processors, optimizing the computation and communication patterns, and reducing load imbalance and redundant work. In summary, the problem definition is to develop an efficient parallel triangle counting algorithm that can scale to large graphs, minimize communication overheads, and optimize computation and communication patterns to achieve high performance and scalability.|Triangle Counting,Distributed Memory,Graph Algorithms,Scalability,Parallel Processing,MPI,Graph Processing,Large-Scale Graphs,Optimization,Performance
b96f45f9-f570-5137-a6e8-87f09274ebb3|2015-05-22T07:03:35|2015|5|Mining maximal cliques from a large graph using MapReduce: Tackling highly uneven subproblem sizes|Michael Svendsen||||||The problem definition addressed in this research is the Maximal Clique Enumeration (MCE) problem, which involves enumerating all maximal cliques in a large graph. The context that makes this problem important is the increasing size and complexity of graphs in various applications, such as social networks, web analysis, and bioinformatics, which require efficient and scalable methods for graph analysis. The key objectives or goals of the authors are to: Develop a scalable method for enumerating maximal cliques in a graph using MapReduce, a popular framework for processing large data sets in parallel. Achieve effective load balancing to ensure that the parallel resources are utilized efficiently. Evaluate the performance of the proposed solution on large real-world graphs and demonstrate its superiority over previous MapReduce solutions. Overall, the authors aim to provide a fast and efficient solution for the MCE problem, which is a fundamental problem in graph analysis, and has numerous applications in various fields.|Maximal Clique Enumeration (MCE),Graph Mining,MapReduce,Parallel Algorithm,Load Balancing,Large Graphs,Dense Substructures,Clique Enumeration,Graph Analysis,Scalability
85503c78-2638-5081-9e38-b3d6bf9793dc|2015-01-09T09:30:35|2015|1|Subgraph Rank: PageRank for Subgraph-CentricDistributed Graph Processing|||||||The problem definition addressed in this research revolves around the efficient computation of graph centrality measures, specifically PageRank, on large-scale graphs. The context that makes this problem important is the increasing size and complexity of real-world networks, which necessitates the development of scalable and efficient algorithms to analyze these networks. The traditional approach of computing PageRank on a single machine is no longer feasible due to memory and computational constraints. The key objectives or goals the authors set to address this problem are: 1. To develop a subgraph-centric programming abstraction that can efficiently compute graph centrality measures on large-scale graphs. 2. To adapt the PageRank algorithm to this new abstraction, ensuring that it can scale to massive graphs while maintaining accuracy. 3. To explore the potential of BlockRank, a variant of PageRank, in the context of subgraph-centric computing and evaluate its performance. Overall, the authors aim to provide a scalable and efficient solution for computing graph centrality measures on large-scale graphs, enabling the analysis of complex networks in various domains.|PageRank,BlockRank,Subgraph,Centrality,Graph,Algorithm,Distributed,Platform,Scalability,Convergence
e3d3010c-d623-5d7a-9a6f-35323ab904c9|2014-02-14T04:42:25|2014|2|Distributed and Scalable Graph Pattern Matching: Models and Algorithms|zhanglj||||||The problem addressed in this research is graph pattern matching, which seeks to find subgraphs of a data graph that are similar to a given query graph. This problem is important due to the massive scales of modern application domains such as social networks and the World Wide Web, which require processing massive graphs in a timely manner. The authors aim to develop new simulation models that are conceptually similar to existing ones but better suited to the vertex-centric programming paradigm, and to design distributed algorithms for these models that can mitigate performance bottlenecks and provide high scalability. The key objectives are to explore how well graph simulation models fit into the vertex-centric distributed processing paradigm, identify major bottlenecks, and develop new simulation models and algorithms that are competitive and scalable.|Graph pattern matching,Distributed algorithms,Graph simulation,Big data,Vertex centric programming,BSP model,Scalability,Pattern matching models,Subgraph isomorphism,Graph processing frameworks
e9a7161d-1181-5d18-ac60-7c6474b08b8c|2019-12-03T07:43:19|2019|12|FastSV: A Distributed-Memory Connected Component Algorithmwith Fast Convergence|||||||The problem definition addressed in this research is the efficient computation of connected components (CC) in massive graphs, which is a fundamental problem in graph processing. The context that makes this problem important is the increasing scale and complexity of modern graph datasets, which has led to a significant gap between the capabilities of existing algorithms and the needs of real-world applications. The key objective of this research is to develop a scalable and efficient algorithm for computing CC in massive graphs, with a focus on parallelization and optimization techniques to minimize computational time and resources. The authors aim to address the limitations of existing algorithms, such as the SV algorithm, which are not designed to handle massive graphs and are often slow and inefficient. Specifically, the authors set out to achieve the following goals: * Develop a parallel algorithm that can efficiently compute CC in massive graphs * Optimize the algorithm to minimize computational time and resources * Improve the scalability of the algorithm to handle large-scale graph datasets * Evaluate the performance of the algorithm on real-world graph datasets. Overall, the problem definition addressed in this research is critical for many applications, including social network analysis, web graph analysis, and data mining, where efficient CC computation is essential for extracting insights and knowledge from massive graph datasets.|Graph,Algorithm,Connected,Components,Parallel,Scalable,Distributed,Memory,GraphBLAS,SV
7d813974-b532-5e8e-9958-f16d014b7394|2022-01-07T04:49:15|2022|1|Distributed Triangle Approximately Counting Algorithms in Simple Graph Stream|||||||The problem definition addressed in this research is the distributed triangle counting in a graph stream. The context that makes this problem important is the increasing scale and complexity of graph data, which necessitates efficient and scalable algorithms for processing and analyzing graph streams. Graph streams are sequences of edges that arrive continuously, and triangle counting is a fundamental graph analysis task that has numerous applications in social network analysis, recommendation systems, and anomaly detection. The key objectives or goals the authors set to address this problem are: 1. **Accuracy**: To estimate the global triangle count and local triangle counts in the graph stream with high accuracy. 2. **Efficiency**: To design a distributed algorithm that can process the graph stream in real-time, with low communication overhead and efficient use of computational resources. 3. **Scalability**: To develop an algorithm that can handle large-scale graph streams and scale to a large number of workers. To achieve these objectives, the authors propose a distributed triangle counting algorithm that uses a novel edge distribution strategy to distribute the graph stream across multiple workers, and a reservoir sampling-based approach to estimate the triangle counts in each worker. The algorithm aims to provide a trade-off between accuracy, efficiency, and scalability, making it suitable for real-world graph stream processing applications.|Federated,Learning,Graph,Neural,Networks,Distributed,Edge,Aggregation,Decentralized,Variance
3903e312-7d07-539a-b005-7ec5cecb18f7|2017-01-17T07:17:07|2017|1|Path-based holistic detection plan for multiple patterns in distributed graph frameworks|Jun Gao||||||Here is a concise summary of the problem definition addressed in this research: Context and Background: The increasing need for multiple pattern detection over large graphs is observed in various applications, such as disease detection in gene regulatory networks and program analysis. Handling these graph analysis tasks one by one results in resource waste and inefficiency. Problem Definition: The problem addressed in this research is to efficiently evaluate multiple graph pattern queries over a large graph in a distributed environment. The queries are represented as patterns, and the goal is to detect these patterns in the graph. Key Objectives: The authors aim to design a holistic evaluation plan that can efficiently evaluate multiple graph pattern queries in a single pass, minimizing redundant computation and communication. The plan should be suitable for distributed environments, allowing each graph vertex to take actions independently. The authors also aim to optimize the plan by capturing and reusing shared subparts among different queries. Overall, the research addresses the important problem of efficient multiple pattern detection in large graphs, which has significant implications for various applications.|Pattern detection,Multiple queries,Holistic evaluation plan,Distributed graph framework,Optimization,Graph pattern query,Evaluation plan,Parallelization,Graph analysis,Query optimization
a18624ce-3d32-507f-8a96-4cd35c6cdc94|2019-01-25T03:09:38+00:00|2019|1|DISTRIBUTED COLORING OF GRAPHSWITH AN OPTIMAL NUMBER OF COLORS|||||||The problem addressed in this research is the distributed coloring of graphs with an optimal number of colors. The context that makes this problem important is the need for efficient algorithms in distributed computing systems, where a graph represents a network of processors or computers that need to communicate with each other. In this setting, graph coloring is a fundamental problem that has many applications, such as scheduling, resource allocation, and network optimization. The key objective of this research is to develop a distributed algorithm that can color a graph with the minimum number of colors, which is known as the chromatic number of the graph. The authors aim to achieve this goal in a limited number of communication rounds, which is a critical factor in distributed computing systems. Specifically, the authors focus on graphs with a chromatic number close to the maximum degree of the graph, which is a challenging scenario. They set out to design a distributed algorithm that can color such graphs with an optimal number of colors, while minimizing the number of communication rounds required to achieve this goal. The authors' approach involves combining techniques from distributed computing, probability, and graph theory to develop an efficient algorithm that can solve this problem in a scalable and efficient manner.|Distributed,Graph,Coloring,Algorithm,Randomized,Maximum,Degree,Chromatic,Number,Complexity
a12cb4ac-0dfb-5f9b-9f11-9a0e881949ae|2022-09-01T02:10:42+00:00|2022|9|Improved Distributed-memory Triangle Counting by Exploiting the Graph Structure|||||||The problem definition addressed in this research is the efficient distributed memory triangle counting in large-scale graphs. The context that makes this problem important is the increasing size and complexity of graph-structured data, which is becoming a bottleneck in various applications, such as social network analysis, web graph analysis, and bioinformatics. The authors note that traditional serial algorithms are no longer feasible for processing these massive graphs, and distributed memory algorithms are necessary to scale up the computation. The key objective of this research is to develop a scalable and efficient distributed memory algorithm for triangle counting that can handle massive graphs with billions of edges. The authors aim to achieve strong scalability, which means that the algorithm should be able to efficiently utilize an increasing number of processing nodes to solve larger problems. Additionally, the authors aim to minimize memory usage and synchronization overhead, which are critical factors in achieving scalability. Overall, the problem definition is important because it addresses a critical bottleneck in graph processing, and the authors' objectives are focused on developing a scalable and efficient solution that can handle massive graphs.|Distributed,Memory,Graph,Triangle,Counting,Edge,Query,Bloom,Filter,Optimization
19aa7194-2413-59f6-b5a9-539316786f6f|2017-01-24T02:39:29|2017|1|GA-LP: A genetic algorithm based on Label Propagation to detect communities in directed networks|Rodrigo Francisquini||||||"The problem definition addressed in this research is the community detection problem in directed networks. The context that makes this problem important is the increasing complexity of real-world networks, such as social networks, biological networks, and web graphs, which are often represented as directed graphs. The ability to identify communities or clusters in these networks is crucial for understanding their structure, behavior, and evolution.\nThe key objective of this research is to develop an efficient and effective algorithm for community detection in directed networks. The authors aim to address the limitations of existing methods, which are often designed for undirected networks or are computationally expensive. Specifically, the authors seek to develop a genetic algorithm (GA) that can optimize the modularity measure, a widely used metric for evaluating community structure, in directed networks.\nThe authors' goal is to design a GA that can efficiently search for high-quality community partitions in large-scale directed networks, while also considering the unique challenges posed by directed networks, such as link reciprocity and asymmetry. By achieving this goal, the authors hope to provide a valuable tool for researchers and practitioners working with directed networks, enabling them to gain insights into the structure and behavior of these complex systems."|Community detection,Directed networks,Genetic algorithm,Label Propagation,Modularity,Network analysis,Clustering,Graph optimization,Heuristics,Complex networks
c5b84731-6c84-5e05-b620-6669e49b1e87|2019-11-29T20:30:35+00:00|2019|11|Approximate Pattern Matching in Massive Graphs with Precision and Recall Guarantees|Tahsin Reza ,Matei Ripeanu,Geoffrey Sanders,Roger Pearce||||||The problem addressed in this research is the efficient identification of approximate matches in large graphs, specifically in the context of exact pattern matching. The background that makes this problem important is the increasing need for graph-based data analysis in various domains, such as social networks, biology, and chemistry, where exact pattern matching is a fundamental operation. However, exact pattern matching is computationally intractable, making it impractical for large graphs. The key objectives or goals the authors set to address this problem are: To develop a solution that can efficiently identify approximate matches in large graphs, with a focus on reducing the computational complexity of exact pattern matching. To achieve full precision and recall, ensuring that all matching vertices and edges are identified without any false positives. To provide a solution that can handle a range of problem scenarios, including those with strict definitions of match similarity and those requiring full precision and recall. Overall, the authors aim to develop an efficient and effective solution for approximate pattern matching in large graphs, which can enable various applications in graph-based data analysis.|Graph,Matching,Approximate,Template,Edit,Distance,Search,Subgraph,Pattern,Algorithm
45818540-6674-51ce-bc2f-8adaf6f0630f|2018-10-03T01:16:26|2018|10|A Vertex-Centric Graph Simulation Algorithm for Large Graphs|Jingdong Li||||||The problem definition addressed in this research revolves around the efficient querying of large-scale data graphs, which is a crucial task in various applications such as social network analysis, recommendation systems, and knowledge graphs. The context that makes this problem important is the rapid growth of data graphs, which has led to a significant increase in query complexity and processing time. The key objective of this research is to develop an efficient algorithm for querying large-scale data graphs, specifically focusing on cyclic query graphs. The authors aim to address the problem of high query complexity and processing time by proposing a novel algorithm that can efficiently match cyclic query graphs with large-scale data graphs. The specific goals of this research include: 1. Developing an efficient algorithm for querying large-scale data graphs with cyclic query graphs. 2. Reducing the query complexity and processing time for large-scale data graphs. 3. Improving the scalability and efficiency of data graph querying algorithms. Overall, the authors aim to provide a solution that can efficiently query large-scale data graphs, enabling faster and more accurate analysis of complex data structures.|Graph simulation,Vertex centric,Graph pattern matching,Large graphs,Distributed computation,Optimization,Query graph,Data graph,Subgraph isomorphism,BSP (Bulk Synchronous Parallel)
ed8055e1-e343-54e5-b6a9-56d058dad194|2018-01-22T12:09:24|2018|1|Parallel Algorithm for Incremental Betweenness Centrality on Large Graphs|||||||The problem definition addressed in this research is the efficient computation of betweenness centrality in dynamic graphs, which is a fundamental metric in graph analysis. The context that makes this problem important is the increasing size and complexity of real-world graphs, such as social networks, web graphs, and biological networks, which require efficient algorithms to analyze and understand their structure and behavior. The key objective of this research is to develop an incremental algorithm, called iCENTRAL, that can efficiently update betweenness centrality values in a graph after an edge insertion or deletion, without recomputing the entire graph from scratch. The authors aim to achieve this goal by exploiting the structural properties of biconnected components in graphs, which allows them to focus on the affected components and reduce the computational complexity of the algorithm. In summary, the problem definition is to develop an efficient algorithm for updating betweenness centrality in dynamic graphs, which is crucial for analyzing and understanding complex networks. The key objective is to design an incremental algorithm that can update centrality values quickly and accurately, without requiring a full recomputation of the graph.|Betweenness Centrality,Incremental Algorithm,Evolving Graphs,Graph Analysis,Dynamic Graphs,Centrality Metrics,Graph Updates,Scalability,Algorithmic Efficiency,Graph Processing
7336d57f-43a5-5f6b-a5bc-57b290145cda|2023-08-21T01:52:39+00:00|2023|8|Maximum Length-Constrained Flows and Disjoint Paths:Distributed, Deterministic and Fast|||||||The problem addressed in this research is the optimization of length-constrained flows in networks, which is a fundamental problem in network optimization. The context that makes this problem important is that it has numerous applications in distributed computing, network coding, and communication networks, where efficient routing and flow management are crucial. In particular, the authors highlight that the maximum length-constrained flow gives the minimum makespan of multiple unicasts in a network, even when network coding is allowed. The key objective of this research is to develop efficient algorithms for computing length-constrained flows and moving cuts in networks. The authors aim to achieve this goal by proposing new technical contributions, including batched multiplicative weights, a new framework for length-constrained flows, and a novel approach for moving cuts. The ultimate goal is to provide a more efficient and scalable solution for optimizing length-constrained flows in networks, which can have a significant impact on the performance of distributed systems and communication networks.|Distributed algorithms,Network optimization,Congest model,Cycle cover,Disjoint paths,Flow decomposition,Graph algorithms,Hop constraint,Parallel algorithms,Approximation ratio
c5ab0417-7445-501a-8756-efac81019599|2015-09-05T02:32:09|2015|9|Arabesque: A System for Distributed Graph Mining|||||||The problem definition addressed in this research revolves around scalable graph mining, which involves enumerating subgraphs that satisfy certain interestingness criteria in large graphs. The context that makes this problem important is the ubiquity of graph data in various fields, such as the Web, advertising, and biology, and the increasing need for graph analytics. However, designing scalable graph mining algorithms is challenging, especially when dealing with large graphs. The key objectives or goals the authors set to address this problem are: 1. To develop a system that can automatically and systematically explore all embeddings of a pattern in a graph, which is essential for graph mining. 2. To design a distributed graph mining system that can scale to large graphs, overcoming the limitations of centralized approaches. 3. To provide a high-level filter process computational model that allows users to specify their own interestingness criteria and algorithms for graph mining. Overall, the authors aim to create a system that can efficiently and effectively mine large graphs, enabling users to extract valuable insights and patterns from graph data.|Graph mining,Arabesque,Distributed graph processing,Subgraph enumeration,Graph analytics,Scalability,Pattern matching,Graph computation,Graph algorithms,Data mining
6b801da8-897e-5b38-b3c0-62b4ab6077e1|2017-05-17T04:49:11|2017|5|Distributively Computing Random Walk Betweenness Centrality in Linear Time|Qiang-Sheng Hua,Ming Ai,Hai Jin,Dongxiao Yu,Xuanhua Shi||||||The problem definition addressed in this research is the distributed computation of random walk betweenness centrality in networks. The context that makes this problem important is the increasing need to analyze and understand the behavior of complex networks, such as social networks, transportation networks, and biological networks. Betweenness centrality is a key measure of a node's influence over the spread of information in a network, and random walk betweenness centrality is a more realistic and nuanced measure that considers not only shortest paths but also other paths. The key objective of this research is to develop a distributed algorithm that can efficiently compute random walk betweenness centrality in large-scale networks. The authors aim to design an algorithm that can achieve a linear time complexity, which is a significant improvement over existing centralized algorithms that have a time complexity of O(nm), where n is the number of nodes and m is the number of edges. The authors' goal is to enable fast and scalable computation of random walk betweenness centrality in distributed networks, which is essential for many applications, such as identifying influential nodes, predicting information diffusion, and optimizing network topology.|Random walk betweenness,Distributed algorithms,Centrality measures,Network analysis,Betweenness centrality,Random walks,Graph algorithms,Distributed computing,Network flow,Pagerank
852cfead-950f-5ce7-8e9f-7b6b53f1b596|2017-05-24T03:05:05|2017|5|Scalable Single Source Shortest Path Algorithms for Massively Parallel Systems|||||||The problem definition addressed in this research is the Single-Source Shortest Paths (SSSP) problem in large-scale graphs, which is a fundamental problem in graph algorithms. The context that makes this problem important is the increasing size and complexity of real-world graphs, such as social networks, web graphs, and traffic networks, which require efficient algorithms to compute shortest paths. The key objective of this research is to develop an efficient parallel algorithm for solving the SSSP problem in large-scale graphs, with a focus on minimizing the number of relaxations, reducing communication overhead, and achieving good load balancing. The authors aim to overcome the limitations of existing algorithms, such as Dijkstra's algorithm and the Bellman-Ford algorithm, which are not scalable for large graphs. Specifically, the authors set out to achieve the following goals: * Develop an algorithm that can efficiently handle large-scale graphs with billions of vertices and edges. * Minimize the number of relaxations, which is a major contributor to the processing time and communication overhead. * Reduce the communication overhead by avoiding redundant relaxations and minimizing the number of phases. * Achieve good load balancing to ensure that the algorithm can be efficiently parallelized. Overall, the authors aim to develop a scalable and efficient algorithm for solving the SSSP problem in large-scale graphs, which is essential for many applications in computer science and other fields.|SSSP (Single Source Shortest Path),Graph,Algorithm,Scalability,Parallelism,Performance,BFS (Breadth First Search),Graph 500,Dijkstra,Relaxation
cec09e6a-a58c-5bd8-a95d-6de7fc18b91e|2005-06-13T09:47:15|2005|6|Finding strongly connected components in distributed graphs|||||||The problem addressed in this research is the efficient identification of strongly connected components (SCCs) in distributed graphs, particularly in the context of radiation transport applications. The background that makes this problem important is the need for scalable and efficient algorithms to solve large-scale graph problems, which are increasingly common in various scientific domains. In radiation transport, SCCs play a crucial role in solving problems using sweeping methods, and their efficient identification is essential for performance. The key objective of this research is to develop a parallel algorithm that can efficiently identify SCCs in distributed graphs, with a focus on minimizing runtime and improving scalability. The authors aim to achieve this by proposing a modified version of the DCSC algorithm, which is designed to reduce the size of the problem before invoking the DCSC algorithm, thereby improving overall performance. The goal is to develop an algorithm that can efficiently handle large graphs with many SCCs, which is critical for solving complex radiation transport problems.|Parallel,Strongly,Connected,Components,Graph,Algorithm,Radiation,Transport,Sweep,Dependence
be8bf615-6f44-563f-b05a-764179768494|2019-05-16T18:22:20+00:00|2019|5|LACC: A Linear-Algebraic Algorithm for Finding Connected Components in Distributed Memory|||||||The problem addressed in this research is the efficient computation of connected components in large-scale graphs, which is a fundamental problem in graph algorithms. The context that makes this problem important is the increasing size and complexity of graphs in various domains, such as social networks, web graphs, and biological networks, which necessitates the development of scalable and efficient algorithms to process them. The key objective of this research is to design and implement a parallel algorithm for computing connected components that can efficiently utilize modern distributed-memory architectures. Specifically, the authors aim to develop an algorithm that can achieve good load balance, minimize communication overhead, and scale well with the number of processors. The authors focus on the Awerbuch-Shiloach (AS) algorithm, which is a well-known algorithm for computing connected components, and explore its implementation using the GraphBLAS matrix algebra framework. The goal is to develop a scalable and efficient parallel algorithm that can handle large-scale graphs and provide a competitive solution to existing methods.|Connected,Components,Graph,Algorithm,Distributed,Memory,Linear,Algebraic,Parallel,GraphBLAS
7464cc70-abca-5122-bdf8-5e9e47d3bd41|2018-10-31T18:08:24|2018|10|Optimal Distributed Coloring Algorithms for Planar Graphs in the LOCAL model|Shiri Chechik,Doron Mukhtar||||||The problem definition addressed in this research is the k-coloring problem in the LOCAL model of distributed computing. The context that makes this problem important is the need for efficient algorithms in distributed systems, where communication networks are represented by graphs, and each vertex corresponds to a processor, and each edge to a communication line. The k-coloring problem is a fundamental problem in graph theory, which asks to assign colors (integers from 1 to k) to the vertices of a graph such that no two adjacent vertices have the same color. The key objective of this research is to develop an efficient algorithm that can solve the k-coloring problem in the LOCAL model, with a focus on minimizing the number of communication rounds required. The authors aim to achieve this goal by designing an algorithm that can properly k-color a triangle-free planar graph in O(log n) communication rounds, where n is the number of vertices in the graph. This is an important problem because it has applications in various areas, such as scheduling, resource allocation, and network optimization, and an efficient solution can lead to significant improvements in the performance of distributed systems.|Distributed,Coloring,Planar,Graphs,Algorithm,LOCAL,Model,Time,Optimal,Complexity
257e68db-3eca-5c2b-a8dd-0670daf02e12|2023-12-18T16:50:57|2023|12|Distributed subgraph counting|||||||The problem definition addressed in this research is the efficient counting of induced subgraphs in large graphs, specifically focusing on local subgraph counting. The context that makes this problem important is the increasing significance of graph data in various domains, such as social networks, bioinformatics, and recommender systems, where characterizing high-order local structures is crucial. Local subgraph counting plays a key role in this characterization, enabling the computation of metrics that quantify node clustering and community structure. The key objective of this research is to develop an efficient approach for local subgraph counting, which involves counting the occurrences of a given pattern graph around every node in a large data graph. The authors aim to address the challenges of scalability, flexibility, and accuracy in existing approaches, which are limited by their ability to handle large graphs, various pattern graphs, and different orbit types. The authors' goal is to design a novel approach that can efficiently count induced subgraphs, support flexible pattern graphs, and handle different orbit types, ultimately enabling the analysis of complex graph structures in large-scale graph data.|Subgraph,Counting,Local,Graph,Isomorphism,Homomorphism,Query,Pattern,Matching,Enumeration
b6fd1cf3-f5aa-5a2a-81b5-7363ea2c44aa|2018-05-21T03:11:23|2018|5|A Parallel Complex Coloring Algorithm for Scheduling of Input-Queued Switches|||||||The problem definition addressed in this research revolves around the scheduling of packets in an Input-Queued (IQ) switch, a critical component in high-speed networks. The context that makes this problem important is the increasing demand for high-speed networks with high throughput and low latency, particularly in data centers and cloud computing environments. In such networks, IQ switches play a crucial role in forwarding packets efficiently. The key challenge in IQ switch scheduling is to determine the optimal connection pattern between inputs and outputs in each time slot, ensuring that packets are transmitted efficiently while minimizing congestion and packet loss. The authors formulate this problem as an edge coloring problem in a bipartite graph, where each edge represents a packet to be transmitted, and the goal is to find a coloring scheme that minimizes the number of colors (time slots) required. The authors' key objectives are to develop a scheduling algorithm that achieves: 1. **Optimality**: Minimizes the number of colors (time slots) required to transmit all packets. 2. **Parallelizability**: Enables fast computation of the scheduling algorithm in a distributed manner. 3. **Rearrangeability**: Allows for efficient updates to the scheduling algorithm when the traffic pattern changes. By addressing these objectives, the authors aim to develop a scheduling algorithm that can efficiently manage packet transmission in IQ switches, ultimately leading to improved network performance and reliability.|Scheduling,Bipartite graph,Edge coloring,Complex coloring,IQ switch,Frame-based,Parallel processing,Deadlocks,Stopping rule,Throughput
2d60f894-2aa3-5060-950d-4e1f46cb5c4c|2015-12-14T07:33:59|2015|12|A Stable and Distributed Community Detection Algorithm Based on Maximal Cliques|||||||The problem definition addressed in this research is the community detection in large-scale networks. The context that makes this problem important is the increasing availability of large-scale network data from various sources, such as social media platforms, online forums, and government institutions. This data contains valuable information about user behavior, preferences, and relationships, which can be leveraged to identify communities or groups of users with similar characteristics. The key objective of this research is to develop an efficient and effective method for community detection in large-scale networks. The authors aim to address the limitations of existing methods, which are often computationally expensive, require prior knowledge of the number of communities, or are prone to oscillation. The authors' goal is to propose a novel approach that can handle large-scale networks, detect high-quality communities, and overcome the limitations of existing methods. Specifically, the authors focus on developing a distributed community detection algorithm that can efficiently process large-scale networks and detect communities by assembling maximal cliques. The authors' objective is to design an algorithm that can scale to large networks, detect communities with high modularity, and provide a good balance between computational efficiency and community quality.|Community Detection,Maximal Clique,Label Propagation Algorithm (LPA),Distributed Label Propagation Algorithm (DLPA),MapReduce,Graph Processing,Network Analysis,Clustering,Parallel Computing,Modularity
7e54bbdf-88ce-53c0-8367-06094a5634fe|2021-01-11T14:56:54|2021|1|Efficient Distributed k-Clique Mining for Large Networks Using MapReduce|||||||The problem definition addressed in this research is the k-clique mining problem, which involves enumerating all cliques of size k in a large network. The context that makes this problem important is the increasing size and complexity of modern networks, such as social media and web graphs, which require efficient and scalable algorithms to analyze and extract valuable insights. The key objectives or goals of the authors are to: Develop an efficient algorithm for k-clique mining that can handle large networks. Design a parallel and distributed solution to take advantage of modern computing architectures. Improve the scalability and performance of k-clique mining algorithms to enable analysis of massive networks. The authors aim to address the limitations of existing solutions, which are often sequential, time-consuming, and not designed for large-scale networks. By developing a scalable and efficient k-clique mining algorithm, the authors hope to enable researchers and practitioners to extract valuable insights from large networks, such as identifying clusters, communities, and patterns.|k-clique mining,MapReduce algorithms,Distributed graph processing,Large network analysis,Clique enumeration,Parallel graph processing,Scalable algorithms,Network analysis,Graph mining,Data processing
0c167b95-0992-59a6-86e6-bdf1935768e7|2016-01-02T03:30:41|2016|1|Processing SPARQL queries over distributed RDF graphs|Peng Peng||||||The problem definition addressed in this research revolves around efficiently processing SPARQL queries over distributed RDF graphs. The context that makes this problem important is the increasing scale and decentralization of RDF data, which necessitates distributed query processing to ensure scalability and performance. In this setting, the key challenge lies in dealing with crossing edges between fragments, which can lead to a significant increase in intermediate results and communication overhead. The authors aim to address this problem by developing a partition-agnostic framework that can efficiently process SPARQL queries over distributed RDF graphs, minimizing the number of involved vertices and edges in intermediate results. The key objectives or goals of this research are: To develop a method that can efficiently compute local partial matches at each site, which are the overlapping parts between a crossing match and a fragment. To minimize the number of involved vertices and edges in intermediate results, thereby reducing communication overhead and improving query performance. By achieving these objectives, the authors aim to provide a scalable and efficient solution for processing SPARQL queries over distributed RDF graphs, which is essential for various applications that rely on large-scale RDF data.|SPARQL,Distributed,RDF,Graph,Queries,Query,Evaluation,Partial,Processing,Optimization
c145a936-3487-53f4-8e79-74ef84df25ba|2019-05-31T19:02:42+00:00|2019|5|Improving Distribued Subgraph Matching Algorithm on Timely Dataflow|||||||The problem definition addressed in this research is the efficient subgraph matching in large-scale graphs, particularly in the context of graph analytics and querying. The background that makes this problem important is the increasing need to analyze and query large-scale graphs in various domains, such as social networks, biological networks, and knowledge graphs. Subgraph matching is a fundamental operation in graph analytics, but it is computationally expensive and challenging to scale. The key objectives or goals the authors set to address this problem are: To develop an efficient subgraph matching algorithm that can scale to large graphs. To minimize the computational cost and memory usage of the algorithm. To optimize the join plan for subgraph matching to reduce the number of intermediate results. The authors aim to achieve these objectives by proposing a novel algorithm called CliqueJoin, which extends the traditional join-based approach to subgraph matching by using clique and star structures as join units. The algorithm is designed to work efficiently on large-scale graphs and to minimize the computational cost and memory usage.|Subgraph,Matching,CliqueJoin,Distributed,Graph,Query,Join,Algorithm,Enumeration,Optimization
fafc10f1-74df-5627-a721-89fdf52b9bee|2016-06-10T09:20:48|2016|6|Distributed Greedy Approximation to Maximum Weighted Independent Set for Scheduling With Fading Channels|||||||The problem definition addressed in this research revolves around designing a distributed scheduling algorithm for wireless networks with fading channels. The context that makes this problem important is the need for efficient and adaptive scheduling in wireless networks, where channel conditions can vary rapidly. The traditional approach of solving the Maximum Weighted Independent Set (MWIS) problem to achieve throughput optimality is NP-hard and impractical for large networks. The key objectives or goals the authors set to address this problem are: 1. To design a distributed scheduling algorithm that can adapt to changing channel conditions in real-time. 2. To achieve a provable fraction of the optimal throughput, ensuring the algorithm's performance is close to optimal. 3. To develop an algorithm with low complexity, making it feasible for implementation in large-scale wireless networks. By addressing these objectives, the authors aim to provide a practical and efficient solution for scheduling in wireless networks with fading channels, enabling better utilization of network resources and improved overall performance.|Scheduling,Fading Channels,Distributed Algorithm,Maximum Weighted Independent Set,Wireless Networks,Throughput Optimality,Greedy Approximation,Opportunistic Gain,Con ict Graph,Channel State
c9a7fb50-d222-5593-8bab-94d873dcec2e|2017-09-27T12:24:35|2017|9|An Eﬃcient Silent Self-stabilizing 1-MaximalMatching Algorithm Under Distributed Daemonfor Arbitrary Networks|||||||The problem addressed in this research is the development of an efficient self-stabilizing algorithm for the 1-maximal matching problem in arbitrary networks. The context that makes this problem important is the need for distributed systems to withstand transient failures, such as memory corruption, erroneous initialization, or topology changes, and recover to a legitimate configuration. In this context, self-stabilization is a versatile technique to ensure the system's resilience. The key objective of this research is to design an algorithm that can efficiently solve the 1-maximal matching problem in arbitrary networks, which is a fundamental problem in distributed computing with applications in communication scheduling, resource allocation, and client-server systems. The authors aim to achieve this objective by proposing an algorithm that is self-stabilizing, meaning it can converge to a legitimate configuration from any initial state, and remains in a legitimate configuration thereafter. Specifically, the authors focus on developing an algorithm that can solve the 1-maximal matching problem in arbitrary networks, without relying on global identifiers, and under the unfair distributed daemon model, which allows the daemon to select any non-empty set of nodes to execute actions. The authors' goal is to design an algorithm that is efficient in terms of the number of moves required to converge to a legitimate configuration, and can operate in arbitrary networks, making it a versatile solution for various distributed systems.|Self-stabilization,Maximal matching,Distributed algorithm,Unfair daemon,Arbitrary networks,Silent algorithm,Ecient algorithm,Matching problem,Distributed systems,Fault tolerance
c56515e5-d741-50d1-8e98-0247488ea450|2017-08-02T00:19:43+00:00|2017|8|Distributed Approximation of Maximum Independent Set andMaximum Matching|||||||The problem definition addressed in this research is the Maximum Independent Set (MaxIS) problem in distributed networks. The context that makes this problem important is that many real-world networks, such as social networks, wireless sensor networks, and peer-to-peer networks, have a distributed nature, and solving the MaxIS problem in these networks has numerous applications, including clustering, resource allocation, and network optimization. The key objective of this research is to develop an efficient distributed algorithm that can approximate the maximum independent set in a weighted graph, where each node has a bounded degree. The authors aim to achieve a good approximation factor while minimizing the number of communication rounds required to solve the problem. Specifically, the authors focus on designing a distributed algorithm that can be executed in the CONGEST model, which is a standard model for distributed computing in networks. The goal is to develop an algorithm that can be implemented in a distributed manner, where each node only communicates with its neighbors, and the algorithm terminates within a reasonable number of rounds. Overall, the authors' objective is to provide an efficient and scalable solution to the MaxIS problem in distributed networks, which can be applied to various real-world scenarios and has the potential to improve the performance and efficiency of these networks.|Distributed,Approximation,Algorithms,Maximum,Independent,Set,Graphs,CONGEST,Model,Weighted
debd9eec-503a-523a-80e4-15410a2d6edb|2022-09-30T11:17:45|2022|9|Scaling Graph 500 SSSP to 140 Trillion Edges with over 40 Million Cores|||||||The problem definition addressed in this research is the scalability issue of Single-Source Shortest Path (SSSP) algorithms on large-scale graphs, particularly on power-law graphs, in the context of high-performance computing (HPC) and distributed graph processing. The background that makes this problem important is the increasing significance of large-scale data-intensive applications, which are characterized by heavy communication loads, latency sensitivity, and load imbalance. These characteristics make it challenging to scale out graph algorithms, including SSSP, on supercomputers. The key objectives or goals the authors set to address this problem are: 1. To analyze the work efficiency of existing SSSP algorithms, including Bellman-Ford and Stepping, and identify the limitations that hinder their scalability. 2. To design and develop a new SSSP algorithm that can efficiently process large-scale graphs on distributed systems, minimizing the repeated visits to edges and reducing the communication overhead. 3. To optimize the algorithm for better scalability, exploring techniques such as sparsity optimization and dynamic sliding windows to reduce the computational complexity and improve the performance. Overall, the authors aim to develop a scalable SSSP algorithm that can efficiently process large-scale graphs on distributed systems, addressing the challenges of heavy communication loads, latency sensitivity, and load imbalance.|SSSP (Single Source Shortest Path),Graph 500,Supercomputers,Scalability,Graph Computing,Benchmarking,Shortest Path Problem,Bellman Ford,Hyper Stepping,Parallel Computing
ce718e2b-ebfe-5cc9-8fca-fb62157ca238|2020-07-12T16:38:07|2020|7|On Distributed Listing of Cliques|||||||The problem addressed in this research is the listing of all instances of a subgraph pattern, specifically Kp (complete graph on p nodes), in a distributed network, where each node has limited bandwidth and can only communicate with its neighbors. The context that makes this problem important is the increasing need for efficient algorithms in distributed networks, such as social networks, web graphs, and peer-to-peer networks, where subgraph listing is a fundamental problem with numerous applications. The key objective of this research is to develop an efficient algorithm that can list all instances of Kp in a distributed network, while minimizing the number of communication rounds required. The authors aim to overcome two main challenges: (1) controlling the sparsity of the problem assigned to each cluster, and (2) ensuring that the bandwidth available to each cluster is proportional to the size of the problem assigned to it. In particular, the authors focus on the CONGEST model, which is a widely used model for distributed networks, and aim to develop an algorithm that can list all instances of Kp in O(n^(3/4)) rounds, which is a significant improvement over existing algorithms. Overall, the goal of this research is to provide an efficient solution for subgraph listing in distributed networks, which can have a significant impact on various applications, such as network analysis, data mining, and graph processing.|Distributed,Listing,Cliques,CONGEST,Subgraph,Algorithm,Arboricity,Expander,Decomposition,Sparsity
faf44597-a5fd-5c6d-82be-f7c2bae1b2a8|2018-09-26T13:40:17|2018|9|Distributed Approximate Maximum Matching inthe CONGEST Model|||||||The problem addressed in this research is the maximum fractional matching problem in the CONGEST model, a synchronous distributed computing model. The context that makes this problem important is the need for efficient distributed algorithms to solve graph problems, particularly in large-scale networks where centralized algorithms are impractical. Maximum fractional matching is a fundamental problem in graph theory, and its distributed solution has applications in various fields, such as network optimization, resource allocation, and data processing. The key objective of this research is to develop a deterministic distributed algorithm that approximates the maximum fractional matching in a graph, with a focus on minimizing the number of communication rounds required. The authors aim to achieve a nearly optimal approximation ratio while keeping the communication complexity low. Specifically, they target a polylogarithmic number of rounds, which is a significant improvement over previous algorithms that require a large number of rounds. In summary, the problem definition addressed in this research is the development of an efficient distributed algorithm for maximum fractional matching in the CONGEST model, with the goal of achieving a good approximation ratio while minimizing communication complexity.|Matching,Bipartite,Graphs,Distributed,Algorithm,Maximum,CONGEST,Model,Complexity,Approximation
13d8f711-860f-5a3b-856f-ddf4b38cede7|2015-11-26T01:39:30+00:00|2015|11|Fast Distributed PageRank Computation ∗|||||||The problem definition addressed in this research is the efficient computation of PageRank in a distributed network. The context that makes this problem important is the increasing significance of distributed networks, such as peer-to-peer (P2P) and overlay networks, where nodes need to communicate with each other to perform tasks. PageRank, a fundamental algorithm in graph theory, is crucial in these networks for tasks like ranking nodes, identifying influential nodes, and detecting spam. The key objective of this research is to design a distributed algorithm that can efficiently compute PageRank in a network with n nodes, where each node has limited knowledge and can only communicate with its neighbors. The authors aim to achieve this goal by minimizing the number of communication rounds required to compute PageRank, while ensuring that the algorithm is scalable, efficient, and accurate. Specifically, the authors focus on developing a distributed algorithm that can estimate the PageRank vector with high probability, using a limited number of communication rounds and message sizes. They also aim to improve upon existing algorithms by reducing the number of rounds required to achieve a certain level of accuracy. Overall, the research addresses a critical problem in distributed networks, with the goal of enabling efficient and accurate computation of PageRank in large-scale networks.|PageRank,Distributed Algorithm,Random Walk,Monte Carlo Method,Graphs,Networks,Distributed Computing,Computational Complexity,Scalability,Undirected Graphs
3ad84924-a7dd-5448-9b4e-6dc8bb5800a4|2020-01-01T07:25:38|2020|1|Insert Your Title Here|FirstName Surname,FirstName Surname,FirstName Surname||||||The problem definition addressed in this research is the graph coloring problem, which is a fundamental problem in computer science with numerous applications in clustering, data mining, image capturing, and timetabling. The context that makes this problem important is that graph coloring is NP-hard, meaning there is no known polynomial-time algorithm that can solve it optimally. This complexity leads to a trade-off between solution quality and computation time, making it challenging to find efficient algorithms that can produce good solutions quickly. The key objective of this research is to develop an efficient graph coloring algorithm that can balance solution quality and computation time. Specifically, the authors aim to design an algorithm that can color a graph using as few colors as possible while minimizing the computation time. To achieve this goal, the authors focus on developing shortcuts and optimizations to improve the efficiency of the algorithm, particularly in the context of shared-memory architectures. Overall, the research aims to contribute to the development of fast and effective graph coloring algorithms that can be applied in various domains.|Graph coloring,Priority heuristics,Jones-Plassmann algorithm,Greedy algorithm,Largest Degree First (LDF),Shortcuts,Optimizations,Parallel graph coloring,Graph algorithms,Heuristics
7f212a83-25e9-534d-a751-23baedb19492|2015-04-15T04:49:20|2015|4|Efficient distributed subgraph similarity matching|Ye Yuan||||||The problem definition addressed in this research revolves around optimizing query decomposition in distributed query processing systems. The context that makes this problem important is the increasing complexity of querying large-scale distributed data, which leads to high processing costs and network overhead. In this context, query decomposition is a crucial step that involves breaking down a query into smaller sub-queries that can be executed in parallel across multiple machines. However, the decomposition process can significantly impact the overall performance of the system. The key objective of this research is to optimize query decomposition by minimizing the number of intermediate results and maximizing the sharing of computation among sub-queries. This is achieved by finding the optimal set of h-trees that cover the query, where h-trees are a data structure used to represent the decomposition of a query. The authors' goal is to develop an efficient algorithm that can solve this optimization problem, which is shown to be NP-complete. By addressing this problem, the authors aim to reduce the processing cost and network overhead associated with distributed query processing, ultimately leading to improved system performance and scalability.|Subgraph,Similarity,Matching,Distributed,Graph,Query,Relaxation,Decomposition,Parallel,Processing
2f1de64d-7a0b-52d2-a71b-b2ed36334e65|2023-07-14T13:09:07|2023|7|Distributed (&#x03B1;, &#x03B2;)-Core Decomposition over Bipartite Graphs|||||||The problem definition addressed in this research revolves around efficiently computing the Bi-index, a measure used to analyze the structure of bipartite graphs. The context that makes this problem important is the increasing prevalence of bipartite graphs in various domains, such as social networks, recommender systems, and biological networks, where understanding the graph structure is crucial for applications like community detection, link prediction, and graph clustering. The key objective of this research is to develop an efficient algorithm for computing the Bi-index, which is a challenging task due to the high computational complexity of existing methods. The authors aim to address this problem by proposing optimizations that reduce the computational cost and improve the scalability of Bi-index computation, making it feasible for large-scale bipartite graphs. Specifically, the authors focus on developing a more efficient algorithm that can handle large bipartite graphs, reduce the number of iterations required for convergence, and minimize the memory usage. By achieving these objectives, the authors aim to provide a practical solution for computing the Bi-index, enabling researchers and practitioners to better analyze and understand the structure of bipartite graphs in various domains.|Core decomposition,Bipartite graphs,Distributed algorithms,Graph processing,Cohesive subgraph,core,Bi-indexes,Distributed computing,Graph partitioning,Scalability
323f7f75-f334-5804-b031-bb8b1f6d563d|2014-03-28T15:36:57|2014|3|Parallel Subgraph Listing in a Large-Scale Graph|||||||The problem addressed in this research is the efficient execution of subgraph listing, a fundamental operation in graph computing, on large-scale undirected graphs in parallel. The context that makes this problem important is the increasing need to analyze massive graphs in various domains, such as social networks, biological networks, and web graphs, to uncover hidden patterns and relationships. Subgraph listing is a computationally challenging task, as the size of the result set can be exponential to the number of vertices in the pattern graph. The key objectives or goals the authors set to address this problem are: 1. To design a parallel framework, PSgL, that can efficiently execute subgraph listing on large-scale graphs. 2. To develop optimization techniques to reduce the computation, communication, and memory costs associated with subgraph listing. 3. To achieve a good workload balance among parallel workers to minimize the impact of workload imbalance on performance. Overall, the authors aim to provide a scalable and efficient solution for subgraph listing on large-scale graphs, enabling faster and more accurate analysis of complex networks.|Subgraph,Listing,Parallel,Graph,Large-scale,Algorithm,Enumeration,Pattern,Distributed,Scalability
3ae1dae8-54fe-5160-8d98-8fc70518ce5b|2014-06-21T10:26:58|2014|6|Distributed Graph Simulation: Impossibility and Possibility|XinWang||||||"The problem definition addressed in this research is the efficient querying of big graphs that are fragmented and distributed across multiple sites. The context that makes this problem important is the increasing prevalence of large-scale graph-structured data in various domains, such as social networks, biological networks, and knowledge graphs, which are often too large to be stored and processed on a single machine. As a result, these graphs are typically fragmented and distributed across multiple sites, making it challenging to query them efficiently. The key objective of this research is to develop algorithms that can efficiently query these distributed graphs while minimizing data shipment and response time. Specifically, the authors aim to design algorithms that are ""parallel scalable"" in response time and data shipment, meaning that the algorithm's performance should improve as the number of processing sites increases, and the amount of data shipped should be independent of the size of the graph. Overall, the problem of querying big graphs is important because it has significant implications for various applications, such as social network analysis, recommendation systems, and knowledge graph querying. The authors' goal is to develop efficient and scalable algorithms that can handle large-scale graph-structured data, enabling faster and more efficient querying and analysis of these datasets."|Graph,Simulation,Pattern,Matching,Distributed,Query,Evaluation,Fragmentation,Algorithm,Scalability
19e3d7ed-ff13-578f-9c1d-63898c64be65|2022-10-04T17:20:53|2022|10|Awake-Efficient Distributed Algorithms for Maximal Independent Set|||||||The problem definition addressed in this research is the design of distributed algorithms for the Maximal Independent Set (MIS) problem in the sleeping model, which is a fundamental problem in distributed computing with various applications. The context that makes this problem important is the need to minimize resource utilization in resource-constrained networks, such as ad hoc wireless, sensor, and IoT networks. In these networks, nodes have limited energy and computational resources, and hence, it is crucial to design algorithms that are energy-efficient and minimize the number of rounds a node needs to be awake. The key objective of this research is to design distributed MIS algorithms that have small awake complexity, which is the worst-case number of awake rounds needed by any node until it terminates. The authors aim to achieve this objective by developing algorithms that can solve the MIS problem in a small number of awake rounds, while also minimizing the traditional time complexity of the algorithm. In other words, the authors want to design algorithms that are not only energy-efficient but also have a fast running time.|Distributed,Algorithms,Maximal,Independent,Set,Awake,Complexity,Random,Graphs,Sleeping
6cae13c9-9af6-5461-8749-57dbe935f3a8|2013-12-25T01:21:17|2013|12|STREAMER: A distributed framework for incremental closeness centrality computation|||||||The problem addressed in this research is the efficient maintenance of closeness centrality (CC) scores in dynamic networks. The context that makes this problem important is the widespread use of networks to model various systems, such as social interactions, web pages, and traffic patterns. In these networks, nodes have varying levels of importance, and CC scores quantify the relative importance of each node. However, when the network changes, the CC scores need to be updated, which is a computationally expensive task. The key objective of this research is to develop an efficient method for incrementally maintaining CC scores in dynamic networks. The authors aim to achieve this by designing a distributed framework that can quickly update CC scores in response to changes in the network, without having to recompute them from scratch. This is crucial for applications that require real-time analysis of network dynamics, such as power grid contingency analysis and social network analysis. Overall, the goal is to provide a scalable and efficient solution for maintaining CC scores in dynamic networks, enabling faster and more accurate analysis of network behavior.|Closeness Centrality,Incremental Computation,Dynamic Networks,Distributed Framework,STREAMER,Network Analysis,Centrality Metrics,Graph Algorithms,Parallel Computing,Scalability
b5e2f9cd-9696-51b1-8b91-f6760b863ece|2021-04-13T22:06:44+00:00|2021|4|Distributed Memory Graph Coloring Algorithms for Multiple GPUs|||||||The problem definition addressed in this research is the graph coloring problem, specifically focusing on distance-1 and distance-2 coloring. The context that makes this problem important is its wide range of applications in various fields, including computer science, operations research, and engineering. Graph coloring is used to model various problems, such as scheduling, resource allocation, and circuit design, where vertices represent entities, and edges represent conflicts or constraints between them. The goal is to assign colors to vertices such that adjacent vertices have different colors, minimizing the total number of colors used. The key objectives or goals the authors set to address this problem are: 1. To develop an efficient parallel algorithm for graph coloring that can handle large-scale graphs. 2. To minimize the number of colors used in the coloring, which is an NP-hard optimization problem. 3. To achieve good weak scaling behavior on multiple GPUs, enabling the solution of massive graphs. Overall, the authors aim to provide a scalable and efficient solution to the graph coloring problem, which is essential for various applications and has significant implications for performance and resource utilization.|Graph coloring,Distributed memory,Multiple GPUs,Parallel algorithms,Coloring problem,Distance-1 coloring,Distance-2 coloring,Speculative approach,Iterative approach,Scalability
257c00c0-89de-5268-8b90-131dd79205d6|2015-05-23T08:40:34|2015|5|BFS-4K: An Efficient Implementation of BFS for Kepler GPU Architectures|||||||The problem addressed in this research is the efficient parallelization of the Breadth-First Search (BFS) algorithm on Graphics Processing Units (GPUs) for large-scale graph processing. The context that makes this problem important is the increasing need to process massive graphs in various domains, such as engineering, finance, medicine, and scientific applications, which can involve millions of vertices. The traditional sequential BFS algorithm is inefficient for large graphs, and existing parallel solutions on GPUs often suffer from workload imbalance and are asymptotically less efficient than the fastest CPU implementations. The key objectives or goals of this research are to: Develop an efficient parallel BFS algorithm on GPUs that can handle large-scale graphs. Overcome the workload imbalance issue in existing parallel solutions. Achieve a computational complexity comparable to or better than the fastest CPU implementations. To address these objectives, the authors propose a novel parallel BFS algorithm that leverages the massively parallel architecture of GPUs, incorporating techniques such as dynamic virtual warps, edge discovery, and two-level exclusive prefix sum to manage the frontier propagation steps efficiently.|BFS (Breadth-First Search),GPU (Graphics Processing Unit),Parallel,Graph,Algorithm,Kepler,CUDA,Implementation,Performance,Optimization
d99c679c-0bdc-5cd0-a780-b6a2387144eb|2023-07-07T14:19:39|2023|7|Engineering a Distributed-Memory Triangle Counting Algorithm|||||||The problem definition addressed in this research is triangle counting in large-scale graphs, which is a fundamental graph analysis problem. The context that makes this problem important is the increasing size and complexity of real-world graph datasets, such as social networks, web graphs, and biological networks, which require efficient algorithms to process and analyze. The key objective of this research is to develop scalable and efficient algorithms for triangle counting in distributed memory architectures, which can handle massive graph datasets. The authors aim to address the limitations of existing sequential and parallel algorithms, which are not designed to handle large-scale graphs and are often memory-bound or communication-intensive. Specifically, the authors set out to develop algorithms that can: 1. Scale to thousands of processing elements (PEs) to handle massive graph datasets. 2. Reduce memory requirements and communication overheads. 3. Provide accurate and efficient triangle counting results. By addressing these objectives, the authors aim to enable fast and efficient graph analysis on large-scale graphs, which is essential for various applications, such as social network analysis, web graph analysis, and biological network analysis.|Triangle counting,Distributed memory algorithm,Graph analysis,Clustering coefficient,MPI,Graph processing,Scalability,Parallel computing,Graph algorithms,Large-scale graphs
3d44812e-0506-5b82-b79f-32194c72258b|2017-08-10T13:05:22|2017|8|QFrag: Distributed Graph Search via Subgraph Isomorphism|Marco Serafini,Gianmarco De Francisci Morales,Georgos Siganos||||||The problem addressed in this research is the inefficiency of Bulk Synchronous Parallel (BSP) systems in handling straggler tasks, which are tasks that take significantly longer to complete than others. This problem is important because BSP systems are widely used in various applications, including graph processing, machine learning, and data analytics, where straggler tasks can lead to significant performance degradation and slow down the entire system. The key objective of this research is to develop a task scheduling technique that can efficiently handle straggler tasks and improve the overall performance of BSP systems. The authors aim to achieve this by developing a task fragmentation approach that breaks down tasks into smaller subtasks, detects and separates straggler subtasks, and redistributes them across workers to balance the load and minimize the impact of straggler tasks. In summary, the problem addressed in this research is the inefficiency of BSP systems in handling straggler tasks, and the key objective is to develop a task scheduling technique that can efficiently handle straggler tasks and improve the overall performance of BSP systems.|Graph,Search,Subgraph,Isomorphism,Distributed,Query,Algorithm,Embedding,Enumeration,Fragmentation
7deed2f4-ab43-58ce-a26e-70266636f14b|2022-07-29T07:44:50|2022|7|Distributed PageRank computation with improved round complexities|Siqiang Luo||||||The problem definition addressed in this research is the efficient computation of PageRank in a distributed setting, specifically in the congested clique model. The context that makes this problem important is the increasing need for distributed computation in various applications, such as spatial databases, road network systems, and graph systems. The congested clique model is a fundamental model in distributed computation, where nodes communicate with each other via message passing in synchronous rounds. The key objective of this research is to design an efficient algorithm for computing PageRank in the congested clique model, with a focus on minimizing the round complexity (i.e., the number of communication rounds) and bandwidth (i.e., the size of messages exchanged between nodes). The authors aim to achieve a better trade-off between these two efficiency measures, which is crucial for large-scale distributed systems. In particular, the authors set out to improve upon existing algorithms, such as the IPRA algorithm, which has a round complexity of O(log n) and a bandwidth of O(log n)^3 bits. The goal is to design an algorithm that can estimate PageRank values with a relative error of O(1/log n) in fewer rounds and with a smaller bandwidth, making it more suitable for real-world distributed systems.|PageRank,Distributed Computation,Congested Clique Model,Graph Algorithms,Random Walks,Distributed PageRank,Communication Rounds,Relative Error,Algorithm Design,Graph Processing
5281aa7b-edb2-5d13-8753-ad91c3a408ef|2020-04-14T18:03:07+00:00|2020|4|G-thinker: A Distributed Framework for Mining Subgraphs in a Big Graph|||||||The problem definition addressed in this research is the efficient and scalable mining of subgraphs in large graphs, which is a fundamental problem in graph data analysis. The context that makes this problem important is the increasing availability of large-scale graph data in various domains, such as social networks, biological networks, and web graphs, which requires efficient algorithms to extract valuable insights and patterns. The key objectives or goals the authors set to address this problem are: To design a scalable and efficient subgraph mining algorithm that can handle large graphs with millions of vertices and edges. To minimize the input/output (IO) cost and maximize the CPU utilization, making the algorithm CPU-bound rather than IO-bound. To develop a framework that can be applied to various subgraph mining problems, such as clique, triangle, and subgraph matching. Overall, the authors aim to develop a scalable and efficient subgraph mining algorithm that can handle large graphs and extract valuable insights and patterns, which is essential for various applications in data analysis and machine learning.|Subgraph,Mining,Graph,CPU-bound,Compute-intensive,Clique,Triangle,Subgraph-centric,Distributed,Scalability
8cee156f-b943-5c40-a323-0e9378d6bb6b|2023-01-18T22:05:53+00:00|2023|1|Khuzdul: Efficient and Scalable Distributed Graph Pattern Mining Engine|Jingji Chen,Xuehai Qian||||||The problem definition addressed in this research is the efficient and scalable distributed graph pattern mining, which is a crucial task in various applications such as social network analysis, recommendation systems, and bioinformatics. The context that makes this problem important is the increasing size and complexity of graph-structured data, which poses significant challenges to existing graph mining systems in terms of scalability, performance, and memory usage. The key objectives or goals the authors set to address this problem are: 1. To design a distributed graph pattern mining engine that can efficiently process large-scale graphs and scale to thousands of machines. 2. To minimize the communication overhead and memory usage, which are major bottlenecks in existing systems. 3. To develop a flexible and adaptive scheduling strategy that can effectively utilize the computation resources and balance the workload across machines. To achieve these objectives, the authors propose a novel distributed graph pattern mining engine called Khuzdul, which employs a fine-grained task scheduling strategy, an extendable embedding abstraction, and a hierarchical caching mechanism to optimize the performance and scalability of graph pattern mining.|Graph,Pattern,Mining,Distributed,Engine,Efficient,Scalable,Embedding,Enumeration,Computation
b5ad8214-7885-571f-bb8e-d6e57eb8e8ab|2018-03-09T04:49:08|2018|3|G-Miner: An Efficient Task-Oriented Graph Mining System|Hongzhi Chen,Miao Liu,Yunjian Zhao,Xiao Yan,Da Yan,James Cheng||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph mining tasks in a distributed computing environment. The context that makes this problem important is the increasing availability of massive graph-structured data, which has led to a growing need for scalable and efficient graph mining algorithms to extract valuable insights from these datasets. However, traditional graph mining approaches are often limited by their computational complexity, memory requirements, and communication overhead, making them unsuitable for large-scale graphs. The key objectives or goals the authors set to address this problem are: 1. To design a scalable and efficient distributed graph mining framework that can handle massive graphs. 2. To minimize the communication overhead and memory requirements associated with graph mining tasks. 3. To develop a task pipeline that can effectively overlap computation and communication, ensuring optimal resource utilization. By achieving these objectives, the authors aim to enable fast and efficient graph mining on large-scale graphs, facilitating the extraction of valuable insights and patterns from these datasets.|Graph Mining,Distributed System,Large Scale Graph,Task Oriented,Graph Processing,CPU Utilization,Memory Consumption,Graph Algorithms,Subgraph Mining,Parallel Processing
dcce33ff-a6a5-5563-85f3-4efa2a4863c6|2022-02-17T11:40:01|2022|2|Distributed CONGEST Approximation of Weighted Vertex Covers and Matchings|||||||The problem definition addressed in this research is the distributed approximation of weighted vertex covers and matchings in graphs. The context that makes this problem important is the increasing need for efficient distributed algorithms to solve graph optimization problems in large-scale networks, such as social networks, communication networks, and the internet. The minimum weighted vertex cover (MWVC) and maximum weighted matching (MWM) problems are fundamental optimization problems in graph theory, and their distributed solutions have numerous applications in network optimization, resource allocation, and data analysis. The key objectives or goals of the authors are to develop efficient distributed algorithms that can approximate the MWVC and MWM problems with a good approximation ratio, while minimizing the number of communication rounds required to solve the problems. Specifically, the authors aim to design algorithms that can achieve an approximation ratio below 2 in the CONGEST model, which is a widely used model for distributed graph algorithms. The authors also aim to explore the limitations of distributed algorithms for these problems, including the trade-offs between approximation ratio and communication rounds. Overall, the research aims to contribute to the development of efficient and scalable distributed algorithms for solving graph optimization problems in large-scale networks.|Distributed,Graph,Algorithms,Vertex,Cover,Minimum,Weighted,Approximation,CONGEST,Model
7334f68d-fe0e-5a8f-ad8a-26476a8a3cb7|2015-10-09T21:29:06|2015|10|PL2AP: Fast Parallel Cosine Similarity Search|||||||The problem definition addressed in this research is the All-Pairs Similarity Search (APSS) problem, which involves finding, for each object in a set, all other similar objects with a similarity value above a certain threshold. The context that makes this problem important is its widespread application in various domains, including clustering, online advertising, recommender systems, near-duplicate document detection, and query refinement. The key objective of this research is to develop efficient parallel algorithms to solve the APSS problem, particularly in the context of high-dimensional sparse data, which is common in many real-world applications. The authors aim to achieve significant speedups over existing serial and parallel methods, while ensuring the accuracy and scalability of their approach.|Similarity,Search,Parallel,Cosine,APSS,AllPairs,Sparse,Multi-core,Inverted,Index
e7e98e43-7120-532e-8a59-82d95efba671|2016-11-01T23:25:35|2016|11|Scalable distributed subgraph enumeration|||||||The problem definition addressed in this research is the subgraph enumeration problem in a distributed environment. The context that makes this problem important is the increasing need to process large-scale graph data in various applications, such as social networks, biological networks, and knowledge graphs. The traditional centralized approach is no longer efficient due to the massive size of these graphs, and distributed processing is necessary. The key objective of this research is to develop an efficient algorithm for subgraph enumeration in a distributed environment. The authors aim to address the following goals: 1. To design an optimal execution plan for solving subgraph enumeration, which minimizes the total cost of processing. 2. To develop a pattern decomposition strategy that can effectively reduce the cost of subgraph enumeration. 3. To propose a join plan that can efficiently process the decomposed patterns in a distributed environment. Overall, the authors aim to provide a scalable and efficient solution for subgraph enumeration in large-scale graph data, which is essential for various applications in data mining, machine learning, and graph analytics.|Subgraph,Enumeration,Distributed,Algorithm,Graph,Pattern,Matching,Join,Optimization,Scalability
b47576a5-a3fa-5f9c-8ece-1953a9f6653c|2021-07-27T15:02:14|2021|7|FlexMiner: A Pattern-Aware Accelerator for Graph Pattern Mining|Xuhao Chen||||||The problem definition addressed in this research is Graph Pattern Mining (GPM), which involves finding all subgraphs in a large data graph that are isomorphic to a given pattern graph. The context that makes this problem important is the increasing availability of large-scale graph data in various domains, such as social networks, biological networks, and web graphs, which can provide valuable insights and knowledge when mined effectively. However, the massive combinatorial search space and expensive graph isomorphism tests make GPM computationally intensive, even on moderate-sized graphs. The key objectives or goals of this research are to: Develop a scalable and efficient solution for GPM problems, which can handle large graphs and patterns. Improve the performance of GPM solvers by exploiting parallelism and reducing memory latency. Design a flexible and programmable hardware architecture that can support various GPM problems and patterns. Overall, the authors aim to address the computational challenges of GPM and provide a scalable and efficient solution that can unlock the potential of graph data in various domains.|Graph Pattern Mining (GPM),Accelerator,Pattern Aware,Software-Hardware Co-Design,Graph,Pattern,Mining,Subgraph,Enumeration,Search Tree
08f7fd69-4f54-57b2-a5b9-f84c056ab92f|2017-03-28T17:29:32|2017|3|Parallelizing Sequential Graph Computations|||||||The problem definition addressed in this research revolves around efficiently processing graph queries in a distributed computing environment. The context that makes this problem important is the increasing scale and complexity of graph-structured data, which is becoming a bottleneck for many applications. The authors identify that existing solutions are limited by their reliance on sequential algorithms, which are not optimized for distributed computing and lead to high computational costs and slow query response times. The key objectives or goals the authors set to address this problem are: 1. To develop a parallel graph query processing framework that can efficiently process graph queries in a distributed computing environment. 2. To minimize the communication overhead and iterative recomputation inherent in incremental graph query processing. 3. To support a wide range of graph queries and algorithms, while ensuring correctness and scalability. Overall, the authors aim to provide a scalable and efficient solution for processing graph queries in distributed computing environments, which is essential for many applications that rely on graph-structured data.|Graph,Parallel,GRAPE,Query,Algorithm,Partition,Incremental,Computation,Optimization,Parallelization
4e2705f2-6f63-5264-9aa0-78defe46b86a|2022-03-03T04:54:58|2022|3|An Efficient Index-Based Approach to Distributed Set Reachability on Small-World Graphs|||||||The problem definition addressed in this research is the efficient processing of set reachability queries in distributed graph systems. The context that makes this problem important is the increasing scale and complexity of graph-structured data, which is common in many applications such as social networks, web graphs, and biological networks. In this context, set reachability queries, which aim to find all reachable pairs of vertices between a set of source vertices and a set of target vertices, are a fundamental operation. However, processing these queries efficiently in a distributed environment is challenging due to the need to balance computation and communication costs. The key objectives or goals of this research are to design an efficient indexing approach that can reduce both computation and communication costs, and to develop a query algorithm that can accurately answer set reachability queries in a distributed graph system. The authors aim to achieve these objectives by proposing a novel indexing structure called ML2hop, which is designed to minimize the number of message exchanges between partitions and reduce the computation cost of local queries.|Reachability,Graph,Indexing,Query,Distributed,2-Hop,Labeling,Partitioning,Scalability,Efficiency
9c4e2d87-5bce-5a63-b8b4-293f2b3ebc41|2019-05-27T00:00:44+00:00|2019|5|Improved Distributed Expander Decomposition and Nearly Optimal Triangle Enumeration|Yi-Jun Chang,Thatchaphol Saranurak||||||The problem addressed in this research is the efficient distributed computation of triangle enumeration and expander decomposition in massive graphs, particularly in the Congested Clique model. The context that makes this problem important is the increasing need to process large-scale graphs in various applications, such as social networks, web graphs, and biological networks, where the graph size exceeds the memory capacity of a single machine. Distributed computing is essential to handle such massive graphs, but it poses significant challenges due to communication constraints. The key objective of this research is to develop efficient distributed algorithms for triangle enumeration and expander decomposition, which are fundamental graph problems. Specifically, the authors aim to design algorithms that can solve these problems in a small number of rounds, ideally in O(1) or O(log n) rounds, where n is the number of vertices in the graph. This is crucial because the number of rounds directly affects the communication overhead and, consequently, the overall computation time. In the Congested Clique model, each vertex can communicate with every other vertex, but the bandwidth constraint limits the amount of information that can be exchanged in each round. The authors' goal is to develop algorithms that can efficiently utilize this communication model to solve triangle enumeration and expander decomposition problems, which are essential for various graph analytics tasks.|Expander Decomposition,Distributed Algorithms,Graph Partitioning,Triangle Enumeration,Congested Clique,Low-Diameter Decomposition,Nearly Optimal,Sparse Cut,Graph Decomposition,Distributed Computing
b5df9f40-dcf3-565e-ba7b-7128203ad3e2|2016-06-12T04:46:00|2016|6|PTE: Enumerating Trillion Triangles On DistributedSystems|||||||The problem definition addressed in this research is the enumeration of triangles in massive graphs, which is a fundamental task in graph data analysis. The context that makes this problem important is the increasing size and complexity of real-world networks, such as social networks and the web, which have millions or billions of vertices and edges. This scale poses significant challenges to existing algorithms, making it difficult to process and analyze these networks efficiently. The key objective of this research is to develop a scalable and efficient algorithm for triangle enumeration that can handle massive graphs. The authors aim to overcome the limitations of existing algorithms, which fail to process large graphs due to massive intermediate data and high computational costs. Specifically, the authors set out to design an algorithm that can enumerate triangles in a distributed manner, reducing the amount of shuffled data and improving the performance and scalability of the algorithm. Overall, the problem definition is critical in graph data analysis, and the authors' objectives are focused on developing a practical solution that can efficiently process massive graphs, enabling various applications such as identifying suspicious users, detecting web spams, and finding communities.|Triangle,Enumeration,Distributed,Algorithm,Graph,Big,Data,MapReduce,Scalable,Network
e1c4d452-15ee-5baf-9057-47d7ad0c4ac6|2020-04-24T08:43:41|2020|4|Label Propagation-Based Parallel Graph Partitioning for Large-Scale Graph Data|||||||The problem definition addressed in this research is the balanced graph partitioning problem, which is a critical issue in distributed parallel processing. The context that makes this problem important is the need to efficiently process large-scale graphs in distributed computing environments, where the graph is divided into smaller subgraphs and processed in parallel across multiple machines. However, the partitioning of the graph can significantly impact the performance of the distributed processing, and an unbalanced partitioning can lead to poor performance, increased communication overhead, and decreased scalability. The key objective of this research is to develop an efficient and scalable graph partitioning algorithm that can achieve a balanced partitioning of the graph, minimizing the edge cut while ensuring that the size of each partition is balanced. The authors aim to address this problem by proposing a novel approach that combines the benefits of heuristic and multilevel approaches, with the goal of achieving a better balance between the quality of the partitioning and the computational efficiency of the algorithm.|Graph Partitioning,Balanced Graph Partitioning,Heuristic Approaches,Multilevel Approach,Label Propagation,Local Search Algorithm,Graph Clustering,Edge Cut,Vertex Balance,Scalability
d5617d85-7698-5a36-a302-4a6864c67aae|2015-07-20T09:07:42|2015|7|S2X: Graph-Parallel Querying of RDF withGraphX|||||||The problem definition addressed in this research revolves around efficiently processing and querying large-scale RDF (Resource Description Framework) data, which is a standard for modeling and exchanging data on the web. The context that makes this problem important is the growing amount of RDF data, driven by initiatives like Schema.org, which requires distributed solutions to store and query it. The key objective of this research is to develop a system that can seamlessly combine the benefits of data parallel frameworks (e.g., Spark) and graph parallel computation (e.g., GraphX) to efficiently process and query large-scale RDF data. The authors aim to bridge the gap between the record-centric view of data parallel frameworks and the graph parallel computation of specialized systems, enabling new applications that can leverage the strengths of both approaches.|GraphX,SPARQL,RDF,Hadoop,Spark,Graph parallel,Data parallel,Distributed querying,Graph abstraction,In-memory cluster computing
591c1fa4-6885-574f-9ec4-4f7533346e25|2022-11-08T14:59:15|2022|11|Distributed Maximal Matching and|||||||The problem addressed in this research is the lower bound for the round complexity of locally checkable problems on hypergraphs in the port numbering model. The context that makes this problem important is the need to understand the fundamental limits of distributed computing, particularly in the context of hypergraphs, which are increasingly used to model complex systems in various domains. The key objective of this research is to develop a new approach to prove lower bounds for the round complexity of locally checkable problems on hypergraphs, which has been a challenging task due to the complexity of hypergraphs. The authors aim to overcome the limitations of previous approaches and provide a more general and powerful framework for proving lower bounds. Specifically, the authors focus on the hypergraph maximum matching (MM) problem, which is a fundamental problem in distributed computing, and aim to prove a lower bound for its round complexity in the port numbering model. The goal is to develop a sequence of problems that are increasingly relaxed versions of the original problem, and to show that each problem in the sequence is not solvable in a certain number of rounds, ultimately leading to a lower bound for the round complexity of the original problem. Overall, the authors' objective is to make a significant contribution to the understanding of the fundamental limits of distributed computing on hypergraphs, and to provide a new approach that can be applied to a wide range of locally checkable problems on hypergraphs.|Hypergraph,Lower bound,Maximal matching,Round elimination,Distributed algorithm,CONGEST model,Graph,Coloring,Maximal independent set,Complexity theory
b00ce766-2716-5cc2-98ed-13375257a479|2021-05-12T18:23:17|2021|5|<sc>Trust</sc>: Triangle Counting Reloaded on GPUs|||||||The problem definition addressed in this research is the efficient counting of triangles in massive graphs, which is a fundamental task in graph analysis with numerous applications in anomaly detection, community detection, and robustness analysis. The context that makes this problem important is the rapid growth of graph sizes, making traditional serial algorithms inefficient and unable to handle large-scale graphs. The key objective of this research is to develop a scalable and efficient triangle counting algorithm that can handle massive graphs, with a specific goal of achieving a trillion triangle enumeration per second (TEPS) rate. The authors aim to overcome the limitations of existing algorithms, which are bottlenecked by memory access patterns, workload imbalance, and collision reduction, and to design a novel algorithm that can efficiently utilize GPU architecture to accelerate triangle counting.|Graph,Triangle,Listing,Parallel,Algorithm,Optimization,Reordering,Partitioning,Hashing,CUDA
479cec88-0c7e-5ce0-b0ce-bc3da9388715|2014-08-06T02:39:51|2014|8|Distributed Maximal Clique Computation|||||||The problem definition addressed in this research is the computation and update maintenance of maximal cliques in a graph. Maximal cliques are important substructures in graph analysis, and their computation is a fundamental problem in graph theory and computer science. The context that makes this problem important is the increasing size and complexity of real-world graphs, which makes it challenging to compute and update maximal cliques efficiently. The authors' key objectives are to develop efficient algorithms for computing maximal cliques and updating them when the underlying graph is updated. They aim to achieve this by proposing a new vertex ordering method, called core number ordering, which can reduce the time complexity of maximal clique computation, and by developing algorithms for incremental update maintenance of maximal cliques. The authors also aim to verify the efficiency of their algorithms through experiments on real-world graphs.|Maximal Cliques,Graphs,Algorithms,Distributed Computing,Big Data,MapReduce,Hadoop,Vertex Ordering,Degeneracy,Core Number
ec118c62-15bd-5fa5-a1ba-5df9463936e0|2018-10-26T04:01:53+00:00|2018|10|Distributed Triangle Detection via Expander Decomposition|Yi-Jun Chang,Seth Pettie,Hengjie Zhang||||||The problem definition addressed in this research is the Triangle Detection problem in distributed networks, specifically in the CONGEST model. The context that makes this problem important is the increasing need for efficient distributed algorithms in modern networks, where bandwidth constraints and locality of information are crucial considerations. The Triangle Detection problem is a fundamental graph problem that has numerous applications in social network analysis, data mining, and network optimization. The key objective of this research is to develop an efficient distributed algorithm for Triangle Detection in the CONGEST model, which takes into account the bandwidth constraints and locality of information. The authors aim to design an algorithm that can detect triangles in a distributed network in a minimum number of rounds, while ensuring that the algorithm is scalable and can handle large networks. In particular, the authors focus on developing an algorithm that can solve the Triangle Detection problem in O(√n) rounds, which is a significant improvement over the existing algorithms. The authors also aim to provide a comprehensive analysis of the algorithm's performance, including its round complexity, message complexity, and the trade-offs between these two metrics. Overall, the goal of this research is to contribute to the development of efficient distributed algorithms for graph problems, with a specific focus on the Triangle Detection problem in the CONGEST model.|Graph Partition,CONGEST Model,Distributed Algorithm,Graph Clustering,Minimum Degree,Mixing Time,Local Graph Exploration,Polylogarithmic Time,Graph Diameter,Sublinear Time
8ad7a29b-5e8d-553b-859d-1fd43ecb70a0|2017-02-01T03:22:53|2017|2|Fast Connected Components Computation in Large Graphs by Vertex Pruning|||||||The problem definition addressed in this research is the efficient detection of connected components (CCs) in large-scale graphs, which is a fundamental problem in graph theory and has numerous applications in various fields, including social network analysis, image clustering, and population estimation. The context that makes this problem important is the increasing size and complexity of graphs, which has led to a significant rise in the computational cost and communication overhead of traditional CC detection algorithms. This has resulted in a need for more efficient and scalable solutions that can handle large graphs. The key objectives or goals of the authors are to design a distributed algorithm that can efficiently detect CCs in large-scale graphs, with a focus on minimizing the number of iterations and messages exchanged between nodes. Specifically, the authors aim to develop an algorithm that can achieve a time complexity of O(log n), where n is the number of nodes in the graph, and reduce the communication overhead by exploiting the graph structure. Overall, the authors' goal is to provide a scalable and efficient solution for CC detection in large-scale graphs, which can be applied in various domains and has the potential to significantly impact the performance of graph-based applications.|Distributed,Graph,Algorithms,Connected,Components,Detection,Labelling,Clustering,MapReduce,Pregel
18311aa5-d82d-54f4-a0a7-2bfea6753af5|2021-12-04T01:56:36+00:00|2021|12|Parallel Scheduling Algorithm based on ComplexColoring for Input-Queued Switches|||||||The problem addressed in this research is the scheduling of input-queued (IQ) switches in high-performance datacenter networks. The context that makes this problem important is the rapid growth of datacenter traffic, driven by the increasing demand for cloud computing, big data, and online services. This growth necessitates the development of high-throughput switches that can efficiently manage large volumes of traffic. The key objective of this research is to design an optimal scheduling algorithm that can maximize the throughput of IQ switches while minimizing the complexity of the scheduling process. Specifically, the authors aim to develop a scheduling algorithm that can achieve 100% throughput, which is the maximum possible throughput of an IQ switch, while using a minimal number of colors (or time slots) in the scheduling process. To address this problem, the authors formulate the scheduling problem as an edge-coloring problem of a bipartite graph, where the graph represents the connections between input and output ports of the switch. The goal is to find an optimal edge-coloring scheme that can schedule the packets in the input queues to the output ports without conflicts, while minimizing the number of colors used.|Scheduling,Bipartite graph,Edge coloring,Packet switch,Frame-based,Online scheduling,Chromatic index,Complex coloring,Kempe chain,Graph theory
71940a3f-1244-598d-95f9-0838896fa0c8|2021-12-08T11:52:42|2021|12|Distributed Vertex Cover Reconfiguration|||||||The problem addressed in this research is the Distributed Vertex Cover Reconfiguration problem. In the context of network monitoring, a vertex cover is a set of nodes that monitor all communication links in a network. The problem arises when the network needs to switch from one vertex cover to another, ensuring that each communication link is always monitored during the transition process. This problem is important because it enables efficient and robust network monitoring, which is crucial in various applications such as distributed systems, social networks, and communication networks. The key objectives of this research are to develop distributed algorithms that can efficiently compute a reconfiguration schedule, which is a sequence of steps that transforms one vertex cover into another while maintaining a valid vertex cover at all times. The authors aim to minimize the number of batches (or rounds) required to complete the reconfiguration process, while ensuring that the size of the intermediate vertex covers remains small. The ultimate goal is to provide a robust and efficient solution for distributed vertex cover reconfiguration, enabling seamless transitions between different monitoring configurations in networks.|Reconfiguration,Vertex Cover,Distributed Algorithm,Graph Decomposition,Network Decomposition,Approximation Algorithm,Scheduling,Batch Scheduling,Graph Theory,Computational Complexity
761ab75b-0aeb-560d-92f4-d46ecaba0412|2022-01-28T09:26:22|2022|1|Svelto: High-Level Synthesis of Multi-Threaded Accelerators for Graph Analytics|||||||The problem definition addressed in this research revolves around the efficient execution of tasks in a multi-threaded accelerator architecture. The context that makes this problem important is the increasing demand for high-performance computing in various domains, such as machine learning, data analytics, and scientific simulations. Traditional architectures struggle to meet these demands due to memory access bottlenecks, leading to underutilization of computing resources. The key objective of this research is to design an efficient task scheduling and memory management system that can effectively utilize the available computing resources in a multi-threaded accelerator architecture. The authors aim to achieve this by developing a novel architecture that can dynamically schedule tasks, manage memory requests, and minimize context switching overhead. The primary goal is to improve the overall system performance, increase resource utilization, and reduce memory access latency. In summary, the problem definition involves addressing the memory access bottlenecks and underutilization of computing resources in multi-threaded accelerator architectures, with the objective of designing an efficient task scheduling and memory management system that can improve system performance, resource utilization, and reduce memory access latency.|Graph,Accelerators,Synthesis,Irregular,Algorithms,Memory,Parallelism,Architecture,HLS (High-Level Synthesis),Latency
07169962-b926-5ec0-9d24-77804ce438be|2021-08-04T13:11:12|2021|8|G-thinker: a general distributed framework for finding qualified subgraphs in a big graph with load balancing|Da Yan||||||The problem definition addressed in this research revolves around efficiently processing and mining large-scale graph data to extract valuable subgraphs that satisfy certain structural or label constraints. The context that makes this problem important is the increasing availability of massive graph datasets in various domains, such as social networks, biological networks, and web graphs, which hold valuable insights and patterns waiting to be uncovered. However, the sheer scale and complexity of these graphs pose significant computational challenges, making it difficult to extract meaningful subgraphs in a timely and efficient manner. The key objectives or goals the authors set to address this problem are: 1. To develop a scalable and efficient framework for mining large-scale graph data, capable of handling massive graphs with billions of vertices and edges. 2. To enable the extraction of valuable subgraphs that satisfy specific structural or label constraints, such as maximum cliques, quasi-cliques, and triangles. 3. To minimize the computational cost and memory requirements of graph mining, while ensuring high throughput and responsiveness. Overall, the authors aim to provide a general-purpose distributed framework for graph mining that can efficiently process large-scale graph data, uncover hidden patterns and insights, and support a wide range of graph mining applications.|Subgraph,Graph,Mining,CPU-bound,Compute-intensive,Clique,Triangle,Subgraph-centric,Parallel,Scalability
521b04ce-540c-556b-8564-6f0fb84829f0|2022-07-02T06:39:10|2022|7|Better Approximation for Distributed Weighted Vertex Cover via Game-Theoretic Learning|||||||The problem addressed in this research is the Minimum Weighted Vertex Cover (MWVC) problem, which is a fundamental problem in computer science and graph theory. The context that makes this problem important is its wide range of applications in distributed networking systems, such as network optimization, resource allocation, and fault tolerance. In these systems, finding a minimum weighted set of nodes that covers all edges is crucial to minimize costs and optimize performance. The key objective of this research is to develop a distributed algorithm that can approximate the MWVC solution in a decentralized manner, where each node makes decisions based on local information only. The authors aim to design an algorithm that can provide high-quality solutions, preferably with a better approximation ratio than existing methods, while also being computationally efficient. In summary, the MWVC problem is important due to its applications in distributed networking systems, and the authors' goal is to develop a distributed algorithm that can efficiently approximate the MWVC solution with high quality, without relying on a central authority or global information.|Distributed,Minimum Weighted Vertex Cover (MWVC),Game Theoretic,Optimization,Nash Equilibrium,Potential Game,Local Search,Distributed Decision Making,Combinatorial Optimization,Graph Theory
a5459a01-0253-5d32-8025-5a6a5b08aedc|2022-12-29T03:44:44|2022|12|Toward the minimum vertex cover of complexnetworks using distributed potential games|||||||The problem definition addressed in this research is the Minimum Vertex Cover (MVC) problem in complex networks. The context that makes this problem important is the increasing complexity of real-world networks, such as social networks, biological networks, and the internet, which hinders the efficiency of network covering. The MVC problem is a fundamental problem in computer science and operations research, and its solution has significant implications for various applications, including network optimization, resource allocation, and fault tolerance. The key objective of this research is to develop a novel approach to solve the MVC problem in complex networks. The authors aim to design a decentralized algorithm that can efficiently find the MVC state in a distributed manner, without relying on a centralized controller. The goal is to minimize the number of covered vertices while ensuring that all edges in the network are covered, thereby optimizing network resources and improving overall network performance. In summary, the problem definition addressed in this research is the MVC problem in complex networks, which is important due to its implications for network optimization and resource allocation. The authors' key objective is to develop a decentralized algorithm that can efficiently find the MVC state in a distributed manner, thereby minimizing the number of covered vertices and optimizing network resources.|Vertex Cover,Complex Networks,Potential Game,Minimum Vertex Cover,Strategic Game,Nash Equilibrium,Optimization Algorithm,Distributed Algorithm,Network Science,Combinatorial Optimization
71823dc4-d6a9-5723-b43b-e8a5339e7bb1|2021-04-03T11:35:09+00:00|2021|4|HUGE: An Efficient and Scalable Subgraph Enumeration System|Zhengyi Yang,Longbin Lai,Xuemin Lin,Kongzhang Hao,Wenjie Zhang||||||The problem addressed in this research is the efficient execution of graph pattern matching (GPM) queries, which are crucial in various applications such as social network analysis, recommendation systems, and knowledge graphs. The context that makes this problem important is the increasing scale and complexity of graph data, which poses significant challenges to existing GPM query execution methods. These challenges include high computational costs, large memory requirements, and inefficient communication between machines. The key objectives or goals the authors set to address this problem are: 1. To develop an efficient GPM query execution method that can handle large-scale graph data. 2. To minimize both computation and communication costs in the execution of GPM queries. 3. To design a system that can scale to handle massive graph data and support various GPM query types. To achieve these objectives, the authors propose a novel system called HUGE, which leverages a bounded-memory execution strategy and an optimized execution plan to efficiently execute GPM queries.|Subgraph,Enumeration,Graph,Query,Optimization,Execution,Plan,Join,Algorithm,Scalability
b7c58b35-4817-5a74-b050-6795220957d6|2015-01-09T02:18:42|2015|1|PRS: Parallel Relaxation Simulation for Massive Graphs|||||||The problem definition addressed in this research revolves around graph pattern matching, specifically in the context of social network analysis. The background that makes this problem important is the rapid growth of data volumes in various domains, including social networks, biological networks, and the World Wide Web, which are often represented as graphs. Traditional notions of graph pattern matching are too restrictive to identify patterns in these emerging fields, particularly in social network analysis where some nodes may be absent or replaced by others. The key objective of this research is to develop a more flexible and realistic approach to graph pattern matching, allowing for the absence of some nodes and their replacement by others. The authors aim to design a framework that can efficiently identify patterns in massive graphs, such as those found in social networks, while considering the complexities and nuances of real-life applications. The ultimate goal is to provide a more effective and practical solution for graph pattern matching in big data analytics, particularly in the context of social network analysis.|Graph pattern matching,Distributed algorithms,Big data,Cloud computing,Social networks,Graph query,Data graphs,Pattern graphs,Bulk synchronous parallel,Emerging applications
c638efb6-1911-500b-893b-d2fa4e7cd816|2017-02-18T13:59:56|2017|2|Distributed Algorithms on Exact Personalized PageRank|Tao||||||The problem definition addressed in this research is the efficient computation of Personalized PageRank (PPR) vectors in large-scale graphs. The context that makes this problem important is the increasing need for personalized recommendations and search results in various applications, such as web search, community detection, link prediction, anomaly detection, and recommendation systems. PPR is a widely used measure to capture node-to-node proximities in graphs, but its computation is time-consuming and memory-intensive, making it challenging to scale to large graphs. The key objective of this research is to develop efficient and scalable algorithms for computing PPR vectors in large-scale graphs, with a focus on reducing the computation time, memory requirements, and communication costs. The authors aim to achieve this by proposing distributed algorithms that can be parallelized on multiple machines, enabling the computation of PPR vectors in a timely and efficient manner. The ultimate goal is to enable fast and accurate personalized recommendations and search results in various applications.|Personalized PageRank,Distributed Algorithms,Exact Computation,Graph Computation,Random Walks,Teleport Probability,Hub Nodes,Partial Vectors,Skeleton Vectors,Scalability
e5c627c1-d755-53bb-9272-54a0a2e294be|2020-01-23T02:34:57+00:00|2020|1|Simple and Fast Distributed Computation ofBetweenness Centrality|||||||The problem definition addressed in this research revolves around the efficient computation of betweenness centrality in large-scale networks, particularly in the context of wireless networks and distributed systems. The background that makes this problem important is the increasing reliance on network analysis in various fields, including social network analysis, epidemiology, and computer networks. Betweenness centrality, a key metric in network analysis, measures the extent to which a node lies on the shortest paths between other nodes, making it a crucial indicator of a node's influence and importance in the network. The key objectives or goals the authors set to address this problem are: 1. To develop a decentralized algorithm that can efficiently compute betweenness centrality in large-scale networks, without relying on a centralized authority or global knowledge of the network. 2. To minimize the communication overhead and computational complexity associated with computing betweenness centrality, making the algorithm scalable and suitable for real-world networks. By achieving these objectives, the authors aim to provide a practical solution for computing betweenness centrality in large-scale networks, enabling researchers and practitioners to better understand and optimize network behavior, particularly in the context of wireless networks and distributed systems.|Betweenness Centrality,Distributed Algorithm,Network Analysis,Graph Algorithm,Centrality Measure,Shortest Paths,Communication Complexity,Congest Model,Decentralized Computation,Graph Theory
a65cf2a3-5a71-510b-b2ce-1340d07b333b|2018-06-05T10:01:18|2018|6|Parallel and Streaming Algorithms for K-Core Decomposition|||||||The problem addressed in this research is the k-core decomposition of large graphs, which is a fundamental task in graph mining and network analysis. The context that makes this problem important is the increasing size of data sets available in various fields, such as social networks, biology, and machine learning. The k-core decomposition is a hierarchical clustering method that assigns a score to every node in the network, making it useful for understanding the structure of the network and the role of nodes in different networks. The authors aim to develop efficient algorithms for computing the k-core decomposition in parallel and streaming models, with the goal of achieving a good approximation of the core labeling using small memory and a small number of rounds. Specifically, the authors aim to develop a 1-approximate core labeling algorithm that can be computed efficiently in both streaming and MapReduce models.|k core decomposition,parallel algorithms,streaming algorithms,MapReduce model,graph mining,data mining,machine learning,graph theory,densest subgraph,approximation algorithms
05e7c88e-975a-5001-96b6-d91c44554aa7|2023-07-14T16:40:43|2023|7|Distributed Near-Maximum Independent Set Maintenance over Large-scale Dynamic Graphs|||||||The problem definition addressed in this research is the computation of the Maximum Independent Set (MIS) in large-scale dynamic graphs in a distributed environment. The context that makes this problem important is the increasing scale and dynamic nature of real-world graphs, such as social networks, web graphs, and biological networks, which require efficient and scalable solutions to maintain and analyze graph data. The key objectives or goals of this research are to: Develop an efficient distributed algorithm to compute an approximate Maximum Independent Set (MIS) in large-scale dynamic graphs. Maintain a high-quality maximal near-Maximum Independent Set (MIS) in the presence of edge insertions and deletions. Address the scalability issue of computing MIS in large graphs that cannot fit in a single machine's memory. Overall, the authors aim to provide a distributed solution that can efficiently compute and maintain an approximate MIS in large-scale dynamic graphs, which is essential for various applications in social network analysis, web search, and biology.|Maximum Independent Set (MIS),Distributed Algorithm,Dynamic Graph,Graph Theory,Approximation Algorithm,Vertex Centric,Large Scale Graph,Graph Processing,Independent Set,Graph Update
e2048e62-c1d0-5e15-84ce-3a0a10a4af28|2022-01-25T19:32:21+00:00|2022|1|A localized distributed algorithm for vertex cover problem|Vahid Khalilpour Akram||||||The problem addressed in this research is the Minimum Vertex Cover (MVC) problem in graph theory, which is an NP-hard problem. The context that makes this problem important is its numerous applications in real-life scenarios, such as covering all streets of a city with a minimum number of security cameras, finding the best nodes for monitoring and controlling links in a wireless sensor network, and solving other related NP-hard problems in graph theory. The key objective of this research is to develop a localized distributed algorithm for the MVC problem, which can efficiently find a minimum vertex cover in a distributed system without requiring a single node to have the entire graph of the network. This is important because collecting the network topology of a wireless sensor or multi-hop network as a graph in a single node may impose a large amount of message passing and energy consumption in the nodes. The authors aim to design an algorithm that uses local information to decide about the minimum vertex cover, reducing the need for message passing and energy consumption in distributed systems. The goal is to develop an efficient and scalable algorithm that can be applied in various distributed applications, particularly in battery-powered networks such as wireless sensor networks or the Internet of Things.|Vertex Cover,Distributed Algorithm,Localized Algorithm,Minimum Vertex Cover,Graph Theory,Optimization,Distributed Systems,NP-Hard Problem,Approximation Algorithm,Graph Optimization
b741a5ea-a8a2-5cfa-8bd0-def8538b7fcc|2022-12-15T08:24:12|2022|12|Distributed Approaches to Butterfly Analysis on Large Dynamic Bipartite Graphs|||||||The problem definition addressed in this research is the efficient analysis of butterfly structures in large dynamic bipartite graphs. The context that makes this problem important is the increasing prevalence of large-scale bipartite graphs in various domains, such as social networks, recommender systems, and bioinformatics, where butterfly structures play a crucial role in understanding the graph's properties and behavior. The key objective of this research is to develop distributed approaches to efficiently analyze butterfly structures in large dynamic bipartite graphs, which is challenging due to the massive scale and dynamic nature of these graphs. The authors aim to address this problem by designing algorithms that can process large-scale graphs in a distributed manner, reducing the computational time and memory requirements while maintaining accuracy. Specifically, the authors focus on developing strategies to incrementally update the butterfly analysis when the graph is updated, rather than recomputing the entire analysis from scratch. This is crucial in real-world applications where graphs are constantly evolving, and timely analysis is essential. Overall, the research aims to provide a scalable and efficient solution for butterfly analysis in large dynamic bipartite graphs, enabling better insights and decision-making in various domains.|Bipartite graph,Distributed algorithm,Butterfly counting,Tip decomposition,Dynamic graph,Large-scale graph,Graph mining,Scalability,Memory allocation,Graph processing
a5fc3e5b-c50b-558f-a371-4e7ab480df84|2018-06-16T09:01:33|2018|6|An FPGA Framework for Edge-Centric Graph Processing|Luís Pina,Anastasios Andronidis,Cristian Cadar||||||The problem definition addressed in this research revolves around the efficient processing of large-scale graph data, particularly in the context of edge-centric graph algorithms. The background that makes this problem important is the increasing significance of graph-structured data in various applications, such as social networks, web graphs, and recommendation systems. The processing of these massive graphs poses significant computational challenges, including memory access patterns, data locality, and parallelization. The key objective of this research is to develop an efficient and scalable framework for edge-centric graph processing on Field-Programmable Gate Arrays (FPGAs). The authors aim to achieve this by designing a novel architecture that can effectively handle the irregular memory access patterns and data dependencies inherent in graph algorithms. The primary goals of this research are to: 1. Develop a flexible and efficient FPGA-based framework for edge-centric graph processing. 2. Optimize the framework to minimize memory access latency and maximize parallelization. 3. Demonstrate the effectiveness of the proposed framework using real-world graph datasets and various edge-centric graph algorithms. By addressing these objectives, the authors aim to provide a solution that can efficiently process large-scale graph data, enabling faster and more accurate analysis in various applications.|FPGA,Graph,Acceleration,PageRank,SpMV,Edge-centric,Paradigm,Architecture,Optimization,Performance
6fce2b81-dcb8-5122-b567-783bf0c72d4f|2017-11-02T12:56:34|2017|11|On the distribution of betweenness centrality in random trees|Kevin Durant||||||The problem definition addressed in this research revolves around understanding the distribution of betweenness centrality (b.c.) in random trees, particularly in simply generated (s.g.) trees and increasing trees. The context that makes this problem important is the widespread use of betweenness centrality as a measure of vertex importance in complex networks, which is crucial in various fields such as social network analysis, epidemiology, and computer science. The key objective of this research is to characterize the distribution of b.c. in random trees, which has been lacking in the literature despite its significance. The authors aim to provide a comprehensive understanding of the b.c. distribution in s.g. trees and increasing trees, including the maximum b.c. and the behavior of the centroid (the vertex with the highest b.c.). By achieving this goal, the authors hope to contribute to a deeper understanding of the structural properties of complex networks and inform the development of algorithms and models that rely on betweenness centrality.|Betweenness centrality,Random trees,Simply generated trees,Increasing trees,Subcritical graph classes,Centroid,Network science,Graph theory,Combinatorial analysis,Random graph models
910f76e7-a566-5a06-a82b-223ea6f474c2|2019-06-23T06:41:18+00:00|2019|6|Distributed subgraph matching on timely dataflow|||||||The problem definition addressed in this research revolves around the efficient processing of subgraph matching queries in large-scale graph datasets. The context that makes this problem important is the increasing prevalence of graph-structured data in various domains, such as social networks, biological networks, and knowledge graphs, which necessitates the development of scalable and efficient query processing techniques. The key objective of this research is to design and implement efficient algorithms for subgraph matching that can handle large graphs and complex queries. The authors aim to achieve this goal by exploring various optimization strategies, such as workload-aware expanding, clique-based optimization, and distributed processing, to reduce the computational cost and improve the scalability of subgraph matching algorithms. Specifically, the authors focus on addressing the challenges of subgraph matching, including the high computational complexity, the need for efficient data access and communication, and the requirement for handling large graphs and complex queries. By developing efficient algorithms and optimization strategies, the authors aim to enable fast and scalable subgraph matching, which is essential for various applications, such as graph pattern mining, graph-based machine learning, and graph querying.|Subgraph,Matching,Distributed,Algorithm,Graph,Query,Optimization,Join,Strategy,Performance
113009e1-9d97-550e-920f-1ff8b1d675f4|2019-08-28T12:44:13|2019|8|DistTC: High Performance Distributed Triangle Counting|||||||The problem definition addressed in this research is the efficient counting of triangles in large-scale undirected graphs, which is a fundamental problem in graph analysis with applications in social network analysis, graph statistics, and k-truss identification. The context that makes this problem important is the increasing size and complexity of modern graphs, which renders traditional serial algorithms inefficient and necessitates the development of distributed and parallel solutions. The key objectives or goals set by the authors to address this problem are: 1. To develop a scalable and efficient distributed algorithm for triangle counting that can handle massive graphs. 2. To minimize communication overhead and optimize data locality in the distributed computation. 3. To design a partitioning strategy that ensures each machine in the distributed cluster can perform triangle counting independently without requiring communication with other hosts. By achieving these objectives, the authors aim to provide a practical solution for triangle counting in large-scale graphs, enabling faster and more accurate graph analysis in various domains.|Triangle counting,Distributed memory,Multi-GPUs,Graph partitioning,Clusters,Graph processing,Scalability,Performance optimization,Parallel processing,Large-scale graphs
b02f8814-ee06-5c98-b083-0f6dbda3f020|2014-01-28T07:15:26|2014|1|"Distributed $(\Delta+1)$-Coloring in Linear (in $\Delta$) Time | SIAM Journal on Computing | Vol. 43, No. 1 | Society for Industrial and Applied Mathematics"|Leonid Barenboim,Michael Elkin,Fabian Kuhn||||||The problem addressed in this research is the distributed 1-coloring problem, a fundamental problem in distributed algorithms. The context is a network of processors, each with a distinct identity number, communicating synchronously in discrete rounds. The problem is important due to its applications in scheduling, resource allocation, symmetry breaking, and workload balancing. The key objective is to devise an efficient algorithm that can color the graph in linear time, specifically O(1) time, which is a significant improvement over previous algorithms. The authors aim to achieve this goal by developing a new approach that employs defective coloring, a generalized variant of coloring, and set-theoretic methods.|coloring,distributed,algorithm,graph,time,vertices,colors,maximum,degree,complexity
3aee1322-a73c-5c4f-bc31-188b3056113f|2014-10-03T10:11:13|2014|10|Faster Parallel Traversal of Scale Free Graphs at Extreme Scale with Vertex Delegates|||||||"The problem definition addressed in this research is the efficient processing and analysis of large-scale free graphs, which are commonly found in various domains such as social networks, biology, chemistry, and social sciences. The context that makes this problem important is the increasing size and complexity of these graphs, which poses significant challenges to their analysis and processing. The key objectives or goals set by the authors to address this problem are: To develop an efficient graph partitioning strategy that can handle large-scale free graphs and minimize communication overhead between processing nodes. To design a scalable and parallelizable algorithm that can perform graph analysis tasks, such as graph traversal, k-core decomposition, and PageRank calculation, on distributed memory architectures. The authors aim to achieve these objectives by proposing a novel graph partitioning strategy called ""delegate partitioning,"" which is designed to reduce communication overhead and improve load balancing among processing nodes. They also develop a parallel graph analysis framework that can efficiently execute graph analysis tasks on distributed memory architectures."|Graphs,Scale-free,Parallel,Traversal,Algorithms,Extreme scale,Vertex,Delegate,Asynchronous,Distributed
2767e0ca-e154-5653-947b-32f9874740ba|2022-05-17T07:24:59|2022|5|Distributed Multimodal Path Queries|||||||The problem addressed in this research is the Distributed Multimodal Path (DMP) query, which involves finding the shortest path in a multimodal transportation network that satisfies certain constraints. The context that makes this problem important is the increasing need for efficient and personalized route planning in modern transportation systems, which often involve multiple modes of transportation such as buses, trains, cars, bikes, and walking. The key objective of the authors is to develop an efficient algorithm that can process DMP queries in a distributed manner, taking into account the constraints and preferences of users. The authors aim to minimize the running time and network overhead of the algorithm, while ensuring that the computed path is feasible and satisfies the user's constraints. Specifically, the authors focus on solving the RegLCSP (Regular Language Constrained Shortest Path) problem, which involves finding the shortest path in a graph that satisfies a given regular language constraint. The authors propose a parallel algorithm, ALGdmp, that can efficiently process DMP queries in a distributed environment, and evaluate its performance through extensive experiments on real and synthetic datasets.|Multimodal,Graph,Distributed,Path,Query,Parallel,Computation,Regular,Language,Algorithm
93779d64-bb41-5658-8d87-84be439b76fe|2017-07-18T06:57:42|2017|7|An Adaptive Parallel Algorithm for Computing Connected Components|||||||The problem addressed in this research is the efficient parallelization of the connected component labeling (CCL) algorithm for large-scale graphs. The context that makes this problem important is the increasing need to process and analyze massive graphs in various domains, such as social networks, web graphs, and biological networks. The CCL algorithm is a fundamental graph processing task that identifies connected components in a graph, which is essential for many graph-based applications. The key objectives of this research are to develop a scalable and efficient parallel CCL algorithm that can handle large graphs and minimize the communication overhead between processors. The authors aim to achieve this by designing an algorithm that can effectively utilize the available parallel processing resources, reduce the number of iterations required for convergence, and optimize the data distribution and communication patterns. In summary, the problem addressed in this research is the development of an efficient parallel CCL algorithm for large-scale graphs, with the goal of minimizing communication overhead and maximizing scalability, to support various graph-based applications.|Connected Components,Undirected Graphs,Parallel Algorithms,Distributed Memory,Graph Analytics,Scalability,Performance,Hybrid Approach,Graph Topology,Metagenomic Graphs
77c7c8c9-e27d-586a-946d-ecbddd29a3a8|2013-10-27T03:14:58|2013|10|Distributed Community Detectionin Web-Scale Networks|myday||||||The problem definition addressed in this research is the scalability issue in community detection algorithms for large-scale networks. The context that makes this problem important is the increasing size and complexity of real-world networks, such as social media platforms, biological networks, and the internet, which require efficient and effective community detection methods to uncover hidden patterns and structures. The key objective of this research is to develop a scalable and distributed community detection algorithm that can handle massive networks with billions of vertices and edges. The authors aim to achieve this by proposing a pre-processing algorithm that reduces the problem size, making it possible to apply traditional community detection algorithms. The goal is to develop an algorithm that can efficiently identify core groups in large networks, which can then be used as input for traditional community detection algorithms to identify cohesive communities. In summary, the problem definition is the need for scalable community detection algorithms that can handle large-scale networks, and the key objective is to develop a distributed algorithm that can efficiently identify core groups in massive networks, enabling the application of traditional community detection algorithms to uncover hidden patterns and structures.|Community Detection,Graph Clustering,Distributed Algorithms,Ensemble Learning,MapReduce,Network Analysis,Core Groups,Graph Partitioning,Scalability,Modularity
12300048-e5cb-5322-af6d-034ad0d186fa|2021-10-31T16:39:05|2021|10|Universally-Optimal Distributed Shortest Paths andTransshipment via Graph-Based ℓ1-Oblivious Routing∗|||||||Here is a clear and concise summary of the problem definition addressed in this research: The research addresses the problem of designing an efficient oblivious routing scheme for the transshipment problem in a distributed network setting. **Problem Definition:** The research addresses the problem of designing an efficient oblivious routing scheme for the transshipment problem in a distributed network setting. **Context and Background:** In a distributed network, the transshipment problem involves finding the minimum-cost flow that satisfies a given demand. This problem is important because it has numerous applications in various fields, such as logistics, telecommunications, and finance. The traditional approach to solving this problem involves computing the optimal solution centrally, which can be computationally expensive and may not be feasible in large-scale networks. Therefore, there is a need for an efficient distributed algorithm that can solve the transshipment problem in a scalable and efficient manner. **Key Objectives and Goals:** The authors aim to design an oblivious routing scheme that can efficiently solve the transshipment problem in a distributed network setting. The key objectives are to: 1. Develop a routing scheme that can route any demand in the network with a low competitive ratio, i.e., the cost of the routing scheme should be close to the optimal solution. 2. Design an algorithm that can be executed efficiently in a distributed manner, without requiring a centralized coordinator. 3. Ensure that the algorithm is scalable and can handle large networks with a large number of nodes and edges. Overall, the research aims to develop a practical and efficient solution for the transshipment problem in distributed networks, which can have a significant impact on various applications that rely on efficient network optimization.|Distributed algorithms,Minor aggregation,Universally optimal,Congestion,Shortcut quality,Graph algorithms,Polylogarithmic,Competitive analysis,Distributed graph processing,CONGEST model
437b471d-c084-56e5-9409-5a6d5f3afeba|2018-10-26T05:24:32|2018|10|A Deterministic Distributed 2-Approximation for Weighted Vertex Cover in Rounds|Ran Ben-Basat||||||The problem addressed in this research is the Minimum Weight Vertex Cover (MWVC) problem, a classical NP-hard problem in computer science and graph theory. The context that makes this problem important is its wide range of applications in various fields, including network optimization, data mining, and machine learning. In MWVC, the input is a graph with non-negative vertex weights, and the goal is to find a subset of vertices (a vertex cover) that covers all edges in the graph while minimizing the total weight of the selected vertices. The key objective of this research is to develop an efficient distributed algorithm for solving the MWVC problem in the CONGEST model, a popular model for distributed computing. The authors aim to design an algorithm that achieves a 2-approximation of the optimal solution while minimizing the number of communication rounds required to solve the problem. The authors also focus on improving the round complexity of existing algorithms, particularly for the case where the maximum degree of the graph is unknown. Overall, the research aims to provide a fast and efficient distributed solution for the MWVC problem, which can be applied to various real-world applications.|Vertex Cover,Minimum Weight,Distributed Algorithm,Approximation Algorithm,CONGEST Model,Graph Algorithm,Weighted Vertex Cover,2-Approximation,Round Complexity,Logarithmic Rounds
c2e8e422-7461-5a74-ae7b-744662c058b2|2013-09-27T08:09:00|2013|9|LNCS 8205 - Distributed Minimum Cut Approximation|Mohsen Ghaffari,Fabian Kuhn||||||The problem addressed in this research is the distributed minimum cut approximation in a network graph. The context that makes this problem important is that minimum cuts and their sizes (i.e., edge connectivity) are crucial in network design and optimization, as they represent the throughput capacity of the network. Decomposing a network using small cuts helps in designing efficient communication strategies and identifying communication bottlenecks. The key objective of this research is to develop efficient distributed algorithms that can approximate the minimum cut in a network graph. The authors aim to design algorithms that can be executed in a distributed manner, where each node in the network can communicate with its neighbors, and the goal is to compute an approximate minimum cut in a time-efficient manner. The authors focus on developing algorithms with low time complexity, specifically in terms of the number of rounds required to compute the approximate minimum cut, while ensuring the correctness of the solution with high probability.|Minimum Cut,Distributed Algorithm,Approximation,Graph,Edge Connectivity,Randomized Algorithm,Communication Network,Cut Size,Graph Decomposition,CONGEST Model
4dfc4653-ea25-5a02-a503-ccffbd41b74b|2021-09-15T05:58:53|2021|9|TriPoll: Computing Surveys of Triangles in Massive-ScaleTemporal Graphs with Metadata|||||||The problem addressed in this research is the efficient processing of triangles in massive scale temporal graphs with metadata. The context is that network scientists seek to understand higher-order interactions within network data, and triangles are a fundamental pattern that is commonly difficult to enumerate in massive distributed real-world networks. The key objective is to develop a system that can survey triangles in massive graphs containing metadata on their edges and vertices, allowing users to answer hypotheses regarding the relevance of metadata triangles on their datasets and leverage their discoveries for various discovery tasks. The authors aim to provide a scalable solution that can handle large-scale graphs with hundreds of billions of edges and support metadata-aware capabilities.|TriPoll,Triangle,Graph,Metadata,Distributed,Scalable,Temporal,Network,Asynchronous,Communication
172ace0f-6fd5-5170-9977-3ad8b604a923|2021-07-01T03:51:57|2021|7|Efficient Distributed Approaches to Core Maintenance on Large Dynamic Graphs|||||||The problem definition addressed in this research revolves around maintaining core decomposition in large dynamic graphs, which is a fundamental problem in graph theory and network analysis. The context that makes this problem important is the increasing scale and dynamism of real-world graphs, such as social networks, web graphs, and biological networks, which undergo frequent updates (e.g., edge insertions or deletions). This dynamism poses significant challenges to maintaining accurate core decomposition, which is essential for various applications, including network analysis, community detection, and anomaly detection. The key objectives or goals the authors set to address this problem are: Efficiently maintaining core decomposition in large dynamic graphs, which involves updating the core numbers of vertices in response to edge updates. Scalability, as the graphs can be massive, with millions of vertices and edges. Accuracy, ensuring that the updated core numbers are precise and reflect the changes in the graph structure. To achieve these objectives, the authors aim to develop distributed algorithms that can efficiently process edge updates, minimize communication overhead, and ensure accurate core decomposition in large dynamic graphs.|Core maintenance,Dynamic graphs,Distributed approaches,Core decomposition,Graph analysis,Large-scale networks,Real-time updates,Edge insertion,Edge deletion,Graph structure
47865235-f272-5186-b89b-e08721ce2c3f|2021-02-04T04:17:02|2021|2|A Minimal Memory Game-based Distributed Algorithm to Vertex Cover of Networks|||||||The problem definition addressed in this research is the Minimum Vertex Cover (MVC) problem, which is a fundamental problem in graph theory and computer science. The context that makes this problem important is its broad applications in various fields, such as wireless sensor networks, crossroads monitoring, military surveillance, and health monitoring, where it is essential to select a minimum set of vertices to cover all edges in a graph. The key objective of this research is to develop a distributed algorithm to solve the MVC problem, which is an NP-hard problem. The authors aim to design a game-theoretic approach that can efficiently find a near-optimal solution to the MVC problem in a distributed manner, without relying on a central administrator. The goal is to overcome the contradiction between individual interests and collective benefits, which often leads to inefficient solutions in distributed algorithms. In summary, the problem definition is to develop a distributed algorithm to solve the MVC problem, which is crucial in various applications, and the key objective is to design a game-theoretic approach that can efficiently find a near-optimal solution to the MVC problem in a distributed manner.|Vertex Cover,Distributed Algorithm,Nash Equilibrium,Bounded Rational Behavioral,Memory Length,Selection Intensity,Snowdrift Game,Networks,Optimization,Game Theory
36b4a881-40c8-5fc3-a04c-4345d6db9b33|2022-06-01T19:39:30+00:00|2022|6|Asynchronous Distributed-Memory Triangle Counting and LCC with RMA Caching|||||||The problem addressed in this research is the efficient computation of Local Clustering Coefficient (LCC) in large-scale graphs, which is a crucial metric in graph analysis with applications in community detection, link prediction, and thematic relationships. The context that makes this problem important is the rapid growth in the size of graphs, exceeding the memory and computational capacities of a single machine, making distributed memory computing necessary. The key objective of this research is to develop an efficient distributed memory algorithm for LCC computation, leveraging MPI Remote Memory Access (RMA) to minimize communication overhead and optimize data reuse. The authors aim to achieve strong scaling capabilities, reduce initialization overheads, and lower per-node memory requirements, enabling the analysis of massive graphs on distributed computing systems.|Distributed,Graph,Analysis,Local,Clustering,Coefficient,Triangle,Counting,Asynchronous,RDMA
a02ffda1-3532-551a-ba06-81e64f58420e|2015-04-16T03:54:06|2015|4|Scalable Subgraph Enumeration in MapReduce|||||||The problem definition addressed in this research is the subgraph enumeration problem, which involves finding all subgraph instances of a given data graph that are isomorphic to a pattern graph. This problem is important in various applications, such as network motif computing, biochemistry, neurobiology, ecology, and bioinformatics. The context that makes this problem important is the increasing size and complexity of graph data, which makes it challenging to efficiently and effectively enumerate all subgraph instances. The authors aim to address this problem by developing a scalable and efficient solution that can handle large graph data. The key objectives or goals of this research are: 1. To develop a distributed algorithm for subgraph enumeration that can efficiently process large graph data. 2. To minimize the computational cost and memory usage of the algorithm. 3. To ensure the correctness and completeness of the subgraph enumeration results. Overall, the authors aim to provide a scalable and efficient solution for the subgraph enumeration problem, which is essential for various applications in graph analysis and data mining.|Subgraph,Enumeration,MapReduce,Graph,Pattern,Join,Algorithm,Scalability,Optimization,Distributed
34e3e49c-b758-593b-bef9-8c7f04d29d10|2017-09-05T04:33:56|2017|9|Towards Practical and Robust Labeled Pattern Matching in Trillion-Edge Graphs|Tahsin Reza,Christine Klymko,Matei Ripeanu,Geoffrey Sanders,Roger Pearce||||||The problem definition addressed in this research is the exact matching of a small template graph within a large background graph. The context that makes this problem important is the increasing need for efficient graph pattern matching algorithms in various applications, such as social network analysis, data mining, and information retrieval. The existing algorithms for exact matching have high computational complexity, limiting their scalability and robustness guarantees for modern large graph datasets. The key objectives or goals the authors set to address this problem are: 1. To develop an efficient algorithm that can exactly match a small template graph within a large background graph, with a focus on reducing the computational complexity. 2. To design a vertex-centric approach that iteratively eliminates vertices that do not meet local constraints imposed by the template graph, thereby reducing the search space and improving the algorithm's scalability. 3. To evaluate the feasibility and effectiveness of this approach in terms of its ability to prune the search space and reduce the computational cost of exact matching. Overall, the authors aim to provide a more efficient and scalable solution for exact graph pattern matching, which can be applied to various real-world applications involving large graph datasets.|Pattern Matching,Graphs,Subgraph,Labeled,Vertex Centric,Distributed Memory,Scalability,Algorithm,Semantic Graphs,Exact Matching
9220e9fa-aff9-5f02-b27c-bac459f345d6|2009-01-17T10:55:27|2009|1|Distributed Data Aggregation Scheduling in Wireless Sensor Networks|||||||Here is a concise summary of the problem definition addressed in this research: Context and Background: In wireless sensor networks, data aggregation is a crucial technique to reduce energy consumption and improve network efficiency. However, existing data aggregation scheduling algorithms have high latencies, and centralized approaches are inefficient due to frequent topology changes. Distributed data aggregation scheduling is essential to address these limitations. Problem Definition: The problem addressed in this research is the Distributed Aggregation Scheduling Problem (DASP), which aims to find a data aggregation schedule in a distributed manner to minimize the aggregation latency while ensuring collision-free data transmission. Key Objectives: 1. Minimize Aggregation Latency: The primary goal is to reduce the time required to aggregate data from all sensor nodes to the sink node. 2. Ensure Collision-Free Transmission: The schedule should prevent data collisions during transmission, ensuring that all data is received correctly at the sink node. 3. Distributed Approach: The algorithm should be distributed, allowing sensor nodes to collaborate and determine the aggregation schedule without relying on a centralized authority. By addressing the DASP, the authors aim to develop an efficient and reliable data aggregation scheduling algorithm that can be applied in various wireless sensor network applications.|Data Aggregation,Scheduling Algorithm,Wireless Sensor Networks,Distributed Algorithm,Collision-Free,Time Latency,Energy Consumption,Network Diameter,Node Degree,Sensor Networks
a931de24-62fc-5145-8a0f-8e6354e0a3b9|2021-09-15T05:58:47|2021|9|cuTS: Scaling Subgraph Isomorphism on Distributed Multi-GPUSystems Using Trie Based Data Structure|||||||The problem addressed in this research is the subgraph isomorphism problem, which involves finding all subgraphs in a large data graph that are isomorphic to a given query graph. This problem is important in various domains such as bioinformatics, computer vision, social network analysis, and cheminformatics. The authors aim to develop an efficient algorithm to solve this problem, with a focus on achieving high performance and scalability on modern GPUs. The key objectives are to reduce memory usage, minimize synchronization requirements, and balance workload between different nodes to achieve the best performance.|Subgraph,Isomorphism,GPU,Graph,Search,Algorithm,Query,Data,Parallel,Optimization
421c598a-3238-5ffa-b0c1-aefbd0ecd126|2018-05-28T07:18:51|2018|5|Accelerating PageRank using Partition-Centric Processing|||||||The problem definition addressed in this research is the optimization of PageRank computation, a fundamental graph algorithm, on shared-memory platforms. The context that makes this problem important is the growing scale of graph-based problems in various fields, such as web and social network analysis, biology, and transportation, which demands high-performance graph analytics. The key objective of this research is to overcome the limitations of traditional Graph Algorithmic Skeleton (GAS) models, which are inherently suboptimal due to their node-centric approach. The authors aim to develop a novel approach that can efficiently utilize the shared-memory architecture, reduce memory accesses, and improve the performance of PageRank computation. Specifically, the authors set out to design a partition-centric processing model that can take advantage of the graph structure, minimize memory traffic, and optimize the computation of PageRank. The goal is to achieve significant performance gains over state-of-the-art methods, making it possible to efficiently process large-scale graphs on shared-memory platforms.|PageRank,Partition Centric Processing,Sparse Matrix-Vector Multiplication,Graph Algorithms,Graph Locality,Memory Access Patterns,Cache Miss Ratio,Memory Bandwidth,Gather-Apply-Scatter Model,Graph Computation
6bd07b97-425d-5f09-9faf-fe16f372b0d9|2016-06-07T11:21:30|2016|6|A distributed approach for graph mining in massive networks|N Talukder||||||The problem definition addressed in this research is the efficient mining of frequent patterns in a large graph distributed across multiple machines. The context that makes this problem important is the increasing availability of large-scale graph data in various domains, such as social networks, web graphs, and biological networks, which requires scalable and efficient methods for pattern discovery. The key objective of this research is to develop a distributed graph mining approach that can handle large graphs by partitioning them across multiple machines, while ensuring that no frequent patterns are missed (no false negatives) and no infrequent patterns are reported as frequent (no false positives). The authors aim to achieve this goal by minimizing communication between machines, allowing local pruning of infrequent patterns, and eliminating false negative patterns via external neighbors. In summary, the problem definition involves developing a scalable and efficient distributed graph mining approach that can handle large graphs, minimize communication, and ensure accurate pattern discovery without missing any frequent patterns or reporting infrequent ones as frequent.|Distributed,Graph,Mining,Frequent,Subgraph,Massive,Networks,Parallel,Algorithm,Scalability
