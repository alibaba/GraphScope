id|name|description
d9a3aa25-e33b-5dad-ba54-29d43f6924e9|Local Database Cache|The authors propose a local database cache to store the adjacency sets fetched from the distributed database. This cache is shared among all working threads and can capture both intra-task and inter-task locality, reducing the communication cost with controllable memory usage.
667214b7-11c5-514f-837c-139e4a5b7b4b|Triangle Cache|The authors propose a triangle cache technique to reduce the redundancy in enumerating triangles. The technique stores the matches of triangles in a cache and reuses them when needed.
69546870-eaf0-54dd-827c-3427105a8a83|Task Splitting|The authors propose a task splitting technique to handle the skewed workloads of local search tasks. The technique splits the tasks into smaller sub-tasks and assigns them to different worker machines.
bbd127b3-e323-50dc-96d7-ac6e826a9228|On-Demand Shuffling|The authors propose an on-demand shuffling technique to query the database as needed during enumeration. The technique avoids shuffling partial matching results and only queries the necessary edges of the data graph.
52035306-c5fe-536b-87ef-bf41991ad57f|Backtracking-Based Framework|The authors propose a backtracking-based framework to enumerate subgraphs. The framework incrementally maps each pattern vertex to data vertices in the match and uses a backtracking search to enumerate all possible matches.
251cf2d8-5e82-5809-9ee1-eeaad0043eb5|Execution Plan Optimization|The authors propose an execution plan optimization technique to optimize the execution plan for the backtracking-based framework. The technique uses a cost estimation model and two pruning techniques to optimize the execution plan.
ad2b4308-0864-5d8a-8610-718445662946|Best Execution Plan Generation|The authors propose a best execution plan generation algorithm to generate the best execution plan for the backtracking-based framework. The algorithm uses a search-based method to find the best execution plan with the least cost.
7dd64307-dbd4-533f-a2d1-b94e2cd2d36b|On-Demand Shuffling Technique|The on-demand shuffling technique is a method for adaptive subgraph enumeration in heterogeneous and irregular graphs. It involves storing the edges of the data graph in a distributed database and querying the database as needed during enumeration, rather than shuffling the entire data graph before enumeration. The technique uses a backtracking-based framework to enumerate subgraphs, which allows it to adapt to the structure of the graph and only query the necessary edges. This approach reduces communication overhead and enhances memory locality by avoiding the need to shuffle large amounts of data. The paper reports that the on-demand shuffling technique reduces communication costs and improves performance compared to existing approaches.
a015b20e-d63c-56c2-a6f0-fad0c8eb9abf|Local Database Cache Technique|The local database cache technique is a method for reducing communication overhead in distributed subgraph enumeration. It involves setting up an in-memory database cache on each machine to store the adjacency sets fetched from the distributed database. The technique uses a replacement policy like LRU to capture the intra-task locality and a shared cache among all working threads to capture the inter-task locality. This approach reduces the number of database queries and enhances memory locality. The paper reports that the local database cache technique reduces communication costs and improves performance compared to existing approaches.
32545439-d5a9-533d-9b41-fa0fc5274da1|Task Splitting Technique|The task splitting technique is a method for balancing workloads in distributed subgraph enumeration. It involves splitting heavy tasks into smaller subtasks to distribute the workload evenly among workers. The technique uses a threshold to determine when to split a task and a load balancing strategy to distribute the subtasks among workers. This approach reduces load imbalance and enhances memory locality by avoiding the need to process large tasks. The paper reports that the task splitting technique reduces load imbalance and improves performance compared to existing approaches.
9c3094af-612a-5054-95bf-4eeb16fa4b15|Triangle Cache Technique|The triangle cache technique is a method for reducing redundancy in subgraph enumeration. It involves storing the results of triangle enumeration to avoid repeated calculations. The technique uses a cache to store the results of triangle enumeration and reuses them when possible. This approach reduces redundancy and enhances memory locality by avoiding the need to recalculate triangle enumeration results. The paper reports that the triangle cache technique reduces redundancy and improves performance compared to existing approaches.
aeefb4b2-ede4-5c50-9ece-6f459ec99b20|DFA G Model|The DFA G model is a unified programming model for vertex-centric parallel graph processing that ensures a BSP program modeled after it can run properly on both synchronous and asynchronous platforms. The DFA G model expresses the computation at a vertex as a process of message-driven state transition, which allows it to process messages in a one-at-a-time manner without regard to their arrival order. This makes the algorithmic correctness of DFA G independent of the processing order of messages. The paper demonstrates the effectiveness of the DFA G model by showing that it can run properly on both synchronous and asynchronous platforms, and that it can achieve better performance than synchronous platforms in some cases.
1cc6b488-7727-5610-8073-8790da17715e|Automaton Scheduler|The automaton scheduler is a component of the DFA G model that arranges the execution order of automatons to control the execution of BSP programs. The automaton scheduler uses a graphical interface to allow users to specify the execution order of automatons, which enables the DFA G model to adapt to different graph processing algorithms and platforms. The paper demonstrates the effectiveness of the automaton scheduler by showing that it can be used to construct DFA G models for different graph processing algorithms, such as PageRank and Bipartite Matching.
c623ba1b-6001-5dff-99b1-0a2fc62fdab2|Model Converter|The model converter is a component of the DFA G model that automatically translates DFA G models into runnable BSP programs. The model converter uses a set of predefined rules to translate DFA G models into BSP programs, which enables the DFA G model to be executed on different platforms. The paper demonstrates the effectiveness of the model converter by showing that it can be used to translate DFA G models into runnable BSP programs for different graph processing algorithms, such as PageRank and Bipartite Matching.
15e0d68e-bd64-54e5-ad14-f5ff4f1d5636|Graphical User Interface|The graphical user interface is a component of the DFA G model that allows users to construct DFA G models using a graphical interface. The graphical user interface uses a set of predefined components, such as automatons, messages, and actions, to enable users to construct DFA G models in a visual and intuitive way. The paper demonstrates the effectiveness of the graphical user interface by showing that it can be used to construct DFA G models for different graph processing algorithms, such as PageRank and Bipartite Matching.
28600e97-7469-514e-9f7e-477808a27ba6|DFA G (Deterministic Finite Automaton for Graph processing)|DFA G is a unified programming model for vertex-centric parallel graph processing that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by expressing vertex computation as a process of message-driven state transition. This approach allows for efficient processing of complex graph structures with varying degrees, weights, and sparsity.
adbd0fcb-1e51-5e12-827a-4e8ec7af9124|Edge Pivoting|Edge pivoting is a technique used to collect 2-hop information without maintaining an explicit 2-hop neighborhood list at each vertex, enabling the computation of all local 3-profiles in parallel with minimal communication. Edge pivoting involves using a gather-apply-scatter model to collect information from neighboring vertices and edges, allowing for efficient computation of local 3-profiles without requiring excessive memory storage. The paper demonstrates that edge pivoting enables the computation of local 3-profiles in approximately the same time as triangle counting, with a running time proportional to the graph's number of edges.
dc961417-7a96-52a2-8998-847af6832a33|3-Profile Sparsifiers|3-profile sparsifiers are sparse graphs that can be used to approximate the full 3-profile counts for a given large graph, enabling memory-efficient scalable graph processing. The authors develop a concentration theorem that shows that local 3-profile estimation via sub-sampling is comparable in runtime and accuracy to local triangle counting, allowing for the use of sparse graphs to approximate 3-profile counts. The paper demonstrates that 3-profile sparsifiers can achieve comparable performance to triangle counting, with a running time reduced by a factor of 4 and network usage reduced by a factor of 2 when using a sampling probability of 0.1.
22e0109b-95e7-5102-8b89-fc3934e206c0|Parallel Ego 3-Profile Computation|The authors propose a parallel algorithm for computing ego 3-profiles, which involves combining edge pivot equations and local clique counts to calculate ego 3-profiles in parallel. The algorithm uses a gather-apply-scatter model to collect information from neighboring vertices and edges, allowing for efficient computation of ego 3-profiles in parallel. The paper demonstrates that the parallel algorithm can compute ego 3-profiles for up to 100,000 ego graphs in the timescale of several minutes, with a running time that scales well with the number of machines.
2c15c962-14e1-519c-8c0d-e66f52fc33de|Ego 3-Profile Algorithm|The ego 3-profile algorithm is a distributed algorithm for estimating the 3-profile of the ego graph of each vertex in a graph. The algorithm uses a combination of edge pivoting and local clique counts to compute the ego 3-profiles in parallel. The algorithm uses a gather-apply-scatter model to compute the ego 3-profiles. In the gather stage, each vertex collects information from its neighbors, and in the apply stage, each vertex computes its ego 3-profile using the collected information. The scatter stage is used to combine the ego 3-profiles to obtain the global 3-profile. The paper shows that the ego 3-profile algorithm can estimate the ego 3-profiles of hundreds of thousands of vertices in parallel, in the timescale of minutes.
e42a6531-46e4-57a3-8229-3d2847fc98d4|Edge Pivot Equations|The authors propose using edge pivot equations to efficiently compute 3-profiles in a distributed graph engine framework. This solution specifically addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by providing a novel approach to handle the irregular structure of graphs.
fa06ee42-eb67-5602-8623-83325ee6443b|Gather-Apply-Scatter Model|The authors utilize the Gather-Apply-Scatter model to design a distributed algorithm for estimating 3-profiles on the subsampled graph, allowing for the computation of local 3-profiles in parallel with minimal communication.
6ff4acac-4a4f-5d2b-8afa-50526d78c81f|Task Tree Design|The authors propose a task tree design to address the challenge of memory-efficient scalable graph processing. This design decouples task generation and execution, allowing for out-of-order task scheduling while being aware of data locality.
3754a949-7860-5de7-b91a-b191aa55587a|Search Tree Merging|The authors propose a search tree merging scheme to address the challenge of memory-efficient scalable graph processing. This scheme allows for the exploration of multiple search trees in parallel, reducing memory consumption and improving scalability.
e59431cb-08d1-5b1c-a552-c0450b62d895|Locality-Aware Task Scheduling|The authors propose a locality-aware task scheduling scheme to address the challenge of memory-efficient scalable graph processing. This scheme takes into account both parallelism and locality when deciding the task execution order.
69e86e6a-68e3-588c-80a0-115be1d35cd2|Shogun Task Scheduling Framework|The Shogun task scheduling framework is designed to optimize communication efficiency in distributed graph mining algorithms by minimizing the number of communication rounds. It achieves this through a novel task scheduling scheme that prioritizes sibling tasks to improve intermediate result locality, while also incorporating a locality monitoring mechanism to avoid performance degradation.
8f289823-15cf-585e-b71a-4c5f175d1796|Task Tree Splitting|Task tree splitting is a technique used in Shogun to optimize communication efficiency by reducing the number of communication rounds. It involves splitting a search tree onto multiple processing elements (PEs) to improve load balance and minimize the impact of inter-depth barriers.
56b0e235-028f-5d1c-b6fa-5f7451a628e3|Quiescing Mechanism|The quiescing mechanism is a recovery mechanism that stalls the processing of one search tree to recover from severe locality loss.
45a6470e-24c6-5901-a9f5-87a5d514d067|Locality-Aware Out-of-Order Task Scheduling|The authors propose a locality-aware out-of-order task scheduling framework, Shogun, which enables the execution of tasks without parent-child relationships in parallel. This approach aims to improve the PE utilization rate and reduce the impact of inter-depth barriers. The Shogun framework uses a task tree design to decouple task generation and execution. The scheduler assesses the parallelism and locality of tasks and determines the task execution order. This approach is different from existing methods as it takes into account the locality of tasks and avoids severe locality loss. The paper shows that Shogun can improve the performance by 63% on average with a hardware overhead of less than 4%.
dfafc1fa-4f5a-50f3-911a-f943cbff02e5|Shogun Scheduling Framework|Shogun is a task scheduling framework designed to improve the resource utilization of graph mining accelerators by enabling out-of-order task scheduling while being aware of data locality. The scheduler in the task tree takes both parallelism and locality into consideration when deciding the execution order of tasks.
9552912a-4059-512a-b763-a63ac86316e2|Symmetric Rectilinear Partitioning|The authors propose a symmetric rectilinear partitioning strategy to divide the graph into smaller subgraphs, allowing for more efficient memory usage and reduced communication overhead. This approach enables the processing of larger graphs that cannot fit into GPU memory.
63c7907f-10c8-5352-98db-0816c5689d28|Block-Based Triangle Counting Formulation|The authors introduce a block-based triangle counting formulation that divides the computation into smaller tasks, each depending on three blocks where edges of a triangle can appear. This approach enables the efficient processing of large graphs by maximizing memory efficiency and bounding data movement.
6b17d07a-1e44-5470-9f5f-6b2cffde730d|Task-Based Execution on Heterogeneous Architectures|The authors propose a task-based execution model that leverages both CPUs and GPUs to process tasks concurrently. This approach enables the efficient processing of large graphs by maximizing memory utilization and overlapping communication with computation.
707e9589-1f83-5e70-b6be-a300ecbde672|Task-Based Execution with Dynamic Scheduling|The authors propose a task-based execution approach with dynamic scheduling to optimize communication efficiency in distributed algorithms. This approach involves dividing the computation into tasks and scheduling them dynamically to minimize communication overhead and maximize memory utilization.
416b5f6e-b194-5b99-a071-028d4d258516|Hybrid Intersection Approach|The authors propose a hybrid intersection approach to optimize communication efficiency in distributed algorithms. This approach involves combining list-based and hashmap-based intersection methods to minimize communication overhead and maximize memory utilization.
2217580f-f6ac-5071-93f3-3ac2e28a1ed5|Overlapping Communication and Computation|The authors propose an approach to overlap communication and computation to optimize communication efficiency in distributed algorithms. This approach involves using multiple streams on multiple GPUs to execute tasks simultaneously and move data asynchronously.
4130bcbe-d6ed-5ee1-8eea-641cb873ac98|Dynamic Task Scheduling Scheme|The authors propose a dynamic task scheduling scheme that schedules tasks on multi-core CPUs and multiple streams on multiple GPUs. This scheme is designed to overlap communication with computation and reduce load imbalance.
58409fe7-655d-57cc-a1f3-c76bd166f6c7|Hybrid Execution Model|The authors propose a hybrid execution model that leverages both CPUs and multiple GPUs to process tasks. This model is designed to maximize memory utilization and reduce data movement.
fea4238d-4c50-5226-90dc-3b01e4ccbc94|Task Workload Estimation|The authors propose a task workload estimation technique to estimate the workload of tasks and assign them to GPUs or CPUs accordingly. The technique involves estimating the workload of tasks using a heuristic-based approach, which takes into account the average degree of blocks in a task and the maximum degree of vertices in the blocks. The authors report that their task workload estimation technique achieves better performance than other estimation functions, with speedups ranging from 1.4 to 2.6.
04fc1ddc-1e60-5bd5-b7d4-d68dc65536d9|Hybrid Execution and Cut-Off|The authors propose a hybrid execution approach that combines CPU and GPU execution, and uses a cut-off point to separate bottleneck tasks from lightweight tasks. The approach involves assigning computationally heavy tasks to GPUs and lighter tasks to CPUs, and using a cut-off point to separate bottleneck tasks from lightweight tasks. The authors report that their hybrid execution approach achieves better performance than a fully dynamic scheduling approach, with speedups ranging from 1.4 to 3.8.
d2b73443-d559-5022-a6b4-6d425a64da01|Block-Based Task Composition|The authors propose a block-based task composition approach to optimize GPU memory access for graph processing. This approach divides the computation into tasks, each depending on three blocks where edges of a triangle can appear, allowing for more efficient memory access patterns.
5bb12fa2-b5b1-5093-b515-8eecec120a31|Dynamic Task Scheduling|The authors propose a dynamic task scheduling approach to optimize GPU memory access for graph processing. This approach schedules tasks on multi-core CPUs and multiple streams on multiple GPUs, allowing for more efficient memory access patterns.
317e8fe8-898e-5cb5-89ab-8651224274f5|Estimation-Based Task Ordering|The authors propose an estimation-based task ordering approach to optimize GPU memory access for graph processing. This approach orders tasks based on their estimated execution time, allowing for more efficient memory access patterns.
1d9085bb-fc34-53f0-b9e2-6a3b5b93c43b|Block-Based Triangle Counting Algorithm|The authors propose a block-based triangle counting algorithm to reduce data movement during both sequential and parallel execution. This algorithm partitions the graph into smaller blocks, allowing for more efficient processing and minimizing the need for accessing the entire graph for each block. The algorithm uses a symmetric rectilinear partitioning strategy to divide the graph into blocks, ensuring that each block can fit into memory. It also employs a block variant of the Compressed Sparse Row (CSR) format to store the graph, reducing memory usage. The algorithm then processes each block in parallel, using a combination of CPU and GPU resources to count triangles. The authors report that their algorithm achieves a speedup of up to 18x on large graphs and up to 22x on smaller instances compared to existing algorithms.
011dae16-b037-5e48-a11e-cdef597709fb|Task-Based Execution Model|The authors propose a task-based execution model for heterogeneous environments, which allows for the efficient distribution of tasks among different CPUs and GPUs. The model uses a lightweight scheduler to schedule tasks on available devices, taking into account the workload of each task and the available resources. The scheduler also handles the movement of data blocks needed for task execution. The authors report that their task-based execution model achieves better scalability and performance compared to existing approaches, with a speedup of up to 5.6x on the Friendster graph.
441c4d72-967a-5349-874a-529595cea3ab|Dynamic Scheduling Technique|The authors propose a dynamic scheduling technique for block-based execution on heterogeneous environments, which avoids the long tail problem by scheduling bottleneck tasks to CPUs. The technique uses a heuristic-based ordering of tasks, which separates bottleneck tasks from lightweight tasks. It then schedules tasks on available devices, taking into account the workload of each task and the available resources. The authors report that their dynamic scheduling technique achieves better performance and scalability compared to existing approaches, with a speedup of up to 2.7x on the Twitter graph.
8b15027f-5bc5-5ca9-9684-feefa7016ba5|Hybrid Execution and Cut-Off Technique|The authors propose a hybrid execution technique that combines CPU and GPU processing, using a cut-off technique to determine the optimal distribution of tasks between CPUs and GPUs. The technique uses a cut-off value to determine the number of tasks to be executed on CPUs and GPUs, taking into account the workload of each task and the available resources. The authors report that their hybrid execution technique achieves better performance and scalability compared to existing approaches, with a speedup of up to 18x on large graphs and up to 22x on smaller instances.
6c3dda7d-9e2e-55d1-b8bc-073dc74a0e77|Double Buffering|The authors propose using double buffering to overlap data transfer with computation, thereby reducing memory consumption and improving scalability.
669350c1-70a3-5467-9626-ec6bd3d4a7b0|Custom Instruction Implementation|The authors propose using custom instruction implementation to reduce memory consumption and improve scalability.
04dcddb1-763b-5671-b22f-06d2dfc5fac7|SIMD Instructions|The authors propose using SIMD instructions to improve parallelism and reduce memory consumption.
9d436af6-7b1d-543b-bcd7-57bdbbaf2673|Graph Partitioning|The authors propose using graph partitioning to reduce memory consumption and improve scalability.
396c8701-112e-5e50-b9a1-ebd0a3f6e414|CoRAM-based DMA Controller|The authors propose using a CoRAM-based DMA controller to optimize memory usage and improve scalability.
85e5d276-e14c-582d-98fa-e3895975cbea|GraphGen Framework|The GraphGen framework is a vertex-centric graph specification that targets FPGA for hardware acceleration of graph computations. It accepts a vertex-centric graph specification and automatically compiles it onto an application-specific synthesized graph processor and memory system for the target FPGA platform.
e069a132-4af2-5239-95dc-00989d9982ac|Custom Instruction Support|GraphGen supports custom instructions that can be defined by the user to compute arbitrary combinational functions using the scope and temporary data variables as input and output.
1d01ebfd-020a-5d7b-b55a-e60f12deacd1|SIMD Instruction Support|GraphGen supports SIMD-style custom instructions that can operate on multiple data in parallel.
73e64fb6-5535-5453-af94-362adcdd8174|Custom Instruction Composition|GraphGen supports the composition of custom instructions to improve parallelism by using a single instruction that operates on multiple data (SIMD).
9f46e7bd-aa3d-5ec3-b12e-668535dc3e07|Automatic Graph Partitioning|GraphGen provides both manual and automatic partitioning capabilities to partition the input graph into subgraphs.
a701f787-4e8d-533f-a7ca-367adaeb7804|Distributed Pagerank Computation|The authors propose a distributed algorithm for computing pageranks in peer-to-peer networks, which enables memory-efficient and scalable graph processing. The algorithm is based on chaotic asynchronous iterative solution of linear systems, where each peer acts as a simple state machine exchanging messages with other peers. This approach allows for incremental computation of pageranks as new documents are entered into the network, reducing memory consumption and communication overhead. The authors demonstrate that the algorithm converges rapidly, produces high-quality pageranks, and enables incremental and continuous computation of pageranks. The algorithm also shows good scalability with graph size, with a maximum error of less than 1% for most documents, even with moderately high thresholds.
24cc0299-7654-5886-b3c7-467f673b88d9|IP Address Caching Scheme|The authors propose an IP address caching scheme to reduce communication overhead and memory consumption in the distributed pagerank computation algorithm. The scheme involves caching IP addresses of peers at each peer, allowing for direct exchange of update messages between source and destination peers, rather than routing through intermediate nodes. The authors demonstrate that the caching scheme reduces communication overhead and memory consumption, with storage requirements scaling linearly with the sum of outlinks in all documents in a peer.
df66135f-311e-5345-9a0f-0cd72f2bb8c4|Incremental Search Mechanism|The authors propose an incremental search mechanism that provides a 10-fold reduction in network traffic while executing keyword search queries. The mechanism involves incrementally returning documents sorted by pagerank, rather than returning all document hits at once. This approach reduces network traffic and memory consumption, while also improving search effectiveness. The authors demonstrate that the incremental search mechanism provides a significant reduction in network traffic, with an average traffic reduction of 12.2 for two-word queries and 11.9 for three-word queries.
b3483b9a-a460-5dd6-9570-da655d3af422|Distributed Pagerank Computation with Asynchronous Iteration|The authors propose a distributed pagerank computation algorithm that utilizes asynchronous iteration to optimize communication efficiency in distributed algorithms. This approach enables the computation of pageranks in peer-to-peer networks, allowing for incremental and continuous computation of pageranks as new documents are entered into or deleted from the network. The algorithm employs chaotic asynchronous iterations, where each peer acts as a simple state machine exchanging messages with other peers. This approach allows for the computation of pageranks without the need for a central server or global synchronization, reducing the number of communication rounds required. The authors report that the algorithm converges rapidly, with 99% of the graph converging in as few as 10 passes, and that the execution time is dominated by network transfer time.
57656e39-f7ce-5e72-ac70-e533bee08361|Incremental Search with Pagerank|The authors propose an incremental search algorithm that uses pagerank to reduce network traffic and improve search efficiency in peer-to-peer networks.
56f31d49-fd84-5900-a1a3-103175e4e54b|Caching of Document Locations|The authors propose a caching scheme that reduces the network traffic generated by pagerank update messages. This scheme addresses the challenge of load balance optimization by allowing peers to cache the IP addresses of other peers, reducing the need for network updates.
67989b95-76f6-566f-a30f-bbdfcae41841|Incremental Searching|The authors propose an incremental search algorithm that uses pagerank values to sort and return documents in response to keyword search queries, reducing network traffic and improving search efficiency. The algorithm uses a combination of pagerank values and document closeness to sort and return documents, and incrementally returns documents to the querying node, reducing the need for transferring large amounts of data. The authors demonstrate that their algorithm provides a 10-fold reduction in network traffic for two-word and three-word queries, and returns a manageable number of results to the user.
a32c9988-974a-553f-ab9d-cde99465b356|IP Address Caching|The authors propose an IP address caching scheme to reduce network traffic and improve the efficiency of pagerank updates. The scheme caches IP addresses of peers and documents, allowing for direct exchange of pagerank update messages between peers without the need for routing. The authors demonstrate that their scheme reduces network traffic and improves the efficiency of pagerank updates, allowing for faster convergence of the distributed pagerank computation algorithm.
38fe640a-109b-5a6b-86d4-d7a399e3f44e|Distributed Treewidth Computation|The authors propose a distributed algorithm for computing the treewidth of a graph, which is a key factor in determining the memory efficiency of graph processing algorithms. The algorithm is designed to work in a distributed computing environment and can handle large-scale graphs.
3d0788d1-d817-5b97-baf3-9816cd15d749|Subgraph Aggregation|The authors propose a subgraph aggregation algorithm that can be used to compute various graph properties, such as maximum independent set, minimum vertex cover, chromatic number, and minimum dominating set. The algorithm is designed to work in a distributed computing environment and can handle large-scale graphs.
8fcb4dad-5266-5aa2-ac77-19ea8f31cb96|Partwise Aggregation|The authors propose a partwise aggregation algorithm that can be used to compute various graph properties, such as maximum independent set, minimum vertex cover, chromatic number, and minimum dominating set. The algorithm is designed to work in a distributed computing environment and can handle large-scale graphs.
707d4988-b87b-53d3-806f-f37d45fe5e0b|Shortcuts Framework|The authors propose a shortcuts framework that can be used to design memory-efficient and scalable algorithms for processing massive graphs within distributed computing environments. The framework is designed to work with various graph properties, such as maximum independent set, minimum vertex cover, chromatic number, and minimum dominating set.
150baa37-3255-5431-bdcf-308e760ffe65|Vertex Disjoint Paths|The authors propose a distributed algorithm for finding k vertex disjoint paths in a graph network. The algorithm runs in O(kO(1) D) rounds, where k is the number of paths and D is the diameter of the network graph.
a8e82408-0902-5fec-b3eb-337ffe6a8152|Distributed Shortcuts|The authors propose a distributed algorithm for computing shortcuts in a graph network, which is a key component of their distributed treewidth computation algorithm. The algorithm runs in O(kO(k) D) rounds, where k is the treewidth of the graph and D is the diameter of the network graph.
6050040f-2667-5abb-afd7-745b14dc5e6b|Vertex Disjoint Paths Algorithm|The authors propose a vertex disjoint paths algorithm for finding k vertex disjoint paths between two vertices in a graph. The algorithm is designed to work on heterogeneous and irregular graphs, and it uses a combination of techniques such as graph contraction and partwise aggregation to efficiently find the paths.
dabf3f1e-8b8d-55d7-b877-6443d5ea2b33|Partwise Aggregation Algorithm|The authors propose a partwise aggregation algorithm for efficiently aggregating information across a graph. The algorithm is designed to work on heterogeneous and irregular graphs, and it uses a combination of techniques such as graph contraction and vertex disjoint paths to efficiently aggregate information.
86e5de24-1cf8-576f-a1f8-24d9398357ee|Distributed Vertex Disjoint Paths Algorithm|The authors propose a distributed algorithm for finding vertex disjoint paths in a graph network of treewidth k, which can be used as a subroutine for approximating treewidth. The algorithm either finds k vertex disjoint s-t paths or outputs an s-t node cut of size less than k in O(kO(1)D) rounds.
24209002-fff3-5bba-af37-727a717699d8|Treewidth Approximation Algorithm|The authors propose a treewidth approximation algorithm that uses the distributed vertex disjoint paths algorithm as a subroutine. The algorithm recursively finds a node set S of size k-1 and, for each connected component U in G-U-S, recursively calls U, N(U)XS.
a783525b-a400-514c-9b13-31f5be8a3b0c|Distributed Maximum Independent Set Algorithm|The authors propose a distributed algorithm for finding a maximum independent set in a graph network of treewidth k. The algorithm uses a dynamic programming approach to find the maximum independent set.
4ddc3b30-cf76-5134-a48d-8f07f138ef47|Distributed Subgraph Aggregation Algorithm|The authors propose a distributed algorithm for subgraph aggregation, which can be used to solve problems such as maximum independent set, minimum vertex cover, chromatic number, and minimum dominating set.
6f6a99ac-9d20-5cbf-b4b2-a711a4cb589c|Distributed Partwise Aggregation Algorithm|The authors propose a distributed algorithm for partwise aggregation, which can be used to solve problems such as maximum independent set, minimum vertex cover, chromatic number, and minimum dominating set.
2b375ff6-c289-540f-908e-2a93e8cd4b49|Distributed Spanning Tree Algorithm|The authors propose a distributed algorithm for finding a spanning tree in a graph network.
e21ce959-22a5-5c2f-933d-6c44b51c9e9c|Distributed Path Aggregation Algorithm|The authors propose a distributed algorithm for path aggregation, which can be used to solve problems such as maximum independent set, minimum vertex cover, chromatic number, and minimum dominating set.
7ee2b81d-d62f-5b90-8a4e-2692992ed8ef|Distributed Rooted Tree Aggregation Algorithm|The authors propose a distributed algorithm for rooted tree aggregation, which can be used to solve problems such as maximum independent set, minimum vertex cover, chromatic number, and minimum dominating set.
2e4cf8f6-98c2-5fe9-aa26-fc2816f8aeec|Distributed Subgraph Aggregation with Operator Algorithm|The authors propose a distributed algorithm for subgraph aggregation with operator, which can be used to solve problems such as maximum independent set, minimum vertex cover, chromatic number, and minimum dominating set.
c38f0f48-b5a4-5741-bdc4-3c49c77d447b|Distributed Partwise Aggregation with Operator Algorithm|The authors propose a distributed algorithm for partwise aggregation with operator, which can be used to solve problems such as maximum independent set, minimum vertex cover, chromatic number, and minimum dominating set.
ea4d204f-5d11-5f13-b855-cfd0f0929feb|Distributed Vertex Disjoint Paths Algorithm with Forbidden Node Set|The authors propose a distributed algorithm for finding vertex disjoint paths in a graph network with a forbidden node set.
92bb82c0-b9a7-5e1a-bfd1-927f522bea82|Distributed Vertex Disjoint Paths Algorithm with Multiple Instances|The authors propose a distributed algorithm for finding vertex disjoint paths in multiple instances of a graph network.
f3817baf-81fb-5aec-b3ab-6182095e6c36|Distributed Treewidth Approximation Algorithm with Multiple Instances|The authors propose a distributed algorithm for approximating treewidth in multiple instances of a graph network.
75c8898e-6829-5e79-b31f-f1da5ff702c7|Distributed Maximum Independent Set Algorithm with Multiple Instances|The authors propose a distributed algorithm for finding a maximum independent set in multiple instances of a graph network.
ac480ec7-6a44-5897-ac7c-59f6420d3104|Distributed Subgraph Aggregation Algorithm with Multiple Instances|The authors propose a distributed algorithm for subgraph aggregation in multiple instances of a graph network.
c786a35d-5cf2-5416-8529-7bbfb830e3f8|Distributed Partwise Aggregation Algorithm with Multiple Instances|The authors propose a distributed algorithm for partwise aggregation in multiple instances of a graph network.
35e862f3-67c2-5c38-9ee7-4942c1af41a9|Distributed Spanning Tree Algorithm with Multiple Instances|The authors propose a distributed algorithm for finding a spanning tree in multiple instances of a graph network.
dcb5cfe6-2096-59ed-9a8d-80ccbc937c67|Distributed Path Aggregation Algorithm with Multiple Instances|The authors propose a distributed algorithm for path aggregation in multiple instances of a graph network.
2e7e7866-a8b8-52c8-8bed-ed3c096734ad|Distributed Rooted Tree Aggregation Algorithm with Multiple Instances|The authors propose a distributed algorithm for rooted tree aggregation in multiple instances of a graph network.
ef7df822-8741-58cb-9845-fea64a836460|Distributed Subgraph Aggregation with Operator Algorithm with Multiple Instances|The authors propose a distributed algorithm for subgraph aggregation with operator in multiple instances of a graph network.
b32d0f84-511a-5f57-9a46-05113319b948|Distributed Partwise Aggregation with Operator Algorithm with Multiple Instances|The authors propose a distributed algorithm for partwise aggregation with operator in multiple instances of a graph network.
3aec4964-64d2-583f-9977-bfc5eb855581|Distributed Vertex Disjoint Paths Algorithm with Forbidden Node Set and Multiple Instances|The authors propose a distributed algorithm for finding vertex disjoint paths in a graph network with a forbidden node set and multiple instances.
e6a9654e-fd1f-53d9-b64b-24c9c1c99904|Distributed Treewidth Approximation Algorithm with Forbidden Node Set and Multiple Instances|The authors propose a distributed algorithm for approximating treewidth in a graph network with a forbidden node set and multiple instances.
4758c6e5-a276-587e-96c5-f5e4a7e6aa0c|Distributed Maximum Independent Set Algorithm with Forbidden Node Set and Multiple Instances|The authors propose a distributed algorithm for finding a maximum independent set in a graph network with a forbidden node set and multiple instances.
1d8a7aea-eb12-5566-8ac0-c82548f9e24e|Distributed Subgraph Aggregation Algorithm with Forbidden Node Set and Multiple Instances|The authors propose a distributed algorithm for subgraph aggregation in a graph network with a forbidden node set and multiple instances.
4480960f-40a6-5377-a021-70ce53661888|Distributed Partwise Aggregation Algorithm with Forbidden Node Set and Multiple Instances|The authors propose a distributed algorithm for partwise aggregation in a graph network with a forbidden node set and multiple instances.
3c615309-0ac4-52d9-b2eb-8841b320a8a8|Distributed Spanning Tree Algorithm with Forbidden Node Set and Multiple Instances|The authors propose a distributed algorithm for finding a spanning tree in a graph network with a forbidden node set and multiple instances.
31bd9760-f32e-5ce6-a847-6b95fe097993|Distributed Path Aggregation Algorithm with Forbidden Node Set and Multiple Instances|The authors propose a distributed algorithm for path aggregation in a graph network with a forbidden node set and multiple instances.
9f17f5f8-0dd4-597e-ac6a-5bd201219faa|Distributed Rooted Tree Aggregation Algorithm with Forbidden Node Set and Multiple Instances|The authors propose a distributed algorithm for rooted tree aggregation in a graph network with a forbidden node set and multiple instances.
c120cf93-1ff8-5028-a39e-a1cbbbae0617|Distributed Subgraph Aggregation with Operator Algorithm with Forbidden Node Set and Multiple Instances|The authors propose a distributed algorithm for subgraph aggregation with operator in a graph network with a forbidden node set and multiple instances.
e2d77be1-0770-5541-9a18-c9ed89e7d9ff|Distributed Partwise Aggregation with Operator Algorithm with Forbidden Node Set and Multiple Instances|The authors propose a distributed algorithm for partwise aggregation with operator in a graph network with a forbidden node set and multiple instances.
f5ae631f-e92a-53b4-a73d-88d174e420af|Distributed Vertex Disjoint Paths Algorithm with Multiple Instances and Forbidden Node Set|The authors propose a distributed algorithm for finding vertex disjoint paths in a graph network with multiple instances and a forbidden node set.
cee5af2c-11eb-534a-b3a8-a47567c22406|Distributed Treewidth Approximation Algorithm with Multiple Instances and Forbidden Node Set|The authors propose a distributed algorithm for approximating treewidth in a graph network with multiple instances and a forbidden node set.
96c598f2-c9df-5fab-8d9c-96d4bd0a21be|Distributed Maximum Independent Set Algorithm with Multiple Instances and Forbidden Node Set|The authors propose a distributed algorithm for finding a maximum independent set in a graph network with multiple instances and a forbidden node set.
6981ff14-a501-57f8-b2a6-ed971a39610b|Distributed Subgraph Aggregation Algorithm with Multiple Instances and Forbidden Node Set|The authors propose a distributed algorithm for subgraph aggregation in a graph network with multiple instances and a forbidden node set.
30162db5-9efb-537c-8160-6b413b843ff3|Distributed Partwise Aggregation Algorithm with Multiple Instances and Forbidden Node Set|The authors propose a distributed algorithm for partwise aggregation in a graph network with multiple instances and a forbidden node set.
09beef5f-3057-559d-a4cd-5d3c333a9450|Distributed Spanning Tree Algorithm with Multiple Instances and Forbidden Node Set|The authors propose a distributed algorithm for finding a spanning tree in a graph network with multiple instances and a forbidden node set.
955b218c-b9ef-58ab-ab55-2c8ba2d8f4ff|Distributed Path Aggregation Algorithm with Multiple Instances and Forbidden Node Set|The authors propose a distributed algorithm for path aggregation in a graph network with multiple instances and a forbidden node set.
90bf3716-e9a9-5e15-a204-3fd12f727d22|Distributed Rooted Tree Aggregation Algorithm with Multiple Instances and Forbidden Node Set|The authors propose a distributed algorithm for rooted tree aggregation in a graph network with multiple instances and a forbidden node set.
18a9151c-7c99-5685-b1ac-bb69b7e7b0d2|Distributed Subgraph Aggregation with Operator Algorithm with Multiple Instances and Forbidden Node Set|The authors propose a distributed algorithm for subgraph aggregation with operator in a graph network with multiple instances and a forbidden node set.
acb4d1df-177d-52ef-a742-7078d726d2c9|Distributed Partwise Aggregation with Operator Algorithm with Multiple Instances and Forbidden Node Set|The authors propose a distributed algorithm for partwise aggregation with operator in a graph network with multiple instances and a forbidden node set.
18eac9f1-5460-5e30-bdea-f2079f72c766|Distributed Vertex Disjoint Paths Algorithm with Multiple Instances, Forbidden Node Set, and Multiple Forbidden Node Sets|The authors propose a distributed algorithm for finding vertex disjoint paths in a graph network with multiple instances, a forbidden node set, and multiple forbidden node sets.
a3140b34-9c00-55dd-8b08-6764afcd8af0|Distributed Treewidth Approximation Algorithm with Multiple Instances, Forbidden Node Set, and Multiple Forbidden Node Sets|The authors propose a distributed algorithm for approximating treewidth in a graph network with multiple instances, a forbidden node set, and multiple forbidden node sets.
4756ee02-508c-5a6a-9c67-51ceeb5f716a|Distributed Maximum Independent Set Algorithm with Multiple Instances, Forbidden Node Set, and Multiple Forbidden Node Sets|The authors propose a distributed algorithm for finding a maximum independent set in a graph network with multiple instances, a forbidden node set, and multiple forbidden node sets.
0ff2e377-a95f-5f3e-9130-7795f783ae34|Distributed Subgraph Aggregation Algorithm with Multiple Instances, Forbidden Node Set, and Multiple Forbidden Node Sets|The authors propose a distributed algorithm for subgraph aggregation in a graph network with multiple instances, a forbidden node set, and multiple forbidden node sets.
750f4b68-b3a8-56fd-b4c7-eff433b2b220|Distributed Partwise Aggregation Algorithm with Multiple Instances, Forbidden Node Set, and Multiple Forbidden Node Sets|The authors propose a distributed algorithm for partwise aggregation in a graph network with multiple instances, a forbidden node set, and multiple forbidden node sets.
5059b5dd-098e-52d7-8b1e-34d45cf17669|Distributed Spanning Tree Algorithm with Multiple Instances, Forbidden Node Set, and Multiple Forbidden Node Sets|The authors propose a distributed algorithm for finding a spanning tree in a graph network with multiple instances, a forbidden node set, and multiple forbidden node sets.
567deaed-3ed9-55aa-a03b-72d1e564c293|Distributed Path Aggregation Algorithm with Multiple Instances, Forbidden Node Set, and Multiple Forbidden Node Sets|The authors propose a distributed algorithm for path aggregation in a graph network with multiple instances, a forbidden node set, and multiple forbidden node sets.
b642a92d-4055-53b6-8945-450c97caf041|Distributed Rooted Tree Aggregation Algorithm with Multiple Instances, Forbidden Node Set, and Multiple Forbidden Node Sets|The authors propose a distributed algorithm for rooted tree aggregation in a graph network with multiple instances, a forbidden node set, and multiple forbidden node sets.
e608d7ee-5847-5d38-b933-c78778b7cb7a|Distributed Subgraph Aggregation with Operator Algorithm with Multiple Instances, Forbidden Node Set, and Multiple Forbidden Node Sets|The authors propose a distributed algorithm for subgraph aggregation with operator in a graph network with multiple instances, a forbidden node set, and multiple forbidden node sets.
b67f0ae6-0ebc-52d0-a44a-e3723abb477d|Distributed Partwise Aggregation with Operator Algorithm with Multiple Instances, Forbidden Node Set, and Multiple Forbidden Node Sets|The authors propose a distributed algorithm for partwise aggregation with operator in a graph network with multiple instances, a forbidden node set, and multiple forbidden node sets.
26eb9c18-6d8f-5b08-ad54-a8af54f243dd|Distributed Vertex Disjoint Paths Algorithm with Multiple Instances, Multiple Forbidden Node Sets, and Multiple Forbidden Edge Sets|The authors propose a distributed algorithm for finding vertex disjoint paths in a graph network with multiple instances, multiple forbidden node sets, and multiple forbidden edge sets.
f991cceb-5a53-54db-a8b2-392be2501376|Distributed Treewidth Approximation Algorithm with Multiple Instances, Multiple Forbidden Node Sets, and Multiple Forbidden Edge Sets|The authors propose a distributed algorithm for approximating treewidth in a graph network with multiple instances, multiple forbidden node sets, and multiple forbidden edge sets.
627502a3-ed7c-53d4-8461-8f1f2351b168|Distributed Maximum Independent Set Algorithm with Multiple Instances, Multiple Forbidden Node Sets, and Multiple Forbidden Edge Sets|The authors propose a distributed algorithm for finding a maximum independent set in a graph network with multiple instances, multiple forbidden node sets, and multiple forbidden edge sets.
80027f09-c1e0-56bc-a291-ba070a945baa|Distributed Subgraph Aggregation Algorithm with Multiple Instances, Multiple Forbidden Node Sets, and Multiple Forbidden Edge Sets|The authors propose a distributed algorithm for subgraph aggregation in a graph network with multiple instances, multiple forbidden node sets, and multiple forbidden edge sets.
e61f9453-c9d2-58c3-9b49-ef9fba2e6889|Distributed Partwise Aggregation Algorithm with Multiple Instances, Multiple Forbidden Node Sets, and Multiple Forbidden Edge Sets|The authors propose a distributed algorithm for partwise aggregation in a graph network with multiple instances, multiple forbidden node sets, and multiple forbidden edge sets.
dfa7d9bf-b486-5378-b608-ce94205b539f|Distributed Spanning Tree Algorithm with Multiple Instances, Multiple Forbidden Node Sets, and Multiple Forbidden Edge Sets|The authors propose a distributed algorithm for finding a spanning tree in a graph network with multiple instances, multiple forbidden node sets, and multiple forbidden edge sets.
91b76da5-76a9-537b-9d8e-7b5f71d346f7|Distributed Path Aggregation Algorithm with Multiple Instances, Multiple Forbidden Node Sets, and Multiple Forbidden Edge Sets|The authors propose a distributed algorithm for path aggregation in a graph network with multiple instances, multiple forbidden node sets, and multiple forbidden edge sets.
60c31b22-9a16-5921-980a-14cab8bf5d7e|Distributed Rooted Tree Aggregation Algorithm with Multiple Instances, Multiple Forbidden Node Sets, and Multiple Forbidden Edge Sets|The authors propose a distributed algorithm for rooted tree aggregation in a graph network with multiple instances, multiple forbidden node sets, and multiple forbidden edge sets.
565e8add-bbe8-5ac2-b282-b97d7483583a|Distributed Subgraph Aggregation with Operator Algorithm with Multiple Instances, Multiple Forbidden Node Sets, and Multiple Forbidden Edge Sets|The authors propose a distributed algorithm for subgraph aggregation with operator in a graph network with multiple instances, multiple forbidden node sets, and multiple forbidden edge sets.
2039306c-3a2a-5256-81c1-760639a1471e|Distributed Partwise Aggregation with Operator Algorithm with Multiple Instances, Multiple Forbidden Node Sets, and Multiple Forbidden Edge Sets|The authors propose a distributed algorithm for partwise aggregation with operator in a graph network with multiple instances, multiple forbidden node sets, and multiple forbidden edge sets.
7ada8bf7-57a2-52c0-8467-4aaa712464cc|Distributed Vertex Disjoint Paths Algorithm with Multiple Instances, Multiple Forbidden Node Sets, Multiple Forbidden Edge Sets, and Multiple Forbidden Edge Sets with Weights|The authors propose a distributed algorithm for finding vertex disjoint paths in a graph network with multiple instances, multiple forbidden node sets, multiple forbidden edge sets, and multiple forbidden edge sets with weights.
5397a1e9-0147-5d67-9154-d0983a03cc65|Distributed Treewidth Approximation Algorithm with Multiple Instances, Multiple Forbidden Node Sets, Multiple Forbidden Edge Sets, and Multiple Forbidden Edge Sets with Weights|The authors propose a distributed algorithm for approximating treewidth in a graph network with multiple instances, multiple forbidden node sets, multiple forbidden edge sets, and multiple forbidden edge sets with weights.
b6e0dd13-9976-54d4-bd8e-c93227b5e6b9|Distributed Maximum Independent Set Algorithm with Multiple Instances, Multiple Forbidden Node Sets, Multiple Forbidden Edge Sets, and Multiple Forbidden Edge Sets with Weights|The authors propose a distributed algorithm for finding a maximum independent set in a graph network with multiple instances, multiple forbidden node sets, multiple forbidden edge sets, and multiple forbidden edge sets with weights.
a37638d9-b992-5fc3-853a-e96e05dd86c8|Distributed Subgraph Aggregation Algorithm with Multiple Instances, Multiple Forbidden Node Sets, Multiple Forbidden Edge Sets, and Multiple Forbidden Edge Sets with Weights|The authors propose a distributed algorithm for subgraph aggregation in a graph network with multiple instances, multiple forbidden node sets, multiple forbidden edge sets, and multiple forbidden edge sets with weights.
df0cb9d7-2aba-529e-a7f8-4a20d5a58cbd|Distributed Partwise Aggregation Algorithm with Multiple Instances, Multiple Forbidden Node Sets, Multiple Forbidden Edge Sets, and Multiple Forbidden Edge Sets with Weights|The authors propose a distributed algorithm for partwise aggregation in a graph network with multiple instances, multiple forbidden node sets, multiple forbidden edge sets, and multiple forbidden edge sets with weights.
c13b80fa-5f3f-5afc-9ff0-166a42054048|Distributed Spanning Tree Algorithm with Multiple Instances, Multiple Forbidden Node Sets, Multiple Forbidden Edge Sets, and Multiple Forbidden Edge Sets with Weights|The authors propose a distributed algorithm for finding a spanning tree in a graph network with multiple instances, multiple forbidden node sets, multiple forbidden edge sets, and multiple forbidden edge sets with weights.
b72676f1-4109-5793-9f13-4df439c2d6cb|Distributed Path Aggregation Algorithm with Multiple Instances, Multiple Forbidden Node Sets, Multiple Forbidden Edge Sets, and Multiple Forbidden Edge Sets with Weights|The authors propose a distributed algorithm for path aggregation in a graph network with multiple instances, multiple forbidden node sets, multiple forbidden edge sets, and multiple forbidden edge sets with weights.
445dff47-b34e-5e5a-83bf-f3b5ee113763|Distributed Rooted Tree Aggregation Algorithm with Multiple Instances, Multiple Forbidden Node Sets, Multiple Forbidden Edge Sets, and Multiple Forbidden Edge Sets with Weights|The authors propose a distributed algorithm for rooted tree aggregation in a graph network with multiple instances, multiple forbidden node sets, multiple forbidden edge sets, and multiple forbidden edge sets with weights.
b7724b2e-a550-52e6-8653-10793f595bc9|Distributed Subgraph Aggregation with Operator Algorithm with Multiple Instances, Multiple Forbidden Node Sets, Multiple Forbidden Edge Sets, and Multiple Forbidden Edge Sets with Weights|The authors propose a distributed algorithm for subgraph aggregation with operator in a graph network with multiple instances, multiple forbidden node sets, multiple forbidden edge sets, and multiple forbidden edge sets with weights.
dcf6eecf-2db5-55de-a795-662566c16ef7|Distributed Partwise Aggregation with Operator Algorithm with Multiple Instances, Multiple Forbidden Node Sets, Multiple Forbidden Edge Sets, and Multiple Forbidden Edge Sets with Weights|The authors propose a distributed algorithm for partwise aggregation with operator in a graph network with multiple instances, multiple forbidden node sets, multiple forbidden edge sets, and multiple forbidden edge sets with weights.
f8c8db34-c6e0-50c1-ac5b-e70d15d2128b|Distributed Vertex Disjoint Paths Algorithm with Multiple Instances, Multiple Forbidden Node Sets, Multiple Forbidden Edge Sets, Multiple Forbidden Edge Sets with Weights, and Multiple Forbidden Edge Sets with Weights|The authors propose a distributed algorithm for finding vertex disjoint paths in a graph network with multiple instances, multiple forbidden node sets, multiple forbidden edge sets, multiple forbidden edge sets with weights, and multiple forbidden edge sets with weights.
73962a7f-370f-5256-85bc-dda3eef8b711|Fractional Push|The authors propose a novel approach called Fractional Push to address the challenge of memory-efficient scalable graph processing. This solution focuses on reducing the bandwidth required for communication between nodes in a distributed graph processing system.
cc703c4b-d068-5927-a504-40df51a4327a|Radar Push|The authors also propose an improved version of the Radar Push algorithm, which is designed to optimize the communication cost in distributed PageRank computation.
9c2839ca-a2e0-5dfc-b407-bf750d901a18|Hybrid Approach|The authors propose a hybrid approach that combines the benefits of Fractional Push and Radar Push to achieve both memory efficiency and scalability in graph processing.
3c8007ab-8bc8-539a-b9ca-be835807934f|Fractional Push Radar Push Algorithm|The authors propose a novel algorithm that combines Fractional Push and Radar Push to optimize communication efficiency in distributed PageRank computation. This algorithm strategically uses Fractional Push in the initial rounds to reduce the bandwidth bottleneck and then switches to Radar Push to maintain the same theoretical guarantee.
d2c77ea4-b21e-5206-aa5a-7e2f668e0524|Hybrid Combiner|The authors propose a hybrid combiner that carefully couples lock-free and lock-based interactions to efficiently handle fine-grain synchronizations in vertex-centric graph processing. This solution addresses the challenge of memory-efficient scalable graph processing by reducing the overhead of message combinations, which is a critical component of vertex-centric programs.
52519922-00cf-56ed-a3e2-58d82c0df2de|Externalized Structure|The authors propose externalizing vertex attributes to improve cache efficiency by grouping them according to their access frequency. This solution addresses the challenge of memory-efficient scalable graph processing by reducing memory accesses and improving locality.
c1da9645-d4da-5bab-8c52-b4f7f431150d|Edge-Centric Workload Representation|The authors propose representing the workload in an edge-centric manner to improve load imbalance while preserving the vertex-centric paradigm and Pregel user interface. This solution addresses the challenge of memory-efficient scalable graph processing by better evaluating the workload and reducing the overhead of message combinations.
fd7a4391-40d8-506b-88e3-e44e819faa17|Dynamic Scheduling|The authors propose using OpenMP dynamic scheduling to improve the performance of vertex-centric graph processing. This solution addresses the challenge of memory-efficient scalable graph processing by reducing the overhead of workload dispatch and improving load balance.
304348c9-f3ef-567d-bb51-7b4b83a450f5|Externalised Structure|The externalised structure is a solution that addresses the challenge of optimizing communication efficiency in distributed algorithms by reorganizing vertex structures to improve cache efficiency.
fae2b34f-46c2-5a2b-ae09-960dbbb18ef1|Edge-Centric Workload|The edge-centric workload is a solution that addresses the challenge of optimizing communication efficiency in distributed algorithms by representing the workload from an edge-centric perspective.
789e5c17-e58a-5d02-b00c-26fe6c17bf4c|Structure Externalisation|The authors propose externalising vertex attributes to improve cache efficiency by grouping them according to their access frequency, thereby reducing cache pollution and improving memory locality. By reorganizing the vertex structure, the authors are able to minimize cache pollution and improve memory locality, leading to a more efficient use of memory. This approach is unique in that it focuses on the specific characteristics of vertex-centric graph processing and adapts the memory layout to optimize performance. The paper reports that structure externalisation reduces the runtime by up to 40% and provides a speedup of 1.30 on average.
94e92cb3-6ff3-5b22-8bc8-f27516b21358|Adaptive Query Planner with Minimal Communication|The authors propose an adaptive query planner that partitions SPARQL queries instead of partitioning RDF datasets, allowing for minimal communication between machines. This approach enables the system to adaptively choose a suitable number of machines based on query complexity, maximizing parallelism and minimizing communication.
ce06b61f-100f-522f-8749-153747197501|Adaptive Query Planner|The authors propose an adaptive query planner that can handle heterogeneous graph structures by dynamically adjusting the number of machines and their constituent subgraphs based on the complexity of the query. This approach allows for efficient processing of complex queries on large-scale RDF graphs. The query planner uses a novel graph-based approach to partition the query graph into multiple subgraphs, each with a basic two-level tree structure. It then maps each subgraph to a separate machine, allowing for parallel processing and minimizing communication overhead. The planner also uses a cost model to estimate the cost of each possible graph plan and selects the one with the minimum cost. The authors demonstrate the effectiveness of their approach through experiments on various RDF datasets and query workloads, showing significant improvements in query performance and reductions in communication overhead compared to existing systems.
b6a16cda-9397-540c-95b0-75ef4f1105bb|Minimal Communication|The Minimal Communication solution is proposed by the authors to address the challenge of efficient graph dynamics processing. This solution involves a novel approach that minimizes intermediate data shuffling by avoiding data partitioning altogether and employing a novel query planner that effectively partitions queries as opposed to partitioning data.
0c652ef3-50de-5511-be7c-f41f168b5e96|PowerGraph|PowerGraph is a graph system that optimizes computation tasks on power-law graphs, addressing the challenge of memory-efficient scalable graph processing.
01abc6e0-9d95-5fa3-9780-a06b06f64976|GraphChi|GraphChi is a disk-based graph system that exploits a low memory for parallel computations on a single machine, addressing the challenge of memory-efficient scalable graph processing.
60b3df08-30ce-5762-a22a-d01a86b4d1fa|Blogel|Blogel is a block-centric framework for distributed computation on large graphs, addressing the challenge of memory-efficient scalable graph processing.
51bbca50-7ae4-548f-93e2-1809c519e5d4|GraphX|GraphX is a Resilient Distributed Graph (RDG) system based on Spark, addressing the challenge of memory-efficient scalable graph processing.
ec876985-54e9-5419-b972-5e3008a85e02|Gradoop|Gradoop is a scalable graph data management and analytics system with Hadoop, addressing the challenge of memory-efficient scalable graph processing.
f2332159-2192-57f0-968d-0251c93dc164|DFEP|DFEP is a distributed graph partitioning algorithm, addressing the challenge of memory-efficient scalable graph processing.
fb7ca553-93a9-5a85-863f-2b91cc5af940|JA-BE-JA|JA-BE-JA is a fully distributed algorithm for large-scale graph partitioning, addressing the challenge of memory-efficient scalable graph processing.
29b79229-1b2e-52ba-9320-be01866b7fd7|Streaming Graph Partitioning|Streaming graph partitioning is a technique for partitioning large-scale graphs in a streaming scenario, addressing the challenge of memory-efficient scalable graph processing.
93183848-3020-5e9d-841a-bfdef9ce7aac|Fennel|Fennel is a novel framework that unifies two heuristics for streaming graph partitioning, addressing the challenge of memory-efficient scalable graph processing.
861612e5-44c1-5e47-b511-2e002eb91ad9|Distributed Funding-based Edge Partitioning (DFEP)|DFEP is a method proposed by the authors to optimize load balance in distributed systems by partitioning edges across nodes. It assigns an amount of funding to each partition for buying edges, with a coordinator monitoring transactions to balance partition sizes.
4c95f3fa-1fdb-5dff-a307-0714bf8387da|Asynchronous Vertex-Centric Pruning|The authors propose an asynchronous vertex-centric pruning approach to address the challenge of memory-efficient scalable graph processing. This solution involves pruning the graph by iteratively eliminating vertices and edges that do not participate in matches, thereby reducing the graph size and memory consumption.
c6c60a42-2862-5ef1-80f1-b6fc7280a12f|Non-Local Constraint Checking (NLCC)|The authors propose a Non-Local Constraint Checking (NLCC) technique to further reduce memory consumption and improve scalability. NLCC involves checking constraints that span multiple vertices, allowing for more aggressive pruning and reduced memory usage.
94a2ba59-26f5-58d2-bf36-370e91859dee|Edge Pruning|The authors propose an edge pruning technique to reduce the edge density in the pruned graph, which can lead to significant memory savings.
15399c3e-65ba-5f23-9584-2e9e1fd66504|Asynchronous NLCC|Asynchronous NLCC is a solution that addresses the challenge of optimizing communication efficiency in distributed algorithms by leveraging asynchronous communication to reduce the number of communication rounds.
771e5d2a-4b26-5633-b051-c75f22120075|Work Aggregation|Work Aggregation is a solution that addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the number of messages sent over eliminated edges.
15eecaa9-6df5-562b-b39a-c873ea28b5e1|Load Balancing|Load Balancing is a solution that addresses the challenge of optimizing communication efficiency in distributed algorithms by rebalancing the workload to evenly distribute vertices and edges across processing cores.
1f1f9158-ffd3-5063-87b2-309c94c48913|Asynchronous Vertex-Centric Pruning Algorithm|The authors propose an asynchronous vertex-centric pruning algorithm to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm is designed to efficiently process and analyze complex graphs by iteratively pruning the original graph and reducing it to a subgraph that represents the union of all matches.
48b7423f-33aa-5d83-a776-0e4c7b9a2497|Pseudo-Dynamic Load Balancing Strategy|The authors propose a pseudo-dynamic load balancing strategy to address the challenge of load imbalance in distributed systems. This strategy involves checkpointing the current state of execution, reshuffling vertex to processor assignment to evenly distribute vertices and edges across processing cores, and then resuming processing on the rebalanced workload.
46242e57-894b-5fc3-94b8-de58adc59c51|Asynchronous Communication|The authors propose the use of asynchronous communication to improve load balance in distributed systems. This approach allows for efficient communication between nodes without requiring synchronization overheads.
438fb8b6-f632-5231-a763-e3f49cec1eca|PruneJuice|PruneJuice is a pruning-based solution that limits the exponential growth of the state space, scales to massive graphs and distributed memory machines with large number of processors, and supports arbitrary search templates.
c7feab00-8b4e-5fe0-9119-00e3490a8dc6|Template-Driven Search (TDS)|TDS is a distributed algorithm that applies to the solution subgraph to verify non-local constraints, ensuring that all non-matching vertices are eliminated.
cae35700-09fb-5b79-8e75-23d7901f14df|Load Balancing and Checkpointing|Load balancing and checkpointing are used to rebalance the workload and resume processing on a smaller deployment, reducing the overall runtime.
32c0d292-aed3-52c1-a4e0-0b8dbf75ede5|Edge Elimination|Edge elimination is used to reduce the number of edges in the graph, improving the overall efficiency of the system.
b1f7fa58-8ef9-58d1-b884-f103f9b94f25|Exponential Backoff with Limited Participation Rate|The authors propose a solution that combines exponential backoff with a limited participation rate to optimize communication efficiency in distributed algorithms. This approach aims to minimize the number of communication rounds by reducing the likelihood of collisions and increasing the chances of successful message transmission.
932bf4ba-5473-5eb6-8c53-df2050dbdb29|Three-Step Handshake Protocol|The authors propose a three-step handshake protocol to ensure that both endpoints of an edge agree on the formation of an edge. This protocol aims to minimize the number of communication rounds by reducing the likelihood of collisions and increasing the chances of successful message transmission.
303cf6fb-97bf-56c7-9504-27fc8e30f9ed|Adaptive Participation Rate|The authors propose an adaptive participation rate mechanism that adjusts the participation rate of nodes based on the number of rounds that have passed. This approach aims to minimize the number of communication rounds by reducing the likelihood of collisions and increasing the chances of successful message transmission.
fa14c95c-c92b-5261-b66c-ef57040cde68|Adaptive Participation Rate Mechanism|The authors propose an adaptive participation rate mechanism to address the challenge of heterogeneous and irregular graphs. This mechanism involves dynamically adjusting the participation rate of nodes in the graph based on the residual degree of the nodes, which helps to balance the load and reduce communication overhead.
b8014f33-9cf6-573a-bdab-a6f2d0e9f6a4|Neighbor Assignment Function (NAF) Load Balancing|The authors propose a method to optimize load balance in distributed systems by utilizing Neighbor Assignment Functions (NAFs). NAFs are used to assign each node to a neighbor, ensuring that each node has a backup device to store its data in case of failure. The goal is to minimize the maximum load, defined as the number of nodes assigned to a single node.
d196d25f-e7a8-5ddc-9974-1b2c0afab294|Distributed Maximal Matching Algorithm|The authors propose a distributed algorithm for finding a maximal matching in a radio network, which can be used to optimize load balance. The algorithm is designed to work in a distributed setting, where nodes can wake up at random times and try to recruit one of their neighbors to pair with them.
8d9bf607-ab83-5cbb-bb4a-84c8f53f3bfd|Partial NAF Load Balancing|The authors propose a method to optimize load balance in distributed systems by utilizing partial NAFs. Partial NAFs are used to assign a subset of nodes to their neighbors, ensuring that each node has a backup device to store its data in case of failure.
a753bf0c-0711-5dbd-9028-e286ffb6e54c|Neighbor Assignment Function (NAF) Algorithm|The authors propose an algorithm for finding a Neighbor Assignment Function (NAF) in a graph, which is a key component in efficient graph dynamics processing. The algorithm is designed to work in a radio network model, where nodes can communicate with each other through broadcasts. The algorithm uses a distributed and low-energy approach, where nodes wake up at random times and try to recruit one of their neighbors to pair with them. The algorithm also uses a three-step handshake protocol to ensure that both endpoints of the edge agree about who they are paired with. The authors show that the algorithm always terminates in O(log n) timesteps, and with high probability, each node uses energy at most O(log n log) and the NAF is optimal.
5f420ded-7122-50f4-bdbd-28f3663b8859|Matching Cover Algorithm|The authors propose an algorithm for finding a matching cover in a graph, which is a key component in efficient graph dynamics processing. The algorithm is designed to work in a radio network model, where nodes can communicate with each other through broadcasts. The algorithm uses a distributed and low-energy approach, where nodes wake up at random times and try to recruit one of their neighbors to pair with them. The algorithm also uses a three-step handshake protocol to ensure that both endpoints of the edge agree about who they are paired with. The authors show that the algorithm always terminates in O(log n) timesteps, and with high probability, each node uses energy at most O(log n log) and the matching cover is optimal.
dd5de9b7-8c45-5f4d-8ba3-cc10b58b2889|H Partition with Degree d^2 and Size log^2(2n)|The authors propose a deterministic distributed algorithm that computes an H partition of the graph with degree d^2 and size log^2(2n) in O(log^2(2n)) rounds. This solution specifically addresses the challenge of memory-efficient scalable graph processing by partitioning the graph into layers, each with a bounded degree, allowing for efficient processing and reducing memory consumption.
2c0bcd4c-c0b2-5345-babc-6c99ed575716|Randomized Distributed Algorithm for Partial Coloring|The authors propose a randomized distributed algorithm that partially colors the graph using O(log) colors, such that the remaining graph has no path longer than O(log n), with high probability. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the number of colors used and minimizing the length of paths in the remaining graph.
8e9d5ea8-1560-597e-8428-a214d767c54a|Tradeo Low Arb Coloring Algorithm|The authors propose a randomized distributed algorithm that partially colors the graph using 2 colors, such that the remaining graph has no path longer than O(log n), with high probability. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the number of colors used and minimizing the length of paths in the remaining graph.
62bbf885-1bf6-5b8c-aade-6546a0c137e3|Deterministic Coloring After Partial Coloring|The authors propose a deterministic algorithm that colors the remaining graph after partial coloring, using d+1 extra colors. This solution specifically addresses the challenge of memory-efficient scalable graph processing by minimizing the number of colors used and ensuring that the remaining graph is colored efficiently.
7bb9f83c-c187-57ae-bf9d-068e4dd0b1b5|H Partition with Degree d and Size log 2 2 n|This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing an H partition method that partitions the nodes into log 2 2 n disjoint subsets, such that every node v Hj with j 1, 2, ..., l, has at most 2 neighbors in subsets y jHj. This approach enables the computation of an acyclic orientation of the edges in O log n rounds, such that each node has out degree at most O, which is crucial for minimizing round complexity.
cfcdeb8c-da12-5449-bffc-31662baba123|Low Out Degree Orientation via H Partition|This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a deterministic distributed algorithm that computes an acyclic orientation of the edges in O log1 2 n rounds, such that each node has out degree at most 2, for a given parameter 0.
36e3156e-321f-545f-a35f-fdb7e3a4ca4e|H Partition Method|The H partition method is a technique used to partition the nodes of a graph into layers, ensuring that each node has a limited number of neighbors in the previous layers. This method is specifically designed to address the challenge of adaptive algorithms for heterogeneous and irregular graphs by providing a structured approach to handling graphs with varying degrees, weights, and sparsity.
aea1866d-6987-54d9-9e5e-58a49a921f3f|Low Out-Degree Orientation via H Partition|This solution involves using the H partition method to compute an acyclic orientation of the edges in a graph, ensuring that the maximum out-degree is limited. This approach is specifically designed to address the challenge of adaptive algorithms for heterogeneous and irregular graphs by providing a structured approach to handling graphs with varying degrees, weights, and sparsity.
a4d99681-ef70-599a-b379-171941a78945|Trade-Off Low Arb Coloring Algorithm|This solution involves using a trade-off low arb coloring algorithm to partially color a graph, ensuring that the remaining graph has a limited number of colors. This approach is specifically designed to address the challenge of adaptive algorithms for heterogeneous and irregular graphs by providing a structured approach to handling graphs with varying degrees, weights, and sparsity.
e670bbdf-1dc1-5ea6-8c30-3ccc19cbb6ab|Low Arb Deterministic Coloring Algorithm|This solution addresses the challenge of efficient graph dynamics processing by proposing a deterministic distributed algorithm that computes an O(2) coloring of any n-node graph G with arboricity , in O(log n) rounds. The algorithm uses a low out-degree orientation of the edges, which is computed in O(log n) rounds, and then applies a variation of Linial's algorithm to color the graph.
adeb4d35-b632-5ce6-9ffc-5cc1fbdc64e0|Randomized O(log) Partial Coloring Algorithm|This solution addresses the challenge of efficient graph dynamics processing by proposing a randomized distributed algorithm that partially colors any n-node graph G with arboricity , using O(log) colors, in a manner that the remaining graph has no path longer than O(log n), with high probability.
baec4540-11e7-5e4d-82ff-e3c59781fa97|Local Ratio Technique for Distributed Vertex Cover|The authors propose a distributed 2-approximation algorithm for the minimum weight vertex cover problem using the local ratio technique. This technique involves iteratively reducing the weights of vertices and their neighbors until a vertex cover is obtained.
13f01285-dbbc-5061-90aa-50192ea707ff|Distributed Implementation of Local Ratio Technique|The authors provide a distributed implementation of the local ratio technique for the minimum weight vertex cover problem. The implementation involves each vertex sending requests to its neighbors to reduce their weights and responding to requests from its neighbors.
8b1d0731-077e-5114-8dde-96182d0372aa|Adaptation to CONGEST Model|The authors adapt their distributed implementation of the local ratio technique to the CONGEST model, where the message size is limited to O(log n) bits.
d61731cd-e476-5af8-a2a3-4ec5ebc5061c|Local Ratio Technique for Adaptive Weight Reduction|The authors propose a local ratio technique to adaptively reduce weights in a distributed setting, allowing for efficient processing of heterogeneous and irregular graphs. This technique involves dividing the weight of a vertex into two parts: a vault and a bank. The vault is used to initiate requests for weight reductions with neighbors, while the bank is used to respond to requests from neighbors. This approach enables adaptive weight reduction, reducing communication overhead and enhancing memory locality.
c75785cd-2899-5c71-9f01-37cd43fbe6e9|Distributed 2-Approximation Algorithm for MWVC|The authors present a distributed algorithm for the minimum weight vertex cover problem that achieves a 2-approximation in O(log log log) rounds. The algorithm uses a distributed implementation of the local ratio technique, which involves iteratively reducing the weights of vertices and their neighbors while ensuring that the reduced weights do not become negative. The algorithm also uses a novel approach to handle the vault and bank of each vertex, allowing for efficient weight reductions. The algorithm achieves a 2-approximation for the minimum weight vertex cover problem in O(log log log) rounds, where is the maximum degree in the graph.
f51ce5ef-a060-5829-8342-890b944f7c5a|Adaptation to the CONGEST Model|The authors propose an adaptation of their distributed algorithm to the CONGEST model, where the message size is limited to O(log n) bits. The adaptation involves modifying the request and budget messages to accommodate the limited message size. The authors also propose a novel approach to handle the vault and bank of each vertex, allowing for efficient weight reductions. The adapted algorithm achieves a 2-approximation for the minimum weight vertex cover problem in O(log log log) rounds, where is the maximum degree in the graph.
ccca0352-30af-5c57-83df-8b2a17af2cb7|Fused Breadth-First Probabilistic Traversals (FBPT)|The authors propose a novel technique called Fused Breadth-First Probabilistic Traversals (FBPT) to address the challenge of memory-efficient scalable graph processing. FBPT combines multiple probabilistic breadth-first traversals (BPTs) into a single traversal, reducing memory consumption and improving scalability.
197c1ecd-733f-54fc-b663-b1869dc8829f|Vertex Reordering Techniques|The authors investigate the use of vertex reordering techniques to improve the performance of FBPT. They evaluate several reordering algorithms, including Grappolo and RCM, and demonstrate their effectiveness in reducing memory consumption and improving scalability.
59cf9fc3-24e7-5b30-bb87-f916fdab8bda|Workload Balancing Mechanisms|The authors propose workload balancing mechanisms to address the issue of workload imbalance between CPU and GPU workers in heterogeneous systems. They design a lightweight micro-benchmarking scheme to adjust the color size of CPU workers and achieve better balance.
7e223ffd-6a30-58e0-8b8d-149370510412|Workload Balancing|The authors propose a workload balancing technique to distribute the workload evenly among CPU and GPU workers, reducing the number of communication rounds required.
f84fd847-ad28-54bb-8028-e8f782c52988|Hierarchical Frontier Queue|The authors propose the use of a hierarchical frontier queue to manage the traversal process in graph algorithms. By using a hierarchical queue structure, the algorithm can reduce the overhead of edge accesses and improve memory locality.
ae8e2ad8-2a4a-5279-8ec1-c2e3d81e4a1c|Fused Breadth-First Traversal (FBFT)|The authors propose a novel approach called Fused Breadth-First Traversal (FBFT) to optimize load balance in distributed systems. FBFT combines multiple breadth-first traversals into a single traversal, reducing the number of traversals and improving load balance.
f1e9f578-ee1a-53a3-818a-5a9f7757c81c|Workload Balancing using CPU-GPU Collaboration|The authors propose a workload balancing technique that utilizes CPU-GPU collaboration to optimize load balance in distributed systems. The technique involves grouping CPU workers in the same L3 cache region to collaborate on one BPT group, resulting in 8 total CPU worker groups executing BPTs.
b292a3dc-9bf8-58ba-a29e-96ffb8dc832a|Vertex Reordering using Grappolo|The authors propose a vertex reordering technique using Grappolo to optimize load balance in distributed systems. Grappolo is a clustering-based heuristic that provides the best performance among the evaluated heuristics.
9321bfb3-150c-5796-a8e6-5650834f74ef|Hierarchical Frontier Queue Management|The authors propose a hierarchical frontier queue management system to optimize memory access patterns for graph processing. This system involves dividing the frontier queue into smaller sub-queues based on vertex degrees, reducing memory access latency and improving memory coalescing.
4d8d5f44-20fd-5df6-a9c0-1290b34a8951|Ripples Framework|The authors propose a framework called Ripples for efficient graph dynamics processing, which uses fused BPTs to process dynamic updates in large graphs.
cbae888d-599c-57cb-be8f-3856c2bae825|Deterministic MPC Algorithm for Degree+1 List Coloring|The authors propose a deterministic MPC algorithm for degree+1 list coloring, which addresses the challenge of memory-efficient scalable graph processing by utilizing a sublinear memory regime. The algorithm achieves a time complexity of O(log^2) rounds with linear memory and O(log^2 log n) rounds with sublinear memory.
8b532e71-cb32-5105-9eaa-c7667307204e|Aggregation Tree Structure|The authors propose an aggregation tree structure to efficiently process graph data in the MPC model. The structure allows for constant-time computation of prefix sums, sorting, and set difference operations.
7c04451c-6f59-5364-813e-37f409ec00eb|Derandomization Technique|The authors propose a derandomization technique to deterministically produce biased random coins from a short random seed. The technique involves iteratively fixing bits of the candidate color for each node.
0647d44a-9e19-5b30-807a-47af5a363cb6|Sorting Algorithm|The authors propose a sorting algorithm to efficiently sort the elements of the sets A1, ..., Ak in the MPC model.
2296c935-3e26-5160-8743-3ac2ebede881|Prefix Sums Algorithm|The authors propose a prefix sums algorithm to efficiently compute the prefix sums of the elements of the sets A1, ..., Ak in the MPC model.
22f9b2f6-1921-56ee-aa55-7a1ea0e34afe|Set Difference Algorithm|The authors propose a set difference algorithm to efficiently compute the set difference of the elements of the sets A1, ..., Ak in the MPC model.
9cb846a2-d3f4-5b66-8089-cda6e442cff4|Deterministic Network Decomposition|The authors propose a deterministic network decomposition algorithm that enables the efficient solution of various graph problems, including k-coloring, in a distributed setting. This algorithm allows for the decomposition of the graph into clusters with small diameter, facilitating the parallel execution of local algorithms and reducing the overall round complexity.
a859cfef-8c89-562b-af93-9aec7f415e00|Derandomization of Local Distributed Algorithms|The authors propose a derandomization technique for local distributed algorithms, which enables the efficient solution of graph problems in a distributed setting. This technique involves the use of a short random seed to produce biased random coins, which are then used to make local decisions.
58344277-7971-5c25-96dc-1da4d82fcd7a|List Coloring Algorithm|The authors propose a list coloring algorithm that enables the efficient solution of the list coloring problem in a distributed setting. This algorithm involves the use of a novel approach to list coloring, which relies on the use of a polylogarithmic-time algorithm to compute a decomposition with polylogarithmic diameter and congestion.
ba6aef64-9d62-5d3b-affe-3427bebf1918|CONGESTED CLIQUE Algorithm|The authors propose a CONGESTED CLIQUE algorithm that enables the efficient solution of graph problems in a distributed setting. This algorithm involves the use of a novel approach to network decomposition, which relies on the use of a polylogarithmic-time algorithm to compute a decomposition with polylogarithmic diameter and congestion.
998f328e-abb3-5792-9700-d03b1f50cdaf|MPC Algorithm|The authors propose an MPC algorithm that enables the efficient solution of graph problems in a distributed setting. This algorithm involves the use of a novel approach to network decomposition, which relies on the use of a polylogarithmic-time algorithm to compute a decomposition with polylogarithmic diameter and congestion.
b61ed1dc-1960-5d2a-9104-8d04c575b4cb|Deterministic Distributed List Coloring Algorithm|The authors propose a deterministic distributed list coloring algorithm that can handle heterogeneous graph structures with varying degrees, weights, and sparsity. The algorithm is designed to overcome load imbalance, reduce communication overhead, and enhance memory locality in distributed systems.
be888956-f508-5fdc-88a5-55eac0d5b806|Adaptive Derandomization Technique|The authors propose an adaptive derandomization technique that can handle irregular graph structures with varying degrees and weights. The technique involves producing biased random coins from a common short random seed, which helps to reduce the potential increase in the derandomization process.
1790efac-c066-5bdc-9e0e-40f1a2337b8c|Network Decomposition Algorithm|The authors propose a network decomposition algorithm that can efficiently handle heterogeneous graph structures with varying degrees, weights, and sparsity. The algorithm is designed to reduce communication overhead and enhance memory locality in distributed systems.
44a863e5-5d2f-55b3-8a3b-a6ab6dfe2d7f|Deterministic Distributed Load Balancing via Network Decomposition|The authors propose a deterministic distributed algorithm that utilizes network decomposition to achieve load balancing in distributed systems. The algorithm first computes a network decomposition with a small diameter and then iterates through the clusters to balance the load.
94ecdbbc-fb3d-5a11-89f2-b06f1eb9a5db|Load Balancing via Aggregation Tree Structure|The authors propose a load balancing algorithm that utilizes an aggregation tree structure to balance the load in distributed systems. The algorithm first constructs an aggregation tree structure and then uses it to balance the load.
aa67235a-cfc2-5f2c-9940-68a0ea0b9f9f|Deterministic Distributed Load Balancing via Derandomization|The authors propose a deterministic distributed algorithm that utilizes derandomization to achieve load balancing in distributed systems. The algorithm first derandomizes the load balancing process and then achieves load balancing.
6917e35b-e5ed-5161-b893-125f96d2822b|Deterministic Distributed Graph Coloring Algorithm|The authors propose a deterministic distributed graph coloring algorithm that solves the list-coloring problem in the CONGEST model. The algorithm is designed to work efficiently in dynamic environments where the graph structure changes frequently. The algorithm uses a combination of techniques such as derandomization, network decomposition, and list coloring to achieve efficient graph dynamics processing. The derandomization technique is used to convert a randomized algorithm into a deterministic one, while the network decomposition technique is used to divide the graph into smaller subgraphs that can be processed independently. The list coloring technique is used to assign colors to vertices based on their degree and list of available colors. The authors show that their algorithm can solve the list-coloring problem in O(D log n log^2 C) rounds, where D is the diameter of the graph, n is the number of vertices, and C is the number of colors. This result demonstrates the effectiveness of the algorithm in handling dynamic graph updates.
87b8b373-d827-588d-a319-93f3919be828|Degree-Based Graph Partitioning|The authors propose a degree-based graph partitioning method to address the challenge of memory-efficient scalable graph processing. This method involves partitioning the graph based on the degree of vertices, where vertices with higher degrees are placed on faster memory (DRAM) and those with lower degrees are placed on slower memory (NVRAM).
52ca281a-50c2-5be0-a84d-1c747295cc39|K-Way Graph Partitioning|The authors also propose a K-way graph partitioning method, which groups vertices into clusters to minimize the number of edges between clusters. This method is used to optimize memory usage and reduce communication overhead.
0c0387de-cf92-55f0-aebf-c96c264e4200|Graph Reordering|The authors propose a graph reordering method to optimize memory usage and reduce communication overhead. This method involves reordering the placement of vertices or edges in memory to boost cache locality.
c0d6ba07-6e8e-51c6-9fd1-e7359614996c|Vertex Accesses Partitioning|The authors propose a vertex accesses partitioning method, which measures the number of data accesses of each vertex for each algorithm and places the vertices with highest average accesses per edge onto DRAM.
edbf1cfb-b29c-54b3-8e05-b2ab4a6e186d|Weighted Degree Partitioning|The authors propose a weighted degree partitioning method, which evaluates vertices by a weighted degree metric to optimize memory usage.
11b1f5f6-7aa2-5028-aff3-3bc5dae9074c|Split Degree Partitioning|The authors propose a split degree partitioning method, which splits vertices into two groups based on their degrees to optimize memory usage.
587ba712-d914-5708-84e4-9e1e6742f768|Ascending Degree Partitioning|The authors propose an ascending degree partitioning method, which places vertices with lower degrees on DRAM and those with higher degrees on NVRAM to optimize memory usage.
ab5b28e7-f79e-5fde-a7b8-1e3c9f4dad98|Descending Degree Partitioning|The authors propose a descending degree partitioning method, which places vertices with higher degrees on DRAM and those with lower degrees on NVRAM to optimize memory usage.
2d373808-a45e-5e07-984c-1fe79abb9c5f|Random Partitioning|The authors propose a random partitioning method, which allocates vertices randomly to optimize memory usage.
a42936c3-2460-5bb5-9a77-10ccbce3b103|METIS Partitioning|The authors propose a METIS partitioning method, which uses the METIS library to solve the K-way graph partitioning problem approximately to optimize memory usage.
39b0c8b1-0596-5c10-9a85-da656fabc8fe|Rabbit Partitioning|The authors propose a Rabbit partitioning method, which uses a vertex-level K-way based reordering method to optimize memory usage.
7617034e-40fa-5fc6-82a1-764651c7a7d3|Vertex-Level K-Way Reordering|The authors propose a vertex-level K-way reordering method, which takes into account the degree of vertices to optimize memory usage.
71ef7f35-bbc6-5bbb-96da-1e7e20ed7819|Edge-Level K-Way Reordering|The authors propose an edge-level K-way reordering method, which takes into account the degree of edges to optimize memory usage.
a1f24487-1029-5f8b-a1a8-237661deb995|Descending Degree Reordering|The authors propose a descending degree reordering method, which places vertices with higher degrees on DRAM and those with lower degrees on NVRAM to optimize memory usage.
540392aa-6bd2-50e9-a47e-dd9e50165cb3|Ascending Degree Reordering|The authors propose an ascending degree reordering method, which places vertices with lower degrees on DRAM and those with higher degrees on NVRAM to optimize memory usage.
f965fdde-b7fd-5513-a9c8-95a3a700ec63|Weighted Degree Reordering|The authors propose a weighted degree reordering method, which evaluates vertices by a weighted degree metric to optimize memory usage.
f37d6fc8-db3d-5540-ad6a-14ba19f5f268|Split Degree Reordering|The authors propose a split degree reordering method, which splits vertices into two groups based on their degrees to optimize memory usage.
a05a5821-9df7-5c35-9652-6e54ca678155|Random Reordering|The authors propose a random reordering method, which allocates vertices randomly to optimize memory usage.
32db8140-f407-5485-acaf-27fa34301e5f|METIS Reordering|The authors propose a METIS reordering method, which uses the METIS library to solve the K-way graph partitioning problem approximately to optimize memory usage.
df2d8466-fb50-5607-bd09-badaae779bd8|Rabbit Reordering|The authors propose a Rabbit reordering method, which uses a vertex-level K-way based reordering method to optimize memory usage.
ea74c65d-48c2-5953-9e9a-42babcff2cde|Vertex-Level K-Way Based Reordering|The authors propose a vertex-level K-way based reordering method, which takes into account the degree of vertices to optimize memory usage.
de34cf98-f04a-503b-8da3-5247bc3646d0|Edge-Level K-Way Based Reordering|The authors propose an edge-level K-way based reordering method, which takes into account the degree of edges to optimize memory usage.
196ad473-6849-55fa-8154-e0443b67852c|Descending Degree Based Reordering|The authors propose a descending degree based reordering method, which places vertices with higher degrees on DRAM and those with lower degrees on NVRAM to optimize memory usage.
c7a864b3-b198-5db7-a97c-9c411987103b|Ascending Degree Based Reordering|The authors propose an ascending degree based reordering method, which places vertices with lower degrees on DRAM and those with higher degrees on NVRAM to optimize memory usage.
9bb724d9-901f-5e87-9aa2-59be465b1d0c|Weighted Degree Based Reordering|The authors propose a weighted degree based reordering method, which evaluates vertices by a weighted degree metric to optimize memory usage.
db94a455-87c7-5b24-8751-c72f326a7080|Split Degree Based Reordering|The authors propose a split degree based reordering method, which splits vertices into two groups based on their degrees to optimize memory usage.
9ce2bdba-3ecd-5af5-87d6-85488c557e32|Random Based Reordering|The authors propose a random based reordering method, which allocates vertices randomly to optimize memory usage.
5dca342b-ad0b-5af4-8ac9-4c6d513ba0ed|METIS Based Reordering|The authors propose a METIS based reordering method, which uses the METIS library to solve the K-way graph partitioning problem approximately to optimize memory usage.
8ea0e162-2655-505e-94b0-23f780992d65|Rabbit Based Reordering|The authors propose a Rabbit based reordering method, which uses a vertex-level K-way based reordering method to optimize memory usage.
cf2203f1-c5b0-5627-a758-58a63d875764|Degree-based Partitioning|The authors propose a degree-based partitioning method to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This method partitions vertices by degree, sorting them in order of decreasing degree and placing the vertices with the largest degree counts on DRAM and the vertices with smaller degree counts on NVRAM. The unique mechanism involved in this solution is the use of degree-based partitioning, which takes into account the degree distribution of the graph to optimize data placement. This approach differs from existing methods that focus on random or naive partitioning. The paper shows that degree-based partitioning can lead to significant speedups, with some algorithms achieving 1.5-3x faster performance compared to naive partitioning methods.
c2abe91d-d3bd-5e2a-b9c9-af9d80f23b17|K-way Graph Partitioning|The authors also propose a K-way graph partitioning method to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This method uses the METIS library to solve the K-way graph partitioning problem approximately, creating clusters of vertices that minimize the number of edges between sets. The unique mechanism involved in this solution is the use of K-way graph partitioning, which takes into account the graph structure to optimize data placement. This approach differs from existing methods that focus on random or naive partitioning. The paper shows that K-way graph partitioning can lead to significant speedups, with some algorithms achieving 1.5-2x faster performance compared to naive partitioning methods.
977c8c48-0c3f-597c-83e8-e6fbce9d3ab0|Priority-based Memory Management|The authors propose a priority-based memory management system that identifies and stores frequently accessed data in a high-priority memory, while less frequently accessed data is stored in a low-priority memory. This approach aims to reduce memory consumption and optimize memory usage by prioritizing the most valuable data.
0df8bfa4-01b3-5589-890f-86c9f6bf8181|Locality-Aware Memory Hierarchy|The authors design a locality-aware memory hierarchy that separates vertex and edge accesses to avoid frequent data thrashing between them. This approach aims to reduce memory consumption and optimize memory usage by exploiting the locality of graph mining applications.
e7b68fcc-8283-57af-a37a-0d414ec88497|Pipeline Specialization|The authors propose a pipeline specialization that maximizes computational parallelism by processing the most intermediate results obtained from each subgraph being extended. This approach aims to reduce computational costs and optimize memory usage by minimizing the number of intermediate results.
d26c14e5-797e-512d-ac0e-db051e8450e8|Locality Aware On-Chip Memory Hierarchy|The authors propose a novel on-chip memory hierarchy that exploits the extension locality in graph mining applications to reduce off-chip communication overheads. This hierarchy consists of a high-priority memory and a low-priority memory, where the high-priority memory permanently stores frequently accessed data, and the low-priority memory dynamically maintains less frequently accessed data using a new replacement policy.
f7771377-ffea-5835-883c-76b391dc5e7d|Pipelined Processing Units (PPUs) with Work Stealing|The authors propose a pipelined processing unit (PPU) architecture that maximizes computational parallelism and reduces load imbalance in graph mining applications. Each PPU has a slot buffer, a stealing buffer, and ancestor buffers, which enable the simultaneous processing of multiple initial embeddings and the efficient reuse of intermediate results.
e3a734b1-154d-5f04-a42e-101d86d7a2cd|Graph Reordering for Fast Computation of Rank ON1|The authors propose a graph reordering technique that enables fast computation of the rank ON1 value at runtime. This technique uses the vertex ID to represent the rank for each data and reorders the vertex IDs to be consistent with their rank order.
fe34f752-7ecf-539c-ab29-62f3e313306b|Locality-Preserved Replacement Policy|The authors propose a locality-preserved replacement policy that integrates the cost-efficient heuristic with hardware implementations. This policy uses a linear function of the ON1 and the recency to replace low-priority data in the low-priority memory.
a87dda83-072b-506c-8ffb-fbdeffaa4d00|Priority-Based Heuristics|The authors propose a priority-based heuristic to identify high-priority data, which is permanently stored in the high-priority memory.
aec8a945-6a14-5d0b-926d-ad5147e93b1e|Work Stealing Mechanism|The authors propose a work stealing mechanism to reduce load imbalance in each processing unit (PU). This mechanism allows idle slots to steal workloads from busy ones, ensuring that all slots are utilized efficiently.
c86c28bf-93d3-510b-a277-f016a64efdc0|Adaptive Dispatching of Initial Embeddings|The authors propose an adaptive dispatching mechanism for initial embeddings to balance the parallel executions of workloads across processing units (PUs).
69c64f2d-130e-5fc2-8532-1591c0eba379|Pipelined Processing Units (PPUs)|The authors design pipelined processing units (PPUs) that minimize off-chip memory accesses and maximize computational parallelism. Each PPU consists of a slot buffer, a stealing buffer, and ancestor buffers, which work together to process embeddings in a pipelined manner.
6bc1fbee-a703-5e5e-8695-f24311d7ada5|Dynamic Slack Allocation Algorithm|This solution specifically addresses the challenge of optimizing load balance in distributed systems by dynamically allocating slack to tasks based on their execution time and dependencies. The algorithm aims to minimize energy consumption while meeting performance constraints.
0b47d76c-fffc-5887-a2c7-b5c1df5e2eec|Energy-Aware Scheduling Algorithm|This solution addresses the challenge of optimizing load balance in distributed systems by scheduling tasks based on their energy consumption and performance requirements. The algorithm aims to minimize energy consumption while meeting performance constraints.
0a1f8420-293a-538d-8ad9-13d371ec5b8a|Virtual Machine Migration|This solution addresses the challenge of optimizing load balance in distributed systems by migrating virtual machines (VMs) based on their workload and resource utilization. The algorithm aims to minimize energy consumption and improve system performance.
1e1ecc46-93dc-5646-be6f-0d9e9bb6b57e|Workload Consolidation|This solution addresses the challenge of optimizing load balance in distributed systems by consolidating workloads onto fewer nodes. The algorithm aims to minimize energy consumption and improve system performance.
4a4c8c25-7372-5f7f-9921-e9acdaa36fde|Thermal-Aware Load Balancing|This solution addresses the challenge of optimizing load balance in distributed systems by taking into account thermal constraints. The algorithm aims to minimize energy consumption and improve system performance while meeting thermal constraints.
6c994ec5-6f1c-5a62-aae8-5cb86eba170e|Dynamic Voltage and Frequency Scaling (DVFS)|This solution addresses the challenge of optimizing load balance in distributed systems by dynamically adjusting voltage and frequency levels based on workload conditions. The algorithm aims to minimize energy consumption while meeting performance constraints.
68ebffd5-9277-5965-a535-5ac0c15123b9|Energy-Efficient Routing|This solution addresses the challenge of optimizing load balance in distributed systems by using energy-efficient routing protocols. The algorithm aims to minimize energy consumption while meeting performance constraints.
7f4257be-f969-5c22-a0f2-a10da562ae7c|Power-Aware Task Scheduling|This solution addresses the challenge of optimizing load balance in distributed systems by scheduling tasks based on their power consumption and performance requirements. The algorithm aims to minimize energy consumption while meeting performance constraints.
2716b0a1-b03a-556e-8728-8b292bcdce0e|Load Balancing with Performance Constraints|This solution addresses the challenge of optimizing load balance in distributed systems by taking into account performance constraints. The algorithm aims to minimize energy consumption while meeting performance constraints.
fea357b9-eaec-58d3-8a99-b31c9f29771a|Energy-Aware Resource Allocation|This solution addresses the challenge of optimizing load balance in distributed systems by allocating resources based on energy consumption and performance requirements. The algorithm aims to minimize energy consumption while meeting performance constraints.
8bbc3b54-c4c9-53be-9d3d-b848d065d21f|Vertex-Centric Programming Model|The authors propose a vertex-centric programming model, where each vertex in the graph is associated with a user-defined value and can send messages to other vertices. This approach allows for efficient processing of large graphs by focusing on local computations at each vertex and reducing the need for global synchronization.
2f027337-e6aa-512f-933d-781761e8d02b|Message Passing with Combiners|The authors propose using message passing with combiners to reduce memory usage and improve performance. Combiners are used to aggregate messages sent to the same vertex, reducing the number of messages that need to be stored and processed.
c95a4c35-b7ec-5c24-a87a-1b669ad5fc11|Aggregators|The authors propose using aggregators to compute global values from local values at each vertex. Aggregators are used to reduce memory usage by avoiding the need to store global values at each vertex.
c0a72b2b-7658-5f61-9393-6d607cb82858|Combiner-based Message Aggregation|The authors propose using combiners to aggregate messages sent to the same vertex, reducing the number of messages transmitted and buffered. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by minimizing the number of messages exchanged between vertices.
aee4413f-1b5b-5eae-b9ae-8453760b20d7|Aggregator-based Global Communication|The authors propose using aggregators to compute a single global value by applying an aggregation function to a set of values supplied by the user. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the need for explicit communication between vertices.
31409de1-d4cd-5760-b155-d2ddf6c46917|Vertex-centric Programming Model|The authors propose a vertex-centric programming model that allows users to focus on local actions, processing each item independently. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the need for explicit communication between vertices.
8c044156-d6cf-566d-a529-20769de5f8f3|Confinement-based Recovery|Confinement-based recovery is a technique used to improve the efficiency of recovery in distributed systems by confining the recovery process to only the lost partitions, rather than recomputing the entire graph. This solution involves logging outgoing messages from healthy partitions and recalculating them from recovering partitions, allowing for a more targeted and efficient recovery process.
7980d7a6-4bfe-5c89-8ab5-d6cc82fb3614|Bulk Synchronous Parallel (BSP) Model|The BSP model is a parallel computation model that allows for efficient graph dynamics processing by dividing the computation into a sequence of supersteps, each consisting of three phases: computation, communication, and barrier synchronization.
8ea98dbb-1a94-5baf-9955-63c66c8ff805|Combiners|Combiners are a technique used to reduce the amount of data sent between vertices, allowing for more efficient graph dynamics processing.
5e914ed8-b65c-5699-a15d-79a0ff94111f|Topology Mutations|Topology mutations are a technique used to change the graphs topology, allowing for more efficient graph dynamics processing.
9414fe94-9f3f-5817-a520-5b53e0d0adcb|Out-of-Core Capabilities|The authors propose using out-of-core capabilities to store part of the information on disk when the tables used by the HashJoin operator exceed the memory. This solution addresses the challenge of memory-efficient scalable graph processing by allowing the system to scale up and handle larger data sets, even when the memory is limited.
c85fcf44-cceb-50a4-bc2a-c7a4c7031fd1|Pipeline Operators|The authors suggest using pipeline operators to improve the scalability of the system. This solution addresses the challenge of memory-efficient scalable graph processing by allowing the system to process messages in a pipeline fashion, reducing the memory consumption and improving the scalability.
9f241666-daea-53cd-81da-157a24682dd5|Temporary Tables|The authors propose using temporary tables to store intermediate results, which can reduce the memory consumption and improve the scalability of the system. This solution addresses the challenge of memory-efficient scalable graph processing by allowing the system to store intermediate results in a more memory-efficient way.
b2ca42f0-ac03-56b6-91f4-0dcb759b0f03|Materialization Algebra|The authors propose using a materialization algebra to express queries in a more memory-efficient way. This solution addresses the challenge of memory-efficient scalable graph processing by allowing the system to express queries in a more memory-efficient way, reducing the memory consumption and improving the scalability.
3c421f4c-46ed-5be0-918d-19eab28468ee|Late Projection|Late projection is a technique used to reduce the size of messages sent between vertices in a distributed graph processing system. By delaying the projection of data until later in the computation, the system can reduce the amount of data that needs to be sent, resulting in improved communication efficiency.
c786d3e3-7a48-50d3-93ec-8497bdccd773|Sharded Aggregators|Sharded aggregators are a technique used to reduce the number of messages sent between vertices by aggregating multiple messages into a single message, and then sharding the aggregated message across multiple machines. By sharding the aggregated message, the system can reduce the number of messages sent between vertices, resulting in improved communication efficiency.
9588ccdf-beda-563e-a70a-b297a588295b|ZooKeeper|ZooKeeper is a technique used to reduce the number of messages sent between vertices by providing a centralized coordination system. By providing a centralized coordination system, the system can reduce the number of messages sent between vertices, resulting in improved communication efficiency.
bd72dd1a-c104-53a3-acd3-bf16876bbdf4|Distributed Sketching Model|The authors propose a distributed sketching model to address the challenge of memory-efficient scalable graph processing. This model allows each vertex to send a message to a referee, who then computes a solution to a combinatorial problem on the graph.
5b8e2db0-7777-5716-8113-4dbb63f43f57|Reduction from Maximal Matching to Maximal Independent Set|The authors propose a reduction from the maximal matching problem to the maximal independent set problem. This reduction allows the authors to prove a lower bound for the maximal independent set problem.
6c980e0a-7abe-5276-920e-a415afb6452d|Information Theoretic Analysis|The authors use information theoretic tools to analyze the lower bound for the maximal matching and maximal independent set problems.
1ecde26b-1122-5b37-92ff-c85cfcff0386|Ruzsa-Szemerdi Graphs|The authors use Ruzsa-Szemerdi graphs to prove communication complexity lower bounds for approximate matching algorithms. These graphs are incompressible in the context of the matching problem and require players to communicate almost their entire graph to the referee.
682586ac-8815-57bf-8df6-d53458fa28c4|Distributed Sketching for Maximal Matching and Maximal Independent Set|The authors propose a distributed sketching model for solving the maximal matching and maximal independent set problems in large graphs. They develop a protocol that allows each vertex to send a single message to a referee, who then computes the final output. The protocol uses a combination of public and private randomness to enable the referee to compute the final output. The authors also introduce a new distribution, DMM, which is used to prove lower bounds for the problems. The authors show that any algorithm for maximal matching or maximal independent set that errs with a small constant probability requires sketches of size n1 2 for any constant 0.
df50b367-ae6c-5738-a13d-1e58cbd5a1b9|Relaxed Greed and Memory-Based Algorithm (RGMA)|The RGMA is a distributed algorithm designed to address the challenge of memory-efficient scalable graph processing in the context of the Minimal Weighted Vertex Cover (MWVC) problem. It focuses on optimizing memory usage by utilizing a relaxed greedy rule and a finite memory-based approach, allowing for efficient processing of large-scale graph data.
ad612ab2-0d99-5f96-a4b7-38c11a2eafad|Potential Game Theoretic Learning|The potential game theoretic learning approach is a method for solving the MWVC problem in a distributed manner. It formulates the problem as a spatial potential game, where each vertex is a player interacting with its neighbors. This approach enables the development of distributed algorithms that can efficiently process large-scale graph data.
1ca9e14f-baf7-53d3-9d68-41a7441093c8|Relaxed Greed and Memory-based Algorithm (RGMA)|The RGMA is a distributed learning algorithm that addresses the challenge of optimizing communication efficiency in distributed algorithms by introducing a relaxed greedy rule and a finite memory mechanism. This approach enables players to update their actions concurrently, reducing the need for extensive communication and improving overall efficiency.
69951e8d-26f3-5f60-a297-6b8381486c0c|Distributed Algorithm for MWVC|The authors propose a distributed algorithm for the MWVC problem, which addresses the challenge of efficient graph dynamics processing. The algorithm is designed to work in a distributed manner, where each vertex updates its strategy concurrently.
41791362-d7bc-54d2-9ccc-5af4bf8ff5f7|Distributed Two-Level Path Index (DTLP)|DTLP is a memory-efficient and scalable graph processing solution that addresses the challenge by partitioning the graph into smaller subgraphs and maintaining a two-level index structure. The first level indexes each subgraph by maintaining a list of bounding paths between any pair of boundary vertices, while the second level indexes the skeleton graph, which is a compact representation of the graph.
dadfecad-5136-5556-98c9-2e4aeb1450f4|Edge Path Index (EP Index)|EP Index is a memory-efficient data structure designed to manage the large number of bounding paths in each subgraph. It utilizes locality-sensitive hashing (LSH) to partition the bounding paths into different groups and employs a modified version of the FP tree to compact the bounding paths.
355184f7-da94-5256-b08b-d0845d54f891|KSP DG Algorithm|KSP DG is a distributed algorithm designed to process k shortest path queries over dynamic graphs. It decomposes the problem of identifying k shortest paths into searching for partial k shortest paths in different subgraphs in parallel.
05a7e143-42d6-5618-9a4e-80fd92b4c7c5|Distributed Two-Level Path (DTLP) Index|The DTLP index is a distributed index structure designed to facilitate efficient k-shortest path query processing over dynamic graphs. It consists of two levels: the first level indexes each subgraph by maintaining a list of bounding paths between any pair of boundary vertices, and the second level is a skeleton graph that provides a global view of the graph structure.
75deef4d-9b63-5d0a-a9e6-3dde7e2a6628|K-Shortest Path Distributed Algorithm (KSP DG)|KSP DG is a distributed algorithm designed to process k-shortest path queries over dynamic graphs. It uses the DTLP index to efficiently identify relevant subgraphs and compute partial k-shortest paths in parallel.
0ef81d81-a170-5d1b-8464-44cac88d2a60|K-Shortest Paths in Dynamic Graphs (KSP DG) Algorithm|The KSP DG algorithm is a distributed algorithm proposed by the authors to process k-shortest path queries in dynamic graphs. It is designed to work in conjunction with the DTLP index to efficiently identify k-shortest paths in a dynamic graph.
7e5b9859-6a7c-5c54-8e59-01e6fe2e42dd|Min Rounds BC (MRBC) Algorithm|The MRBC algorithm is a distributed memory algorithm designed to compute betweenness centrality (BC) in unweighted, directed graphs. It addresses the challenge of memory-efficient scalable graph processing by reducing the number of rounds and messages required for BC computation.
b6011260-481f-52d4-8618-7443f4593579|D-Galois Implementation|The D-Galois implementation is a distributed graph analytics system that provides a programming model for scalable graph processing. It addresses the challenge of memory-efficient scalable graph processing by optimizing communication volume and reducing memory overhead.
bbac006e-6f58-56fa-9ece-85ed07210fd0|Data Structure Optimization|The data structure optimization solution involves using efficient data structures to store and retrieve shortest path distances in the MRBC algorithm. It addresses the challenge of memory-efficient scalable graph processing by reducing memory overhead and improving computation efficiency.
95265e3c-2647-5b29-bf23-64a94b5b25f2|Timestamp Pipelining Technique|The authors propose a timestamp pipelining technique to optimize communication efficiency in distributed algorithms. This technique involves assigning a timestamp to each message based on the round number in which it will be sent, allowing for the efficient pipelining of messages and reducing the number of communication rounds.
8d92f354-f4bf-5395-afa8-6aa78a6d9eff|Proxy Synchronization Rule|The authors propose a proxy synchronization rule to optimize communication efficiency in distributed algorithms. This rule involves synchronizing only the necessary information between proxies, reducing the amount of data that needs to be communicated.
f2b28384-bcec-597d-948b-8a100993045f|Accumulation Technique|The authors propose an accumulation technique to optimize communication efficiency in distributed algorithms. This technique involves accumulating the necessary information at each node, reducing the need for additional communication rounds.
2fd625e2-b039-5403-b588-9d8d19afe429|Boost Flat Map Data Structure|The Boost flat map data structure is a novel data structure used in the MRBC algorithm to efficiently store and retrieve shortest path distances. It addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by providing a compact and efficient data structure that can handle large-scale graphs.
13c2c6b9-66cb-5faf-b9c3-2203d1664092|Proxy Synchronization Rule for Min Rounds BC|The authors propose a novel proxy synchronization rule specifically designed for the Min Rounds BC algorithm to optimize load balance in distributed systems. This rule dictates that a vertex v synchronizes only the necessary information with its proxies in each round, reducing the communication volume and load imbalance.
6512c27c-c314-5524-894d-e687b83179bd|Pipelining Technique|The pipelining technique is a method used in the MRBC algorithm to reduce the number of messages sent across edges. It addresses the challenge of efficient graph dynamics processing by minimizing the number of messages required for computation.
6087afed-9bd7-5c48-92a7-9ed1f68afbd4|Distributed Expander Decomposition|The authors propose a distributed algorithm for computing an expander decomposition of a graph, which is a partition of the vertex set into clusters with high conductance and a small number of inter-cluster edges. This decomposition enables the efficient processing of large graphs by reducing the memory requirements and communication overhead.
2c15299f-3005-5e3c-be13-9688e1fff603|Nearly Most Balanced Sparse Cut Algorithm|The authors propose a distributed algorithm for finding a nearly most balanced sparse cut in a graph, which is a cut that separates the graph into two clusters with a small number of edges between them. This algorithm is used as a subroutine in the expander decomposition algorithm.
59d5844d-dcc5-5616-8c0f-48bb3c2d4a4b|Low-Diameter Decomposition|The authors propose a distributed algorithm for computing a low-diameter decomposition of a graph, which is a partition of the vertex set into clusters with a small diameter. This decomposition is used to reduce the diameter of the clusters in the expander decomposition algorithm.
e559aa31-29f8-5718-8ff5-ccfe1632b985|Hierarchical Routing Structure|The authors propose a hierarchical routing structure for efficient communication between processors in a distributed computing environment. This structure is used to reduce the communication overhead in the expander decomposition algorithm.
75108c1c-f5d5-586c-b9bc-01c1106768bc|Triangle Enumeration Algorithm|The authors propose a distributed algorithm for enumerating all triangles in a graph, which is a fundamental problem in graph processing. This algorithm uses the expander decomposition algorithm as a subroutine.
371c00f4-d035-5c98-a2ee-ba432a6f4840|Distributed Expander Decomposition Algorithm|The authors propose a distributed expander decomposition algorithm that efficiently partitions the graph into clusters with low conductance and diameter. This algorithm is designed to optimize communication efficiency in distributed algorithms by reducing the number of communication rounds required for various graph problems.
4400b731-94a8-5f72-b5fb-9968075bb2a3|Parallelized Nibble Algorithm|The authors propose a parallelized version of the Nibble algorithm, which is a key component of the distributed expander decomposition algorithm. This parallelized algorithm enables the efficient execution of multiple Nibble instances simultaneously, reducing the overall round complexity.
0fe9c2ec-50e0-58f4-9985-3764bbbaa18b|Low-Diameter Decomposition Algorithm|The authors propose a low-diameter decomposition algorithm that efficiently partitions the graph into clusters with low diameter. This algorithm is designed to optimize communication efficiency in distributed algorithms by reducing the number of communication rounds required for various graph problems.
9ef467f5-957f-58d4-bf68-c6c967f216e0|Routing Algorithm|The authors propose a routing algorithm that efficiently routes messages between vertices in the graph. This algorithm is designed to optimize communication efficiency in distributed algorithms by reducing the number of communication rounds required for message delivery.
fdb67f66-9742-5156-90be-53c12ef2c3fc|Adaptive Routing Algorithm|"The authors propose an adaptive routing algorithm that can handle heterogeneous graph structures with varying degrees, weights, and sparsity. The algorithm is used to route messages between vertices in the graph while minimizing communication overhead and load imbalance. The algorithm uses a combination of random sampling, parallel computation, and iterative refinement to adaptively route messages between vertices. The authors also employ a technique called ""hierarchical routing"" to reduce the communication overhead and improve the efficiency of the routing algorithm. The authors demonstrate that their algorithm can route messages between vertices with high probability in O(poly(log n, 1/)) rounds, where  is the conductance parameter."
c386d0eb-5fee-5cad-a809-1f667c3312b5|Distributed Triangle Enumeration Algorithm|"The authors propose a distributed algorithm for enumerating triangles in a graph, which is a fundamental problem in graph processing. The algorithm is used to adaptively handle heterogeneous graph structures with varying degrees, weights, and sparsity. The algorithm uses a combination of random sampling, parallel computation, and iterative refinement to enumerate triangles in the graph. The authors also employ a technique called ""expander decomposition"" to reduce the graph size and improve the efficiency of the triangle enumeration algorithm. The authors demonstrate that their algorithm can enumerate triangles with high probability in O(poly(log n, 1/)) rounds, where  is the conductance parameter."
326f4641-359f-5880-ab4e-4267517452a4|Parallel Nibble|The authors propose a parallel version of the Nibble algorithm, called ParallelNibble, which involves a simultaneous execution of a moderate number of ApproximateNibble instances. This algorithm is designed to optimize load balance in distributed systems by reducing the number of iterations required to achieve a nearly most balanced sparse cut.
c2b8f558-4e75-51eb-9d41-cf0e45d2ee0e|Random Nibble|The authors propose a randomized version of the Nibble algorithm, called RandomNibble, which executes ApproximateNibble with a random starting vertex. This algorithm is designed to optimize load balance in distributed systems by reducing the number of iterations required to achieve a nearly most balanced sparse cut.
506cdf14-7992-5569-bf2a-164b3b9db61b|Distributed Triangle Enumeration|The authors propose a distributed algorithm for triangle enumeration, which is designed to optimize load balance in distributed systems. The algorithm uses a combination of techniques, including a distributed expander decomposition algorithm, a variant of the multi-commodity routing scheme, and an adaptation of the CONGESTED CLIQUE Triangle Enumeration algorithm.
368dc401-9ef6-5390-afcb-152288295e91|Parallelized ApproximateNibble|The authors propose a parallelized version of the ApproximateNibble algorithm to efficiently compute sparse cuts in large graphs. This algorithm is designed to work in a distributed setting, allowing for faster computation of sparse cuts even in the presence of dynamic updates.
e53e1438-aaa0-5e73-9830-3e06d6ca690b|Deterministic Distributed Algorithm for Exact Weighted All Pairs Shortest Paths (APSP)|The authors propose a deterministic distributed algorithm for computing exact weighted APSP in both directed and undirected graphs. The algorithm runs in O(n^3/2 log n) rounds in the Congest model, where n is the number of nodes in the graph.
83f9addb-272e-50c1-aa5a-238da3ffcb4c|Pipelined Algorithm for Updating Scores at Ancestors|The authors propose a pipelined algorithm for updating scores at ancestors of the newly chosen blocker node. The algorithm runs in O(n) rounds and is used in conjunction with the blocker set algorithm.
8eccd974-2384-542f-8140-30b340d2a46b|Distributed Blocker Set Algorithm|The authors propose a distributed algorithm for computing a blocker set, which is used to reduce the number of paths that need to be considered in the APSP algorithm. The algorithm runs in O(nh + n^2 log n/h) rounds.
978cb8b1-a877-5834-85cd-a6aff031c13b|Pipelined Algorithm for Updating Scores|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by introducing a pipelined algorithm for updating scores at nodes in a tree. The algorithm allows for the simultaneous update of scores at multiple nodes, reducing the number of communication rounds required.
5882e7fa-01bb-5ef2-8064-e4840886e44d|Blocker Set Algorithm|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by introducing a blocker set algorithm. The algorithm identifies a set of nodes that can be used to reduce the number of communication rounds required to compute shortest paths.
9fafd28e-df94-5e1e-a586-c31c6156d481|Distributed Algorithm for Computing All-Pairs Shortest Paths|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by introducing a distributed algorithm for computing all-pairs shortest paths. The algorithm uses a combination of graph theory and distributed computing techniques to reduce the number of communication rounds required.
87d85708-7b3b-59ec-9cc6-f8907a9102a2|Pipelined Algorithm for Updating Scores at Ancestors of a Blocker Node|The authors propose a pipelined algorithm to update scores at ancestors of a blocker node in all trees. The algorithm runs in O(n) rounds.
138ced65-9767-5106-97c6-318b628b31b1|Deterministic Algorithm for Computing a Blocker Set|The authors propose a deterministic algorithm to compute a blocker set for a collection of rooted h-hop trees. The algorithm runs in O(nh n^2 log n/h) rounds.
8117fe5b-738f-5d9f-8e41-d094bf45ecb8|Algorithm for Updating Scores at Descendants of a Blocker Node|The authors propose an algorithm to update scores at descendants of a blocker node in all trees. The algorithm is a local computation at each node.
c6503ff0-29b8-5d0b-aa3e-0ba20c57fa26|Algorithm for Computing Initial Scores for a Node in a Tree|The authors propose an algorithm to compute initial scores for a node in a tree. The algorithm runs in O(h) rounds.
48485d23-796d-56fc-b7dd-1724a1c52753|Algorithm for Computing Ancestors of a Node in a Tree|The authors propose an algorithm to compute ancestors of a node in a tree. The algorithm runs in O(h) rounds.
3d8f1cc5-d304-5940-a1d7-dc8f5b60cb0c|Linear Graph Sketching|Linear graph sketching is a technique used to efficiently encode the adjacency list of a vertex or a component in a graph, allowing for the sampling of edges incident to a vertex or component. This method is used to reduce memory consumption and optimize memory usage in graph processing.
51b7377c-6c3b-51c8-be79-9808c1530c84|Distributed Random Ranking (DRR)|DRR is a technique used to break up long chains of components in a graph, reducing the number of merges required to compute connected components. This method is used to optimize memory usage and reduce computational costs in graph processing.
9863f1d2-41ee-5fa9-99ac-3e34652375cf|Communication via Random Proxy Machines|This technique uses random proxy machines to facilitate communication between components in a graph, reducing communication overhead and optimizing memory usage.
2acdf42b-baaf-5d30-9bef-ddf8d98f3aa1|Randomized Proxy Computation|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by introducing a randomized proxy computation technique. This technique is used to load balance congestion at any given machine by redistributing it evenly across the k machines. This is achieved by re-assigning the computation tasks to random proxy machines, which helps to avoid congestion at any particular machine.
be12c16e-2e8d-57c5-b452-f6eb5e70f4a2|Merging of Components|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by introducing a merging of components technique. This technique is used to merge components across randomly chosen inter-component edges, which helps to reduce the number of communication rounds required.
e2065387-7e86-505e-b777-5053ea9469f6|Distributed Random Ranking (DRR) Technique|The DRR technique is a method for breaking long chains of components into more manageable directed trees of depth O(log n) to reduce communication overhead and enhance memory locality. The DRR technique involves assigning a random rank to each component, which is used to determine the parent-child relationship between components. This approach helps to reduce the height of the component tree, resulting in faster communication and improved memory locality. The paper demonstrates that the DRR technique can reduce the height of the component tree to O(log n) with high probability, leading to improved performance in distributed graph processing.
b3f44b74-db11-575e-90f1-26e575e7138a|Proxy-Based Communication|Proxy-based communication is a method for reducing communication overhead in distributed graph processing by using proxy machines to communicate between components. Proxy-based communication involves assigning a proxy machine to each component, which is responsible for communicating with other components on behalf of the component. This approach helps to reduce the number of messages exchanged between components, leading to improved performance. The paper demonstrates that proxy-based communication can reduce the number of messages exchanged between components, leading to improved performance in distributed graph processing.
b147bde5-0bdd-5e62-b0f3-0441a91d8f88|Random Linear Sketching Without Shared Randomness|Random linear sketching without shared randomness is a technique for constructing linear sketches without requiring shared randomness among machines. This approach involves using log n-wise independent random bits to construct linear sketches, which can be used to sample edges incident to a component. The paper shows that random linear sketching without shared randomness can be used to construct linear sketches that are equivalent to those constructed using shared randomness, leading to improved performance in distributed graph processing.
320cccb5-97bf-5000-80a0-88e6eb378598|Graph Partitioning with Linear Regression-based Cost Model|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a graph partitioning strategy that utilizes a linear regression-based cost model to balance the workload and minimize communication cost. The cost model takes into account the number of vertices, edges, and the degree of vertices in each partition, allowing for more accurate estimation of the computation cost.
fb0b55b4-b45c-50e6-9e08-f79c336443e1|Column-wise Feature Partitioning|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a column-wise feature partitioning strategy, which partitions the feature matrix along the feature dimension. This approach reduces the memory consumption and communication overhead by only transferring the necessary features for each vertex.
14a3a5dd-d8b2-54e7-b914-cc98c70be627|Factored Execution Model|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a factored execution model, which assigns a sampling operator and training operator to individual devices. This approach reduces the memory consumption and communication overhead by avoiding resource contention and improving data locality.
3baaf43f-63f2-517c-b384-2fa8115e779c|Dynamic Cache Mechanism|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a dynamic cache mechanism, which applies the FIFO policy to cache frequently accessed vertices. This approach reduces the memory consumption and communication overhead by minimizing the number of cache misses.
24114401-e462-5930-9e6f-85eab562cd7b|Pre-sampling Based Caching Policy|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a pre-sampling based caching policy, which caches frequently accessed vertices before the actual sampling process. This approach reduces the memory consumption and communication overhead by minimizing the number of cache misses.
a2d756a0-9190-5438-9c06-9c07260ecade|Operator Parallel Execution Model|This solution addresses the challenge of memory-efficient scalable graph processing by proposing an operator parallel execution model, which enables the inter-batch parallelism and generates multiple computation graphs in parallel. This approach reduces the memory consumption and communication overhead by improving the execution efficiency.
b063cd2c-72b6-5ba0-a2a6-90484b7c9d54|Hybrid Dependency Management|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a hybrid dependency management strategy, which combines the benefits of different dependency management approaches. This approach reduces the memory consumption and communication overhead by improving the execution efficiency.
da037be6-e587-5d2d-94b0-72cd9139b639|Hierarchical Dependency Graphs|This solution addresses the challenge of memory-efficient scalable graph processing by proposing hierarchical dependency graphs, which compactly store the neighbors with different definitions and hierarchical aggregation strategies in GNN models. This approach reduces the memory consumption and communication overhead by improving the execution efficiency.
306dc5c5-7bcf-5693-8f3d-986c61c05cc0|Chunk-based Execution Model|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a chunk-based execution model, which processes the graph in chunks to reduce the memory consumption and communication overhead. This approach reduces the memory consumption and communication overhead by improving the execution efficiency.
43312812-5caf-58ec-8a6b-db3e7b66eff5|Pipeline-based Communication Protocol|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a pipeline-based communication protocol, which accumulates the partial aggregations to get the final graph aggregation. This approach reduces the memory consumption and communication overhead by improving the execution efficiency.
16fa1a87-52cf-581d-9094-e51cef223a57|Asynchronous Embedding and Gradient Aggregation|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting an asynchronous execution model that allows the computation to start with historical states and avoids expensive synchronization costs. Specifically, it involves aggregating historical information with fixed stale embeddings and gradients, which effectively reduces communication costs.
608d1dc1-97be-5b5f-b46b-30cd23929bd3|Pipeline-Based Communication Protocol|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a pipeline-based communication protocol that aggregates the neighborhood of a vertex in different orders. Specifically, it involves dividing the computation pipeline into three phases and using a pipeline-based protocol to transfer partial aggregation results.
3d6b7021-cd8e-5af6-981a-0f1970e28353|Point-to-Point Transmission|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a point-to-point transmission method that only transfers data alongside cross edges in an edge-cut graph partition. Specifically, it involves retrieving or sending the hidden embeddings or partial aggregations to a specific remote worker.
f8aa1c5f-2529-5b36-bc03-0e9a6368c686|Decoupling Computation Graph Generation and Execution|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by decoupling the computation graph generation and execution. Specifically, it involves generating the computation graph in advance and executing it in parallel with the computation graph generation of the next batch.
87f5ae15-377b-5974-828e-bd3b92454ac4|Model Parallelism|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting model parallelism, which splits the model into slices and applies message passing times based on MapReduce. Specifically, it involves dividing the model into slices and applying message passing times based on MapReduce.
16677ee2-5424-56ac-96f7-db857aa96d14|Locality-Aware Neighbor Sampling|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a locality-aware neighbor sampling method that reduces the communication cost by sampling neighbors based on their locality.
19902822-676a-5734-8e2a-e82bdd84bb4c|Hybrid Aggregation Strategy|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a hybrid aggregation strategy that distinguishes the aggregation operations in different contexts and designs suitable methods according to their characteristics. Specifically, it involves using a hybrid aggregation strategy to distinguish the aggregation operations in different contexts.
da1f863b-e577-5f00-9e39-9e48a1c1b096|Pull-Push Parallelism|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a pull-push parallelism method that partitions the graph using fast hash partition and the feature matrix using column-wise hash partition. Specifically, it involves partitioning the graph and feature matrix using fast hash partition and column-wise hash partition.
13ba45eb-d354-5d6d-bcdc-ccf28ce93b3c|Two-Level Scheduling Strategy|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a two-level scheduling strategy that considers both the scheduling among DAGs (coarse-grained) and among operators (fine-grained). Specifically, it involves using a two-level scheduling strategy to schedule operators in the computation graph.
4202b94e-b40c-5bbc-be08-c314610ea748|Pre-Sampling-Based Caching Policy|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a pre-sampling-based caching policy that considers the access frequency of vertices and edges in the graph. Specifically, it involves using a pre-sampling-based caching policy to cache frequently accessed vertices and edges.
ea3e3408-51c8-5200-a303-a12d09f51459|Communication-Efficient Sampling|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a communication-efficient sampling method that reduces the communication cost by sampling vertices and edges based on their locality. Specifically, it involves using a communication-efficient sampling method to sample vertices and edges.
cb2c8bdc-bf48-5672-b001-e2167d0bd85f|Model Quantization|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a model quantization method that reduces the communication cost by quantizing the model parameters. Specifically, it involves using a model quantization method to quantize the model parameters.
151cc4bc-cda2-526e-94ae-91f11994ab35|Feature Partitioning|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a feature partitioning method that reduces the communication cost by partitioning the feature matrix into smaller sub-matrices. Specifically, it involves using a feature partitioning method to partition the feature matrix.
e677a9a7-e2cf-5f99-808b-88fdb66ef6fe|Hierarchical Aggregation Strategy|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a hierarchical aggregation strategy that reduces the communication cost by aggregating the features in a hierarchical manner. Specifically, it involves using a hierarchical aggregation strategy to aggregate the features.
1881fe7a-cf84-5e5e-8ffb-d9646f9c4d48|Locality-Aware Data Placement|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a locality-aware data placement method that reduces the communication cost by placing the data in a locality-aware manner. Specifically, it involves using a locality-aware data placement method to place the data.
8a0f2d18-b20d-5bc9-aed9-421d5cbe709e|Cost Model-Based Data Placement|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a cost model-based data placement method that reduces the communication cost by placing the data based on a cost model. Specifically, it involves using a cost model-based data placement method to place the data.
fb302aaa-227e-5abe-a494-9e34f2a8d81a|Decentralized Training|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a decentralized training method that reduces the communication cost by training the model in a decentralized manner. Specifically, it involves using a decentralized training method to train the model.
53125762-f727-5485-9a04-dbdad4ebe590|Serverless Threads|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a serverless threads method that reduces the communication cost by using serverless threads to execute the computation. Specifically, it involves using serverless threads to execute the computation.
a20c9e3c-8300-550f-a374-20df6ea5497d|Message Compression|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a message compression method that reduces the communication cost by compressing the messages. Specifically, it involves using a message compression method to compress the messages.
d17cfafb-3400-5057-a711-ad8a41f77005|Compensation Methods|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a compensation method that reduces the communication cost by compensating for the errors induced by message compression. Specifically, it involves using a compensation method to compensate for the errors.
f9d79c6c-2ff5-51c1-90da-d0ff4030572d|Bit Tuner|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a bit tuner method that reduces the communication cost by tuning the number of bits used to represent the messages. Specifically, it involves using a bit tuner method to tune the number of bits.
20b916a2-dc80-5d25-b6b2-3a3d7dfc7c3b|Graph Neural Network Compression|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a graph neural network compression method that reduces the communication cost by compressing the graph neural network. Specifically, it involves using a graph neural network compression method to compress the graph neural network.
220194b6-a961-5181-ae8f-cb68ee75f886|Product Quantization|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a product quantization method that reduces the communication cost by quantizing the graph neural network using product quantization. Specifically, it involves using a product quantization method to quantize the graph neural network.
6659a9e7-6bff-56a9-a1d1-3cae3cffd675|Graph Sampling|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a graph sampling method that reduces the communication cost by sampling the graph. Specifically, it involves using a graph sampling method to sample the graph.
7f6eca38-01ce-547a-80ac-738746e72ff6|Inductive Learning Method|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting an inductive learning method that reduces the communication cost by learning the graph structure inductively. Specifically, it involves using an inductive learning method to learn the graph structure.
285b2116-2bfd-5e55-8bc4-fbd3d47d6fcd|Heterogeneous Graph Neural Network|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a heterogeneous graph neural network method that reduces the communication cost by using a heterogeneous graph neural network. Specifically, it involves using a heterogeneous graph neural network method to process the graph.
fce80798-3147-5d1d-91d1-f22ad8cf79b0|Graph Attention Network|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a graph attention network method that reduces the communication cost by using a graph attention network. Specifically, it involves using a graph attention network method to process the graph.
748f68b4-a0a9-5f16-aa91-10539161465c|Relational Inductive Biases|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a relational inductive biases method that reduces the communication cost by using relational inductive biases. Specifically, it involves using a relational inductive biases method to process the graph.
9cbb6e45-8fb3-5a6f-a275-97ed10aeaf7a|Deep Learning on Graphs|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a deep learning on graphs method that reduces the communication cost by using deep learning on graphs. Specifically, it involves using a deep learning on graphs method to process the graph.
fd265476-01ea-58f4-ba44-de4ea268ad83|Graph Neural Network Compression Approach|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a graph neural network compression approach method that reduces the communication cost by compressing the graph neural network. Specifically, it involves using a graph neural network compression approach method to compress the graph neural network.
0f580ba4-655f-57e5-af1a-6eab00260f11|Product Quantization-Based Graph Neural Network Compression|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a product quantization-based graph neural network compression method that reduces the communication cost by compressing the graph neural network using product quantization. Specifically, it involves using a product quantization-based graph neural network compression method to compress the graph neural network.
fe7f9cb3-194e-5589-a00b-9f3fc2b506e2|Graph Neural Network Training and Data Tiering|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a graph neural network training and data tiering method that reduces the communication cost by training the graph neural network and tiering the data. Specifically, it involves using a graph neural network training and data tiering method to train the graph neural network and tier the data.
4ecdb1fe-c3b0-52d8-8659-4776219a6f92|Distributed Hybrid CPU and GPU Training|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a distributed hybrid CPU and GPU training method that reduces the communication cost by training the graph neural network using a distributed hybrid CPU and GPU architecture. Specifically, it involves using a distributed hybrid CPU and GPU training method to train the graph neural network.
5a2d6430-e1a8-563e-a304-41d826a1031c|ByteGNN|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a ByteGNN method that reduces the communication cost by using a ByteGNN architecture. Specifically, it involves using a ByteGNN method to process the graph.
e94a82a4-4d7b-5833-87b7-cf85ea1c05d2|Graph Multi-Attention Network|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a graph multi-attention network method that reduces the communication cost by using a graph multi-attention network. Specifically, it involves using a graph multi-attention network method to process the graph.
af792dc0-ee9a-5e2a-8601-ae5b2f2eacc0|Graphiler|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a Graphiler method that reduces the communication cost by using a Graphiler architecture. Specifically, it involves using a Graphiler method to process the graph.
02585de2-6d3f-563a-888b-c7a4924fc2e7|Learning Sparse Nonparametric DAGs|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a learning sparse nonparametric DAGs method that reduces the communication cost by learning sparse nonparametric DAGs. Specifically, it involves using a learning sparse nonparametric DAGs method to learn the graph structure.
eda0627a-f8ce-55aa-aa28-efcde7ff3d73|Reasoning Over Semantic Level Graph|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a reasoning over semantic level graph method that reduces the communication cost by reasoning over the semantic level graph. Specifically, it involves using a reasoning over semantic level graph method to reason over the graph.
f058c5be-7693-5e60-b7fa-7d0cbe7cdeba|Graph Neural Networks in Recommender Systems|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a graph neural networks in recommender systems method that reduces the communication cost by using graph neural networks in recommender systems. Specifically, it involves using a graph neural networks in recommender systems method to process the graph.
432e4279-d88c-5e31-9878-0f862c98ffb1|Graph Convolutional Networks with Markov Random Field Reasoning|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a graph convolutional networks with Markov random field reasoning method that reduces the communication cost by using graph convolutional networks with Markov random field reasoning. Specifically, it involves using a graph convolutional networks with Markov random field reasoning method to process the graph.
39f8cab2-9bb3-59ab-9b5a-5c9984904080|Graph Neural Networks and Their Current Applications in Bioinformatics|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a graph neural networks and their current applications in bioinformatics method that reduces the communication cost by using graph neural networks and their current applications in bioinformatics. Specifically, it involves using a graph neural networks and their current applications in bioinformatics method to process the graph.
7ad31cc3-6986-5045-a9b8-b1ec13249895|Every Document Owns Its Structure|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting an every document owns its structure method that reduces the communication cost by using an every document owns its structure approach. Specifically, it involves using an every document owns its structure method to process the graph.
d7336aee-df74-54d9-94da-47da035e62de|Inductive Graph Neural Networks|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting an inductive graph neural networks method that reduces the communication cost by using inductive graph neural networks. Specifically, it involves using an inductive graph neural networks method to process the graph.
d64b212f-8c34-5444-8d8d-74a4611a463a|Graph Neural Networks for Social Recommendation|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a graph neural networks for social recommendation method that reduces the communication cost by using graph neural networks for social recommendation. Specifically, it involves using a graph neural networks for social recommendation method to process the graph.
c44010de-2fb3-57e1-b97b-acfd68a5cb35|Open Graph Benchmark|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting an open graph benchmark method that reduces the communication cost by using an open graph benchmark. Specifically, it involves using an open graph benchmark method to process the graph.
8c0a49b7-30b0-5e0a-b9b5-71ff4bd28773|Graph Neural News Recommendation|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a graph neural news recommendation method that reduces the communication cost by using graph neural news recommendation. Specifically, it involves using a graph neural news recommendation method to process the graph.
2f1ed7fe-4edc-5d0e-b0a7-8d398f46d020|Heterogeneous Graph Attention Network|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a heterogeneous graph attention network method that reduces the communication cost by using heterogeneous graph attention networks. Specifically, it involves using a heterogeneous graph attention network method to process the graph.
590d2ad2-655e-51a0-a17b-75037f9fd993|Structure-Aware Convolutional Neural Networks|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a structure-aware convolutional neural networks method that reduces the communication cost by using structure-aware convolutional neural networks. Specifically, it involves using a structure-aware convolutional neural networks method to process the graph.
987c256f-9163-528d-b55b-0c7c4483861a|Graph Neural Networks for Traffic Prediction|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a graph neural networks for traffic prediction method that reduces the communication cost by using graph neural networks for traffic prediction. Specifically, it involves using a graph neural networks for traffic prediction method to process the graph.
85058ac9-77de-5990-bae3-1d555416efc9|MixGCF|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a MixGCF method that reduces the communication cost by using MixGCF. Specifically, it involves using a MixGCF method to process the graph.
166a5f43-ef68-548e-a250-f159ad461ed6|Adaptive Sampling|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting an adaptive sampling method that reduces the communication cost by adaptively sampling the graph. Specifically, it involves using an adaptive sampling method to sample the graph.
a9fb4f7a-d58e-5cae-b622-7ba7c384b4d7|Graph Convolutional Network|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a graph convolutional network method that reduces the communication cost by using graph convolutional networks. Specifically, it involves using a graph convolutional network method to process the graph.
720b590f-82af-5a75-8f0a-ee8dc67fa039|ROC|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a ROC method that reduces the communication cost by using ROC. Specifically, it involves using a ROC method to process the graph.
2e30bb03-b3ac-5ae7-b1fd-d0705655ae43|FlexFlow|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by adopting a FlexFlow method that reduces the communication cost by using FlexFlow. Specifically, it involves using a FlexFlow method to process the graph.
73e4a910-1d7e-5b08-a964-2d31fb4f40ee|Hierarchical Partitioning|Hierarchical partitioning is a technique used to partition heterogeneous graphs into smaller subgraphs while minimizing the communication cost and load imbalance. This approach is specifically designed to handle the heterogeneity in vertex and edge attributes, irregular structures, and variations in density, connectivity, and weights. Hierarchical partitioning involves a two-level partitioning strategy, where the graph is first partitioned into smaller subgraphs based on vertex types, and then each subgraph is further partitioned into smaller chunks based on edge connectivity. This approach helps to reduce the communication cost by minimizing the number of edges that need to be transferred between partitions. The paper reports that hierarchical partitioning achieves a 2-3x speedup in training time compared to traditional partitioning methods.
4341da80-0ef3-5895-a738-7a588a82cbfa|Chunk-Based Execution Model|The chunk-based execution model is designed to reduce memory consumption and network communication by processing chunks of vertices instead of the entire graph. The chunk-based execution model involves processing chunks of vertices in parallel and using a selective scheduling strategy to filter useful vertices only from a large chunk. The paper reports that the chunk-based execution model achieves a 1.5-2x speedup in training time compared to traditional execution models.
ed4d55b2-8428-5498-9063-6edbf7a7e843|Column-Wise Feature Partitioning|Column-wise feature partitioning is a technique used to partition the feature matrix into smaller sub-columns based on feature dimension. Column-wise feature partitioning involves partitioning the feature matrix into smaller sub-columns and distributing them across different workers. This approach helps to reduce communication overhead and improve memory locality. The paper reports that column-wise feature partitioning achieves a 1.5-2x speedup in training time compared to traditional partitioning methods.
5324f3bc-29e5-57d8-8084-281329b31c7b|Model-Aware Cost Model|The authors propose a model-aware cost model that estimates the computation cost of each partition based on the GNN model's characteristics. This solution specifically addresses the challenge of optimizing load balance in distributed systems by providing a more accurate estimation of the computation cost, which enables better workload balancing.
88270d6f-6b4e-5f40-9594-03f39e5e0f5c|Range Graph Partitioning|The authors propose a range graph partitioning method that partitions the graph into cohesive mini-batches, achieving a 2.2 approximation of the optimal computation cost. This solution specifically addresses the challenge of optimizing load balance in distributed systems by reducing the communication overhead and improving the computation efficiency.
4d1fd3f2-1c3a-5ad3-b3d7-d80bdefb20fe|Asynchronous Execution Model|The authors propose an asynchronous execution model that allows different workers to start the computation of the next layer as soon as all its neighbors' embeddings are received. This solution specifically addresses the challenge of optimizing load balance in distributed systems by reducing the communication overhead and improving the computation efficiency.
8b43f8f9-c7ff-52c5-9db5-a4659e7bb80f|Staleness Embedding|The authors propose a staleness embedding approach that uses stale embeddings to form a pipeline and reduce the communication overhead. This solution specifically addresses the challenge of optimizing load balance in distributed systems by reducing the communication overhead and improving the computation efficiency.
153fedad-567b-5617-b022-fe7a6a619acc|Pipeline Parallel Execution Model|The authors propose a pipeline parallel execution model that divides the computation pipeline into three phases: data partition, GNN model optimization, and gradient aggregation. This solution specifically addresses the challenge of optimizing load balance in distributed systems by reducing the communication overhead and improving the computation efficiency.
3b8c9dfe-22ce-52fb-8cb1-e8a1676571b7|Column-Wise Feature Partition|The authors propose a column-wise feature partition approach that partitions the feature matrix into sub-columns of all vertices' features. This solution specifically addresses the challenge of optimizing load balance in distributed systems by reducing the communication overhead and improving the computation efficiency.
102b8bd8-428e-543d-9be4-b0b80d43a8b6|Graph Slicing and Edge Table Coarsening|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a graph slicing technique that divides the input graph into smaller partitions based on vertex IDs. This allows for the processing of large graphs that do not fit in the on-chip scratchpad memory. The solution also involves coarsening the edge table by a factor of N, where N is the scaling factor, to reduce the memory requirements.
24568a5e-a65f-55e2-be66-5a47fdb80b00|Symmetric Graph Optimization|This solution addresses the challenge of memory-efficient scalable graph processing by proposing an optimization technique for symmetric graphs. The technique involves pre-processing the graph to generate a directed graph and then processing the graph using a single pass, reducing the number of edges that need to be processed.
b51dae47-eb24-520c-a579-e6a58e2712c3|Parallel Edge and Destination Vertex Property Access|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a parallelization scheme for edge and destination vertex property access. The scheme involves replicating the Read DST Property module and allocating each destination vertex ID to the lowest occupied module to fetch the destination VProperty.
b00bb1f1-a3ee-5d16-bda7-037e5a6b52ca|On-Chip Scratchpad Memory Usage|This solution addresses the challenge of memory-efficient scalable graph processing by proposing the use of on-chip scratchpad memory to store temporary destination vertex property data. This approach reduces the number of random memory accesses and improves memory efficiency.
d40d9748-9b3a-56ce-bd1a-f48ea525c04c|Data Prefetching|This solution involves prefetching data to reduce the latency of memory accesses. The authors propose a data prefetching technique that prefetches data before it is actually needed, reducing the latency of memory accesses. The paper shows that data prefetching can result in significant performance improvements, particularly for algorithms that have high memory access latency.
1f113868-3373-54a4-a2bc-ff162464ad3d|Graphicionado|Graphicionado is a domain-specific accelerator designed to efficiently process graph analytics workloads. It features a pipeline that is inspired by the vertex programming paradigm, coupled with a few reconfigurable blocks.
e646bdde-8ef6-5833-bb4a-029ac1cc21c8|Edge Access Pattern Optimization|This solution optimizes the edge access pattern in graph algorithms to reduce off-chip memory accesses.
87ed69da-5d99-507c-98f1-41be5e71e72d|Graph Slicing|This solution slices large graphs into smaller sub-graphs to reduce the memory requirements and improve performance.
7de4c187-6a62-5c09-a8ec-3e39c8c60d61|Edge Table Coarsening|This solution coarsens the edge table to reduce the memory requirements and improve performance.
af252d3e-e9b8-5e01-b62d-2f54c238b15b|Prefetching|This solution prefetches data to reduce the memory access latency and improve performance.
c0cdbb99-1eea-5640-8723-5a120b71e700|Stream Parallelization|This solution parallelizes the processing of multiple streams to improve performance.
617145df-78fe-5903-b77a-29b58e26be7d|Parallelizing the Graph Processing Pipeline|This solution involves parallelizing the graph processing pipeline to improve load balance and reduce the memory requirements in distributed systems.
d2bee2fd-c043-5dd1-92a4-517e75d3efe9|Destination Vertex Property Parallelization|This solution involves parallelizing the destination vertex property accesses to improve load balance and reduce the memory requirements in distributed systems.
0600735a-0d5b-5a51-bfec-4785ec2d1812|Value-Driven Differential Scheduling|This solution addresses the challenge of memory-efficient scalable graph processing by differentially scheduling subgraphs based on their values. The authors propose a value-driven approach that prioritizes the processing of high-value subgraphs, which contain a significant amount of useful data, over low-value subgraphs, which contain a lot of never-used data.
3ec333f9-6fff-51b5-81de-cbf57e48af6d|Adaptive Subgraph Partitioning|This solution addresses the challenge of memory-efficient scalable graph processing by adaptively partitioning subgraphs based on their sizes and values. The authors propose a dynamic partitioning algorithm that adjusts the size of subgraphs based on their values and the available memory resources.
daf416e4-db35-5bc0-891f-4deb51e564fd|Memory-Aware Data Structures|This solution addresses the challenge of memory-efficient scalable graph processing by employing memory-aware data structures that minimize memory usage and maximize scalability. The authors propose a novel data structure that compresses graph data and reduces memory usage.
acfefa97-68e6-51d7-a157-f72fed3d088f|Communication-Efficient Data Transfer|This solution addresses the challenge of memory-efficient scalable graph processing by minimizing communication overhead between processors. The authors propose a novel data transfer algorithm that reduces communication overhead and improves scalability.
bd153fbd-b453-56f6-9627-288ae435b68f|Value-Driven Differential Scheduling (VDDS)|VDDS is a scheduling technique that classifies subgraphs into high-value and low-value subgraphs based on their potential usefulness in current and future iterations. This classification enables the system to prioritize the processing of high-value subgraphs, reducing the amount of redundant data transferred and improving the utilization of the host-GPU bandwidth.
1f615f73-d720-5149-86eb-9ad41af1f137|High-Value Subgraph Processing (HVSP)|HVSP is a processing technique that focuses on fully exploiting the potentially useful data (PUD) in high-value subgraphs. This technique involves running each loaded subgraph multiple times to extract its PUD, which is then used to update the vertices in the subgraph.
16093e13-9afb-5e83-87c5-b6c45f015d35|Low-Value Subgraph Processing (LVSP)|LVSP is a processing technique that focuses on efficiently extracting the updated values (UD) from low-value subgraphs. This technique involves using multiple CPU cores to parallelize the UD extraction and to reduce the amount of redundant data transferred.
dec8b84a-ffbe-5213-a45e-93e4165b4b8c|NUMA-Aware Parallel UD Extraction|This solution involves extracting the useful data (UD) from low-value subgraphs in parallel, using multiple CPU cores at the host. The extraction is done in a NUMA-aware manner, where each subgraph is bound to the NUMA node storing it, to improve inter-node load balancing.
26398487-e915-5e10-8f38-63465e4ec58a|Delayed Scheduling Mechanism|This solution involves delaying the scheduling of low-value subgraphs until their useful data (UD) is extracted and transferred to the GPU. This mechanism enables the system to exploit the external value across subgraphs, improving the overall performance.
66e56d92-0254-5c0e-99eb-567b531cd397|Heuristic Subgraph Identification|This solution involves using heuristics to identify high-value subgraphs, which are then processed using the high-value subgraph processing engine.
18256477-39f5-5452-83f4-4447b3a2988e|Bulk Synchronous Parallel BSP|BSP is a model that ensures consistency by synchronizing between each computation and communication phase. This approach is used in MapReduce, where programs are guaranteed to output a correct result if they are serializable.
c957624c-5203-5822-ad05-89f7294e6703|Stale Synchronous Parallel SSP|SSP relaxes the synchronization overhead by allowing faster workers to move ahead for a certain number of iterations. If this number is exceeded, then all workers are paused.
8e7e9689-e7f7-5758-be57-58531aca16b8|Approximate Synchronous Parallel ASP|ASP limits how inaccurate a parameter can be. This contrasts with SSP, which limits how stale a parameter can be.
6a2a3ec8-f3b0-5468-8c12-1bd4ac8fd1b5|Barrierless Asynchronous Parallel BAP|BAP lets worker machines communicate in parallel without waiting for each other.
5d77e951-05eb-51a4-ae81-e3bf02c83212|Total Asynchronous Parallel TAP|TAP is similar to BAP but allows for more flexibility in the communication pattern.
bacc464e-18c6-5e69-aa5f-ed6f35b8f065|Sufficient Factor Broadcasting SFB|SFB is a communication method that reduces the amount of data exchanged between machines.
2b301eb1-bdb9-54e1-8a9e-d8e79ca8ae4c|Parameter Server PS|PS is a decentralized set of servers that store and update the model parameters.
96da5358-b201-5f54-b31d-c30598bc44b8|bComm|bComm is a communication method that combines Parameter Servers PS with Sufficient Factor Broadcasting SFB.
19b456bb-7192-5847-8f37-4565ab6d8395|Ring AllReduce|Ring AllReduce is a communication pattern that simplifies the structure of the distributed system by only requiring neighbor nodes to synchronize through messages. This approach addresses the challenge of load balance optimization by reducing the communication overhead and improving load balance.
3dd41c0d-96a1-5264-8038-0b73f1b2b5a7|Distributed Graph Decomposition|The authors propose a distributed graph decomposition algorithm that partitions the graph into smaller subgraphs, allowing for more efficient processing and reducing memory consumption.
bfae376d-4794-5d40-978c-bd039a970582|Memory-Aware Data Partitioning|The authors develop a memory-aware data partitioning strategy that optimizes the distribution of graph data across processors, minimizing memory consumption and reducing communication overhead.
24368428-cdcf-5b6a-a9e7-a3f0fa23803c|Advanced Data Structures for Graph Processing|The authors propose the use of advanced data structures, such as compressed sparse rows and adjacency lists, to optimize memory usage and improve scalability in graph processing.
1cdee558-f466-540a-a472-52277430f9ed|Distributed Spanner Approximation Algorithm|The authors propose a distributed algorithm for approximating minimum k-spanners in the Local model, which guarantees an approximation ratio of O(log m/n) and takes O(log n log ) rounds w.h.p. The algorithm uses a greedy approach, where dense stars are added to the spanner one by one. The key challenge is to break symmetry among the candidates while balancing the need to choose many stars in parallel for a fast running time with the need to bound the overlap in spanned edges among the candidates for a small approximation ratio. The algorithm achieves an approximation ratio of O(log m/n), which matches the best known ratio for polynomial sequential algorithms. The time complexity of the algorithm is O(log n log ) rounds w.h.p.
354c2134-3f9d-5dfd-9f60-fc678a2667b3|Distributed 2-Spanner Approximation Algorithm|The authors propose a distributed algorithm for approximating the minimum 2-spanner problem in the Local model, which guarantees an approximation ratio of O(log m/n) and takes O(log n log ) rounds w.h.p.
c7e138d1-65be-539a-a3e9-a846f0599956|Weighted 2-Spanner Approximation Algorithm|The authors propose a distributed algorithm for approximating the weighted 2-spanner problem in the Local model, which guarantees an approximation ratio of O(log W) and takes O(log n log W) rounds w.h.p.
e6a59d23-ad8e-5dd5-8831-471461044981|Client-Server 2-Spanner Approximation Algorithm|The authors propose a distributed algorithm for approximating the client-server 2-spanner problem in the Local model, which guarantees an approximation ratio of O(log m/n) and takes O(log n log ) rounds w.h.p.
5f88c6c8-5acd-5375-9351-6c017c3ab7a0|Distributed 1-Approximation Algorithm for Minimum k-Spanners|The authors propose a distributed algorithm for approximating the minimum k-spanner problem in the Local model, which guarantees a 1-approximation ratio and takes O(log n) rounds w.h.p.
c19d7407-c829-5ad1-af8c-a9234e36ffe6|Lower Bound for Distributed 2-Spanner Approximation|The authors show that any distributed algorithm for approximating the 2-spanner problem in the Congest model requires at least log log log n rounds to achieve a constant or polylogarithmic approximation ratio.
2356a36b-9f62-59c9-827b-04aafb6f0663|Distributed Vertex Hashing (DVHT)|DVHT is a distributed algorithm that addresses the challenge of memory-efficient scalable graph processing by utilizing a hash function to map edges from the graph stream to different workers. This approach enables the efficient distribution of the graph stream, reducing memory consumption and computational costs.
44d1152c-a493-5714-9959-f0f168b5ba32|Distributed Edge Hashing with TRI EST IMPR (DEHT)|DEHT is a distributed algorithm that addresses the challenge of memory-efficient scalable graph processing by utilizing a hash function to map edges from the graph stream to workers with a fixed probability. This approach enables the efficient distribution of the graph stream, reducing memory consumption and computational costs.
3322a035-aec0-5739-8b82-753262369af4|Distributed Vertex Hashing (DVHT) Algorithm|The DVHT algorithm is a distributed improvement of the streaming algorithm TRI EST BASE, which uses a hash function to assign edges from the graph stream to different workers by comparing the mapping values of the two vertices of each edge. This approach optimizes communication efficiency by reducing the number of edges sent to each worker, thereby minimizing the communication overhead.
9403df8d-ad72-5933-b219-9a547da29b0c|Distributed Edge Hashing (DEHT) Algorithm|The DEHT algorithm is a distributed improvement of the streaming algorithm TRI EST IMPR, which uses a hash function to map edges to workers with a fixed probability. This approach optimizes communication efficiency by reducing the number of edges sent to each worker and minimizing the communication overhead.
c2f2c21e-5dde-598a-8437-f2416c558822|Grouped and Hierarchical Aggregators|The grouped and hierarchical aggregators approach is used in the DEHT algorithm to optimize communication efficiency. This approach involves dividing workers into different communication groups, each with an aggregator, and using a main aggregator to aggregate the values of all secondary aggregators.
fe158fd6-30c8-5487-8ed7-d8387f3c80dd|Distributed Vertex Hashing TRI EST BASE (DVHT b)|DVHT b is a distributed improvement of the streaming algorithm TRI EST BASE, which uses a Master Worker Aggregator architecture to assign edges from the graph stream to different workers by comparing the mapping values of the two vertices of each edge. DVHT b uses a hash function to map edges to workers, ensuring that each worker stores a different subgraph of the graph stream and counts the number of global triangles in the subgraph stream independently. This approach reduces the variance of estimates by dividing the graph into multiple subgraphs stored in different workers and reducing the number of shared edge triangles in each worker. The paper presents experimental results showing that DVHT b reduces the estimation error and is several times more accurate than state-of-the-art streaming algorithms.
7591cec7-22d2-52f4-b92c-6a8836eb9e5f|Distributed Edge Hashing TRI EST IMPR (DEHT i)|DEHT i is a distributed improvement of the streaming algorithm TRI EST IMPR, which uses a Master Worker Aggregator architecture with grouped and hierarchical aggregators to assign edges from the graph stream to different workers with a fixed probability. DEHT i uses a hash function to map edges to workers directly with a fixed probability, and workers execute the TRI EST IMPR algorithm, which is more accurate than TRI EST BASE. This approach enables the algorithm to handle heterogeneous graph structures with varying degrees, weights, and sparsity. The paper presents experimental results showing that DEHT i reduces the estimation error and is several times more accurate than state-of-the-art streaming algorithms.
98b13f43-39e3-5a4c-8aee-ec9b5c18ec92|Partition Transparency|The authors propose a solution called partition transparency, which enables graph algorithms to work correctly regardless of the underlying graph partitioning strategy. This approach allows for the development of algorithms that can adapt to different partitioning schemes, including edge cut, vertex cut, and hybrid partitions.
b401dad0-5c0f-5b67-8ec7-23513944c678|Hybrid Partitioning|The authors propose a hybrid partitioning strategy that combines the benefits of edge cut and vertex cut partitioning. This approach aims to balance the trade-offs between locality and load balancing, reducing memory consumption and communication overhead.
4360c840-fe9b-5042-bd53-15ccdf93d66c|Application-Driven Partitioning|The authors propose an application-driven partitioning strategy that takes into account the specific requirements of the graph algorithm and the characteristics of the graph. This approach aims to optimize memory usage and reduce communication overhead by partitioning the graph in a way that minimizes the number of edges cut.
ffd76afa-1518-5649-bce3-a8e95d40cb48|PIE Model|The authors propose a graph-centric programming model called PIE, which provides a framework for developing parallel graph algorithms. This approach aims to simplify the development of parallel graph algorithms and improve their performance.
44afb84d-6a15-5e40-850c-849abdb5fff8|GAS Model|The authors propose a vertex-centric programming model called GAS, which provides a framework for developing parallel graph algorithms. This approach aims to simplify the development of parallel graph algorithms and improve their performance.
aaf13db8-0f96-5dcc-886c-469351f9a00b|Partition Transparency Conditions|The authors propose conditions for partition transparency, which ensure that graph algorithms work correctly regardless of the underlying partitioning strategy. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by providing a framework for developing algorithms that can adapt to different partitioning strategies without compromising their correctness or efficiency.
95efe2a0-bc5c-5644-97e8-4e73eb01378c|Hybrid Partitioning Strategy|The authors propose a hybrid partitioning strategy that combines the benefits of edge cut and vertex cut partitioning. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by providing a partitioning strategy that can adapt to different graph structures and algorithms.
f6f0bbf1-2cb2-5bdc-86d3-8c3680a6b0eb|Partition-Aware Algorithm Design|The authors propose a partition-aware algorithm design approach that takes into account the underlying partitioning strategy. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by providing a framework for designing algorithms that can adapt to different partitioning strategies.
db8a366a-6fa5-53c1-8ac2-276d87a72be4|Hybrid Partitioning Strategies|The authors discuss the use of hybrid partitioning strategies, which combine edge cut and vertex cut, to improve the performance of graph algorithms on heterogeneous graphs. These strategies can help reduce communication overhead and enhance memory locality.
be40f207-0182-54af-a708-dcd7dd9f34af|PIE and GAS Programming Models|The authors propose the use of graph-centric and vertex-centric programming models, specifically PIE and GAS, to develop adaptive algorithms for heterogeneous graphs. These models allow for the development of algorithms that can handle irregular graph structures and varying degrees, weights, and sparsity.
b778b96d-a93e-555c-affa-95744945dac7|Hybrid Partitioning with Application-Driven Cost Model|The authors propose a hybrid partitioning strategy that combines the benefits of edge-cut and vertex-cut partitioning methods. This approach is driven by an application-driven cost model that learns the cost of different partitioning strategies for a given graph algorithm. The cost model is used to guide the partitioning process, resulting in a hybrid partition that balances workload and minimizes communication overhead.
af432dd0-1c4b-59bd-8636-458eca8db3f1|Partition-Transparent Algorithms|The authors propose a new approach to designing graph algorithms that are partition-transparent, meaning they can work correctly and efficiently on different partitioning strategies without requiring any changes. This approach involves developing algorithms that can adapt to different partitioning strategies and can take advantage of the benefits of hybrid partitioning.
b6e51d35-bf6c-56ed-9d75-91b4e67940ef|Adaptive Asynchronous Parallelization|The authors propose an adaptive asynchronous parallelization strategy that can adapt to different partitioning strategies and can take advantage of the benefits of hybrid partitioning. This approach involves developing algorithms that can adapt to different partitioning strategies and can take advantage of the benefits of hybrid partitioning.
4d672f69-9208-5bae-b287-8c890efe26f9|Application-Driven Hybrid Partitioning|The authors propose an application-driven hybrid partitioning strategy that combines the benefits of edge-cut and vertex-cut partitioning methods. This approach is driven by an application-driven cost model that learns the cost of different partitioning strategies for a given graph algorithm. The cost model is used to guide the partitioning process, resulting in a hybrid partition that balances workload and minimizes communication overhead.
66dc9000-2b48-5d40-b014-78352f4735b4|Hybrid Partitioning for Efficient Graph Dynamics Processing|The authors propose a hybrid partitioning strategy that combines the benefits of edge cut and vertex cut partitioning to efficiently process dynamic updates in large graphs. This approach aims to minimize computational costs and iterations by adaptively adjusting the partitioning strategy based on the graph topology and update patterns.
b8f08fdb-2c29-5fba-9184-7dba5df1b529|Incremental Algorithm for Maximal k-Truss Maintenance|The authors propose an incremental algorithm for maintaining maximal k-trusses under dynamic updates, which minimizes the number of iterations required to update the trussness values.
947f71b0-6ba5-50de-aa2e-bfce14cd89c5|Batch Processing of Vertex and Edge Updates|The authors propose a batch processing approach for vertex and edge updates, which preserves structural integrity while minimizing computational costs.
71ebd5c2-0b82-592a-9881-9ee91d9df4c3|Adaptive Algorithm for Betweenness Centrality Maintenance|The authors propose an adaptive algorithm for maintaining betweenness centrality under dynamic updates, which minimizes the number of iterations required to update the centrality values.
f68f65ab-e70d-5d2c-bd37-d6cc9ddcef6f|Fine-grained Framework for Distributed Clustering|The authors propose a fine-grained framework for distributed clustering that divides the local vertices in each machine into batches and processes each of them separately. This approach reduces the size of stored remote adjacency lists at each time, which is based on the lemma that the clustering results for local vertices can be obtained by first dividing them into disjoint batches and then clustering separately for each batch. The framework adopts a share-nothing architecture and uses a fixed-size cache to store the fetched adjacency lists of remote vertices. The authors also design an effective work-stealing mechanism to handle unbalanced workloads. The experiment results demonstrate that the proposed algorithm is efficient and scalable for SCAN in billion-scale graphs, outperforming existing distributed algorithms.
21f34154-94a5-576d-92c9-ff5295c26d7e|Ne-grained Clustering with Fixed-size Cache|The authors propose a ne-grained clustering approach that processes clustering in a finer granularity to reduce the size of stored remote adjacency lists at each time. The approach uses a fixed-size cache to store the fetched adjacency lists of remote vertices. The approach uses a shared cache that stores the fetched adjacency lists of remote vertices, and the cache has a fixed size and is designed to be lock-free. The authors also use a local similarity table to record the pairwise similarity between the local vertices and their neighbors. The experiment results demonstrate that the proposed algorithm is efficient and scalable for SCAN in billion-scale graphs, outperforming existing distributed algorithms.
05bedcd2-574f-544a-b32c-7b86e57580af|Dynamic Work Stealing Mechanism|The authors propose a dynamic work-stealing mechanism to handle unbalanced workloads. The mechanism is triggered by idle machines to steal work from other machines. The mechanism uses a WorkSteal RPC function to dynamically steal work from other machines. The authors also use a StealWork function to trigger dynamic work stealing once a machine finishes processing all its batches. The experiment results demonstrate that the proposed algorithm is efficient and scalable for SCAN in billion-scale graphs, outperforming existing distributed algorithms.
9a6d99d2-18e2-55d6-b7c0-be6e6095f4cb|Fine-Grained Clustering Framework|The authors propose a fine-grained clustering framework for distributed SCAN algorithms, which divides the local vertices in each machine into batches and processes each batch separately. This approach reduces the communication overhead by minimizing the number of remote adjacency lists that need to be fetched and stored in each machine.
b5c69898-02ac-58a6-b96f-9f140454a1a1|CoreCluster Algorithm|The authors propose the CoreCluster algorithm, which clusters a list of local vertices and their neighbors. The algorithm fetches the cluster IDs of the neighbors of every vertex in the batch, and then merges the clusters of similar vertices.
f249fc7f-e01c-5d9c-a462-e704a1fda3ea|Dynamic Work-Stealing Mechanism|The authors propose a dynamic work-stealing mechanism to handle unbalanced workloads in the distributed system. The mechanism allows idle machines to steal unprocessed batches from busy machines, reducing the overall processing time.
64c6a20d-d7ab-565c-9e74-0c7122bf8378|Pruning Technique for Core Checking|The authors propose a pruning technique for core checking that reduces the number of similarity computations required during the clustering process. This technique uses similar and effective degrees to early terminate the core checking process.
1915b695-9d03-54f9-8ec5-66872df67cef|Fixed-Size Cache for Remote Adjacency Lists|The authors propose a fixed-size cache to store remote adjacency lists, which helps to manage memory consumption and improve scalability.
cac79eb7-55cd-56d1-b373-2895ab4813d9|Graph Coloring and Parallel Execution|The authors propose using graph coloring to schedule conditionally independent updates in parallel, allowing for memory-efficient and scalable graph processing.
b05a5b3b-5e01-50ba-a9e1-795d194bfdcc|Incremental Junction Tree Construction|The authors propose an incremental junction tree construction algorithm to efficiently build and update junction trees for graph processing.
3db1aa1e-66fe-577f-a17e-1a2f2d987374|Parallel Splash Generation|The authors propose a parallel splash generation algorithm to construct multiple conditionally independent bounded treewidth blocks (Splashes) in parallel.
49e27f61-56e1-513d-87ff-c0af42ca4f5d|Adaptive Splash Generation|The authors propose an adaptive splash generation algorithm to accelerate burn-in by explicitly grouping strongly dependent variables.
ff378e4d-1b6c-5552-abed-b87210c99f1f|Chromatic Sampler|The Chromatic Sampler is a parallel Gibbs sampler that uses graph coloring to schedule conditionally independent updates in parallel. It addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by providing a direct parallelization of the classic sequential scan Gibbs sampler.
90620b4d-be5c-50eb-aa79-9c0bc229b88a|Splash Sampler|The Splash Sampler is a parallel Gibbs sampler that incrementally constructs multiple bounded tree-width blocks (Splashes) and jointly samples each Splash using parallel junction tree inference and backward sampling. It addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by providing a novel locking protocol and an iterative junction tree generation algorithm.
bc511d50-a0b6-510d-8dc8-d1f732cafb5e|Parallel Junction Tree Extension Algorithm|The Parallel Junction Tree Extension Algorithm is a novel algorithm that efficiently updates the junction tree as new variables are added. It addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by providing a method to extend the junction tree over a subset of variables to a junction tree over a larger subset of variables.
05fa2d5d-c1a4-5e72-8318-f8b1d531a37e|Markov Blanket Locking Protocol|The Markov Blanket Locking Protocol is a novel locking protocol that ensures that simultaneously constructed Splashes are conditionally independent given all remaining unassigned variables. It addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by providing a method to handle the issues of random and irregular memory access patterns.
65e6cde2-11da-570e-bc16-f0a905e9975c|Incremental Junction Tree Extension Algorithm|The incremental junction tree extension algorithm is a technique used by the Splash sampler to efficiently update the junction tree as new variables are added, addressing the challenge of efficient graph dynamics processing by minimizing computational costs and iterations.
d89559ea-5010-593d-8ce2-13d8a344b5c2|Weighted Memory-based Algorithm (WMA)|The WMA is a distributed algorithm that addresses the challenge of memory-efficient scalable graph processing by introducing a weighted memory-based heuristic into the framework of learning in games. This approach enables the algorithm to converge to high-efficiency Nash equilibria with high probabilities, which correspond to near-optimal vertex cover solutions.
841c4a5d-c126-510d-8118-4f62c37498d6|Restricted Best Response (RBR)|The RBR is a technique used in the WMA to update the memory and action of each player. It involves finding the restricted best response rbt_i by following a relaxed greedy rule, which takes into account the regret value and the best response of each player.
b7757d41-451d-5f30-8e48-f07361469ee4|Potential Game Model|The potential game model is a framework used in the WMA to model the interactions between players in the graph. It enables the algorithm to converge to high-efficiency Nash equilibria with high probabilities, which correspond to near-optimal vertex cover solutions.
ca3e8535-87cf-5155-b356-06eb558641f7|Restricted Greedy Algorithm (RGA)|The RGA is a distributed algorithm designed to optimize communication efficiency in solving the MWVC problem. It employs a restricted greedy rule, which updates the action of each node based on the weights and degrees of its neighbors.
2087f9ea-14d3-57d4-875b-860dda5fde65|Feedback-based Rule (FBR)|The FBR is a distributed algorithm designed to optimize communication efficiency in solving the MWVC problem. It employs a feedback-based rule, which updates the action of each node based on the weights and degrees of its neighbors.
d087571a-aa3d-54eb-ae48-99c926aed072|CSR Format with Regular Memory Access|The authors propose using the Compressed Sparse Row (CSR) format to represent the graph structure, which enables regular memory access and maximizes parallelism. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing memory consumption and optimizing memory usage.
854242cd-0ff2-5211-a70f-c8ee77ea8c59|Hybrid HYB Format for SpMV|The authors propose a hybrid HYB format for Sparse Matrix-Vector Multiplication (SpMV), which combines the ELL format for non-zero entries and the COO format for the remaining entries. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing memory consumption and optimizing memory usage.
b6c8bf4b-4fea-5903-9abb-7c9f7be90972|Nested Loop Theta Join Operation|The authors propose a nested loop theta join operation to achieve coalesced memory access and reduce communication overhead. This solution specifically addresses the challenge of memory-efficient scalable graph processing by minimizing communication between processors and optimizing memory usage.
2bf57e34-6381-5531-8057-f577f5f8e511|Slotted Page Format for Graph Storage|The authors propose using the slotted page format to store the graph in disk and memory, which allows for efficient storage and reduces memory consumption. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing memory consumption and optimizing memory usage.
2f4b282d-e8e0-5a59-8f57-99b72657dc16|Unified Memory and NVLink for GPU Processing|The authors propose using unified memory and NVLink to enable direct communication between the GPU and CPU, which reduces memory consumption and optimizes memory usage. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing memory consumption and optimizing memory usage.
bb0b5021-c718-57bd-a11a-0a556c13d2d9|Hybrid System|The authors propose a hybrid system that combines the benefits of both CPU and GPU to optimize communication efficiency in distributed algorithms. This solution specifically addresses the challenge by leveraging the strengths of each processing unit to minimize communication rounds and improve overall efficiency.
94281eaa-8690-517e-ad5e-f47a346f2b05|Hybrid Coloring Model|The hybrid coloring model is a graph partitioning technique that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution specifically targets the issue of load imbalance and poor data locality in distributed and GPU-based systems.
01a2aa36-243a-5326-a7f5-ff287cbbf300|Dynamic Scheduling Strategy|The dynamic scheduling strategy is a workload mapping technique that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution specifically targets the issue of load imbalance and poor data locality in distributed and GPU-based systems.
a794a55d-309c-579e-be9f-62968276ba70|Virtual Warp|The virtual warp is a workload mapping technique that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution specifically targets the issue of load imbalance and poor data locality in distributed and GPU-based systems.
82a1d96b-dda6-580f-a892-f82623716669|Two-Phase Decomposition Scheduling Strategy|The two-phase decomposition scheduling strategy is proposed to address the challenge of optimizing load balance in distributed systems. This strategy attempts to achieve optimal workload mapping performance for threads within and across CTAs. The strategy decomposes the scattering process into two phases: the scheduling phase and the computation phase. In the scheduling phase, the target of assigning the adjacent edges is to ensure the number of edges is the same as the CTA size. In the computation phase, the same number of adjacent vertices are visited by each thread.
de502a22-a4d7-5c98-abaa-ff31421a35e9|Per-Thread Fine-Grained and Per-Warp Per-CTA Coarse-Grained Workload Mapping Strategies|The per-thread fine-grained and per-warp per-CTA coarse-grained workload mapping strategies are proposed to address the challenge of optimizing load balance in distributed systems. The per-thread fine-grained strategy maps one thread to the neighbor list of a frontier vertex. In this method, each thread loads the offset in the adjacency list of the assigned node. Next, all the edges in the adjacency list are processed sequentially by the thread. The per-warp per-CTA coarse-grained strategy divides the adjacency list into three categories according to the size of the adjacency list and then maps each category to a strategy that targets specifically at the corresponding size.
c9a6101c-939a-5f3f-819a-9e0e1f51e2ee|Coalesced Memory Access through CSR Representation|The authors propose using a Compressed Sparse Row (CSR) representation to enable coalesced memory access for graph processing on GPUs. This solution specifically addresses the challenge of optimizing GPU memory access by ensuring that threads in a warp access contiguous memory locations, reducing memory access overhead.
3702aa9b-b9fe-53f8-9110-fe146f3ecb37|Memory Access Pattern Optimization through v-Graph Representation|The authors propose using a v-graph representation to optimize memory access patterns for graph processing on GPUs. This solution specifically addresses the challenge of optimizing GPU memory access by ensuring that threads in a warp access memory locations in a regular and contiguous manner.
566a7aec-f4ad-5eac-9a34-de3894f79c40|Asynchronous Data Transfer through CUDA Streams|The authors propose using CUDA Streams to enable asynchronous data transfer for graph processing on GPUs. This solution specifically addresses the challenge of optimizing GPU memory access by overlapping data transfer with computation, reducing memory access latency.
20f7af07-1c55-529a-a782-465dcbbfc6a2|Memory Access Pattern Optimization through Nested Loop Theta Join|The authors propose using a nested loop theta join operation to optimize memory access patterns for graph processing on GPUs. This solution specifically addresses the challenge of optimizing GPU memory access by ensuring that threads in a warp access memory locations in a regular and contiguous manner.
4115c6a3-bbb2-58e1-8234-57d4d8742cad|Hybrid HYB Format for Sparse Matrix Representation|The authors propose using a hybrid HYB format for sparse matrix representation to optimize memory access patterns for graph processing on GPUs. This solution specifically addresses the challenge of optimizing GPU memory access by ensuring that threads in a warp access memory locations in a regular and contiguous manner.
6467138a-99c2-5e56-bd2b-fc9915ecd466|Blocked CSR BCSR Format for Sparse Matrix Representation|The authors propose using a blocked CSR (BCSR) format for sparse matrix representation to optimize memory access patterns for graph processing on GPUs. This solution specifically addresses the challenge of optimizing GPU memory access by ensuring that threads in a warp access memory locations in a regular and contiguous manner.
706bb580-934c-574b-b95d-e8e7a0eb87b6|Hybrid BCSR BCOO Format for Sparse Matrix Representation|The authors propose using a hybrid BCSR BCOO format for sparse matrix representation to optimize memory access patterns for graph processing on GPUs. This solution specifically addresses the challenge of optimizing GPU memory access by ensuring that threads in a warp access memory locations in a regular and contiguous manner.
b479fbd9-79a0-51ea-9d30-68758ae407b8|Slotted Page Format for Graph Representation|The authors propose using a slotted page format for graph representation to optimize memory access patterns for graph processing on GPUs. This solution specifically addresses the challenge of optimizing GPU memory access by ensuring that threads in a warp access memory locations in a regular and contiguous manner.
f1c53d78-dfd0-5225-9874-b99b1b244880|Vector-Based CSR Format for Sparse Matrix Representation|The authors propose using a vector-based CSR format for sparse matrix representation to optimize memory access patterns for graph processing on GPUs. This solution specifically addresses the challenge of optimizing GPU memory access by ensuring that threads in a warp access memory locations in a regular and contiguous manner.
836c30bd-513b-5773-829d-af1b6a730865|Scalar-Based CSR Format for Sparse Matrix Representation|The authors propose using a scalar-based CSR format for sparse matrix representation to optimize memory access patterns for graph processing on GPUs. This solution specifically addresses the challenge of optimizing GPU memory access by ensuring that threads in a warp access memory locations in a regular and contiguous manner.
60115d8b-5397-598c-a606-6f169ab5f5dc|Incremental Iterative Computation Model|The authors propose an incremental iterative computation model, called IncGraph, to improve the execution efficiency of iterative graph algorithms on dynamic graphs. This model calculates the results of the changed vertices and merges the results of the previous graph, reducing the computation time and resources required.
c8d0f53c-7dbd-5931-9a44-044746a4f58a|Incremental Update Method|The authors propose an incremental update method to accelerate the iterative process within the iterative graph algorithm. This method uses the previously computed vertex states of the current iteration to update the uncomputed vertex states immediately.
69bc9146-5400-52ce-a141-da7fff40f6f9|Preprocess Step|The authors propose a preprocess step to identify the newly added vertices between the changed vertices and the vertices of the previous graph. This step uses a BFS algorithm to obtain the newly added vertices.
f57ed6b6-4c05-5bf0-88a6-4c798f58704c|Preprocess Step Optimization|The authors optimize the preprocess step of IncGraph by using a BFS algorithm to quickly identify the newly added vertices between the changed vertices and the vertices of the previous graph. The preprocess step is optimized by using a BFS algorithm, which reduces the time overhead of the preprocess step. The paper demonstrates that the time overhead of the preprocess step increases as the percentage of vertices with edge change size increases, but the cost for preprocess is approximately 3.6 seconds when the percentage of vertices with edge change is up to 10%.
11d72b26-ebd0-5fff-8442-e1f760f9d945|Reinforcement Learning-based Weight Matrix|The authors propose a reinforcement learning-based weight matrix to optimize communication efficiency in distributed algorithms. This solution specifically addresses the challenge by dynamically adjusting the application probability of a particular action under a specific condition, allowing the algorithm to adapt to the search progress and make informed decisions about which agents to trigger.
d4c69e36-c88e-57fa-970a-a026741fe39b|Distributed Hybrid Algorithm for Graph Coloring Problem (DH GCP)|The DH GCP algorithm is designed to efficiently solve the graph coloring problem by utilizing a distributed hybrid approach that combines tabu search, crossover operators, and perturbation techniques. This algorithm is specifically tailored to address the challenge of efficient graph dynamics processing by adapting to changes in the graph structure and minimizing computational costs.
22ce3897-f220-5459-808b-15eea687bdbe|Vertex-Cut Partitioning|The authors propose a vertex-cut partitioning technique to address the challenge of memory-efficient scalable graph processing. This technique involves partitioning a large graph into smaller connected components by removing a vertex cut component, allowing for independent processing of each component.
a3315028-70b6-5b0b-bc86-dfa9fcbcd1dd|Multi-Query Optimization|The authors propose a multi-query optimization technique to optimize the coloring of a set of graphs. This technique involves designing a vertex cut partition hierarchy to represent the common subgraphs of the graphs, allowing for shared processing and reduced memory consumption.
3d7eae24-e324-5757-908e-11d6c351fc52|Cost Model-Based Parameter Estimation|The authors propose a cost model-based parameter estimation technique to optimize the coloring of a set of graphs. This technique involves estimating the optimal values of the parameters using a cost model, ensuring efficient and scalable graph processing.
e6080524-2418-5d1b-b2e8-003cc095b274|VColor|VColor is a vertex cut based coloring technique that optimizes communication efficiency in distributed algorithms by partitioning the graph into smaller connected components (CCs) and coloring them independently.
eb0af5eb-2ea5-5e86-a34c-4ee2b402bded|MGColor|MGColor is a multi-graph coloring technique that optimizes communication efficiency in distributed algorithms by constructing a vertex cut partition hierarchy (VPH) to represent the common subgraphs of the graphs as common CCs.
b797e1fb-7098-5a9c-bca2-a0b8acade46a|Cost Model|The cost model is a mathematical framework that optimizes communication efficiency in distributed algorithms by estimating the optimal values of the parameters that minimize the running time on a given set of graphs.
4c1dfbf3-1341-5eb8-a088-c8d148be3a96|Vertex-Cut Partition Hierarchy (VPH)|The authors propose a vertex-cut partition hierarchy (VPH) to represent the common subgraphs of the graphs in a set, which enables the optimization of graph coloring by minimizing the number of colors used. The VPH is constructed by iteratively partitioning the graphs into connected components (CCs) and vertex-cut components (VCCs), and then clustering the CCs to minimize the number of colors used. The VPH is used to color the graphs by combining the colorings of the CCs and VCCs. The authors report that their technique is significantly faster than existing methods, with a speedup of up to 500 times on certain datasets.
23f128b9-bf98-5b08-90bf-1e78eb7a5dea|Maximum Matching-Based Coloring Combination|The authors propose a maximum matching-based algorithm to combine the colorings of the CCs and VCCs in the VPH. The algorithm constructs a bigraph where each vertex represents a color class, and each edge represents a conflict between two color classes. The algorithm then finds a maximum matching in the bigraph to combine the color classes. The authors report that their algorithm is able to combine the colorings of the CCs and VCCs efficiently, resulting in a significant reduction in the number of colors used.
93ac669c-d900-556e-9fd2-c983f8ca21f9|Sampling-Based Search Method|The authors propose a sampling-based search method to estimate the optimal values of the parameters in their technique. The method involves sampling a set of possible values of the parameters and evaluating the running time of the algorithm for each set of values. The authors report that their method is able to estimate the optimal values of the parameters accurately, resulting in a significant reduction in the running time of their technique.
641c6699-ea8c-5ba1-aaaa-55f8b90907a7|Vertex-Cut Partition Based Graph Coloring (VColor)|VColor is a novel approach to graph coloring that partitions a large graph into smaller connected components (CCs) by removing a vertex cut component (VCC). This allows for efficient coloring of the CCs and the VCC separately, reducing computational costs.
c709d030-ca3d-5a39-bf40-ce2fe7d61ffc|Multi-Graph Coloring using Vertex-Cut Partition Hierarchy (MGColor)|MGColor is an extension of VColor that optimizes the coloring of a set of graphs by extracting common subgraphs and using them as common CCs of the vertex-cut partitions (VPs) of the graphs.
707b324a-a413-50db-b30c-4c98f9effa0a|Cost Model for Coloring Time Estimation|A cost model is proposed to estimate the coloring time of VColor in terms of certain parameters, allowing for the optimization of the coloring process.
0d5b95d9-4e03-57aa-986e-c3ea0605c232|P-Match Graph Computation|The authors propose a novel approach to compute a small intermediate result called p-match graph, which over-approximates strong simulation. This approach is designed to reduce memory consumption and improve scalability in processing massive graphs.
b7bd3681-542b-5d28-923b-a036002ff694|Distributed Graph Redistribution|The authors propose an offline algorithm to redistribute the data graph to make it locally determinable, which enables efficient distributed pattern matching.
adfe7742-029b-5cdb-af7e-240e7ff49e55|Partition Tree Computation|The authors propose a method to compute a partition tree for each node in the pattern graph, which is used to compute the p-match graph.
b18112b6-e65d-5921-8c9f-ca783b8601ec|Dual Simulation|The authors propose a method to compute strong simulation on the p-match graph using dual simulation.
2d057eee-838f-5d33-be14-dd3b5544cd94|P-Match Graph Redistribution|The authors propose a novel approach to optimize communication efficiency in distributed algorithms by redistributing the data graph to make it locally determinable. This approach involves computing a p-match graph, which overapproximates strong simulation, and then conducting strong simulation on this p-match graph. The redistribution process is designed to minimize data shipment and ensure that each node in the data graph can be determined locally.
0ce500d8-356d-51c1-8ae7-25834da1a757|Online Parallel Matching|The authors propose an online parallel matching algorithm, PMatchStrSim, which computes the p-match graph in parallel across multiple machines. The algorithm involves computing local p-match graphs, merging them, and conducting strong simulation on the resulting p-match graph.
6f60ebe6-913a-541e-ac8c-baa379353c17|Distributed P-Match Graph Computation|The authors propose a distributed algorithm for computing the p-match graph, which can handle large data graphs by distributing the computation across multiple machines.
359aadec-21b8-567e-a74f-43f99b1baf48|Strong Simulation on P-Match Graph|The authors propose a novel approach to conduct strong simulation on the p-match graph, which can efficiently find perfect subgraphs in large data graphs.
683a3334-56a8-574f-b9cf-fe00493aa5d6|Distributed Algorithm for Strong Simulation|The authors propose a distributed algorithm for strong simulation, which is designed to work in conjunction with the p-match graph computation. This solution specifically addresses the challenge of efficient graph dynamics processing by enabling the parallel processing of strong simulation in large graphs.
da5646b0-21c8-546c-9320-df2798b8a02a|Locally Determinable Data Graph Redistribution|The authors propose a method to redistribute the data graph to make it locally determinable, which is used in conjunction with the distributed algorithm for strong simulation. This solution specifically addresses the challenge of efficient graph dynamics processing by enabling the parallel processing of strong simulation in large graphs.
46e81b96-3de5-5aeb-8df5-76b782729f17|Distributed Randomized PageRank Algorithm|The authors propose a distributed randomized algorithm to compute the PageRank, which is a special case of eigenvector centrality, under two scenarios depending on whether an individual node has the knowledge of the network size. The algorithm is based on a randomized Kaczmarz algorithm and can be implemented in a fully distributed manner by using only local information of an individual page.
bc0f11d0-0e16-5668-bf65-ce3afa07108e|Persistent Graph for Temporal Networks|The authors introduce the concept of a persistent graph to eliminate the effect of spamming nodes in temporal networks. The persistent graph adds large weights on persistent hyperlinks and a larger weight on more recent hyperlinks.
0b49238d-85ee-517d-9d9e-c336b1939278|Distributed Computation of Degree, Closeness, and Betweenness Centralities|The authors propose a distributed algorithm to compute the degree, closeness, and betweenness centralities in a directed graph. The algorithm is based on a dynamical system approach and can be implemented in a fully distributed manner by using only local information of an individual node.
d6fbc9f0-cb7c-52b0-bacb-0f4fdb35da4c|Randomized Incremental Algorithm with Diffusion Vector|The authors propose a randomized incremental algorithm with a diffusion vector to compute the PageRank, which is a special case of eigenvector centrality. This algorithm is designed to optimize communication efficiency in distributed algorithms by minimizing the number of communication rounds.
dbaf2239-b6b7-5fcf-8ade-468b8d7206d0|Distributed Randomization with Markovian Randomization|The authors propose a distributed randomization scheme with Markovian randomization to optimize communication efficiency in distributed algorithms. This scheme is designed to reduce the number of communication rounds by allowing nodes to communicate with their neighbors.
462a359a-9875-5db8-848a-5e1221faf421|Persistent Graph with Time-Varying Links|The authors propose a persistent graph with time-varying links to optimize communication efficiency in distributed algorithms. This graph is designed to reduce the number of communication rounds by encoding each node with a processor to record its hyperlinks.
17d2db22-f139-586f-82eb-a74f5b0c08fd|Distributed Estimation of Network Size|The authors propose a distributed estimation scheme to estimate the network size, which is a global information and is unknown to an individual node. This scheme is designed to optimize communication efficiency in distributed algorithms by reducing the number of communication rounds.
e9fa4a40-aee2-521a-8b30-0c683d2d2a27|Persistent Graph-based PageRank Algorithm|The authors propose a persistent graph-based algorithm to compute the PageRank for temporal networks with time-varying links. This algorithm is designed to handle the challenges of irregular memory access patterns and load imbalance in distributed systems.
47f5ed97-0368-5c43-9b23-f646540503cc|Distributed Betweenness Centrality Algorithm|The authors propose a distributed algorithm to compute the betweenness centrality for heterogeneous and irregular graphs. This algorithm is designed to handle the challenges of load imbalance, communication overhead, and memory locality in distributed systems.
0bbc9434-be03-5502-93c0-f40496a1948f|Distributed Closeness Centrality Algorithm|The authors propose a distributed algorithm to compute the closeness centrality for heterogeneous and irregular graphs. This algorithm is designed to handle the challenges of load imbalance, communication overhead, and memory locality in distributed systems.
cf32c3c6-11cd-5465-a22c-0008814311f5|Distributed Computation of Betweenness Centrality|The authors propose a distributed algorithm to incrementally compute the betweenness centrality of an oriented tree from the perspective of partitioning the network into multi-levels of neighbors. This algorithm takes advantage of the fact that a tree does not contain any loop, and therefore every pair of nodes has at most one shortest path.
e37cdbfd-206e-5f04-a299-d6c66640f562|Randomized Incremental Algorithm for PageRank Computation|The authors propose a randomized incremental algorithm to compute the PageRank of a graph, which can be used to address the challenge of efficient graph dynamics processing. The algorithm uses a Markovian randomization approach to update the PageRank estimate at each iteration.
534ae11e-f007-5362-a02a-2e8d1411db38|Persistent Graph Concept for Temporal Networks|The authors propose a persistent graph concept to address temporal networks with time-varying links, which can be used to address the challenge of efficient graph dynamics processing. The concept uses a weighted average of past hyperlink matrices to compute the PageRank.
5d61ef77-7270-50ae-a75a-4641c4629293|Selective Stateful Iterative Model|This solution addresses the challenge of memory-efficient scalable graph processing by selectively tracking the intermediate states of a subset of vertices in the graph, rather than maintaining the entire graph's intermediate state. This approach reduces memory consumption while still allowing for incremental processing and accurate results.
cceb4b29-7356-57d5-b9a1-0fc12a43b840|Minimal Stateful Iterative Model|This solution addresses the challenge of memory-efficient scalable graph processing by exploiting the distributive update property in graph algorithms to minimize the memory footprint. This approach reduces the memory consumption by only tracking the minimal necessary information for incremental processing.
75b033d5-37bc-5118-a210-8113266d15c1|Memory-Aware Data Partitioning Strategy|The authors propose a memory-aware data partitioning strategy to address the challenge of memory-efficient scalable graph processing. This strategy involves partitioning the graph data into smaller subgraphs based on the available memory resources, ensuring that each subgraph fits within the memory constraints of the processing nodes. The strategy aims to minimize memory consumption and reduce communication overhead by assigning subgraphs to nodes with sufficient memory capacity.
64f71a94-903c-5744-ae12-db9c83533238|Hierarchical Clustering-Based Communication Reduction|This solution proposes a hierarchical clustering approach to reduce communication overhead in distributed algorithms. By grouping nodes into clusters based on their proximity, the algorithm minimizes the number of inter-cluster communications, thereby reducing the overall round complexity.
3ee594e1-bc13-54c6-a8e7-fedbb512ed2a|Asynchronous Gradient Descent with Adaptive Learning Rates|This solution introduces an asynchronous gradient descent algorithm with adaptive learning rates to optimize communication efficiency in distributed optimization problems. By allowing nodes to update their parameters asynchronously and adaptively adjusting learning rates, the algorithm reduces the number of communication rounds required for convergence.
714b3173-751b-5c5c-90a4-51c5e9e21775|Distributed k-Means with Quantized Communication|This solution proposes a distributed k-means algorithm that employs quantized communication to reduce the communication overhead. By quantizing the data points and transmitting only the quantized values, the algorithm minimizes the amount of data exchanged between nodes.
d3ddc60f-456f-5139-bda9-13163c322486|Hybrid Graph Partitioning|The authors propose a hybrid graph partitioning approach that combines the benefits of both edge-cut and vertex-cut partitioning methods to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This approach specifically targets the issue of load imbalance and poor data locality by adaptively adjusting the partitioning strategy based on the graph's structure and density.
d4c7fa6d-8d98-51f6-97e6-f7706f53e10b|Adaptive Caching and Prefetching|The authors propose an adaptive caching and prefetching mechanism that addresses the challenge of irregular memory access patterns in heterogeneous and irregular graphs. This approach specifically targets the issue of poor memory locality by adaptively adjusting the caching and prefetching strategy based on the graph's structure and access patterns.
c90f9c4c-c8c2-545b-8e82-eee07e868889|Load-Aware Scheduling|The authors propose a load-aware scheduling approach that addresses the challenge of load imbalance in heterogeneous and irregular graphs. This approach specifically targets the issue of load imbalance by adaptively adjusting the scheduling strategy based on the graph's structure and density.
62f3e909-cf0a-500d-8f3d-56f965e5188b|Dynamic Load Balancing using Work-Stealing|This solution proposes a dynamic load balancing approach that utilizes work-stealing to optimize load balance in distributed systems. The approach involves each node maintaining a queue of tasks and periodically checking neighboring nodes for available tasks to steal. This technique ensures that nodes with lighter workloads can assist nodes with heavier workloads, thereby achieving a more balanced distribution of tasks.
09888f10-a9f8-5f79-b38e-48c6d024bfa2|Load Balancing using Machine Learning-based Predictive Modeling|This solution proposes a load balancing approach that leverages machine learning-based predictive modeling to forecast workload patterns and optimize task allocation. The approach involves training a machine learning model on historical workload data to predict future workload patterns, and then using these predictions to allocate tasks to nodes in a way that minimizes workload imbalance.
42492534-c2c0-584a-b657-0e35532a1e9d|Hierarchical Load Balancing using Graph Partitioning|This solution proposes a hierarchical load balancing approach that uses graph partitioning to optimize task allocation and minimize workload imbalance. The approach involves partitioning the graph of tasks and nodes into smaller sub-graphs, and then allocating tasks to nodes within each sub-graph in a way that minimizes workload imbalance.
d6fd536c-0847-57c2-871c-5e8a22d51e25|Coalesced Memory Access Pattern Optimization|The authors propose a novel memory access pattern optimization technique that ensures coalesced memory access for graph processing on GPUs. This technique involves reordering the graph data to minimize non-coalesced memory accesses, thereby reducing memory access latency and improving memory bandwidth utilization.
ab3fceb8-780b-50cc-a95a-c183715e7399|Synchronization-Aware Memory Access Optimization|The authors propose a synchronization-aware memory access optimization technique that minimizes synchronization overhead for graph processing on GPUs. This technique involves optimizing memory access patterns to reduce synchronization latency, thereby improving overall performance.
7e564fd3-29cb-5267-a02f-3ffad0f3486b|Memory Bandwidth-Aware Graph Partitioning|The authors propose a memory bandwidth-aware graph partitioning technique that optimizes graph partitioning to leverage memory bandwidth effectively. This technique involves partitioning the graph into smaller subgraphs that can be processed efficiently within the available memory bandwidth.
d9d4cf7a-bde3-5df2-b8d9-39f5a3935d11|Adaptive Trussness Recomputation|This solution addresses the challenge of efficient graph dynamics processing by proposing an adaptive approach to recomputing trussness values in response to edge and vertex updates. The method involves maintaining a set of affected vertices and edges, and iteratively updating the trussness values based on the changes. The approach is designed to minimize the number of iterations required to maintain the graph structure.
4a5ece42-95a0-55e3-8878-ed2531252a00|Incremental Betweenness Centrality|This solution addresses the challenge of efficient graph dynamics processing by proposing an incremental approach to updating betweenness centrality measures in response to edge modifications. The method involves maintaining a set of affected vertices and edges, and iteratively updating the betweenness centrality values based on the changes.
186d4537-9480-512d-a0dc-f772bbd16ac6|G thinker|G thinker is a distributed graph processing system that optimizes communication efficiency by minimizing data shipment. It uses a subgraph-centric approach to process graph patterns, reducing the need for communication between workers.
7048ba2b-d29e-5421-a423-4302738232be|BENU|BENU is a distributed algorithm designed to optimize communication efficiency in subgraph matching. It employs a novel approach to decompose the search tree into regions, reducing the number of communication rounds required.
3781c352-d809-5403-8b01-5ff598a459dd|CECI|CECI is a distributed algorithm designed to optimize communication efficiency in subgraph matching. It employs a novel approach to estimate the cost of evaluating each embedding cluster, reducing the number of communication rounds required.
77ad0590-ab58-5759-876c-5f825908943b|Dynamic Load Balancing with Task Fragmentation|This solution involves dynamically splitting tasks into smaller fragments to achieve load balance in distributed systems. The approach is designed to handle large-scale graph processing and is implemented in the QFrag system.
db4929b9-0666-52b9-8ae0-bad4c9b66c4e|Load Balancing with Scheduling Algorithm|This solution involves using a scheduling algorithm to assign tasks to workers in a way that minimizes make span and data shipment. The approach is designed to handle large-scale graph processing and is implemented in the CCJoin system.
63923a8a-d6e6-5bc3-bcc8-ceff73b67a93|Load Balancing with Vertex-Centric Approach|This solution involves using a vertex-centric approach to process tasks in parallel and achieve load balance. The approach is designed to handle large-scale graph processing and is implemented in the Lighthouse system.
67c4df64-6d82-5366-8437-428272462086|Load Balancing with Subgraph-Centric Approach|This solution involves using a subgraph-centric approach to process tasks in parallel and achieve load balance. The approach is designed to handle large-scale graph processing and is implemented in the BENU system.
f348b450-8010-5de6-a909-d6817f930d5f|Incremental Graph Simulation|The authors propose an incremental algorithm for evaluating graph simulation in a vertex-centric fashion on top of GPS. This approach is designed to handle streaming graphs by adopting a BSP-based subgraph-centric approach. The algorithm maintains the previous matching information instead of resetting the variables and starting from scratch whenever the data graph is modified. The update event triggers a new iteration of graph simulation, and the vertices concerned will update their match flags and match sets accordingly.
fd578bde-17ad-5f9e-943c-ccbe912cb953|DFA G Automaton Complexity Analysis|The authors propose a framework for complexity analysis of DFA G automaton, which can significantly simplify complexity analysis on asynchronous BSP programs. This solution specifically addresses the challenge of memory-efficient scalable graph processing by providing a method to estimate the worst-case computational cost of an automaton, which can help in optimizing memory usage and reducing computational costs.
5802f279-275c-5f22-8cdc-fd27cc21a1f8|GraphU Execution Engine|The authors propose a new execution engine for the GraphU platform, which can effectively optimize the execution efficiency of DFA G programs. This solution specifically addresses the challenge of memory-efficient scalable graph processing by decoupling vertex computation from remote communication and removing global synchronization.
2c557b6d-7022-5251-b710-f8be219f594f|State-Based Vertex Execution Optimization|The authors propose a state-based vertex execution optimization technique, which can improve the efficiency of DFA G programs by optimizing vertex execution order based on vertex state. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the number of messages generated by the automaton.
55080c75-e6c9-514a-a86d-3ddbae678ca7|DFA G Automaton-Based Vertex Execution Optimization|The authors propose a method to optimize vertex execution order based on DFA G automaton, which can help in handling heterogeneous graph structures with varying degrees, weights, and sparsity. This solution specifically addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by providing a method to optimize vertex execution order, which can help in reducing communication overhead and enhancing memory locality.
d1e52944-1c88-5a73-8c81-cd63b7427309|Neighborhood-based Message Transition Rule Attachment|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a neighborhood-based message transition rule attachment technique. This technique reduces memory consumption and message transfer cost by attaching message transition rules to data vertices based on their neighborhood structural requirements.
ce50f060-b6c8-5cba-b309-779691e0d2bb|Single Sink DAG (SSD) Plan|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a Single Sink DAG (SSD) plan for pattern detection. The SSD plan converts a pattern query into a DAG with a single sink vertex, which guides the exploration in the data graph via messages and reduces memory consumption.
46427866-8a83-5b9d-a345-3734830c9508|Incremental Evaluation over Dynamic Graphs|This solution addresses the challenge of memory-efficient scalable graph processing by proposing an incremental evaluation approach for pattern detection over dynamic graphs. The approach updates the query results incrementally as the graph changes, reducing memory consumption and computational cost.
22659e4f-3157-5995-9f55-1d5e296a0e16|Neighborhood-based Transition Rule Attachment|This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the number of messages transferred between vertices. The authors propose a neighborhood-based transition rule attachment method, which attaches message transition rules to data vertices based on their neighborhood structural requirements. This approach reduces the number of messages transferred by pruning unnecessary search space and minimizing the number of transition rules attached to each data vertex.
4a08e183-78e1-5097-bebb-a09ac7c19e73|Incremental Evaluation of Pattern Detection|This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the number of messages transferred during pattern detection. The authors propose an incremental evaluation method, which updates the pattern detection results incrementally as the graph evolves, rather than re-computing the results from scratch.
95bbe6eb-e602-5a8d-81f9-0907640cdbb5|Single Sink DAG (SSD) Evaluation Plan|This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the number of messages transferred during pattern detection. The authors propose an SSD evaluation plan, which converts a pattern query into a DAG with a single sink vertex and guides the exploration of the data graph via messages.
63430fbb-0f73-5d40-a5b9-c67c1ffb5acd|Single Sink DAG (SSD) Plan Construction|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by proposing a SSD plan construction method. This method converts a pattern query into a DAG with a single sink vertex, guiding the exploration in the data graph via messages and reducing the number of super steps required in the query evaluation.
dcb5a9b6-7605-56e6-9c78-50207af0558b|Encoding Identifiers in Messages|This solution involves encoding the identifiers of data vertices into messages to overcome the limitation of not being able to distinguish between vertices with the same label. This approach aims to improve the precision of pattern detection.
4c4a3815-bfa9-5e05-88f8-ef796ef9c6f8|Join Operations on Partial Vertices|This solution involves conducting join operations on partial vertices during graph exploration to improve the precision of pattern detection. This approach aims to reduce false positives and improve the accuracy of pattern detection.
3cc6e9ee-d6c2-5dde-b958-0c532f814a7c|Neighborhood-Based Optimized Rule Attachment|The neighborhood-based optimized rule attachment is a technique that optimizes the attachment of message transition rules to data vertices based on their neighborhood structural requirements. This technique reduces the number of attached rules and prunes the search space, leading to improved efficiency and precision.
5864d88f-7ffc-5caf-b392-43124414536e|k-Shortcut Hopset Construction|The authors propose a method for constructing a k-shortcut hopset, which is a sparse subgraph that preserves the shortest path distances between vertices in the original graph. This construction is designed to be memory-efficient and scalable for processing massive graphs.
7776f7f0-cb9d-5d1c-9673-5c686892a7b4|Pipelined Broadcast and Upcast|The authors propose a pipelined broadcast and upcast method for disseminating information in a distributed graph processing system. This method is designed to minimize communication overhead and reduce memory consumption.
c6ac142f-f5fa-5b10-96fa-853db259da9e|Memory-Efficient SPT Computation|The authors propose a method for computing shortest paths trees (SPTs) in a memory-efficient manner. This method is designed to reduce memory consumption and optimize memory usage.
41f588b8-b80c-53e4-b15c-f9965bc004f8|Pipelined Broadcast|The authors propose a pipelined broadcast technique to optimize communication efficiency in distributed algorithms. This method involves dividing the data into smaller chunks and transmitting them in a pipelined manner, allowing for concurrent processing and reducing the overall communication time.
f35412e5-1677-5326-9b04-ea840324f38e|k-Shortcut Hopset|The authors introduce the concept of a k-shortcut hopset, which is a graph structure that enables efficient computation of shortest paths in distributed algorithms. The k-shortcut hopset is constructed by selecting a subset of vertices and edges that provide a good approximation of the shortest paths in the original graph.
fab3a975-778d-57cb-8390-e333f4e313d9|Upcast Algorithm|The authors propose an upcast algorithm to optimize communication efficiency in distributed algorithms. The upcast algorithm involves aggregating data from multiple nodes and transmitting it to a central node, reducing the overall communication time.
062ebd78-a207-5937-ba04-6a226684f3af|Streaming Algorithm|The authors propose a streaming algorithm to optimize communication efficiency in distributed algorithms. The streaming algorithm involves processing data in a continuous stream, reducing the overall communication time.
0dde1b03-2b69-5463-acf8-76fb754991ca|Efficient k-Shortcut Hopset Construction|The authors propose a novel method for constructing a k-shortcut hopset in a graph, which is a crucial component for efficient graph dynamics processing. This method involves a variant of the Bellman-Ford algorithm that computes the k closest virtual vertices to each vertex in the graph, without requiring the computation of the virtual graph itself.
8397a69d-c177-5263-98d6-cbd2634f09a6|Adaptive Bellman-Ford Exploration|The authors propose an adaptive Bellman-Ford exploration algorithm that can efficiently compute distances in a graph, even in the presence of dynamic updates. This algorithm uses the k-shortcut hopset constructed in the previous solution to guide the exploration and reduce the number of iterations required.
972aa077-327a-506b-abbf-0512c75eb985|Negative Weight Cycle Detection|The authors propose a method for detecting negative weight cycles in a graph, even in the presence of dynamic updates. This method involves modifying the Bellman-Ford algorithm to detect negative weight cycles, rather than just computing distances.
e3c60fe6-73c2-5bf9-86ed-84b5a50e2d09|Distributed Sketching Protocol|The authors propose a distributed sketching protocol to address the challenge of memory-efficient scalable graph processing. This protocol involves each vertex in the graph sending a sketch to a referee, who then computes a spanning forest of the graph with a certain probability.
30952897-9651-5f4a-a358-a09606d77950|Fully Dynamic Spanning Forest Data Structure|The authors propose a fully dynamic spanning forest data structure to address the challenge of memory-efficient scalable graph processing. This data structure supports both edge insertions and deletions, and can output a spanning forest of the graph with a certain probability.
5c611c8d-57c9-5e1e-baef-d6d2adab01bb|AGM Sketch|The authors propose the AGM sketch, a data structure that can be used to solve the fully dynamic spanning forest problem. The AGM sketch uses a combination of random sampling and hashing to reduce the amount of memory used, while still allowing for efficient updates and queries.
8ff772c9-dc28-512e-87a2-ef2791ee6d73|Direct Product Lemma|The authors propose a direct product lemma to optimize communication efficiency in distributed algorithms. This lemma shows that a protocol for n-fold UR with cost C and error probability p can be used to obtain a new protocol for the original UR problem with cost C*n and error probability p^n.
b9f672b3-5fa8-53d5-8ef5-ffaac207ec90|Distributed Sketching Protocol for Spanning Forest Computation|The authors propose a distributed sketching protocol for computing spanning forests in dynamic graphs, which addresses the challenge of efficient graph dynamics processing by minimizing the communication cost and iterations required for maintaining graph structures under updates. The protocol involves each vertex sending a sketch of its neighborhood to a referee, who then computes a spanning forest of the graph using the sketches. The key technique is the use of a linear sketch, which allows for efficient computation of the spanning forest while minimizing the communication cost. The authors show that their protocol achieves a communication cost of O(log^3 n) bits, which is optimal for the problem.
16e1ba74-933f-58fe-a8e1-9a5b25de162c|Reduction from Universal Relation to Spanning Forest Computation|The authors propose a reduction from the universal relation problem to the spanning forest computation problem, which addresses the challenge of efficient graph dynamics processing by providing a new approach for solving the problem. The reduction involves constructing a graph from the universal relation instance and using a spanning forest computation algorithm to solve the problem. The key technique is the use of a linear sketch, which allows for efficient computation of the spanning forest while minimizing the communication cost. The authors show that their reduction achieves a communication cost of O(log^3 n) bits, which is optimal for the problem.
bd63a351-1736-5b3f-9274-0cc223181e69|Direct Product Lemma for Distributed Sketching|The authors propose a direct product lemma for distributed sketching, which addresses the challenge of efficient graph dynamics processing by providing a new approach for solving the problem. The lemma involves constructing a new graph from multiple instances of the original graph and using a distributed sketching algorithm to solve the problem. The key technique is the use of a linear sketch, which allows for efficient computation of the spanning forest while minimizing the communication cost. The authors show that their lemma achieves a communication cost of O(log^3 n) bits, which is optimal for the problem.
16172268-1601-5124-ac42-934894b93bab|Adaptive Computation in GraphLab|GraphLabs adaptive computation technique is designed to optimize memory usage and improve scalability in graph processing. This solution involves dynamically adjusting the computation schedule based on the graph structure and the available memory resources.
5b1dc6c2-7cfe-519c-b5da-9a4ba2c07205|Large Adjacency List Partitioning (LALP) in GPS|LALP is a technique used in GPS to optimize memory usage by partitioning large adjacency lists into smaller chunks. This solution is designed to reduce memory consumption and improve scalability in graph processing.
1e035dba-5c04-54bc-bd03-a7a759fc8a68|Dynamic Migration in GPS|Dynamic migration is a technique used in GPS to optimize memory usage by dynamically migrating vertices between workers. This solution is designed to reduce memory consumption and improve scalability in graph processing.
79eb3923-33bc-5bb8-b752-0dd2b699412f|Byte Array Data Structure in Giraph|Giraphs byte array data structure is designed to optimize memory usage by storing graph data in a compact binary format. This solution is designed to reduce memory consumption and improve scalability in graph processing.
c3ad8603-814b-551c-8328-4fd1babc7eea|Hash Map Data Structure in Giraph|Giraphs hash map data structure is designed to optimize memory usage by storing graph data in a hash-based data structure. This solution is designed to reduce memory consumption and improve scalability in graph processing.
a527d9d2-d9a9-5bdc-9543-1f4791ea1d96|Large Adjacency List Partitioning (LALP)|LALP is a technique used in the GPS system to optimize communication efficiency in distributed algorithms. It works by partitioning the adjacency lists of high-degree vertices, allowing for more efficient message passing and reducing the number of communication rounds.
70485946-03d0-540e-83d4-9fcd139a78ff|Dynamic Migration|Dynamic migration is a technique used in the GPS and Mizan systems to optimize communication efficiency in distributed algorithms. It involves dynamically repartitioning the graph during computation to improve workload balance and reduce network usage.
2966f1da-4a6e-589d-942c-9be2b9eb7c1c|Adaptive Computation|Adaptive computation is a technique used in the GraphLab system to optimize communication efficiency in distributed algorithms. It involves adaptively adjusting the computation based on the convergence of vertices, reducing the number of communication rounds.
11163110-7443-52da-b062-cb3caa5357d4|Triangle Complete Subgraph (TC Subgraph) Construction|The authors propose constructing a triangle complete subgraph (TC subgraph) for each computing node to address the challenge of memory-efficient scalable graph processing. This solution specifically addresses the challenge by reducing memory consumption and optimizing memory usage.
719e74a0-b4e0-5f34-96cb-06888c718a7b|Subgraph-Oriented Model|The authors propose a subgraph-oriented model to address the challenge of memory-efficient scalable graph processing. This solution specifically addresses the challenge by providing a flexible and efficient way to operate on local graphs.
80004dda-a1db-5f73-98be-d489120e5555|Edge Support Law|The authors propose the edge support law to address the challenge of memory-efficient scalable graph processing. This solution specifically addresses the challenge by providing a way to estimate the memory consumption of the graph processing algorithm.
6458e243-17f1-5cbc-b29e-0df53d443ae0|Edge Support Law-based Partitioning|The authors propose a novel partitioning strategy based on the Edge Support Law, which ensures that the parallel algorithm achieves a low space cost for graphs in the real world. This approach reduces the number of edges that need to be replicated, resulting in improved communication efficiency.
36cc4e87-b826-57db-b143-c31626ea28a2|Seamless Detection without Restart|The authors propose a seamless detection approach that avoids the need for repeated triangle counting and restarts between successive iterations. This approach enables the algorithm to detect local maximal k-trusses in parallel, reducing communication overhead.
6db65bb6-98f3-56d8-9324-03e4010abd07|Subgraph-oriented Model|The authors propose a subgraph-oriented model that allows for more flexible and efficient expression of local graph algorithms. This model enables the algorithm to access vertices and edges on demand, reducing communication overhead.
89b0c059-5e6d-53d9-8f40-6eb733eb6b57|PETA (Parallel Ef cient Truss detection Algorithm)|PETA is a novel parallel algorithm designed to ef ciently detect k-truss in large-scale graphs. It addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by constructing a triangle-complete subgraph (TC subgraph) for each computing node, which enables the detection of local maximal k-truss subgraphs in parallel.
95ad6301-b5b0-5598-8c02-2652b20436a5|Edge Balanced Partition Scheme|The authors propose an edge balanced partition scheme to optimize load balance in distributed systems. This scheme aims to balance the core edges with a small edge cut ratio, which can help decrease the communication cost and improve the overall performance of the system.
52d16220-03c1-5c93-9759-b61b9945da80|Subgraph Oriented Model|The authors propose a subgraph oriented model to optimize load balance in distributed systems. This model treats the local subgraph as the minimal operable unit and allows users to directly access and update the local graph, which can help reduce the communication cost and improve the overall performance.
0a5660cb-018e-5a96-9a93-038469e9a62d|Triangle Complete Subgraph|The authors propose a triangle complete subgraph to optimize load balance in distributed systems. This subgraph includes all the edges that belong to the same triangle, which can help reduce the communication cost and improve the overall performance.
f3ead103-76c9-5a21-bbfa-4a392cae48fd|Seamless Detection Technique|The authors introduce a seamless detection technique that enables the detection of local maximal k-trusses without restarting the computation from scratch. This approach reduces the number of iterations and computational costs by reusing previously computed results.
1ed86f01-09af-5c8c-b039-f024cf045ce3|Edge-Centric Processing Model|The authors propose an edge-centric processing model to address the challenge of memory-efficient scalable graph processing. This model iterates over the edges rather than vertices, allowing for more efficient memory access and reducing the need for random memory accesses.
a566b776-1ee1-5df3-a8db-f76fb811d2e6|Concatenated Edge List (CEL) Method|The authors propose the CEL method to process graphs that are larger than the GPU's global memory. This method involves constructing a concatenated edge list for each subgraph, allowing for efficient processing of large graphs.
c8f78a07-1bba-56ac-b9de-8088ef414a4f|Shared Memory-Based Vertex Value Update|The authors propose a shared memory-based approach to update vertex values, reducing the need for random memory accesses and improving performance.
a9c8a6b9-139a-58ce-bfd4-f67b8121322b|Minimizing Preprocessing Time|The authors propose minimizing preprocessing time by reducing the time spent in building the user-defined data structure.
9441927b-c823-5acb-b1ea-c95c71cdcd9c|Shared Memory-Based Vertex Value Array|The authors propose using a shared memory-based vertex value array to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This approach reduces the number of random accesses to global memory and minimizes data synchronization in global memory.
fcfd5692-0bb0-51c6-9018-55ff410be0c3|Load-Balanced Edge Blocks|The authors propose using load-balanced edge blocks to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This approach reduces load imbalance and improves the performance of the graph processing engine.
9f1fd26c-2f44-5e87-8d73-589554f3888e|Edge-Centric Processing with Concatenated Edge List (CEL)|The authors propose an edge-centric processing approach that iterates over the edges rather than vertices, which helps to mitigate irregular memory access and load imbalance in GPU. They also introduce a data structure called Concatenated Edge List (CEL) to store and process a graph, which enables coalesced memory access and reduces synchronization overhead.
66262688-54a2-58e0-bdb9-76a5b7e061b6|Two-Level GPU Processing and Memory Access Pattern|The authors propose a two-level execution mode that reduces random access and synchronization overhead in global memory. They use a shared memory to cache data access and synchronization, which improves performance when the degree of synchronization is less than a threshold.
eb2b045a-14a9-5462-bf5d-5cd3eeaa0750|Minimizing Preprocessing Time with Edge List Representation|The authors propose an edge list representation to store and process a graph, which minimizes preprocessing time and enables coalesced memory access.
54100216-f5cd-5ffb-a210-4ca90aac1aac|Optimizing Memory Access Pattern with Shared Memory|The authors propose using shared memory to cache data access and synchronization, which improves performance when the degree of synchronization is less than a threshold.
b1433e3c-a1f0-5522-8e3b-a40afcf6712e|Nested Windowed Streaming Model (NWSM)|NWSM is a novel processing model that efficiently processes the k-walk neighborhood query with a fixed-size memory. It abstracts vertices and their adjacency lists as streams and windows, allowing for the processing of very large data with a fixed-size memory. It nests k pairs of vertex streams and adjacency list streams inside one another, enabling the enumeration of all walks w, v in Wk and accessing all attribute values of the vertices in w. It also supports two types of adjacency list iterating modes: partial and full adjacency list modes. The paper demonstrates that NWSM can efficiently process neighborhood-centric analytics with a fixed memory budget, outperforming existing systems in terms of scalability and efficiency.
17dfcc20-ab41-54c2-884e-3dad904cb6f8|Balanced Buffer-Aware Partitioning (BBP)|BBP is a novel partitioning scheme that balances workloads among machines while being aware of the necessary memory budget for buffers of vertex attribute values and adjacency lists. It sorts vertices by their degrees and distributes them among machines in a round-robin manner, ensuring balanced workloads and bounded memory usage. It also partitions each edge chunk by the destination vertices into sub-chunks, balancing the number of edges among the sub-chunks. The paper shows that BBP achieves a reasonable partitioning cost and balanced workloads, outperforming existing partitioning schemes in terms of scalability and efficiency.
2eeee2f8-7563-5418-9651-d9f71807f1a2|Three-Level Parallel and Overlapped Processing (3LPO)|3LPO is a novel execution strategy that fully parallelizes and overlaps CPU computations, disk I/O processing, and network I/O processing, maximizing hardware resource utilization. It carefully aligns the three tasks to prevent I/O tasks from interfering with or blocking computation tasks, ensuring that query processing time is determined by the most bottlenecked hardware resource. The paper demonstrates that 3LPO achieves near-linear speedup and outperforms existing systems in terms of scalability and efficiency.
fd447128-3d41-5c98-9f58-87d814a934f6|Three-Level Parallel and Overlapping Processing (3LPO)|3LPO is a processing approach designed to fully utilize CPU, disk, and network resources in a cluster by aligning computation, disk I/O, and network I/O tasks.
2c57751d-7edb-5958-875b-4bef5bb44daf|Distributed Memory Triangle Counting by Exploiting the Graph Structure (TriC)|TriC is a distributed memory implementation of triangle counting in graphs using the Message Passing Interface (MPI), which exploits the graph structure to minimize the volume of communication. TriC uses a vertex-based graph distribution, where each process owns a subset of vertices and their associated edges. It employs a two-round communication approach, where the first round involves exchanging remotely owned vertices and associated edge lists, and the second round involves returning the status of the query to specific source processes. TriC also uses batching and asynchronous communication to alleviate synchronization overhead. TriC demonstrates a speedup of up to 90 relative to previous work on 32 processor cores of a NERSC Cori node and exhibits good strong scaling characteristics for large graphs.
017e7810-5fe8-5977-89c7-78b6aa94d9b7|Batching of Communication Rounds|The authors propose batching of communication rounds to reduce the communication pressure by aggregating the outgoing data and implementing a policy that trades off computation with communication. The solution involves storing the vertices for e.g., 7, 9, 11, 14, 17, and demarcating vertex boundaries such that the remote side can translate the sequence of vertex IDs in its incoming message buffer into the correct combination of vertices as edges. Since the sequence of vertices in the outgoing communication buffer can supersede the integer range, the authors internally batch communications if the data count is beyond the integer range allowed by MPI.
3516eebd-0a2a-5589-8431-d7509e92b3e7|Adaptive Routing Schemes|The authors experiment with different adaptive routing schemes for MPI Alltoallv to optimize communication efficiency. The solution involves using the MPICH GNI A2A ROUTING MODE environment variable to select different routing schemes, such as A0, A1, A2, and A3, to optimize the performance of MPI Alltoallv.
17334e67-3875-54ed-a02a-8e059ace63a5|TriC: Distributed Triangle Counting by Exploiting the Graph Structure|TriC is a distributed memory triangle counting algorithm that exploits the graph structure to minimize the volume of communication. It uses a vertex-based graph distribution and attempts to distribute the edges equally, reducing the standard deviation of edge distribution across processes and improving load balance. TriC uses a combination of local edge enumeration and communication to count triangles. It employs a batching strategy to reduce communication pressure and trades off computation with communication. The algorithm also uses a policy to store vertices and demarcate vertex boundaries, allowing the remote side to translate the sequence of vertex IDs in its incoming message buffer into the correct combination of vertices as edges. TriC demonstrates a speedup of up to 90 relative to miniTri on 32 processor cores of a NERSC Cori node and exhibits good strong scaling characteristics for large graphs.
aeaa01d8-4fcf-57de-b84a-c4f86f4fbf80|Batching and Asynchronous Communication|The authors propose using batching and asynchronous communication to alleviate the issue of load imbalance in distributed systems. This approach involves aggregating outgoing data and implementing a policy that trades off computation with communication.
82f946db-ed40-5da0-8d11-89e3ffbc4b77|Subgraph-Centric Data Model|The authors propose a subgraph-centric data model to reduce network traffic and optimize memory usage. This approach involves partitioning the data graph into disjoint subgraphs, where each subgraph is mapped to a single vertex in the GPS system. The messages from a subgraph are merged and encapsulated before transmission, reducing network traffic and memory consumption.
af381f14-3968-55d1-90af-12b2e1bf66bf|Incremental Graph Pattern Matching Algorithm|The authors propose an incremental graph pattern matching algorithm to reduce computational costs and memory consumption. This approach involves updating the matchset and match flag of vertices only when necessary, reducing the number of computations and memory accesses.
bb0038eb-391c-58bc-b2cd-43a9d900d648|Incremental Pattern Matching Algorithm|The authors propose an incremental pattern matching algorithm that only triggers computations on the vertices whose matching status is changed by the graph update events. This approach aims to reduce the number of communication rounds by minimizing the number of vertices that need to be updated.
84bed4c7-ca51-545d-8b9d-a0c4e9dea544|Dynamic Subgraph Sizing|The authors propose dynamically adjusting the size of subgraphs to optimize load balance in distributed systems. This approach involves finding the optimal subgraph size that balances the trade-off between network traffic reduction and workload imbalance.
a3678a12-368d-54f2-95c3-671c2219ed88|Vertex-Centric Incremental Algorithm|The authors propose a vertex-centric incremental algorithm to handle graph updates. This algorithm involves three phases: update, checking, and propagation.
63e46a79-6d16-534b-b705-3326ae244a87|Edge Sampling after Randomized Response (RR)|The authors propose using edge sampling after applying Randomized Response (RR) to reduce memory consumption and improve scalability in graph processing. This solution specifically addresses the challenge by reducing the number of edges that need to be processed, thereby decreasing memory usage and computational costs.
535d6312-6aa5-5438-9142-2ad72223683a|Double Clipping Technique|The authors propose a double clipping technique to further reduce memory consumption and improve scalability in graph processing. This solution specifically addresses the challenge by clipping the number of edges and then the number of noisy triangles, thereby reducing the amount of data that needs to be processed.
40725f84-2808-5df0-926b-178c63b1ed2d|4-Cycle Trick|The authors propose a 4-cycle trick to reduce the estimation error in graph processing tasks. This solution specifically addresses the challenge by reducing the number of 4-cycles in the graph, thereby improving the accuracy of graph processing tasks.
5e2006e5-2a41-5b9a-a982-620d272889b4|Asymmetric Randomized Response (ARR)|The Asymmetric Randomized Response (ARR) is a method proposed by the authors to optimize communication efficiency in distributed algorithms. This method involves using an asymmetric randomized response mechanism to reduce the number of communication rounds.
88b69051-89f4-5838-8b3c-8155a5322ab4|Double Clipping|Double Clipping is a technique proposed by the authors to reduce the sensitivity of each user's query in triangle counting algorithms. This solution specifically addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by clipping the number of edges and then the number of noisy triangles, thereby reducing the sensitivity of each user's query.
f816ba06-45a4-51bd-8c7d-b349ca1eb862|Edge Sampling|Edge Sampling is a technique proposed by the authors to reduce the number of noisy edges in triangle counting algorithms. This solution specifically addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by sampling edges after applying Warner's Randomized Response, thereby reducing the number of noisy edges.
bcefc7f3-c6a9-582d-955b-55354136957c|Selective Download|Selective Download is a technique proposed by the authors to reduce the communication overhead in triangle counting algorithms. This solution specifically addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by selectively downloading parts of a centrally computed quantity, thereby reducing the communication overhead.
462e9fad-65f9-58d3-b039-a52b58be9ec1|Edge Sampling after Warner's RR|Edge Sampling after Warner's RR is a method proposed by the authors to address the challenge of efficient graph dynamics processing. This method involves sampling edges after applying Warner's Randomized Response (RR) to reduce the number of noisy edges.
5be269fe-c21b-505e-b6d8-f539e297face|Selective Download of Noisy Edges|Selective Download of Noisy Edges is a method proposed by the authors to address the challenge of efficient graph dynamics processing. This method involves selecting some edges rather than all edges in a more clever manner to reduce the estimation error.
e993242d-9111-5c4b-ad13-d6dfdc0687d9|CentLocal Algorithm for Maximum Cardinality Matching|The authors propose a CentLocal algorithm for maximum cardinality matching, which is a key component in addressing the challenge of memory-efficient scalable graph processing. This algorithm is designed to work in a distributed computing environment and focuses on minimizing memory consumption while optimizing memory usage.
470969b8-57cb-5dd2-bb41-11e94edc6717|CentLocal Algorithm for Maximum Weighted Matching|The authors also propose a CentLocal algorithm for maximum weighted matching, which is another key component in addressing the challenge of memory-efficient scalable graph processing. This algorithm is designed to work in a distributed computing environment and focuses on minimizing memory consumption while optimizing memory usage.
e7721e32-4369-56b9-9873-97e5f9578fbc|Probe Radius Reduction|The authors propose a technique to reduce the probe radius of the CentLocal algorithm, which is essential in addressing the challenge of memory-efficient scalable graph processing. This technique involves simulating probes to the intersection graph via probes to the original graph.
3097237a-4270-5abd-a0d6-b35e00deebca|Distributed Algorithm for Maximum Cardinality Matching|The authors propose a distributed algorithm for maximum cardinality matching, which is designed to work in a distributed computing environment and focuses on minimizing memory consumption while optimizing memory usage.
0b7964a2-3240-5a73-a076-0942ee6e22ba|Distributed Algorithm for Maximum Weighted Matching|The authors propose a distributed algorithm for maximum weighted matching, which is designed to work in a distributed computing environment and focuses on minimizing memory consumption while optimizing memory usage.
19db3b4e-ae1e-5c55-bdd2-895fdf28add9|CentLocal Algorithm for Approximate Maximum Cardinality Matching|The authors propose a CentLocal algorithm for approximate maximum cardinality matching, which is a deterministic stateless algorithm that computes a 1-approximation of a maximum matching. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by minimizing the number of communication rounds.
73ca5839-5cd7-5001-828f-f46e92df3aa8|CentLocal Algorithm for Approximate Maximum Weighted Matching|The authors propose a CentLocal algorithm for approximate maximum weighted matching, which is a deterministic stateless algorithm that computes a 1-approximation of a maximum weighted matching. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by minimizing the number of communication rounds.
fedfa017-e597-5c28-8b92-e42557452572|Simulation of CentLocal by DistLocal|The authors propose a simulation technique that transforms a CentLocal algorithm into a DistLocal algorithm. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by minimizing the number of communication rounds.
1cfbbbd4-f562-5226-8e51-f99b8516a3ec|CentLocal Algorithm for Adaptive Matching|The authors propose a CentLocal algorithm for computing approximate maximum cardinality matchings and approximate maximum weighted matchings in bounded degree graphs. This algorithm is designed to adapt to heterogeneous graph structures by using a local improvement technique that simulates a global algorithm.
46a39731-17eb-522b-a10d-42568d7c0db0|Distributed Local Algorithm for Adaptive Matching|The authors propose a DistLocal algorithm for computing approximate maximum cardinality matchings and approximate maximum weighted matchings in bounded degree graphs. This algorithm is designed to adapt to heterogeneous graph structures by using a local improvement technique that simulates a global algorithm.
e51e57ae-abe8-5d0f-afa8-2f09cddff4aa|Distributed Algorithm for Approximate Maximum Cardinality Matching|The distributed algorithm is designed to approximate maximum cardinality matching in bounded degree graphs. It works by simulating the CentLocal algorithm for approximate maximum cardinality matching.
0f0ed33f-31a0-592f-b531-24c9bf18f737|Distributed Algorithm for Approximate Maximum Weighted Matching|The distributed algorithm is designed to approximate maximum weighted matching in bounded degree graphs. It works by simulating the CentLocal algorithm for approximate maximum weighted matching.
085ba730-664f-544f-b124-cc0dd82f662e|CentLocal Algorithm for Efficient Graph Dynamics Processing|The authors propose a CentLocal algorithm that efficiently processes dynamic updates in large graphs by minimizing computational costs and iterations. This algorithm is designed to maintain graph structures such as maximal k-trusses under updates, efficiently adjust centrality measures like betweenness in response to edge modifications, and batch process vertex and edge updates while preserving structural integrity.
42456c88-5156-5eba-8579-cb1befd78e57|Distributed Local Algorithm for Efficient Graph Dynamics Processing|The authors also propose a Distributed Local algorithm that builds upon the CentLocal algorithm to efficiently process dynamic updates in large graphs. This algorithm is designed to work in a distributed setting, where each vertex in the graph can only access its local neighborhood.
a3a6b998-6d38-5723-8826-f4ea7ac819f7|Probe Simulation Technique|The authors propose a probe simulation technique that enables the efficient processing of dynamic updates in large graphs. This technique involves simulating probes to auxiliary graphs to reduce computational costs.
16de1ce3-c606-5452-b359-57313be01bf0|Recursive Oracle for Membership in Matching|The authors propose a recursive oracle for determining membership in the matching, which enables efficient processing of dynamic updates in large graphs.
a12e929b-57b5-524a-8768-617b8bc23eaf|Matrix Algebraic Primitives for Graph Processing|The authors propose using matrix algebraic primitives to represent and process graphs in a distributed memory environment. This approach enables the development of memory-efficient and scalable algorithms for graph processing.
47b98001-822f-5d55-bcd2-b84a32f14ba0|Distributed Memory Parallel Algorithms for Maximal Cardinality Matching|The authors design and implement distributed memory parallel algorithms for computing maximal cardinality matching in bipartite graphs. These algorithms are built upon matrix algebraic primitives and are optimized for memory efficiency and scalability.
8fbc553b-bae1-576c-8aff-b24793e339fb|Combinatorial BLAS (CombBLAS) Framework|The authors utilize the CombBLAS framework, which provides a set of matrix algebraic primitives optimized for distributed memory environments. This framework enables the development of memory-efficient and scalable algorithms for graph processing.
f39cab8a-9c68-5452-8da5-49e28ca000d6|Matrix Algebraic Primitives for Distributed Memory Parallel Algorithms|The authors propose using matrix algebraic primitives to develop distributed memory parallel algorithms for computing maximal cardinality matching in bipartite graphs. This approach enables the exposure of a higher degree of parallelism on distributed memory platforms, leading to improved communication efficiency. The authors utilize matrix algebra building blocks, such as sparse matrix-vector multiplication (SpMV) and vector manipulations, to update the current matching. This approach allows for the efficient traversal of the bipartite graph from both sides without storing the transpose of the matrix, reducing communication overhead. The authors report achieving up to 300 speedup on 1024 cores of a Cray XC30 supercomputer, demonstrating the effectiveness of their approach in optimizing communication efficiency.
8a90d203-7905-551b-b549-8c16e1e2eaf1|Select2nd, Min Semiring for SpMV|The authors propose using the select2nd, min semiring for SpMV operations to efficiently explore row vertices from unmatched column vertices. The select2nd, min semiring operates on a set of binary numbers and a set of pairs of integers, allowing for the efficient selection of minimum products from the selected columns. The authors report achieving good performance on various input graphs, demonstrating the effectiveness of the select2nd, min semiring in optimizing communication efficiency.
b88352c2-6de9-5cee-ad8b-be02e67c98e2|Invert Function for Vector Manipulation|The authors propose using the INVERT function to efficiently update the mates of column vertices by selecting a unique child for each column vertex. The INVERT function swaps the indices and values of nonzero entries in a sparse vector, allowing for efficient vector manipulation. The authors report achieving good performance on various input graphs, demonstrating the effectiveness of the INVERT function in optimizing communication efficiency.
096b13b0-0ecf-5399-b623-73655fbca439|Matrix Algebraic Formulation of Matching Algorithms|The authors propose a matrix algebraic formulation of matching algorithms to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution involves representing a bipartite graph as a sparse matrix and using matrix algebraic primitives to compute maximal cardinality matching.
f5ff70eb-9e54-5fcb-8355-94bc153c4858|Distributed Memory Parallel Algorithm|The authors propose a distributed memory parallel algorithm for computing maximal cardinality matching in a bipartite graph. This solution involves distributing the graph across multiple processors and using parallel matrix algebraic primitives to compute the matching.
ef4ec283-393f-55f7-a0f6-0f14ff69c536|Semiring-based Graph Traversal|The authors propose a semiring-based graph traversal approach to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution involves using a semiring to define the operations for graph traversal and computing maximal cardinality matching.
74fc6858-0da2-53a3-a1ec-0f94b56c804c|Load Balancing and Random Permutations|The authors propose using load balancing and random permutations to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution involves randomly permuting the input matrix to balance the load across processors and using parallel matrix algebraic primitives to compute the matching.
c56015b8-02de-5353-9e62-e3da5819a6b5|Random Permutation of Input Matrices|The authors propose a solution to optimize load balance in distributed systems by randomly permuting the input matrix A before running the matching algorithms. This approach aims to balance the load across processors by distributing the nonzeros of A and x evenly.
c5c27b3a-8411-5385-a73b-5c84e982f542|2D Processor Grid Distribution|The authors propose a solution to optimize load balance in distributed systems by distributing the sparse matrix A on a 2D processor grid. This approach aims to balance the load across processors by dividing the matrix into smaller submatrices and assigning them to different processors.
b715c976-f909-5875-874b-0dbb065f84ed|Compressed Sparse Blocks (CSB) Data Structure|The authors propose a solution to optimize load balance in distributed systems by using a Compressed Sparse Blocks (CSB) data structure to store the sparse matrix A. This approach aims to reduce the storage requirements and improve the performance of the algorithms.
88705621-9b36-5780-8de1-6016580e2414|Semiring-based Matrix Algebra|The authors propose a solution to optimize load balance in distributed systems by using a semiring-based matrix algebra to perform the matching algorithms. This approach aims to reduce the communication overhead and improve the performance of the algorithms.
dd1d06d2-48b3-5476-bf76-9b1ceddda36d|Matrix Algebraic Primitives for Maximal Cardinality Matching|The authors propose using matrix algebraic primitives to develop distributed memory parallel algorithms for computing maximal cardinality matching in bipartite graphs. This approach enables efficient graph dynamics processing by leveraging fast parallel implementations of matrix algebraic primitives.
fafcc096-5074-57fc-86a9-a4048f01214e|Parallel Sliding Window (PSW) Method|The PSW method is a graph data layout technique that partitions vertices into disjoint sets, associating with each interval a shard containing all of the intervals incoming edges, sorted by source vertex. Intervals are selected to form balanced shards, and the number of intervals is chosen so any interval can fit completely in memory.
dcbf1abf-7fc3-5ff1-8102-63660758c4ce|Streaming Partitions|Streaming partitions is a technique that processes the graph in a streaming fashion, where the graph is partitioned into smaller chunks that can fit in memory.
5ad8029c-00bc-59e7-86ad-9c9409129ce9|Semi-External Memory (SEM) Model|The SEM model is a memory management technique that stores vertices in RAM and edges on external storage, using asynchronous message passing to access edges.
421bb9fb-0c29-585e-a8cf-fc2e132660c5|Path-Centric Compact Storage System|The path-centric compact storage system is a storage technique that stores edge traversal trees in depth-first search order, compressing data structure information.
8b3d6005-a2d1-5e07-bdef-937019ef8067|Combiner Technique|The Combiner Technique is a method for reducing network traffic in distributed graph processing by aggregating messages destined for the same vertex.
268fa353-647b-5fac-bd66-8385cef05bed|Receiver Side Scatter|The Receiver Side Scatter is a technique for reducing network traffic by sending only one message and then having the destination worker distribute multiple copies.
b18bcdf5-4e47-50fa-8ad4-7031a56b6dbf|Message Online Computing Model|The Message Online Computing Model is a method for improving memory usage by processing messages in the queue as they are delivered.
75c1150a-17d5-563a-b808-41b9ddb45876|Finishing Computation Serially (FCS) Method|The FCS Method is a technique for improving performance by adopting a scope of the graph other than vertex-centric.
af8fb645-1b3c-51d2-9afa-78c02ae340a2|Algorithm-Specific Message Optimizations|Algorithm-Specific Message Optimizations are techniques developed for specific algorithms to reduce message passing.
618756cc-a4c3-5cca-a06b-b8405b8b55c7|Partition-Driven Optimizations|Partition-Driven Optimizations are techniques that exploit the graph layout across machines to reduce message passing.
6756e6fd-9875-5eb5-9b8f-55fde204b81d|Dynamic Repartitioning|Dynamic Repartitioning is a technique for improving performance by dynamically repartitioning the graph during computation.
52f90eaf-d33b-5cfb-a5bd-d9a07986b976|PowerSwitch|PowerSwitch is a solution proposed by the authors to address the challenge of optimizing load balance in distributed systems. This approach involves adaptively switching between synchronous and asynchronous execution modes to improve performance. The authors discuss how PowerSwitch can overcome the weaknesses of a particular execution model by dynamically adjusting the execution mode.
4c5bde69-838a-529d-811d-4f64719da4e6|Delta Caching|Delta caching is a technique used to reduce the pulling of redundant data by tracking value changes. It stores a cached copy of the accumulator for each vertex, requiring additional storage. If the change in the accumulator is minimal, the cached copy is used instead of recalculating the value.
40e5af69-7b43-50c1-afec-33a1b2e40cee|Vertex Cut Approach|The vertex cut approach is a method for reducing network communication and balancing computation by breaking up high-degree vertices. It involves partitioning the graph along vertices and maintaining consistency across cached mirrors of the cut vertex.
f6cd4502-9d34-5928-ae96-69d66f8fdbf9|Streaming Partitioning|Streaming partitioning is a method for partitioning large-scale graphs by processing the graph in a streaming fashion. It uses a linear memory bound and can provide results comparable to METIS.
d0538b97-3418-5483-82dc-00d74c47d520|Path Centric Approach|The path centric approach is a method for processing large-scale graphs by storing edge traversal trees in depth-first search order. It improves compactness and locality, reducing memory footprint and runtime.
60e5647e-27ea-5a1a-8787-87aae5d985ce|Subgraph Centric Frameworks|Subgraph centric frameworks are a type of framework that adopts a scope that is greater than a vertex but less than the graph. They can provide better performance and scalability than vertex-centric frameworks.
b9df2777-82c6-5d8e-b2a3-a3fbed018cd0|Finishing Computation Serially (FCS)|FCS is a method for improving performance by adopting a scope of the graph other than vertex-centric. It is applicable when an algorithm with a shrinking set of active vertices converges slowly near the end.
c3d1d0fc-cb22-5937-b24f-ec9964fd92a3|Storing Edges at Subvertices (SEAS)|SEAS is an optimization for TLAV frameworks that improves performance by adopting a scope of the graph other than vertex-centric. It is applicable for algorithms that combine vertices into a supervertex.
8dde671b-7d73-5cbe-870f-7a8939d657b1|Edge Cleaning on Demand (ECOD)|ECOD is an optimization for TLAV frameworks that improves performance by adopting a scope of the graph other than vertex-centric. It is applicable for algorithms that remove edges.
09ddc003-c2dc-55ba-9c9c-11f7121319db|Quantum Distributed Search Framework|The authors propose a quantum distributed search framework to address the challenge of memory-efficient scalable graph processing. This framework enables the detection of triangles in a graph by partitioning the set of vertices into smaller subsets and assigning each subset to a vertex. The framework then uses quantum search to find a triple of vertices that form a triangle.
6a7a6654-6969-59b6-8219-ac4601b3e2c2|Expander Decomposition|The authors propose an expander decomposition technique to address the challenge of memory-efficient scalable graph processing. This technique involves decomposing the graph into smaller components with low mixing time, which enables efficient routing and communication between vertices.
a67f4bae-ecb2-5328-be42-8c3cc3214075|Distributed Data Structure|The authors propose a distributed data structure to address the challenge of memory-efficient scalable graph processing. This data structure enables the vertices to implement the routing task with probability at least 1 - 1/poly(n) in O(mix(G)) rounds.
194aa487-7a98-5ded-a08c-241955a5a7e8|Quantum Algorithm for Triangle Finding|The authors propose a quantum algorithm for triangle finding to address the challenge of memory-efficient scalable graph processing. This algorithm uses a combination of classical and quantum techniques to detect triangles in a graph.
c495ad49-a120-50ef-bf3c-bbf6358dc742|Classical Distributed Algorithm for Triangle Finding|The authors propose a classical distributed algorithm for triangle finding that uses a combination of expander decomposition and routing techniques. This algorithm can detect triangles in a graph with high probability in O(n^{1/3}) rounds.
db879f08-760b-5002-93ae-c71365d84fe6|Routing Scheme|The authors propose a routing scheme to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This scheme is specifically designed to handle the issues of random and irregular memory access patterns in distributed systems.
cfeab062-8f2c-5afa-b84d-5e427e7c4e08|Classical Distributed Algorithm|The authors propose a classical distributed algorithm to optimize load balance in distributed systems. This algorithm is specifically designed to address the challenge of load balance optimization by enabling efficient search and detection of triangles in a distributed graph. The classical distributed algorithm leverages classical computing principles to achieve faster search times and improved load balancing.
07452e68-e8ea-5ebd-b0a1-e46b3bbf0ffb|Quantum Distributed Algorithm|The authors propose a quantum distributed algorithm to optimize load balance in distributed systems. This algorithm is specifically designed to address the challenge of load balance optimization by enabling efficient search and detection of triangles in a distributed graph. The quantum distributed algorithm leverages quantum computing principles to achieve faster search times and improved load balancing.
0c1610ec-1cd1-5f73-9c5a-05ef808699d9|Quantum Distributed Algorithm for Triangle Finding|The authors propose a quantum distributed algorithm for triangle finding in the CONGEST model, which addresses the challenge of efficient graph dynamics processing by utilizing quantum techniques to speed up the computation.
c1b56d1b-2d10-5648-8f23-9411cdfac2a8|Classical Routing Techniques|The authors use classical routing techniques to gather information about the edges of the graph, which is necessary for the triangle finding problem.
7f5c5910-d9a0-5ac6-afa8-153a43b6e586|Distributed Quantum Search|The authors use a distributed quantum search framework to find a triple of vertices that form a triangle.
6e89eb1e-b6a1-5f9f-99ea-37bb12651315|2D Cartesian Partitioning Scheme|The authors propose a 2D Cartesian partitioning scheme to distribute the graph among MPI processes, which helps to reduce communication overheads and achieve better strong scaling behavior.
85c90b0a-8c2c-5c26-9d1a-fd1c3bd99634|Cilk-based Shared Memory Parallelism|The authors use Cilk, a work-stealing, multithreaded runtime, for shared memory parallelism within each MPI process, which helps to improve the computation efficiency.
ea6c1d4e-e327-51ff-95ce-136897f4ecf2|Compressed Row Storage (CRS) Format|The authors use the CRS format to store the graph data, which helps to reduce memory consumption and improve communication efficiency.
5eb9c450-e52f-5cf7-80df-0e818b35b0d1|Over-Partitioning Rate|The authors use an over-partitioning rate to divide the computation into smaller tasks, which helps to improve the load balancing and reduce the communication overhead.
3846a3e9-dc53-5857-ada8-5ed9926fc377|Dense Hashmaps|The authors use dense hashmaps to store the intermediate results, which helps to reduce the memory consumption and improve the computation efficiency.
cef4f878-3fb5-587a-bb75-07318ba482cd|Hybrid Parallelization using MPI and Cilk|The authors propose a hybrid parallelization approach that combines MPI for inter-process communication with Cilk for shared-memory parallelism. This approach allows for efficient communication and computation in distributed algorithms.
6eed1e1a-bc21-5fa9-b054-48f68801ba64|Over-Partitioning of Row Blocks|The authors propose an over-partitioning approach for row blocks, where each row block is divided into smaller blocks to balance the number of nonzeros in each block.
01139d78-e556-58f7-8cfa-80ee8fe4736b|Hybrid Parallel Triangle Counting Algorithm|The authors propose a hybrid parallel triangle counting algorithm that uses MPI on distributed memory compute nodes and Cilk for shared memory parallelism. This algorithm is designed to efficiently process and analyze complex, real-world graphs characterized by heterogeneity in vertex and edge attributes, irregular structures leading to poor data locality, and variations in density, connectivity, and weights.
c8f7306a-8791-549d-b30b-c77da4b6fbed|Block Partitioning Scheme|The authors use a block partitioning scheme to divide the rows of the matrix among Cilk threads, which balances the number of nonzeros in each block and reduces load imbalance.
368cf0fd-84fd-57cc-83a5-b7362eed9568|Subgraph-Centric Processing|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a subgraph-centric approach, where the graph is partitioned into smaller subgraphs, and each subgraph is processed independently. This approach reduces memory consumption by only loading the necessary subgraph data into memory, rather than the entire graph.
b139b653-7e1b-5678-bd21-66a979469098|Linear Algebra-Based Processing|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a linear algebra-based approach, where graph algorithms are expressed as linear algebra operations. This approach reduces memory consumption by representing the graph as a sparse matrix, which can be processed efficiently using linear algebra operations.
bfd067d5-5f6b-56eb-8a3a-52c49fbc0590|Data Flow-Based Processing|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a data flow-based approach, where graph algorithms are expressed as data flow graphs. This approach reduces memory consumption by processing the graph in a streaming fashion, rather than loading the entire graph into memory.
6554a3ee-226d-565c-81a6-4ac69c53b2ce|Datalog-Based Processing|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a Datalog-based approach, where graph algorithms are expressed as Datalog queries. This approach reduces memory consumption by using a declarative query language to express graph algorithms, which can be optimized and executed efficiently.
89b481c8-dba4-5065-a6d5-7577943ad3a0|Mirroring|Mirroring is a message reduction technique that creates copies of high-degree vertices on different machines to reduce communication overhead.
ab1926cf-ae5e-52b5-baf1-a281b2d7eeb2|Breaking Supersteps|Breaking supersteps is a technique that divides each superstep into smaller supersteps to reduce memory requirements and improve communication efficiency.
6504856a-c575-5735-bcd2-9afe22a246cb|Pre-Aggregation|Pre-aggregation is a technique that allows for the aggregation of results at the outbox to reduce communication overhead.
c3d2f202-7c7a-524e-ade2-d41738ae1d5d|Barrierless Asynchronous Parallel (BAP) Model|The BAP model is a execution model that allows vertices to immediately access messages they have received and utilizes only barriers local to each worker.
f823e813-0f80-5a71-985c-5bebab45b2af|Incremental Execution|Incremental execution is a technique that allows for the efficient update of computation when the input changes, without halting and re-computing everything from scratch.
3cffac88-df58-5562-a205-4c3770fae318|Differentiated Vertex Computation Model|The Differentiated Vertex Computation Model is a solution proposed by the authors to address the challenge of optimizing load balance in distributed systems. This model differentiates between high-degree and low-degree vertices in the graph and applies different computation models to each type of vertex. High-degree vertices are processed using the GAS model, while low-degree vertices are processed using a GraphLab-like vertex-centric model.
1b3fddc7-2af8-5a93-ac57-55ecd118e3c0|Mirroring Mechanism|The Mirroring Mechanism is a solution proposed by the authors to address the challenge of optimizing load balance in distributed systems. This mechanism creates copies of high-degree vertices on different machines, allowing for more efficient communication and computation.
eaac70e1-2f0a-5ca2-8cb7-9b5ff702fd10|Breaking Supersteps into Smaller Supersteps|Breaking Supersteps into Smaller Supersteps is a solution proposed by the authors to address the challenge of optimizing load balance in distributed systems. This approach breaks each superstep into a number of smaller supersteps and processes only a subset of the vertices in each smaller superstep.
145f29cc-9496-50d6-b13f-5172647d2643|Incremental Execution Model|The Incremental Execution Model is a solution proposed by the authors to address the challenge of optimizing load balance in distributed systems. This model allows for efficient updates to the computation when the input changes, without halting and re-computing everything from scratch.
60eebe3f-787f-571c-aa44-913223592f78|Asymmetric Convergence Optimization|This solution specifically addresses the challenge of efficient graph dynamics processing by exploiting the phenomenon of asymmetric convergence in iterative graph algorithms. The authors propose a technique to monitor the convergence of different parts of the graph and adjust the computation accordingly, reducing the number of iterations required for convergence.
cd5199e9-4f34-5169-a9a2-ffea27691bd6|Neighborhood-Centric Processing|This solution addresses the challenge of efficient graph dynamics processing by proposing a neighborhood-centric approach to graph processing. The authors suggest that by extracting subgraphs based on specific application criteria, the computation can be more efficient and scalable.
ff8ce7ed-2ac8-5d86-95d2-ca806a9a416c|Filter Process-Based Processing|This solution addresses the challenge of efficient graph dynamics processing by proposing a filter process-based approach to graph processing. The authors suggest that by using a filter process to dynamically generate embeddings, the computation can be more efficient and scalable.
7cf448ee-84a9-5444-a21c-e6750b681750|Graph Traversal-Based Processing|This solution addresses the challenge of efficient graph dynamics processing by proposing a graph traversal-based approach to graph processing. The authors suggest that by using graph traversals to perform computations, the computation can be more efficient and scalable.
2b2fcb90-fd79-5e43-acaf-6e357d69a344|Radar Push (RP) Algorithm|The RP algorithm is a method for generating random walks in a graph, which is a crucial component of PageRank computation. It addresses the challenge of memory-efficient scalable graph processing by proposing a deterministic push approach to extend random walks, rather than relying on random extensions. This approach allows for more controlled and efficient use of memory, as it avoids the creation of hubs that can lead to high memory consumption.
e494001c-985f-503f-89f1-8a6464f3f447|MRP (Multi-Round Push) Algorithm|The MRP algorithm is an extension of the RP algorithm that allows for the generation of random walks of varying lengths. It addresses the challenge of memory-efficient scalable graph processing by proposing a hierarchical approach to generating random walks, which reduces the memory consumption and communication overhead.
01252b86-d300-58aa-b0e0-ac77542d0a85|Multi-Round Radar Push (MRP) Algorithm|The MRP algorithm is an extension of the RP algorithm that generates random walks of varying lengths. It addresses the challenge of optimizing communication efficiency by reducing the number of communication rounds required for generating random walks of different lengths.
48dabc88-d3ec-5e75-a96a-b43291313f48|Batch One-Hop Personalized PageRanks (BPPR) Algorithm|The BPPR algorithm is a method for computing personalized PageRanks in a distributed manner. It addresses the challenge of optimizing communication efficiency by reducing the number of communication rounds required for computing personalized PageRanks.
7b2a2f46-a00e-5e88-889c-90fbc58eca3c|Multiple Round RP (MRP) Algorithm|The MRP algorithm is an extension of the RP algorithm that recursively connects shorter walks to longer walks, reducing the round complexity and edge bandwidth in distributed PageRank computation. This approach helps to overcome load imbalance and reduce communication overhead in handling heterogeneous graph structures.
cc74e1ee-2ab8-5167-8f1b-625a6b0a3f7c|Adaptive Trussness Updates|The paper proposes a method for adaptively updating trussness values in response to edge modifications. It works by using the MRP algorithm to efficiently compute random walks and update trussness values.
bbce38c8-8bde-5afb-b2a1-885385e9c281|Incremental Betweenness Centrality Updates|The paper proposes a method for incrementally updating betweenness centrality measures in response to edge modifications. It works by using the MRP algorithm to efficiently compute random walks and update betweenness centrality measures.
1e1fa07b-95f2-546b-a365-086aff92b029|I/O-Efficient Algorithm for Top-r Non-Contained k-Influential Communities|This solution addresses the challenge of memory-efficient scalable graph processing by proposing an I/O-efficient algorithm for computing the top-r non-contained k-influential communities. The algorithm assumes that all vertices of the graph can be stored in the main memory and computes the k-influential communities following the decreasing order of their weights. The communities and edges in communities with large weights can be safely deleted without affecting the correct computation of the top-r non-contained k-influential communities.
7ad61583-378a-5049-99f2-c0571c855770|Bulk Deletion and Local Exploration|This solution addresses the challenge of efficient graph dynamics processing by proposing a greedy algorithm that uses bulk deletion and local exploration to quickly find the closest truss community in the local neighborhood of query vertices. The algorithm uses a greedy strategy to delete vertices far away from query vertices, which speeds up the pruning process and achieves quick termination while sacrificing some approximation ratio. Additionally, it uses a heuristic strategy of local exploration to quickly find the closest truss community in the local neighborhood of query vertices. The algorithm achieves a 2-approximation to the optimal solution, which is essentially matching the lower bound.
ef41d9ac-6a83-5695-8359-754fed13129f|Truss Decomposition|This solution addresses the challenge of efficient graph dynamics processing by proposing an algorithm for truss decomposition in massive networks. The algorithm uses a peeling framework to iteratively remove vertices with the minimum degree, which allows for efficient computation of trussness values and adjustment to graph topology changes. The algorithm takes O(m1.5) time and O(m+n) space, making it efficient for large graphs.
4a456d3d-cddf-5c32-be44-334289b28e30|EquiTruss Index|This solution addresses the challenge of efficient graph dynamics processing by proposing an indexing technique called EquiTruss, which represents the k-truss equivalence classes of edges in a graph. The EquiTruss index preserves the truss number and triangle connectivity across different communities, allowing for efficient query processing and community search. The EquiTruss index can be constructed in O(u,v E min(deg_G(u), deg_G(v))) time and stored in O(m) space, making it efficient for large graphs.
6158cc91-8579-53b3-a291-c854897364f6|ATindex|This solution addresses the challenge of efficient graph dynamics processing by proposing an indexing technique called ATindex, which consists of two components: structural trussness and attribute trussness. The ATindex allows for efficient detection of a small neighborhood subgraph around query vertices, which tends to be densely connected and have similar attributes. The ATindex can quickly identify a good candidate of k, d-truss to the answer, making it efficient for community search in attributed graphs.
2d779c85-1ffa-5a72-8461-7b68966a73fc|Incremental k-Truss Maintenance|This solution addresses the challenge of efficient graph dynamics processing by proposing an algorithm for incremental k-truss maintenance in evolving networks. The algorithm uses a combination of graph decomposition and random contraction to efficiently update the k-truss structure in response to edge insertions and deletions. The algorithm takes O(h*l*E) time if the first k-ECC enumeration algorithm is adopted, or O(V*t*E) time if the second one is used, where h and l are bounded by small constants for real graphs, and t=O(log2(V)).
4d102abf-12bb-5774-9109-3786709d9691|Hsync Mode|Hsync is a hybrid graph computation mode that adaptively switches between synchronous (Sync) and asynchronous (Async) modes to achieve optimal performance. Hsync uses online sampling, offline profiling, and a set of heuristics to predict future performance and determine when a mode switch could be profitable. It also employs a fast and consistent switch between the two modes, allowing for seamless transitions between Sync and Async modes. The paper demonstrates that Hsync outperforms both Sync and Async modes in various graph algorithms, with a speedup ranging from 9 to 73.
2c9c1ed6-6b90-5c35-95b0-91072f84b566|Neural Network-based Offline Profiling|This solution uses a neural network model to predict the throughput of Async mode for a given input graph. The neural network model takes into account the average degree of vertices and the average number of replicas as input dimensions to predict the throughput of Async mode. The paper demonstrates that the neural network model can accurately predict the throughput of Async mode, with an error ranging from 10.8 to 11.5.
1757fd50-9f4d-5311-b21e-ce40863a0072|Online Sampling|This solution uses online sampling to predict the throughput of Sync and Async modes. Online sampling collects execution statistics of a graph algorithm on a small input graph and uses these statistics to predict the throughput of Sync and Async modes. The paper shows that online sampling can accurately predict the throughput of both Sync and Async modes, with an error ranging from 0.5 to 7.9.
756ce3ef-4de1-5629-b179-0e3d894e3bff|Hsync|Hsync is a hybrid graph computation mode that adaptively switches a graph parallel program between synchronous and asynchronous modes for optimal performance. Mechanisms: Hsync constantly collects execution statistics on the fly and leverages a set of heuristics to predict future performance and determine when a mode switch could be profitable. Results: PowerSwitch, a prototype built based on PowerGraph, consistently outperforms the best of both modes, with a speedup ranging from 9 to 73 due to timely switch between two modes.
4cd5344e-04fb-5d42-a18e-9a08a8a35c88|Of-line Profiling|Of-line profiling is a method used to predict the throughput of the current mode by building a neural network model using a set of training graphs. Mechanisms: The method uses a set of training graphs to build a neural network model that predicts the throughput of the current mode, which is then used to determine when to switch to the other mode. Results: The error of predicted throughput using of-line profiling is from 10.8 to 11.5.
b58fc819-0d9d-5b37-b89b-72f197023cf9|Convergence Ratio|Convergence ratio is a method used to normalize the throughput of different modes by considering the total amount of vertex computation in asynchronous mode to that of synchronous mode. Mechanisms: The method uses the convergence ratio to normalize the throughput of different modes, which allows for a fair comparison of the performance of different modes. Results: The convergence ratio is used to normalize the throughput of different modes, which allows for a fair comparison of the performance of different modes.
65a6afa7-0096-5921-818e-6ff93efb2f22|Hsync Mode Switching|The authors propose a hybrid graph computation mode called Hsync, which adaptively switches between synchronous (Sync) and asynchronous (Async) modes to optimize performance for different graph algorithms and execution stages. Hsync uses online sampling, offline profiling, and a set of heuristics to predict the optimal mode switch points. It also employs a fast and consistent mode switching mechanism to minimize overhead. The paper shows that Hsync outperforms both Sync and Async modes alone by up to 2.23X and 2.71X, respectively, and achieves a speedup of up to 73 over the best of both modes.
1bbc3282-e438-53b4-a5dd-d596739ff9f0|Neural Network-based Throughput Prediction|The authors propose a neural network-based model to predict the throughput of Async mode for different input graphs. The model uses two dimensions of input vectors (average degree of vertex and average number of replicas) to model the throughput of Async mode. The paper shows that the predicted throughput of Async mode using the neural network model is close to the real throughput, with an error of 10.8-11.5.
5d0cdbcb-ea70-5496-b473-2ec00cb31330|Online Sampling-based Throughput Prediction|The authors propose an online sampling-based approach to predict the throughput of Sync and Async modes. The approach collects execution statistics of a new graph algorithm on PowerSwitch by running an algorithm with a small input for several seconds. The paper shows that the predicted throughput of Sync and Async modes using online sampling is close to the real throughput, with an error of 0.5-7.9.
5e0c4d8b-e513-5a84-84de-cdb6399c105e|Hybrid Synchronous Mode (Hsync)|Hsync is a hybrid graph computation mode that adaptively switches between synchronous (Sync) and asynchronous (Async) modes to achieve optimal performance.
1aec17af-a8f8-5c5a-93b5-c6a576a822eb|Adaptive Mode Switching|Adaptive mode switching is a technique used in Hsync to switch between Sync and Async modes based on the execution progress and performance prediction.
f85997f7-63e5-5a3d-b0df-1e69f90cb71f|Performance Prediction|Performance prediction is a key component of Hsync, which predicts the throughput of both Sync and Async modes based on execution statistics and heuristics.
8bc2a378-749f-5d0e-8a2f-d8ab8f53c39e|Offline Profiling|Offline profiling is a technique used in Hsync to build a neural network model to predict the throughput of Async mode.
af396597-9810-5f2c-a39f-71df31d4be80|Mixed Structure-Based Approach|The authors propose a mixed structure-based approach to address the challenge of memory-efficient scalable graph processing. This approach involves identifying a mixed structure, which is a set of edges and vertices that can be inserted or deleted simultaneously, allowing for batch processing of graph updates.
87220944-700f-52c9-8fdf-13d633cb74e0|Pre-Truss Concept|The authors introduce the concept of pre-truss, which is used to determine the initial trussness of newly inserted edges in a mixed structure. This concept helps to reduce the number of iterations needed for processing both inserted and deleted edges and vertices.
af131499-de3b-5e54-ae8c-783ccb098a78|Parallel Implementations|The authors propose parallel implementations of their algorithms to further reduce the number of iterations needed for truss maintenance. This approach involves assigning the task of searching edges that may update the trussness to different processes for different k values.
e493371e-11a7-50e1-a2aa-90d480d77671|Parallelized Truss Maintenance Algorithm|The authors propose a parallelized truss maintenance algorithm to optimize communication efficiency in distributed algorithms. This solution specifically addresses the challenge by allowing multiple edges to be processed simultaneously, reducing the number of communication rounds required.
d190ba35-3d46-5aea-a533-62287f15f375|Mixed Structure-Based Truss Maintenance|The authors propose a novel approach to truss maintenance in fully dynamic graphs, which involves the use of a mixed structure composed of inserted/deleted edges and vertices. This approach enables the efficient handling of heterogeneous graph structures with varying degrees, weights, and sparsity.
8f656f77-641b-51f3-9e73-23cee90f94c9|Parallel Traversal Algorithm|The authors propose a parallel traversal algorithm that can efficiently handle the traversal of edges in the mixed structure, reducing the computational overhead associated with truss maintenance.
109a7e5a-35cd-504a-80a2-bfccb04f805b|Triangle Support-Based Trussness Update|The authors propose a novel approach to updating the trussness of edges in the mixed structure, which involves the use of triangle support.
e7bc0b9e-6ecc-5e6b-9582-f60a6b6c34d5|Distributed Graph Simulation Algorithm|The authors propose a distributed graph simulation algorithm that addresses the challenge of memory-efficient scalable graph processing by utilizing a vertex-centric approach. This approach allows for the distribution of the query graph among all workers, reducing memory consumption and enabling the processing of large-scale graph data.
8f86556a-33f2-59a1-b85a-15e749b505d1|Strict Simulation Model|The authors propose a new pattern matching model, called strict simulation, which addresses the challenge of memory-efficient scalable graph processing by providing a more stringent and scalable alternative to strong simulation.
b2fd529c-0128-53ec-97f0-f58185cde61b|Ball Creation Optimization|The authors propose an optimization technique for ball creation in strong simulation, which addresses the challenge of memory-efficient scalable graph processing by reducing the overhead of ball creation.
569a7de8-8964-5a72-926d-26bff6c40f12|Strict Simulation|The authors propose a new pattern matching model called strict simulation, which is designed to be more scalable and efficient than strong simulation while preserving its important properties. Strict simulation is specifically tailored to optimize communication efficiency in distributed algorithms by reducing the number of communication rounds required.
f7b74afe-5ff6-5e3e-a8f8-40db5326d8af|Dual Simulation with BFS|The authors propose a distributed algorithm for dual simulation that uses a breadth-first search (BFS) approach to optimize communication efficiency.
57896132-383f-549a-a545-11da680a7bdb|2D Distributed Graph Partitioning|The authors propose a 2D distributed graph partitioning approach to address the challenge of memory-efficient scalable graph processing. This solution involves partitioning the graph into smaller sub-matrices and distributing them among processors, reducing memory consumption and communication overhead.
c6b64d02-46c3-5a5e-9168-8b7e29185fd4|Doubly-Compressed Sparse Columns (DCSC) Data Structure|The authors propose the use of a Doubly-Compressed Sparse Columns (DCSC) data structure to represent the graph, which is designed to reduce memory consumption and improve scalability.
9720583b-5daf-5d18-b464-1a8647704055|Hybrid Parallel BFS Algorithm|The authors propose a hybrid parallel BFS algorithm that combines the benefits of 1D and 2D partitioning approaches to address the challenge of memory-efficient scalable graph processing.
c0d5a425-7168-5088-b2de-29ace2e2bbd9|2D Vector Distribution|The authors propose a 2D vector distribution approach to optimize load balance in distributed systems. This method involves distributing the vertices and edges of a graph among processors in a two-dimensional manner, allowing each processor to have approximately the same number of vertices. This approach aims to mitigate the load imbalance issue by ensuring that each processor has a similar workload.
b2e6b4ca-0c60-5f85-96bd-4340aec30270|Hybrid Parallel BFS with Vertex Partitioning|The authors propose a hybrid parallel BFS algorithm that combines vertex partitioning with shared memory traversal parallelism. This approach aims to optimize load balance by dividing the graph into smaller subgraphs and processing them in parallel using shared memory.
c1b66cc7-adc8-53b0-97ec-5e25db18cc4f|Random Shuffling of Vertex Identifiers|The authors propose a simple yet effective method to optimize load balance by randomly shuffling the vertex identifiers prior to partitioning the graph. This approach aims to reduce the likelihood of load imbalance by ensuring that each processor has a similar workload.
b8e62987-a520-5dd9-95ba-ce558f4a2f6c|2D Partitioning for BFS|The authors propose a 2D partitioning approach for Breadth-First Search (BFS) to efficiently process large-scale graphs. This solution specifically addresses the challenge of efficient graph dynamics processing by reducing the computational costs and iterations required for BFS.
41092733-5fac-5fa8-ba84-83fb52ec15a5|MemoGFK Optimization|The MemoGFK optimization is a memory-efficient technique for scalable graph processing, specifically designed to reduce memory consumption and improve performance in the context of Euclidean Minimum Spanning Tree (EMST) and Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) algorithms.
9ff4a4af-d06b-5656-99b2-0d1c32240ec3|Parallel Dendrogram Construction|The parallel dendrogram construction algorithm is a technique for efficiently generating a dendrogram, a hierarchical representation of clusters, from a minimum spanning tree (MST) of a graph.
f5025f93-4ce5-5049-9dd5-f16c1357d688|New Definition of Well Separation|The new definition of well separation is a technique for improving the efficiency of the WSPD construction algorithm by incorporating the concept of mutual unreachability.
ed841343-d7b2-5add-aef3-341754de52a7|Parallel WSPD Construction|The authors propose a parallel algorithm for constructing the Well-Separated Pair Decomposition (WSPD), which is a crucial step in computing the EMST and HDBSCAN. The parallel algorithm uses a d-tree to partition the points and recursively constructs the WSPD in parallel.
223435b3-6ecc-5d73-9618-1469f1feff17|Parallel GFK Algorithm|The parallel GFK algorithm is a parallelization of the GFK algorithm for EMST and HDBSCAN. It uses Kruskal's MST algorithm as a subroutine and passes batches of edges to it, where each batch has edges with weights no less than those in the previous batch.
b247b6bd-4b3e-5470-8f24-173fda6ddab1|Parallel Dendrogram Algorithm|The parallel dendrogram algorithm is a parallelization of the dendrogram algorithm for HDBSCAN. It uses a parallel thread spawning strategy to parallelize the computation of the dendrogram.
de00136b-6318-5009-9a79-500b90c603be|Parallel HDBSCAN Algorithm|The authors propose a parallel HDBSCAN algorithm to efficiently process dynamic updates in large graphs. This algorithm is designed to minimize computational costs and iterations by leveraging parallel processing techniques.
60db10ac-4509-5753-88c9-ed0bd7323cb4|Decoupled Architecture with Smart Query Routing|The authors propose a decoupled architecture that separates query processing from graph storage, allowing for more flexible and efficient query routing. This approach enables the use of smart query routing strategies that can effectively leverage the cache of query processors, reducing the impact of how the graph is partitioned across storage servers.
fead93af-a0d9-5bcc-b1ff-3d0d671f1d38|Landmark-based Routing|The authors propose a landmark-based routing strategy that uses a set of landmarks to partition the graph into regions, and then assigns each region to a query processor. This approach enables the system to adapt to workload changes and graph updates, reducing the need for expensive graph re-partitioning and re-partitioning.
48ab9f5b-19d7-5399-b7a2-1521eb2f14ee|Graph Embedding-based Routing|The authors propose a graph embedding-based routing strategy that uses graph embedding to assign coordinates to nodes in the graph, and then uses these coordinates to determine the distance between nodes. This approach enables the system to adapt to workload changes and graph updates, reducing the need for expensive graph re-partitioning and re-partitioning.
65d05287-bf7b-54d6-8e0a-571cf91f731c|Query Stealing|The authors propose a query stealing strategy that allows query processors to steal queries from other processors that are currently handling high-degree nodes. This approach enables the system to adapt to workload changes and graph updates, reducing the need for expensive graph re-partitioning and re-partitioning.
8d7563f7-b3bc-59af-bb88-f894424966de|Landmark Routing|Landmark routing is a solution proposed by the authors to optimize communication efficiency in distributed graph querying. This method involves selecting a set of landmarks that partition the graph into regions, and then deciding a one-to-one mapping between those regions and processors. The distance of a query node to each processor is computed based on its distance to the landmarks, and the query is routed to the processor with the smallest distance.
49f3b39c-2bb1-5fc2-befc-06de406b162e|Embed Routing|Embed routing is another solution proposed by the authors to optimize communication efficiency in distributed graph querying. This method involves embedding the graph in a Euclidean plane and using the resulting node coordinates to determine how far a query node is from the recent history of queries that were sent to a specific processor.
e3a28874-0326-5ffb-b3ca-f7436c44c8fe|Embedding-based Routing|The authors propose an embedding-based routing algorithm that uses graph embedding to assign coordinates to nodes in a D-dimensional Euclidean plane. This approach enables the router to determine the distance between nodes and make routing decisions based on their coordinates.
1fec5163-12e0-5ecf-a4ba-82d7b41c900e|Load Balanced Distance Routing|Load balanced distance routing is a technique that calculates the load-balanced distance between a query node and a processor, taking into account the processor's load and a load factor. The load-balanced distance is calculated using the formula dLB(u, p) = d(u, p) * Processor Load / Load Factor, where d(u, p) is the distance between the query node and the processor. The paper shows that load balanced distance routing achieves better throughput and cache hit rates compared to baseline routing schemes.
9fc2510e-9d0e-574f-a582-8d1bd9411119|CSMR: Cosine Similarity with MapReduce|The authors propose a scalable algorithm for text clustering using Cosine Similarity and MapReduce. This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by utilizing the MapReduce model to improve the efficiency of traditional tf-idf algorithm and Cosine Similarity measurements on distributed text processing.
f185d299-08da-5cdc-aa4f-c0919d44841b|Hybrid Matrix Multiply Algorithm|The authors propose a hybrid matrix multiply algorithm that combines the strengths of both the adjacency-only and adjacency+incidence algorithms to address the challenge of memory-efficient scalable graph processing. This algorithm is designed to handle high-degree vertices more efficiently, reducing memory consumption and computational costs.
c64ea59a-24f6-589d-a6f7-2855959e646b|Degree-Aware Matrix Multiply Algorithm|The authors propose a degree-aware matrix multiply algorithm that takes into account the degree of each vertex in the graph when distributing workloads across processors. This algorithm is designed to optimize load balance in distributed systems by reducing the impact of high degree vertices on skewness.
a8fd8351-62e4-520c-973b-b4881975b476|Local Approximation Algorithm for Max Min Linear Programs|The authors propose a local approximation algorithm for max min linear programs, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
afc0766c-c8b7-5e97-8845-d32951d1b570|Distributed Constant Size Problem Solving|The authors propose a framework for solving distributed constant size problems, which is a specific method for optimizing communication efficiency in distributed algorithms. This framework is designed to achieve a constant time complexity for solving problems in large-scale distributed systems.
15c7a266-e9df-56b1-ac25-bc150f39b4d3|Local Algorithm for Sleep Scheduling in Sensor Networks|The authors propose a local algorithm for sleep scheduling in sensor networks, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale sensor networks.
49a4bef4-5113-5322-9189-5b4973093c35|Local Algorithm for Distributed Approximations|The authors propose a local algorithm for distributed approximations, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
a7345cb5-a5a8-5325-be08-9312707c9037|Local Algorithm for Max Min Linear Programs with Local Advice|The authors propose a local algorithm for max min linear programs with local advice, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
d97db0bf-37e4-5104-bb60-a64880dde04e|Local Algorithm for Distributed LP Rounding|The authors propose a local algorithm for distributed LP rounding, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
2096e6ab-1f57-5dbd-8a6b-3f426f889dba|Local Algorithm for Distributed SAT|The authors propose a local algorithm for distributed SAT, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
0b37b5d3-b8ac-5dfc-9758-da002deeedb3|Local Algorithm for Distributed Vertex Cover|The authors propose a local algorithm for distributed vertex cover, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
9416004f-a27f-5c8f-97a4-2a3d31bd5769|Local Algorithm for Distributed Edge Cover|The authors propose a local algorithm for distributed edge cover, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
67c73345-2479-5b68-92dd-730b85cb5604|Local Algorithm for Distributed Matching|The authors propose a local algorithm for distributed matching, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
d425a100-6bb0-5cb2-b19b-5c17e6c8c6c5|Local Algorithm for Distributed Set Cover|The authors propose a local algorithm for distributed set cover, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
c2cb71f1-c026-5019-973e-7e81a78d7fbd|Local Algorithm for Distributed Dominating Set|The authors propose a local algorithm for distributed dominating set, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
4551887e-d55a-54ba-bc76-8d276408ab82|Local Algorithm for Distributed Coloring|The authors propose a local algorithm for distributed coloring, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
c8fa2d92-bb51-5046-bd13-85572aef053d|Local Algorithm for Distributed Clique Cover|The authors propose a local algorithm for distributed clique cover, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
3fa5aecb-ce4d-5685-838c-353b5aac7e85|Local Algorithm for Distributed Cluster Cover|The authors propose a local algorithm for distributed cluster cover, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
3ce9fb28-4d76-5425-b15b-f1829acc3e02|Local Algorithm for Distributed Facility Location|The authors propose a local algorithm for distributed facility location, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
1aea7b09-7a5e-5760-bb3f-ba36729b7694|Local Algorithm for Distributed k-Median|The authors propose a local algorithm for distributed k-median, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
4370b952-ace2-58e8-948e-50a07ab3e79e|Local Algorithm for Distributed k-Center|The authors propose a local algorithm for distributed k-center, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
a47857ae-7871-51a6-9aee-188509dc5783|Local Algorithm for Distributed k-Means|The authors propose a local algorithm for distributed k-means, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
ac93ec44-c7b2-5dcd-b24c-b37b145c6cf2|Local Algorithm for Distributed k-Median with Outliers|The authors propose a local algorithm for distributed k-median with outliers, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
c5f6a83f-5c65-5772-b1cf-d3287e2ff567|Local Algorithm for Distributed k-Center with Outliers|The authors propose a local algorithm for distributed k-center with outliers, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
c4d0f1ff-e1ce-539a-925f-2e487472aa3c|Local Algorithm for Distributed k-Means with Outliers|The authors propose a local algorithm for distributed k-means with outliers, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
8ce78a6b-f4ae-5ce2-b284-3c3439ac1803|Local Algorithm for Distributed Facility Location with Outliers|The authors propose a local algorithm for distributed facility location with outliers, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
a062cace-1931-5348-9da9-5394e554e48e|Local Algorithm for Distributed k-Median with Multiple Facilities|The authors propose a local algorithm for distributed k-median with multiple facilities, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
20ffa8d4-6bef-5c48-9c9a-4a14b79576fb|Local Algorithm for Distributed k-Center with Multiple Facilities|The authors propose a local algorithm for distributed k-center with multiple facilities, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
2172b1ca-57a6-507b-87bc-1a8113514db2|Local Algorithm for Distributed k-Means with Multiple Facilities|The authors propose a local algorithm for distributed k-means with multiple facilities, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
7e10af8b-156f-5be0-a480-1981de954e46|Local Algorithm for Distributed Facility Location with Multiple Facilities|The authors propose a local algorithm for distributed facility location with multiple facilities, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
3f90bb75-9476-5d62-9632-889ad1646f75|Local Algorithm for Distributed k-Median with Multiple Facilities and Outliers|The authors propose a local algorithm for distributed k-median with multiple facilities and outliers, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
a1d5f284-a558-5efc-970c-d8b2dba4e72c|Local Algorithm for Distributed k-Center with Multiple Facilities and Outliers|The authors propose a local algorithm for distributed k-center with multiple facilities and outliers, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
69d800d6-7534-54ca-ab58-d63888c35d52|Local Algorithm for Distributed k-Means with Multiple Facilities and Outliers|The authors propose a local algorithm for distributed k-means with multiple facilities and outliers, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
3565e41d-6f22-5072-a09f-46367320e8bc|Local Algorithm for Distributed Facility Location with Multiple Facilities and Outliers|The authors propose a local algorithm for distributed facility location with multiple facilities and outliers, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
d209ced4-c007-530a-a235-af8c33c7a3db|Local Algorithm for Distributed k-Median with Multiple Facilities, Outliers, and Non-Uniform Capacities|The authors propose a local algorithm for distributed k-median with multiple facilities, outliers, and non-uniform capacities, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
0cd75202-3334-574b-b511-e4bfcee73dd9|Local Algorithm for Distributed k-Center with Multiple Facilities, Outliers, and Non-Uniform Capacities|The authors propose a local algorithm for distributed k-center with multiple facilities, outliers, and non-uniform capacities, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
7fd0ed14-68b3-5ff9-a4a2-1b479568e24e|Local Algorithm for Distributed k-Means with Multiple Facilities, Outliers, and Non-Uniform Capacities|The authors propose a local algorithm for distributed k-means with multiple facilities, outliers, and non-uniform capacities, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
03558904-3b63-569c-bda7-d57ac12680ec|Local Algorithm for Distributed Facility Location with Multiple Facilities, Outliers, and Non-Uniform Capacities|The authors propose a local algorithm for distributed facility location with multiple facilities, outliers, and non-uniform capacities, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
cbb60a10-6c1b-5ab8-ba4b-99a723d5addb|Local Algorithm for Distributed k-Median with Multiple Facilities, Outliers, Non-Uniform Capacities, and Non-Uniform Weights|The authors propose a local algorithm for distributed k-median with multiple facilities, outliers, non-uniform capacities, and non-uniform weights, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
4247cd08-22a8-5e91-aef3-aed0b2511e22|Local Algorithm for Distributed k-Center with Multiple Facilities, Outliers, Non-Uniform Capacities, and Non-Uniform Weights|The authors propose a local algorithm for distributed k-center with multiple facilities, outliers, non-uniform capacities, and non-uniform weights, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
8b9e058b-ecfe-55b9-80d6-5d695e800d65|Local Algorithm for Distributed k-Means with Multiple Facilities, Outliers, Non-Uniform Capacities, and Non-Uniform Weights|The authors propose a local algorithm for distributed k-means with multiple facilities, outliers, non-uniform capacities, and non-uniform weights, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
7d3be3d3-50b3-5cf4-835f-3176e4c16823|Local Algorithm for Distributed Facility Location with Multiple Facilities, Outliers, Non-Uniform Capacities, and Non-Uniform Weights|The authors propose a local algorithm for distributed facility location with multiple facilities, outliers, non-uniform capacities, and non-uniform weights, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
96fc8890-3fa9-543a-88f6-4715a729a213|Local Algorithm for Distributed k-Median with Multiple Facilities, Outliers, Non-Uniform Capacities, Non-Uniform Weights, and Non-Uniform Demands|The authors propose a local algorithm for distributed k-median with multiple facilities, outliers, non-uniform capacities, non-uniform weights, and non-uniform demands, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
d026e77a-2697-5165-8f07-9fa30f9a2d36|Local Algorithm for Distributed k-Center with Multiple Facilities, Outliers, Non-Uniform Capacities, Non-Uniform Weights, and Non-Uniform Demands|The authors propose a local algorithm for distributed k-center with multiple facilities, outliers, non-uniform capacities, non-uniform weights, and non-uniform demands, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
d2f71e7a-06a2-51cc-a335-88fbda97bdf7|Local Algorithm for Distributed k-Means with Multiple Facilities, Outliers, Non-Uniform Capacities, Non-Uniform Weights, and Non-Uniform Demands|The authors propose a local algorithm for distributed k-means with multiple facilities, outliers, non-uniform capacities, non-uniform weights, and non-uniform demands, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
e7c0832a-1a7f-5b38-9e50-ae86fff31fff|Local Algorithm for Distributed Facility Location with Multiple Facilities, Outliers, Non-Uniform Capacities, Non-Uniform Weights, and Non-Uniform Demands|The authors propose a local algorithm for distributed facility location with multiple facilities, outliers, non-uniform capacities, non-uniform weights, and non-uniform demands, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
35a1803b-040f-51cb-b72f-bb05f0edd916|Local Algorithm for Distributed k-Median with Multiple Facilities, Outliers, Non-Uniform Capacities, Non-Uniform Weights, Non-Uniform Demands, and Non-Uniform Costs|The authors propose a local algorithm for distributed k-median with multiple facilities, outliers, non-uniform capacities, non-uniform weights, non-uniform demands, and non-uniform costs, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
6baf04a1-be8c-5ba8-b7ab-0dcdafc8e47a|Local Algorithm for Distributed k-Center with Multiple Facilities, Outliers, Non-Uniform Capacities, Non-Uniform Weights, Non-Uniform Demands, and Non-Uniform Costs|The authors propose a local algorithm for distributed k-center with multiple facilities, outliers, non-uniform capacities, non-uniform weights, non-uniform demands, and non-uniform costs, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
015bec9b-4a15-52e6-ac66-50372ab86506|Local Algorithm for Distributed k-Means with Multiple Facilities, Outliers, Non-Uniform Capacities, Non-Uniform Weights, Non-Uniform Demands, and Non-Uniform Costs|The authors propose a local algorithm for distributed k-means with multiple facilities, outliers, non-uniform capacities, non-uniform weights, non-uniform demands, and non-uniform costs, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
a058b0ad-85ff-5f5d-9715-97aa50a57631|Local Algorithm for Distributed Facility Location with Multiple Facilities, Outliers, Non-Uniform Capacities, Non-Uniform Weights, Non-Uniform Demands, and Non-Uniform Costs|The authors propose a local algorithm for distributed facility location with multiple facilities, outliers, non-uniform capacities, non-uniform weights, non-uniform demands, and non-uniform costs, which is a specific method for optimizing communication efficiency in distributed algorithms. This algorithm is designed to achieve a constant factor approximation in a constant number of communication rounds, making it efficient for large-scale distributed systems.
15a857af-004f-576b-86bc-1e43fd9cf3d0|Local Approximation Algorithms for Heterogeneous Graphs|The authors propose local approximation algorithms that can handle heterogeneous graph structures with varying degrees, weights, and sparsity. These algorithms are designed to overcome load imbalance, reduce communication overhead, and enhance memory locality in distributed and GPU-based systems.
4d39ad84-3c91-5cb2-a971-f795f45c1543|Graph Decomposition and Orientation|The authors propose a graph decomposition technique that partitions the graph into smaller subgraphs, each with a bounded degree, to handle heterogeneous graph structures. They also use graph orientation to reduce communication overhead and enhance memory locality.
31cfa778-282c-5d4b-9852-ed3c54968ffb|Randomized Local Algorithms|The authors propose randomized local algorithms that can handle heterogeneous graph structures with varying degrees, weights, and sparsity. These algorithms are designed to overcome load imbalance, reduce communication overhead, and enhance memory locality in distributed and GPU-based systems.
4c78570e-d2cb-50b5-bd5c-3b1559f15621|LP Rounding|The authors propose LP rounding techniques to enhance memory locality and improve performance in handling weighted graphs and irregular network topologies.
08506428-ab52-556e-8b23-62382bb3321d|Local Approximation Algorithm for Semi-Matching|The authors propose a local approximation algorithm for the semi-matching problem, which is a key component in optimizing load balance in distributed systems. The algorithm aims to minimize the total processing delay of all tasks by balancing the load as equally as possible.
ec0a6ea7-755b-5233-8bbd-9b258d904b3e|Distributed Constant Size Problem|The authors propose a framework for solving distributed constant size problems, which is relevant to optimizing load balance in distributed systems. The framework focuses on developing local algorithms that can solve problems in constant time, regardless of the size of the input.
7da6d685-2cbd-5b60-ac4c-4bc985534701|Local Computation for Load Balancing|The authors propose a local computation approach for load balancing in distributed systems. The approach focuses on developing algorithms that can solve load balancing problems in constant time, regardless of the size of the input.
27ed19f4-bee0-55fb-9bb2-5a6eed08510b|Local Algorithms for Efficient Graph Dynamics Processing|The authors propose the use of local algorithms to address the challenge of efficient graph dynamics processing. Local algorithms are designed to run in a constant number of synchronous communication rounds, independent of the number of nodes in the network. This approach enables the efficient processing of dynamic updates in large graphs by minimizing computational costs and iterations.
698a00d3-9688-57fd-afa3-e8c4cb119d5a|Pipelined Routing of Chunks|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by introducing a pipelined routing mechanism for chunks of data. The approach involves breaking down large data into smaller chunks and transmitting them in a pipelined fashion, allowing for more efficient use of communication rounds.
2debe693-7315-563a-ac1d-0f2221829843|MIS Computation with Reduced Message Size|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the message size required for MIS computation. The approach involves using a randomized reduction from general graphs to bipartite graphs, allowing for more efficient message transmission.
06d9abe1-1ba3-5b51-89a7-75cddf03d937|Emulation of Distributed Algorithms on the Conflict Graph|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by introducing an emulation mechanism for distributed algorithms on the conflict graph. The approach involves constructing the conflict graph and emulating the distributed algorithm on it, allowing for more efficient communication.
ef852250-dd26-5aa5-acce-4c9b4d8e07f0|Counting Half Augmenting Paths of Length d|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by introducing a counting mechanism for half augmenting paths of length d. The approach involves using a breadth-first search to count the number of half augmenting paths, allowing for more efficient communication.
22bff832-292b-5810-8427-1428b2bd4909|Distributed Approximate Matching Algorithm|The authors propose a distributed approximate matching algorithm that efficiently processes graph dynamics by utilizing a combination of techniques such as maximal independent set (MIS) computation, augmenting paths, and edge weight functions. This algorithm is designed to minimize computational costs and iterations in large graphs.
95b50df9-e38c-58c2-97c4-2ffe0a162f93|Preprocessing-based Random Walk Sampling|The authors propose a preprocessing-based approach to optimize communication efficiency in distributed random walk sampling algorithms. This solution involves preprocessing short random walks at each node, which are then used to answer subsequent random walk requests. The preprocessing stage reduces the number of communication rounds required for each random walk request.
673adc01-14be-5373-9491-1743b1dfb868|Continuous Random Walk Algorithm|The authors propose a continuous random walk algorithm that can handle a sequence of random walk requests in a distributed network. This solution involves using the preprocessing-based approach to answer each request, and then updating the preprocessing table as needed to ensure that the algorithm can continue to answer requests efficiently.
07f6047f-3002-5a74-9366-9f8c152d798d|Doubling-based Preprocessing|The authors propose a doubling-based preprocessing approach to reduce the number of short random walks required at each node. This solution involves storing short walks in a doubling fashion, where each node stores short walks corresponding to lengths that are powers of 2.
9f08a07b-e227-5c18-94d4-3db7b8a4e8f5|Stitching-based Random Walk Extension|The authors propose a stitching-based approach to extend short random walks to longer walks. This solution involves using a combination of preprocessing and online processing to extend short walks to longer walks.
a947888f-4cde-544e-88a7-c1e091a9b8ac|Preprocessing-based Random Walk Algorithm|The authors propose a preprocessing-based random walk algorithm to efficiently process continuous random walk requests in a distributed network. The algorithm preprocesses short random walks from each node and stores them in a table, which can be used to answer multiple random walk requests without recomputing the walks from scratch.
16f96279-c7de-56ea-98fe-cb7ecd571919|3-Level Degree-Aware 1.5D Graph Partitioning|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a novel graph partitioning method that divides vertices into three levels of degree. Vertices with the highest degree level are delegated on all nodes, while those in the second level are delegated only on columns and rows of the communication mesh. This approach reduces memory consumption by minimizing the number of shared vertices and edges, thereby optimizing memory usage.
dd416a32-85b0-5f14-a88c-db04dd506b58|On-Chip Sorting with RMA|This solution addresses the challenge of memory-efficient scalable graph processing by proposing an efficient on-chip sorting mechanism using Remote Memory Access (RMA). This approach enables the sorting of messages in a distributed manner, reducing memory consumption and communication overhead.
5243aa34-75e7-542f-bb3c-ab2457692ed1|Core Group-Aware Core Subgraph Segmenting|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a core group-aware core subgraph segmenting approach. This approach divides the core subgraph into smaller segments, each processed by a separate core group, reducing memory consumption and communication overhead.
5d8637b6-029d-5415-a91d-a2b45e4c4226|Sub-Iteration Direction Optimization|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a sub-iteration direction optimization approach. This approach optimizes the direction of graph traversal in each sub-iteration, reducing memory consumption and communication overhead.
a1d90420-b249-58ce-9cd2-629cc8d38d0f|CG-Aware Core Subgraph Segmenting|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a CG-aware core subgraph segmenting technique. The technique segments the core subgraph into smaller segments and processes them in parallel, reducing the number of communication rounds.
ffabec79-026b-5db8-9668-b39eaf32f00b|Edge-Aware Vertex Cut Load Balancing|This solution addresses the challenge of optimizing load balance in distributed systems by proposing an edge-aware vertex cut load balancing method. This method calculates the prefix sum of locally available frontier vertices' degree at each EH2EH top-down traversal and divides the frontier by accumulated degrees, generating a balanced workload for each CPE.
5aa82bee-7813-50cf-9bbd-d5d4c5f8147b|On-Chip Sorting with RMA (OCS RMA)|OCS RMA is a novel on-chip sorting algorithm designed to optimize GPU memory access for graph processing. It leverages the Remote Memory Access (RMA) mechanism to enable efficient sorting of random messages into buckets. OCS RMA utilizes the RMA mechanism to send batched messages from producers and consumers, allowing for efficient sorting and minimizing memory access overhead. The algorithm divides cores into producers and consumers, with each consumer responsible for a group of buckets. This approach enables OCS RMA to achieve high memory bandwidth utilization and outperform existing on-chip sorting algorithms. OCS RMA achieves 47.0% memory bandwidth utilization, outperforming the previous on-chip sorting algorithm on SW26010, which had a utilization of 33.7%. This results in a significant improvement in graph processing performance.
931d1270-3673-5c34-b148-dd6b92da5032|Deterministic Distributed Algorithm for Coloring Sparse Graphs|The authors propose a deterministic distributed algorithm for coloring sparse graphs with fewer colors, which addresses the challenge of optimizing communication efficiency in distributed algorithms. The algorithm achieves a round complexity of O(log^3 n) for coloring planar graphs with 6 colors and O(a^4 log^3 n) for coloring graphs of arboricity a with 2a colors.
eea05ad0-ebad-558e-8b57-694f641a08fc|Deterministic Distributed Algorithm for Graph Coloring|The authors propose a deterministic distributed algorithm for graph coloring that efficiently processes dynamic updates in large graphs. The algorithm focuses on minimizing computational costs and iterations by identifying a set of happy vertices that can be easily and efficiently colored, given a partial coloring of the rest of the graph. The algorithm uses a combination of graph-theoretic arguments and distributed computing techniques to identify the set of happy vertices. It employs a ruling forest with respect to the set of happy vertices, which allows for efficient coloring of the graph. The algorithm also uses a partition of the graph into stable sets to reduce the number of colors needed. The authors demonstrate the effectiveness of their algorithm by showing that it can color graphs of arboricity a with 2a colors in O(a^4 log^3 n) rounds, and that it can color planar graphs with 6 colors in O(log^3 n) rounds.
86969c2a-b42d-5730-a9a8-bb65af241dd4|Vertex Marking Functional Block (VMFB)|The authors propose a novel functional block-based approach, called Vertex Marking Functional Block (VMFB), to break down complex graph operations into smaller, similar blocks, reducing redundant computation and improving memory efficiency. VMFB allocates parallel CUDA threads for each element of the operand array, allowing each thread to perform a function and alter flag values in a shared array. The VMFB has three main functional steps: vertex marking, filtering, and gathering. This approach enables the processing of large graphs by minimizing memory usage and reducing communication overhead. The paper demonstrates that the VMFB-based approach achieves significant performance improvements, with up to 5.6x speedup compared to the state-of-the-art GPU-based recomputing approach.
4a7503b5-c5b2-5d1c-b9d7-9622b46c3a12|Dynamic Scheduling with Asynchronous Updates|The authors propose a dynamic scheduling approach with asynchronous updates to improve memory efficiency and scalability in graph processing. This approach allows for the processing of changed edges in batches, reducing memory consumption and communication overhead. The approach uses a combination of dynamic scheduling and asynchronous updates to process changed edges in batches. This enables the reduction of memory usage and communication overhead, making it more efficient for large-scale graph processing. The paper shows that the dynamic scheduling approach with asynchronous updates achieves significant performance improvements, with up to 5.6x speedup compared to the state-of-the-art GPU-based recomputing approach.
df315d7c-1420-56f0-a237-d354b319cbbb|Compressed Sparse Row (CSR) Data Structure|The authors propose the use of a Compressed Sparse Row (CSR) data structure to store the graph, which is a memory-efficient representation of sparse matrices. The CSR data structure is used to store the graph, which enables the reduction of memory usage and improves memory efficiency. The paper demonstrates that the use of the CSR data structure achieves significant memory savings, making it more efficient for large-scale graph processing.
163c8b7b-f830-54c8-85bd-421414a56482|Packed Compressed Sparse Row (PCSR) Data Structure|The authors propose the use of a Packed Compressed Sparse Row (PCSR) data structure to store the graph, which is a memory-efficient representation of sparse matrices. The PCSR data structure is used to store the graph, which enables the reduction of memory usage and improves memory efficiency. The paper demonstrates that the use of the PCSR data structure achieves significant memory savings, making it more efficient for large-scale graph processing.
88c430b6-2ffd-5ad4-ab89-3f9675664d4d|Dynamic CSR-based Data Structure (DCSR)|The authors propose the use of a Dynamic CSR-based Data Structure (DCSR) to store the graph, which is a memory-efficient representation of sparse matrices. The DCSR data structure is used to store the graph, which enables the reduction of memory usage and improves memory efficiency. The paper demonstrates that the use of the DCSR data structure achieves significant memory savings, making it more efficient for large-scale graph processing.
4df6b6ac-5f2e-5ba1-a80e-3c3a3b78c1a3|Asynchronous Updates|The authors propose an asynchronous update approach to optimize load balance in distributed systems. This approach involves allowing threads to synchronize less frequently, reducing the overhead of synchronization constructs.
aa40b499-e356-5db5-b336-e0ffb3240ab7|Iterative Vertex Marking Functional Block (VMFB) Approach|The authors propose an iterative VMFB approach to efficiently update the Single Source Shortest Path (SSSP) in large-scale dynamic networks. This solution specifically addresses the challenge of efficient graph dynamics processing by identifying the subgraphs affected by changes and updating only these subgraphs.
ddf0c596-5d03-5823-9485-eecbb35e768a|Asynchronous Update of SSSP|The authors propose an asynchronous update of SSSP to reduce synchronization overhead. This solution specifically addresses the challenge of efficient graph dynamics processing by allowing threads to synchronize less frequently.
8150d76b-412b-54bf-aa5b-7c76b0c5f2d6|Batch Processing of Changed Edges|The authors propose batch processing of changed edges to improve performance. This solution specifically addresses the challenge of efficient graph dynamics processing by processing changed edges in batches.
d522cead-7211-5920-b465-2db3285bfed4|Shared Memory Implementation|The authors propose a shared memory implementation to efficiently update SSSP in large-scale dynamic networks. This solution specifically addresses the challenge of efficient graph dynamics processing by leveraging shared memory parallelism.
a1adb038-6ac1-5532-b41c-971705ba6c3b|GPU Implementation|The authors propose a GPU implementation to efficiently update SSSP in large-scale dynamic networks. This solution specifically addresses the challenge of efficient graph dynamics processing by leveraging GPU parallelism.
298ce0f9-e00c-5724-9c38-b12d9325cead|Color Selection Mechanism|The authors propose a color selection mechanism that allows each vertex to choose a color based on its degree and the degrees of its neighbors. This mechanism is designed to minimize the number of colors used while ensuring that no two adjacent vertices have the same color.
daf89092-9a9a-5b91-bf15-abf3f37da33d|Conflict Resolution Mechanism|The authors propose a conflict resolution mechanism that allows vertices to resolve conflicts by recomputing their orders within the set of neighboring vertices that have the same color.
8813e08a-6f36-598e-884d-548341b95fd4|Message Reduction Mechanism|The authors propose a message reduction mechanism that reduces the number of messages sent between vertices by only sending messages when necessary.
bf7c8760-80e7-57f4-91ab-9d8856c41629|DistG Distributed Graph Coloring Algorithm|The DistG algorithm is a distributed graph coloring algorithm designed for the vertex-centric model, specifically tailored to handle heterogeneous and irregular graphs. It addresses the challenge by proposing a novel approach to graph coloring that adapts to the varying degrees, weights, and sparsity of the graph structure.
f297b5a8-8412-52d5-9628-20da0125989c|Distributed Algorithm for Minimum Dominating Set Problem|The authors propose a distributed algorithm for the minimum dominating set problem, which is designed to be memory-efficient and scalable for processing massive graphs within distributed computing environments. The algorithm uses a local computation approach, where each node in the graph communicates with its neighbors to determine the minimum dominating set. The algorithm iteratively marks nodes as part of the dominating set based on their weights and the weights of their neighbors. The authors demonstrate the effectiveness of their algorithm through experiments on various graph benchmarks, showing that it achieves good results in terms of solution quality and running time.
df26b6bf-ab44-5c9f-b1d6-58aee738bd31|Local Computation with Edge Weighting and Configuration Checking|This solution involves using local computation with edge weighting and configuration checking to optimize communication efficiency in distributed algorithms. The authors propose a local search algorithm that incorporates edge weighting and configuration checking to reduce the number of communication rounds required to achieve a solution.
e7f9e8e6-1cc6-5e24-a1aa-34890f381460|Distributed Algorithm with Neighbor Selection|This solution involves using a distributed algorithm that selects neighbors based on their weights to optimize communication efficiency. The authors propose a distributed algorithm that selects neighbors with maximum weights to reduce the number of communication rounds required to achieve a solution.
9777345c-1d4c-53a2-b70d-629bb40f4ce4|Distributed Algorithm for Minimum Dominating Set (MDS) Problem|The authors propose a distributed algorithm for the MDS problem, which is an extension of a previous algorithm. The algorithm is designed to work in a distributed model, where each node in the graph has a unique identifier and can communicate with its neighbors. The algorithm iteratively marks nodes as part of the dominating set based on their weights and the weights of their neighbors.
83a77772-6b80-5088-9794-9266d3602871|Local Algorithm for Minimum Total Dominating Set (MTDS) Problem|The authors propose a local algorithm for the MTDS problem, which is a variant of the MDS problem. The algorithm is designed to work in a distributed model, where each node in the graph has a unique identifier and can communicate with its neighbors. The algorithm iteratively marks nodes as part of the total dominating set based on their weights and the weights of their neighbors.
58e4911b-6447-5957-a3fb-d9b9286a1a2c|Extension of the Algorithm for Minimum Distance Dominating Set (MDDS) Problem|The authors propose an extension of their algorithm for the MDDS problem, which is a variant of the MDS problem. The algorithm is designed to work in a distributed model, where each node in the graph has a unique identifier and can communicate with its neighbors. The algorithm iteratively marks nodes as part of the distance dominating set based on their weights and the weights of their neighbors.
469ab217-16fa-5358-a4ee-b79b7f06f71b|Pipelined Algorithm for Weighted Shortest Paths|The authors propose a pipelined algorithm for computing weighted shortest paths in a distributed environment, which addresses the challenge of memory-efficient scalable graph processing by reducing memory consumption and optimizing memory usage.
d597ab35-b9e8-5125-af46-56566f489687|Simplified Short-Range Algorithm|The authors propose a simplified short-range algorithm for computing weighted shortest paths in a distributed environment, which addresses the challenge of memory-efficient scalable graph processing by reducing memory consumption and optimizing memory usage.
7999809e-9a96-5ff7-8696-40c9e397299f|Randomized Algorithm for Weighted APSP|The authors propose a randomized algorithm for computing weighted all-pairs shortest paths in a distributed environment, which addresses the challenge of memory-efficient scalable graph processing by reducing memory consumption and optimizing memory usage.
1ee97dfd-dae9-5c7c-8e62-449504b667b7|Approximation Algorithm for Weighted APSP|The authors propose an approximation algorithm for computing weighted all-pairs shortest paths in a distributed environment, which addresses the challenge of memory-efficient scalable graph processing by reducing memory consumption and optimizing memory usage.
7e84379d-b9ae-5f71-8814-871f2763cbeb|Pipelined Algorithm for Updating Scores at Descendants of a Newly Chosen Blocker Node|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a pipelined algorithm for updating scores at descendants of a newly chosen blocker node. The algorithm ensures that each node receives at most one message in a given round, reducing the number of communication rounds required.
64904846-7627-5a6a-8197-5ffac25f2f29|Simplified Short-Range Algorithm for Single Source|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a simplified short-range algorithm for single source. The algorithm reduces the number of communication rounds required by only sending messages when the current estimate of the shortest path distance satisfies a certain condition.
21673d98-9538-579f-8465-0af82ae5b631|Improved Blocker Set Algorithm|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing an improved blocker set algorithm. The algorithm reduces the number of communication rounds required by developing faster algorithms for two steps that take O(nh) rounds in the original algorithm.
f19f17ed-f958-5d43-a8e7-7c60b5546386|Randomized Algorithm for Weighted APSP with Arbitrary Edge Weights|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a randomized algorithm for weighted APSP with arbitrary edge weights. The algorithm reduces the number of communication rounds required by using a randomized framework.
02efa23b-83c0-5421-b001-344dc44f1302|Deterministic Algorithm for 1-Approximation Weighted APSP with Non-Negative Integer Edge Weights|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a deterministic algorithm for 1-approximation weighted APSP with non-negative integer edge weights. The algorithm reduces the number of communication rounds required by using a pipelined algorithm.
992a5594-ea86-5200-bde9-5ea006331577|Approximate APSP Algorithm|The authors propose an approximate algorithm for computing weighted all-pairs shortest paths (APSP) in graphs with non-negative integer edge weights. This algorithm is designed to handle heterogeneous graph structures and irregular memory access patterns.
75b33e52-e4c7-51ee-b4c6-f5c91bb521f7|Pipelined Algorithm for Load Balancing|The authors propose a pipelined algorithm to optimize load balance in distributed systems. This algorithm is designed to handle non-negative integer edge weights, including zero weights, and can be used for both directed and undirected graphs. The algorithm works by constructing a blocker set, which is a set of nodes that can be used to reduce the load on other nodes in the system. The blocker set is computed using a greedy algorithm that selects nodes that lie in the maximum number of paths in the h-hop trees that have not yet been covered by the already selected blocker nodes.
2cb43181-8d49-5877-bfd0-6746722b2949|Randomized Algorithm for Load Balancing|The authors propose a randomized algorithm to optimize load balance in distributed systems. This algorithm is designed to handle arbitrary edge weights and can be used for both directed and undirected graphs. The algorithm works by using a randomized approach to select nodes to be added to the blocker set.
17b10ba4-a90d-5b9c-becd-c0ff958b06ff|Blocker Set Algorithm for Weighted Shortest Paths|The authors propose a blocker set algorithm for computing weighted shortest paths in graphs with non-negative edge weights, including zero-weight edges. This algorithm is designed to minimize computational costs and iterations by identifying a set of nodes that can be used to block updates and reduce the number of iterations required.
95716b7f-022c-52c0-aa8d-85d77ca5dd9b|Simplified Short-Range Algorithm for Weighted Shortest Paths|The authors propose a simplified short-range algorithm for computing weighted shortest paths in graphs with non-negative edge weights, including zero-weight edges. This algorithm is designed to minimize computational costs and iterations by reducing the number of iterations required for computing shortest paths.
e054cefe-52e8-55a7-946c-ff3cfbe0be36|Randomized Algorithm for Weighted Shortest Paths|The authors propose a randomized algorithm for computing weighted shortest paths in graphs with arbitrary edge weights. This algorithm is designed to minimize computational costs and iterations by using randomization to reduce the number of iterations required.
95377e9f-55c7-58f4-b6a9-83b942938c68|Approximation Algorithm for Weighted Shortest Paths|The authors propose an approximation algorithm for computing weighted shortest paths in graphs with non-negative edge weights, including zero-weight edges. This algorithm is designed to minimize computational costs and iterations by using approximation techniques to reduce the number of iterations required.
8d308d82-8cff-5983-a671-d3112fb0bc3b|Local Smallest Largest Degree First Algorithm|This solution proposes a novel graph coloring algorithm that addresses the challenge of memory-efficient scalable graph processing by utilizing a greedy approach with a focus on the smallest and largest degree vertices between neighbors. The algorithm iteratively identifies the smallest and largest degree vertices in each superstep and assigns different colors to them, ensuring that no neighboring vertices have the same color. This approach reduces the number of supersteps required, leading to improved runtime performance and memory efficiency. The paper presents experimental results showing that the Local Smallest Largest Degree First algorithm outperforms other heuristic-based algorithms in terms of runtime and number of colors used.
30effe25-75cb-5220-809e-5e6818a3bd7c|Local Largest Degree First Algorithm|This solution proposes a graph coloring algorithm that addresses the challenge of memory-efficient scalable graph processing by utilizing a greedy approach with a focus on the largest degree vertices between neighbors. The algorithm iteratively identifies the largest degree vertices in each superstep and assigns colors to them, ensuring that no neighboring vertices have the same color. This approach reduces the number of supersteps required, leading to improved runtime performance and memory efficiency. The paper presents experimental results showing that the Local Largest Degree First algorithm performs better than other heuristic-based algorithms in terms of number of colors used, but takes more computation time.
8c458932-8ee0-5b09-b901-5f93cb98dee0|Local Minima Maxima First Algorithm|This solution proposes a graph coloring algorithm that addresses the challenge of memory-efficient scalable graph processing by utilizing a greedy approach with a focus on the minimum and maximum values between neighbors. The algorithm iteratively identifies the minimum and maximum values in each superstep and assigns different colors to them, ensuring that no neighboring vertices have the same color. This approach reduces the number of supersteps required, leading to improved runtime performance and memory efficiency. The paper presents experimental results showing that the Local Minima Maxima First algorithm performs better than the Local Maxima First algorithm in terms of runtime, but takes almost the same number of colors.
97e0b522-6b48-5ccf-a3c6-6fa7c7745609|Local Maxima First Algorithm|This solution proposes a graph coloring algorithm that addresses the challenge of memory-efficient scalable graph processing by utilizing a greedy approach with a focus on the maximum values between neighbors. The algorithm iteratively identifies the maximum values in each superstep and assigns colors to them, ensuring that no neighboring vertices have the same color. This approach reduces the number of supersteps required, leading to improved runtime performance and memory efficiency. The paper presents experimental results showing that the Local Maxima First algorithm performs poorly in terms of number of colors used and computation time compared to other heuristic-based algorithms.
dffd07b7-1934-5b63-b35f-c50984000aa6|2D Cyclic Distribution of Adjacency Matrix|The authors propose a 2D cyclic distribution of the adjacency matrix to balance the computations and reduce the communication overheads. This approach structures the communication and computational steps to reduce memory overhead and leverage the sparsity of the graph.
6ed575ec-d66e-50ea-85ea-b7d3b1746134|Doubly Compressed Sparse Row (CSR) Structure|The authors use a doubly compressed sparse row structure to store the task matrix and the upper and lower triangular portions of the adjacency matrix. This approach reduces memory overhead by avoiding unnecessary storage of empty adjacency lists.
5069ed0f-7b76-5335-952c-d0e8d767fcd8|Modifying Hashing Routine for Sparser Vertices|The authors modify the hashing routine to take advantage of the sparsity of the graph. This approach reduces the number of unnecessary intersection operations and improves performance.
7be4abb4-7de3-5286-9d42-913749af97aa|Reducing Overheads Associated with Communication|The authors reduce the overheads associated with communication by allocating memory for the adjacency lists and using a sequence of communication steps similar to Cannons parallel matrix-matrix multiplication algorithm.
6d354907-d8ac-543c-ba12-723a7d77d1f3|2D Cyclic Distribution of the Graph|The authors propose a 2D cyclic distribution of the graph to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This approach involves distributing the graph across a 2D processor grid, where each processor is responsible for a subset of the graphs vertices and their corresponding adjacency lists. The 2D cyclic distribution is designed to balance the workload among processors and reduce communication overhead by minimizing the number of messages exchanged between processors.
b4ac7530-1822-520d-acc1-ec34e76e5f12|Map-Based Triangle Counting with Hashing|The authors propose a map-based triangle counting approach with hashing to improve the performance of triangle counting on heterogeneous and irregular graphs. This approach involves using a hash map to store the adjacency list of each vertex and then looking up the vertices in the hash map to find triangles.
52171ca8-2668-5074-8813-e343e6c9778a|Redundant Work Reduction|The authors propose a technique to reduce redundant work in the triangle counting phase. This approach involves counting the number of tasks that result in the map-based set intersection operation and reducing the number of tasks that do not contribute to the final result.
91426f79-8d87-5794-801b-c0abaaf4c1b5|2D Cyclic Distribution of C|The authors propose a 2D cyclic distribution of the task matrix C to balance the load among processors. This approach ensures that each processor has a similar number of non-zero tasks and a similar number of light and heavy tasks, reducing load imbalance.
84a07c43-c798-5f81-a61c-aa27cf3c7e41|Modifying the Hashing Routine for Sparser Vertices|The authors propose modifying the hashing routine to optimize the set intersection operation for sparser vertices. This approach reduces the number of unnecessary intersection operations and improves the overall performance.
5a24d746-1f64-55aa-8538-772369c63a57|2D Parallel Triangle Counting Algorithm|The authors propose a 2D parallel triangle counting algorithm for distributed memory architectures, which utilizes a 2D cyclic decomposition to balance computations and reduce communication overheads. This algorithm is designed to efficiently process large graphs in a distributed memory setting.
029036a7-98da-56c9-8f67-18709389fed3|Event-driven Computation Model|The authors propose an event-driven computation model to address the challenge of memory-efficient scalable graph processing. This model expresses computations as events, which are generated when the value of a vertex changes, to update the vertex properties. The events are stored in a coalescer queue, which eliminates the need for atomic operations and reduces memory accesses.
358b9ddc-eeef-544f-8f06-879626443be9|In-place Coalescing and Retrieval|The authors propose an in-place coalescing and retrieval technique to reduce memory accesses and improve memory efficiency. This technique uses a direct-mapped event storage approach, where events are stored in a structured manner to facilitate coalescing and retrieval.
8218f292-d9fa-5b4a-bc9b-e818ac86a2be|Prefetching and Streaming Scheduler|The authors propose a prefetching and streaming scheduler technique to improve memory efficiency and reduce memory accesses. This technique uses a prefetching scheme to prefetch vertex properties and a streaming scheduler to schedule events.
57a22eb1-9be0-51f3-9afa-aee48af437ec|Binning and Spatial Locality|The authors propose a binning and spatial locality technique to improve memory efficiency and reduce memory accesses. This technique uses a binning approach to group events and improve spatial locality.
3eea7a86-2298-59b0-8939-b593d641676f|Lookahead and Event Execution Prole|The authors propose a lookahead and event execution prole technique to improve memory efficiency and reduce memory accesses. This technique uses a lookahead approach to predict event execution and improve memory efficiency.
6e802e2f-5e1b-5de3-9d29-86102edd6fd5|Event Coalescing|The authors propose an event coalescing technique to reduce the number of events and, subsequently, event storage and processing overheads. This technique combines multiple events carrying deltas to the same vertex using a reduce operator specific to the application.
62734309-7417-5734-a3a1-7de0c37a45e2|In-Place Coalescing|The authors propose an in-place coalescing technique to reduce the memory pressure and congestion caused by buffering uncoalesced events. This technique uses a direct-mapped event storage to coalesce events in-place, eliminating the need for large memory.
03b83aa9-8e75-580f-bed8-2a7ee6aba037|Event-Driven Graph Processing Model|The authors propose an event-driven graph processing model that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs. This model expresses computations as events, typically generated when the value of a vertex changes, to update neighboring vertices. The event-driven approach decouples the communication and control tasks of the graph computation, allowing for more efficient processing of heterogeneous graph structures.
e5428036-1656-5380-888c-ec956b0bccfa|In-Place Coalescing and Retrieval|The authors propose an in-place coalescing and retrieval technique to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This technique uses a direct-mapped event storage structure to coalesce events destined to the same vertex, reducing the number of events and memory accesses.
4255e26f-c0a0-577b-af0a-09833af1acc0|Coalesced Memory Access via Event-Driven Processing|The authors propose an event-driven processing model that coalesces memory accesses to optimize GPU memory access for graph processing. This approach involves processing events in a coalesced manner, reducing the number of memory accesses and minimizing synchronization overhead.
9f28af4a-dd8d-500b-bca3-f041eb8b06b2|Lookahead and Binning|The authors propose a lookahead and binning approach to reduce memory accesses and improve memory efficiency. This approach involves looking ahead to identify events that can be coalesced and binning events to reduce memory accesses.
575170f1-0a1d-53fe-b3dc-b46a9880cdb4|Partitioning and Inter-Slice Communication|The authors propose a partitioning approach to handle large graphs that do not fit in on-chip memory. This approach involves partitioning the graph into multiple slices and communicating events between slices.
3e40eaf5-1b79-5439-a8e3-561ef80388c5|Parallel Sliding Windows (PSW)|PSW is a mechanism designed to process very large graphs from disk, requiring only a small number of non-sequential accesses to the disk.
26abcc93-4418-51e2-9ea5-f9bb6cfe2c50|GAS Decomposition|GAS decomposition is a technique used to factor vertex programs over edges, allowing for the distribution of computation of a single vertex-program over the entire cluster.
61c13514-30bd-5cee-bdca-de770bbf8fec|PowerGraph's Partitioning Scheme|PowerGraph's partitioning scheme is designed to cut the vertex set in a way that the edges of a high-degree vertex are handled by multiple workers, reducing the degree-dependent communication overhead.
542b7a19-6542-5017-b5b9-e33bc0f4241e|Trinity's Memory Cloud|Trinity's memory cloud is a distributed key-value store that supports a memory storage module and a message passing framework, allowing for efficient graph computation.
099f5056-2457-53d9-94e0-4c5f7e7035f9|GraphChi's Selective Scheduling|GraphChi's selective scheduling mechanism is designed to converge faster on some parts of the graph, especially on those where the change on values is significant.
2cc2f8e5-3ceb-50af-baca-26e6b55eb455|GRACE's Asynchronous Execution|GRACE's asynchronous execution is designed to combine synchronous programming with asynchronous execution for large-scale graph processing.
0d8657fb-08a0-5ab0-9ff3-2c1501511aed|Signal Collect's Graph Algorithms|Signal Collect's graph algorithms are designed to support fast graph exploration as well as efficient parallel graph computations.
4901745f-fd57-5718-8146-c4aa331d2163|TurboGraph's Column-View Matrix Vector Multiplication|TurboGraph's column-view matrix vector multiplication is designed to restrict the computation to just a subset of the vertices, utilizing two types of thread pools and a buffer manager.
da94cdfe-2b05-5529-bf22-427c1885de78|GraphX's Resilient Distributed Graph (RDG)|GraphX's RDG is designed to associate records with vertices and edges in a graph and provide a collection of expressive computational primitives.
4917eec8-5860-56f7-a89d-178747269212|PowerGraph Partitioning Scheme|The PowerGraph partitioning scheme is designed to tackle the challenge of handling heterogeneous graph structures by cutting the vertex set in a way that the edges of a high degree vertex are handled by multiple workers. This approach eliminates the degree dependent partitioning problem and allows for more efficient processing of irregular graphs.
fa0874d2-020c-5478-bc8f-4021bcb52c09|GraphChi Selective Scheduling|GraphChi's selective scheduling mechanism attempts to converge faster on some parts of the graph, especially on those where the change on values is significant. This approach helps to reduce the processing time for irregular graphs.
8a24905f-6fdb-5bd2-bb87-9bb71bdd9668|Dynamic Repartitioning Strategy|The authors propose a dynamic repartitioning strategy to optimize load balance in distributed systems. This strategy involves exchanging vertices between workers based on the amount of data sent by each vertex, aiming to balance the workload among all workers and reduce the number of exchanged messages over the network.
97ba85a6-0cf2-58e1-953a-3457fc8fb073|PECO Map Function|The PECO map function is designed to efficiently process the adjacency list of a graph and emit the necessary information for the reduce tasks to construct the induced subgraph Gv. The map function takes as input a single line of the adjacency list and sends the tuple v, v to each neighbor of v. This information is enough for the reducer for vertex v to construct the graph Gv. The map function is trivial and uses memory equal to the size of a single adjacency list entry, which is of the order of the maximum degree of a vertex in the graph.
4232c984-3eb2-50f6-8168-33b1a41e69b2|PECO Reduce Function|The PECO reduce function is designed to efficiently enumerate all maximal cliques in the induced subgraph Gv. The reduce function uses a modified version of the Tomita algorithm, which is a worst-case optimal algorithm for enumerating all maximal cliques. The algorithm uses a total ordering among vertices in Gv to avoid duplicate enumeration of cliques. The reduce function also uses a Fini set to keep track of vertices that have already been processed, which helps to avoid redundant work.
20a2f74c-dbe4-5427-b09b-b2e202bc5630|Vertex Ordering|The vertex ordering is a technique used to improve load balancing among parallel tasks. The vertex ordering is used to assign vertices to reduce tasks in a way that balances the workload. The ordering is based on the degree of each vertex, which helps to distribute the workload evenly among the reduce tasks.
3b5935b9-bcca-582d-976d-46cd85a14deb|Triangle Counting|The triangle counting is a technique used to improve load balancing among parallel tasks. The triangle counting is used to assign vertices to reduce tasks in a way that balances the workload. The ordering is based on the number of triangles that each vertex is part of, which helps to distribute the workload evenly among the reduce tasks.
70599828-2b87-5e5a-b099-f2693e6aa048|Vertex Ordering for Reduced Communication|The authors propose a solution that involves ordering the vertices in the graph to reduce the communication cost. Specifically, they use a vertex ordering that minimizes the number of edges that need to be transmitted between nodes.
4bce23e3-a2be-5802-97c0-7612f50fd54a|Subgraph Division for Load Balancing|The authors propose a solution that involves dividing the graph into subgraphs to achieve load balancing. Specifically, they divide the graph into subgraphs Gv, where each subgraph is induced by a vertex v and its neighbors.
aa13faef-30d0-52c1-8dbb-27d51588a259|Modified Tomita Algorithm for Reduced Redundant Work|The authors propose a solution that involves modifying the Tomita algorithm to reduce redundant work among nodes. Specifically, they add the entire set of vertices in Lv to the Fini set, which allows them to avoid searching for maximal cliques that contain a vertex from Lv.
eb1f3e54-607d-5202-bbd2-387b998625fe|PECO Parallel Enumeration of Cliques using Ordering|PECO is a parallel algorithm for Maximal Clique Enumeration (MCE) that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by using a carefully chosen total ordering among all vertices in the graph. This ordering is used to eliminate redundant work among processors and improve load balancing.
327aa7ba-9b73-5865-87d9-04447ebb6476|PECO - Parallel Enumeration of Cliques using Ordering|PECO is a parallel algorithm designed to optimize load balance in distributed systems by utilizing a carefully chosen total ordering among all vertices in the graph. This ordering is used to eliminate redundant work among processors and to improve load balancing.
cfd007c9-b0a8-5426-b048-daeecb7f689b|Degree Ordering|Degree ordering is a technique used in PECO to optimize load balance by ordering vertices based on their degree. This approach helps to improve load balancing by assigning vertices with higher degrees to reduce tasks earlier.
70b7357e-48f5-5e0f-8530-bf146d53f812|Triangle Ordering|Triangle ordering is a technique used in PECO to optimize load balance by ordering vertices based on the number of triangles they are part of. This approach helps to improve load balancing by assigning vertices that are part of more triangles to reduce tasks earlier.
47215345-2eb1-543e-b70a-0604e6f1be1c|PECO (Parallel Enumeration of Cliques using Ordering)|PECO is a parallel algorithm designed to efficiently enumerate maximal cliques in large graphs using the MapReduce framework. It addresses the challenge of efficient graph dynamics processing by utilizing a total ordering of vertices to eliminate redundant work and improve load balancing among processors.
c3ad8d6a-9f34-5f80-9b1f-35312e7885b2|Asynchronous Messaging Mechanism|The authors propose an asynchronous messaging mechanism that allows messages to be processed as soon as they are received, rather than waiting for the next superstep. This mechanism can reduce the number of network messages and improve performance.
f3b3fd08-09a6-5bf4-9120-a35db8450b0f|Accumulative Iterative Update BSP Algorithm|The authors propose an accumulative iterative update BSP algorithm that accumulates intermediate updates to an existing PageRank value, rather than propagating the updates immediately. This algorithm can reduce the number of network messages and improve performance.
03aef6e4-4604-5187-9c8d-3047d496375c|GraphHP Platform|The authors propose the GraphHP platform, which is designed to support the hybrid execution model and asynchronous messaging mechanism. The platform provides a vertex-centric programming interface and supports bulk vertex processing without heavy scheduling overhead.
cc141755-3fe9-5e54-a320-9c3767412982|Pseudo Superstep Iteration|The authors propose a pseudo superstep iteration mechanism that iteratively processes active local vertices within a partition until they all become inactive. This mechanism is designed to reduce the number of global iterations and synchronization overhead.
816f189c-18bd-5241-b371-fce0fa7e3a55|Hybrid Execution Model for Load Balancing|The authors propose a hybrid execution model that differentiates between computations within a graph partition and across partitions, decoupling computations within a partition from distributed synchronization and communication. This approach enables the reduction of global iteration frequency, thereby optimizing load balance in distributed systems.
c0577dd3-7c13-5bfe-bbe8-519eb5d467dd|Combiner Functionality|The authors propose a combiner functionality that combines multiple messages intended for a vertex into a single message, reducing communication overhead.
d7cef4e6-cb1f-53fb-b97b-556408e9d6f6|Hybrid Communication Mechanism|The authors propose a hybrid communication mechanism that performs message passing between vertices within a same partition directly in memory, reducing communication overhead.
d258cf5f-b1ba-56f5-ac54-7f7e1be33c16|Hybrid Execution Model for Graph Dynamics Processing|The authors propose a hybrid execution model that combines the benefits of BSP and asynchronous processing to efficiently handle graph dynamics. This model allows for the execution of a sequence of pseudo-supersteps for local computations at each global iteration, reducing the frequency of global iterations and communication overhead.
855a23e9-089c-523d-81c4-546716f0d7c2|Asynchronous Messaging Mechanism for Graph Updates|The authors propose an asynchronous messaging mechanism that allows for the optimization of message passing between vertices within a partition, reducing the number of iterations and network messages required for graph updates.
e91258a1-d392-5a60-a1c5-5791e59f1dcf|Pseudo-Superstep Iteration for Local Computations|The authors propose the use of pseudo-superstep iteration for local computations within a partition, allowing for the efficient execution of iterative algorithms on dynamic graphs.
c53e4803-4511-5fee-aca0-c0389f0de01c|Graph Partitioning for Efficient Graph Dynamics Processing|The authors propose the use of graph partitioning techniques to divide the input graph into smaller partitions, enabling the efficient processing of graph updates and reducing the computational costs.
f0a0e28b-e75a-5331-a04b-f94f7682b31a|Edge-Centric Scatter-Gather|The authors propose an edge-centric approach to the scatter-gather model, which involves iterating over edges and updates on edges rather than over vertices. This approach avoids random access into the set of edges, instead streaming them from storage.
0472dd8c-26e4-583d-bbf3-7f65b44e614c|Multi-Stage Shuffle|The authors propose a multi-stage shuffle approach to enable parallelism in the shuffle phase. This approach involves assigning disjoint equally sized chunks of the stream buffer to threads, allowing them to work in parallel without needing synchronization.
3cf00bb3-32ec-591b-b860-a1b330c812a6|Edge-Centric Scatter-Gather Model|The authors propose an edge-centric scatter-gather model as a solution to optimize communication efficiency in distributed algorithms. This model focuses on streaming edges and updates rather than performing random access through an index, which reduces the number of communication rounds and improves overall efficiency.
1999ef06-06b1-5bac-9d57-1e9efe74c49f|Multi-Stage Shuffling|The authors propose the use of multi-stage shuffling to optimize communication efficiency in distributed algorithms. Multi-stage shuffling reduces the number of communication rounds by allowing for the efficient distribution of updates across partitions.
86386136-278c-5512-baf1-0faa3af094f3|Edge-Centric Scatter-Gather Processing|The authors propose an edge-centric approach to scatter-gather processing, which involves iterating over edges and updates rather than vertices. This approach avoids random access into the set of edges, instead streaming them from storage.
3e0d7a55-5377-5b4e-845d-858a613fea41|Parallel Multistage Shuffler|The authors propose a parallel multistage shuffler to efficiently shuffle edges and updates between partitions. This approach is designed to minimize the number of random accesses and maximize the usage of sequential streaming bandwidth.
b0a49bda-1aa4-596c-b71e-aab601d6f8d1|Layering over Disk Streaming|The authors propose layering the in-memory engine over the out-of-core engine to efficiently process graphs that do not fit in memory. This approach allows the disk engine to independently choose its count of streaming partitions.
07ac3319-2804-5d46-bf77-ce917feaea46|Edge-Centric Processing|The authors propose an edge-centric processing model, which focuses on processing edges rather than vertices. This approach is designed to reduce memory access overhead by minimizing the need for random memory accesses.
c62efcac-3443-5f18-9685-95110016db2e|Multistage Shuffler|The authors propose using a multistage shuffler to improve the performance of their streaming partitions approach. The multistage shuffler involves dividing the graph into smaller partitions and then shuffling the edges between partitions to minimize random access.
8f1d3ed4-d412-5d19-b829-ec3df92820fc|Dynamic Distributed MIS Algorithm|The authors propose a dynamic distributed algorithm for maintaining a maximal independent set (MIS) in a graph, which is a fundamental problem in distributed computing. The algorithm is designed to optimize communication efficiency by minimizing the number of rounds required to update the MIS after a topology change. The algorithm uses a novel analysis of the greedy sequential MIS algorithm with a random ordering of the nodes. It simulates the sequential algorithm in a distributed setting, using a random permutation to determine the order in which nodes are processed. This approach allows the algorithm to maintain a high-quality MIS while minimizing the number of rounds required to update the MIS after a topology change. The authors show that their algorithm requires only a single adjustment and O(1) rounds for all topology changes, with high probability. They also demonstrate that the algorithm achieves a 3-approximation for correlation clustering, a problem closely related to MIS.
b1c913a3-913a-540b-98e4-0551dc0c7a3f|Constant Broadcast Implementation|The authors propose a constant broadcast implementation of their dynamic distributed MIS algorithm, which further optimizes communication efficiency by minimizing the number of broadcasts required to update the MIS after a topology change. The implementation uses a modified version of the algorithm, where each node waits until it knows the maximal i for which it belongs to Si, and changes its state only once. This approach reduces the number of broadcasts required to update the MIS after a topology change. The authors show that their constant broadcast implementation requires O(1) broadcasts, with high probability, and achieves the same approximation ratio as the original algorithm.
13419d01-6f5f-531f-b241-223fa8edb98c|History-Independent Algorithm|The authors propose a history-independent algorithm for maintaining an MIS in a graph, which is designed to optimize communication efficiency by minimizing the number of rounds required to update the MIS after a topology change. The algorithm uses a novel approach that composes nicely with other algorithms, allowing it to maintain a high-quality MIS while minimizing the number of rounds required to update the MIS after a topology change. The authors show that their history-independent algorithm achieves a 3-approximation for correlation clustering, a problem closely related to MIS, and requires only a single adjustment and O(1) rounds for all topology changes, with high probability.
78c0387e-c361-5071-94ae-4555eeb2b59b|Dynamic Truss Maintenance Algorithm|The authors propose a novel algorithm for maintaining maximal k-trusses in dynamic graphs, focusing on minimizing the number of iterations required to update the truss structure after edge insertions or deletions. This algorithm leverages a combination of local and global updates to efficiently maintain the truss decomposition.
fc3f1f2e-e140-5c33-89ab-bb5254f98f94|Incremental Betweenness Centrality Algorithm|The authors present an incremental algorithm for updating betweenness centrality measures in response to edge modifications in dynamic graphs. This algorithm focuses on minimizing the number of iterations required to update the centrality measures after an edge insertion or deletion.
d3433379-f4d7-5900-b99d-b9049e358c02|Batch Processing Algorithm for Vertex and Edge Updates|The authors propose a batch processing algorithm for efficiently processing vertex and edge updates in dynamic graphs while preserving structural integrity. This algorithm focuses on minimizing the number of iterations required to update the graph structure after a batch of updates.
522e4318-1e1e-546c-ab54-f60a535f0705|Adaptive Truss Maintenance Algorithm|The authors propose an adaptive algorithm for maintaining maximal k-trusses in dynamic graphs, focusing on minimizing the number of iterations required to update the truss structure after edge insertions or deletions. This algorithm leverages a combination of local and global updates to efficiently maintain the truss decomposition.
d61e546f-9682-5f54-90b3-60a11a4f3f81|Subgraph-Centric Triangle Counting Algorithm|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a subgraph-centric algorithm for triangle counting. The algorithm is designed to reduce communication costs and memory consumption by leveraging the information available within each subgraph.
bc48555a-0ed7-51e1-b7ef-9be8548da95c|Subgraph-Centric K-Way Clustering Algorithm|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a subgraph-centric algorithm for k-way clustering. The algorithm is designed to reduce memory consumption and communication costs by leveraging the information available within each subgraph.
cf6f28ec-fe3c-5a33-910b-76812706980f|Subgraph-Centric Minimum Spanning Forest Algorithm|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a subgraph-centric algorithm for minimum spanning forest. The algorithm is designed to reduce memory consumption and communication costs by leveraging the information available within each subgraph.
a1e2a89a-c279-55a3-a039-5cec624dd992|Subgraph-Centric k-Way Clustering Algorithm|This solution presents a subgraph-centric algorithm for k-way clustering that reduces communication complexity by selecting cluster centers and assigning vertices to clusters within subgraphs.
36dbf440-4255-5a0f-8444-bfd74732acba|Subgraph-Centric Distributed Graph Processing|The authors propose a subgraph-centric distributed graph processing framework to optimize load balance in distributed systems. This approach partitions the graph into subgraphs and processes each subgraph in parallel, reducing the communication overhead and improving the load balance.
17574332-09b7-518e-abb2-a9f29e6a9f48|Subgraph-Centric PageRank (SGPR)|SGPR is a memory-efficient algorithm for scalable graph processing that leverages subgraph-centric programming abstractions to compute PageRank values. SGPR operates on subgraphs, which are weakly connected components within each partition, to reduce memory consumption and optimize memory usage. It employs a localized vertex update strategy, where each subgraph updates its vertices PageRank values independently, minimizing communication between processors. The paper demonstrates that SGPR achieves a performance gain of 23-74% for equivalent PageRank quality compared to the native PageRank algorithm.
374e736d-832e-5bab-beb9-16122349dd50|BlockRank with PageRank-like Distribution Logic (BRDL)|BRDL is a variation of the BlockRank algorithm that uses a PageRank-like distribution logic to improve memory efficiency and scalability. BRDL employs a PageRank-like distribution logic to reduce the number of supersteps required for convergence, thereby minimizing memory consumption and communication overhead. The paper shows that BRDL outperforms the native BlockRank algorithm and achieves a performance gain of 23-74% for equivalent PageRank quality.
41eaee6b-68e8-5c9e-b353-ec114520f33f|Subgraph Rank (SGRK)|SGRK is a memory-efficient algorithm that combines the benefits of subgraph-centric programming and BlockRank to achieve scalable graph processing. SGRK uses a subgraph-centric approach to compute PageRank values, employing a localized vertex update strategy and minimizing communication between processors. It also leverages the BlockRank algorithm to reduce the number of supersteps required for convergence. The paper demonstrates that SGRK achieves a performance gain of 23-74% for equivalent PageRank quality compared to the native PageRank algorithm and outperforms other BlockRank variations.
089af030-e356-567f-ad7f-04563ceaa746|BlockRank with SG by G Initialization Vector (BRIV)|BRIV is a variation of the BlockRank algorithm that uses an SG by G initialization vector to improve memory efficiency and scalability. BRIV employs an SG by G initialization vector to reduce the number of supersteps required for convergence, thereby minimizing memory consumption and communication overhead. The paper shows that BRIV outperforms the native BlockRank algorithm and achieves a performance gain of 23-74% for equivalent PageRank quality.
6b0a5880-fd01-5c62-9612-c1cca03afecf|BlockRank with PageRank-like Distribution Logic and SG by G Initialization Vector (BRDI)|BRDI is a variation of the BlockRank algorithm that combines PageRank-like distribution logic and an SG by G initialization vector to improve memory efficiency and scalability. BRDI employs a PageRank-like distribution logic and an SG by G initialization vector to reduce the number of supersteps required for convergence, thereby minimizing memory consumption and communication overhead. The paper demonstrates that BRDI outperforms the native BlockRank algorithm and achieves a performance gain of 23-74% for equivalent PageRank quality.
290b2126-466b-51b6-8cd8-94f591f1e833|Subgraph Rank (SGRK) Algorithm|The SGRK algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms. It leverages the subgraph structure of the graph to reduce the number of communication rounds required for PageRank computation.
08537aa2-f314-5689-a749-2cd4da9cf364|BlockRank with Native Distribution Logic (BRNA)|The BRNA algorithm is a variant of the BlockRank algorithm that uses a native distribution logic. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
8b5f9e21-3448-584a-a036-818b3fc83c59|BlockRank with No Initialization Vector (BRNO)|The BRNO algorithm is a variant of the BlockRank algorithm that does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
08129ffa-e3d1-595d-8bf5-a7dd7f5fd28a|BlockRank with PageRank-like Distribution Logic and No Initialization Vector (BRDI_NO_IV)|The BRDI_NO_IV algorithm is a variant of the BlockRank algorithm that uses a PageRank-like distribution logic and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
5ca6a463-4dc9-5da6-8ec3-c6138150578d|BlockRank with SG by G Initialization Vector and No Initialization Vector (BRIV_NO_IV)|The BRIV_NO_IV algorithm is a variant of the BlockRank algorithm that uses an SG by G initialization vector and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
5296894b-7d3f-56b0-80b1-b4e475ca5423|BlockRank with PageRank-like Distribution Logic and SG by G Initialization Vector and No Initialization Vector (BRDI_BRIV_NO_IV)|The BRDI_BRIV_NO_IV algorithm is a variant of the BlockRank algorithm that uses a PageRank-like distribution logic, an SG by G initialization vector, and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
3333ae10-b32d-59d6-80d4-55a339e8bb21|Subgraph Centric PageRank (SGPR) Algorithm|The SGPR algorithm is a variant of the PageRank algorithm that uses a subgraph centric approach. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
14fa067c-1b2c-547c-8c90-0bb67c43e297|BlockRank Algorithm|The BlockRank algorithm is a variant of the PageRank algorithm that uses a block-based approach. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
c23af882-c830-591b-b120-86c4b97d3a13|PageRank Algorithm|The PageRank algorithm is a variant of the PageRank algorithm that uses a traditional approach. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
0416d552-b7ff-5ca9-af44-5017ce1c6132|Local PageRank (LPR) Algorithm|The LPR algorithm is a variant of the PageRank algorithm that uses a local approach. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
024631e6-4179-53e3-b72d-a99ef10b2e86|Global PageRank (GPR) Algorithm|The GPR algorithm is a variant of the PageRank algorithm that uses a global approach. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
0d3c6c64-c8eb-558a-ab2b-9f1ce1d99a54|Subgraph Centric BlockRank (SCBR) Algorithm|The SCBR algorithm is a variant of the BlockRank algorithm that uses a subgraph centric approach. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
e5d0fae2-ac4e-5cd0-8e84-a51debfdaade|Subgraph Centric Local PageRank (SCLPR) Algorithm|The SCLPR algorithm is a variant of the Local PageRank algorithm that uses a subgraph centric approach. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
f7f28433-3e94-5920-a303-4991f527045a|Subgraph Centric Global PageRank (SCGPR) Algorithm|The SCGPR algorithm is a variant of the Global PageRank algorithm that uses a subgraph centric approach. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
6aa184ce-a16c-5bc7-b0b4-803444d32fb0|Subgraph Centric BlockRank with PageRank-like Distribution Logic (SCBRDL) Algorithm|The SCBRDL algorithm is a variant of the BlockRank algorithm that uses a subgraph centric approach and a PageRank-like distribution logic. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
512ed015-a88b-5623-a0aa-bd02083b1b0d|Subgraph Centric BlockRank with SG by G Initialization Vector (SCBRIV) Algorithm|The SCBRIV algorithm is a variant of the BlockRank algorithm that uses a subgraph centric approach and an SG by G initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
c45b83a7-abf3-52cb-9c70-355031c0c799|Subgraph Centric BlockRank with PageRank-like Distribution Logic and SG by G Initialization Vector (SCBRDLIV) Algorithm|The SCBRDLIV algorithm is a variant of the BlockRank algorithm that uses a subgraph centric approach, a PageRank-like distribution logic, and an SG by G initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
57949991-b4eb-5725-9d54-c0b95048d6aa|Subgraph Centric BlockRank with PageRank-like Distribution Logic and No Initialization Vector (SCBRDL_NO_IV) Algorithm|The SCBRDL_NO_IV algorithm is a variant of the BlockRank algorithm that uses a subgraph centric approach, a PageRank-like distribution logic, and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
9839fc17-8fc6-5648-bc73-5418b1c7fe7a|Subgraph Centric BlockRank with SG by G Initialization Vector and No Initialization Vector (SCBRIV_NO_IV) Algorithm|The SCBRIV_NO_IV algorithm is a variant of the BlockRank algorithm that uses a subgraph centric approach, an SG by G initialization vector, and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
3985058c-45f2-5f7b-a061-d69ef18c376d|Subgraph Centric BlockRank with PageRank-like Distribution Logic and SG by G Initialization Vector and No Initialization Vector (SCBRDLIV_NO_IV) Algorithm|The SCBRDLIV_NO_IV algorithm is a variant of the BlockRank algorithm that uses a subgraph centric approach, a PageRank-like distribution logic, an SG by G initialization vector, and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
7317ca70-d49f-5f05-8962-853cce4a19e4|Subgraph Centric PageRank with PageRank-like Distribution Logic (SCPRDL) Algorithm|The SCPRDL algorithm is a variant of the PageRank algorithm that uses a subgraph centric approach and a PageRank-like distribution logic. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
d1cbb3b0-c668-5f34-9bc6-ae5047da1654|Subgraph Centric PageRank with SG by G Initialization Vector (SCPRIV) Algorithm|The SCPRIV algorithm is a variant of the PageRank algorithm that uses a subgraph centric approach and an SG by G initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
db3b0950-c167-5537-95b4-0cf2810bb502|Subgraph Centric PageRank with PageRank-like Distribution Logic and SG by G Initialization Vector (SCPRDLIV) Algorithm|The SCPRDLIV algorithm is a variant of the PageRank algorithm that uses a subgraph centric approach, a PageRank-like distribution logic, and an SG by G initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
75e3c9d3-6726-5ff7-ac56-c08dbf09c4ae|Subgraph Centric PageRank with PageRank-like Distribution Logic and No Initialization Vector (SCPRDL_NO_IV) Algorithm|The SCPRDL_NO_IV algorithm is a variant of the PageRank algorithm that uses a subgraph centric approach, a PageRank-like distribution logic, and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
48567d7d-7743-5cf1-b326-532255dc6e4a|Subgraph Centric PageRank with SG by G Initialization Vector and No Initialization Vector (SCPRIV_NO_IV) Algorithm|The SCPRIV_NO_IV algorithm is a variant of the PageRank algorithm that uses a subgraph centric approach, an SG by G initialization vector, and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
5329dab2-c1e0-545f-a05c-db4f946756af|Subgraph Centric PageRank with PageRank-like Distribution Logic and SG by G Initialization Vector and No Initialization Vector (SCPRDLIV_NO_IV) Algorithm|The SCPRDLIV_NO_IV algorithm is a variant of the PageRank algorithm that uses a subgraph centric approach, a PageRank-like distribution logic, an SG by G initialization vector, and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
81441ddf-f6e4-5c05-a187-b7645958b922|Subgraph Centric Local PageRank with PageRank-like Distribution Logic (SCLPRDL) Algorithm|The SCLPRDL algorithm is a variant of the Local PageRank algorithm that uses a subgraph centric approach and a PageRank-like distribution logic. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
8cef123e-8fa1-556c-b37a-46af2c00d840|Subgraph Centric Local PageRank with SG by G Initialization Vector (SCLPRIV) Algorithm|The SCLPRIV algorithm is a variant of the Local PageRank algorithm that uses a subgraph centric approach and an SG by G initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
41afdc51-7512-57ac-be5d-090f41185b01|Subgraph Centric Local PageRank with PageRank-like Distribution Logic and SG by G Initialization Vector (SCLPRDLIV) Algorithm|The SCLPRDLIV algorithm is a variant of the Local PageRank algorithm that uses a subgraph centric approach, a PageRank-like distribution logic, and an SG by G initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
a4b2a65c-c11f-54d3-b75c-e08c35cc79b2|Subgraph Centric Local PageRank with PageRank-like Distribution Logic and No Initialization Vector (SCLPRDL_NO_IV) Algorithm|The SCLPRDL_NO_IV algorithm is a variant of the Local PageRank algorithm that uses a subgraph centric approach, a PageRank-like distribution logic, and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
975d68f3-314d-546e-a11e-52d5cb6dde74|Subgraph Centric Local PageRank with SG by G Initialization Vector and No Initialization Vector (SCLPRIV_NO_IV) Algorithm|The SCLPRIV_NO_IV algorithm is a variant of the Local PageRank algorithm that uses a subgraph centric approach, an SG by G initialization vector, and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
c23bead2-1a84-5323-9f7b-20380547c7e2|Subgraph Centric Local PageRank with PageRank-like Distribution Logic and SG by G Initialization Vector and No Initialization Vector (SCLPRDLIV_NO_IV) Algorithm|The SCLPRDLIV_NO_IV algorithm is a variant of the Local PageRank algorithm that uses a subgraph centric approach, a PageRank-like distribution logic, an SG by G initialization vector, and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
36e93050-f7b7-50db-9bd3-d2c9b3c7df6d|Subgraph Centric Global PageRank with PageRank-like Distribution Logic (SCGPRDL) Algorithm|The SCGPRDL algorithm is a variant of the Global PageRank algorithm that uses a subgraph centric approach and a PageRank-like distribution logic. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
5e6f666d-78f1-5f49-8f43-f6e67969a5de|Subgraph Centric Global PageRank with SG by G Initialization Vector (SCGPRIV) Algorithm|The SCGPRIV algorithm is a variant of the Global PageRank algorithm that uses a subgraph centric approach and an SG by G initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
6aa06394-7cd6-56ed-910b-024d0b6213f8|Subgraph Centric Global PageRank with PageRank-like Distribution Logic and SG by G Initialization Vector (SCGPRDLIV) Algorithm|The SCGPRDLIV algorithm is a variant of the Global PageRank algorithm that uses a subgraph centric approach, a PageRank-like distribution logic, and an SG by G initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
704d7f1d-a294-5a1a-9d61-516625b3bdb9|Subgraph Centric Global PageRank with PageRank-like Distribution Logic and No Initialization Vector (SCGPRDL_NO_IV) Algorithm|The SCGPRDL_NO_IV algorithm is a variant of the Global PageRank algorithm that uses a subgraph centric approach, a PageRank-like distribution logic, and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
50d0852f-c9fd-51a2-885d-5674a9352f8a|Subgraph Centric Global PageRank with SG by G Initialization Vector and No Initialization Vector (SCGPRIV_NO_IV) Algorithm|The SCGPRIV_NO_IV algorithm is a variant of the Global PageRank algorithm that uses a subgraph centric approach, an SG by G initialization vector, and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
4580b0a1-123a-5976-b290-b66963259b11|Subgraph Centric Global PageRank with PageRank-like Distribution Logic and SG by G Initialization Vector and No Initialization Vector (SCGPRDLIV_NO_IV) Algorithm|The SCGPRDLIV_NO_IV algorithm is a variant of the Global PageRank algorithm that uses a subgraph centric approach, a PageRank-like distribution logic, an SG by G initialization vector, and does not use an initialization vector. This algorithm is proposed as a solution to optimize communication efficiency in distributed algorithms.
c3b6cf69-1e86-507e-8e6a-594f280aca0b|BlockRank with No BlockRank Phase (BRNO)|The BRNO solution modifies the native BlockRank algorithm to skip the BlockRank phase, aiming to improve the load balance by reducing the number of supersteps required for convergence.
f1e491bc-6e72-5c88-9209-c44a7c6b221d|Subgraph Rank Algorithm|The Subgraph Rank algorithm is a novel approach to efficiently process graph dynamics by leveraging subgraph-centric programming abstractions. This algorithm is specifically designed to address the challenge of efficient graph dynamics processing by minimizing computational costs and iterations. The Subgraph Rank algorithm works by first computing local PageRank values for each subgraph, followed by a BlockRank phase that estimates the relative importance of each subgraph. The algorithm then combines these values to obtain the final PageRank values. This approach reduces the number of iterations required to maintain graph structures and centrality measures, making it more efficient than traditional PageRank algorithms. The unique mechanism involved in the Subgraph Rank algorithm is its use of subgraph-centric programming abstractions, which allows for more efficient processing of graph dynamics. This approach is different from existing approaches that rely on vertex-centric or edge-centric processing. Results: The paper presents experimental results that demonstrate the effectiveness of the Subgraph Rank algorithm. For example, the algorithm shows a performance gain of 23-74% for equivalent PageRank quality on various graphs, and it successfully exploits the subgraph structure of the graph to offer high-quality initial PageRank values.
668cac6f-b517-5fb9-8a16-b984d8cba650|BlockRank Algorithm Variations|The paper proposes variations of the BlockRank algorithm, including BlockRank with PageRank-like distribution logic (BRDL), BlockRank with SG by G initialization vector (BRIV), and BlockRank with PageRank-like distribution logic and SG by G initialization vector (BRDI). These variations aim to improve the performance of the BlockRank algorithm by leveraging subgraph-centric programming abstractions. The BlockRank algorithm variations work by modifying the initialization vector and distribution logic of the BlockRank algorithm to better suit subgraph-centric processing. These variations reduce the number of iterations required to maintain graph structures and centrality measures, making them more efficient than traditional BlockRank algorithms. The unique mechanism involved in the BlockRank algorithm variations is their use of subgraph-centric programming abstractions, which allows for more efficient processing of graph dynamics. This approach is different from existing approaches that rely on vertex-centric or edge-centric processing. Results: The paper presents experimental results that demonstrate the effectiveness of the BlockRank algorithm variations. For example, the BRDL variation shows a performance gain of 23-74% for equivalent PageRank quality on various graphs, and it successfully exploits the subgraph structure of the graph to offer high-quality initial PageRank values.
f448d0c5-c77a-5a58-87c1-660ea6d6f563|Fractional Packing Algorithm|The authors propose a fractional packing algorithm to optimize communication efficiency in distributed algorithms. This algorithm is designed to find a maximal fractional packing in a set cover instance, which can be used to approximate the minimum weight set cover problem.
a554e7d3-2744-5771-abe6-15b3a3734bc0|Edge Packing Algorithm|The authors propose an edge packing algorithm to optimize communication efficiency in distributed algorithms. This algorithm is designed to find a maximal edge packing in a weighted graph, which can be used to approximate the minimum weight vertex cover problem.
6b4a51c2-9784-5b51-a315-6a2d3bf83d14|Weak Colour Reduction Algorithm|The authors propose a weak colour reduction algorithm to optimize communication efficiency in distributed algorithms. This algorithm is designed to reduce the number of colours used in the colouring phase of the fractional packing algorithm.
dfe97f08-4fde-5a9d-bd64-d90b4ad6516a|Cole-Vishkin Style Colour Reduction Algorithm|The authors propose a Cole-Vishkin style colour reduction algorithm to optimize communication efficiency in distributed algorithms. This algorithm is designed to reduce the number of colours used in the colouring phase of the edge packing algorithm.
813a232d-4028-5e62-b2a7-5543527d54b1|Distributed Set Cover Algorithm|The authors propose a distributed set cover algorithm to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm is designed to find a set cover in a distributed system, where each node has a limited view of the graph.
8d475e93-7fb5-558a-b88c-27fa4f26d3ae|Maximal Edge Packing Algorithm|The authors propose a deterministic distributed algorithm that finds a maximal edge packing in O(log W) synchronous communication rounds, where W is the maximum weight of the graph. This algorithm is designed to address the challenge of efficient graph dynamics processing by minimizing the number of iterations required to maintain graph structures under updates.
57ea7519-7513-5437-ab44-758038ad4988|LEC Feature-based Optimization|The authors propose a solution called LEC Feature-based Optimization, which aims to compress local partial matches into a compact data structure named the LEC feature. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the memory consumption and communication overhead associated with processing massive graphs.
41a24519-4248-5c4d-a024-14bcd19b3168|Assembling Variables' Internal Candidates|The authors propose a solution called Assembling Variables' Internal Candidates, which aims to assemble variables' internal candidates to filter out some false positive candidates. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the memory consumption and communication overhead associated with processing massive graphs.
475b0b41-1b41-5b42-a06f-7ac7c1b11fed|LEC Feature-based Pruning Algorithm|This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a pruning algorithm that leverages Local Partial Match Equivalence Class (LEC) features to filter out irrelevant partial results. The algorithm compresses local partial matches into LEC features, which are then communicated among sites to prune out irrelevant partial results, reducing the number of communication rounds.
707431e0-97aa-5134-ab36-38c90fcc7c18|LEC Feature-based Assembly Algorithm|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing an assembly algorithm that leverages LEC features to merge partial results. The algorithm partitions LEC features into equivalence classes and joins them to form the final results, reducing the number of communication rounds.
87322dd6-f477-573b-9793-f37b9df35122|Partitioning Strategy|The Partitioning Strategy is a technique used to partition the RDF graph into smaller fragments. This technique is designed to address the challenge of adaptive algorithms for heterogeneous and irregular graphs by efficiently partitioning the graph and reducing the number of crossing edges.
4c3a06b0-eea4-5a14-bcc2-8fdb9104bd7d|Partitioning Strategy Optimization|Partitioning Strategy Optimization is a technique used to optimize load balance in distributed systems by selecting the best partitioning strategy. This technique is designed to reduce the cost of partitioning and improve the performance of distributed SPARQL query evaluation.
0242919d-afe0-57a3-adb4-90d61974e0c7|Assembling Variables Internal Candidates|This solution proposes an optimization technique that communicates variables internal candidates among sites to prune some irrelevant candidates and reduce the search space of the partial evaluation and assembly process.
f91572f7-10f9-5578-88d1-a961fae92464|LEC Feature-based Assembly|This solution proposes an algorithm that assembles the LEC features to form the final results, reducing the search space of the partial evaluation and assembly process.
03d470ab-7ac8-55aa-9e9b-d7b9ddfa1db6|Harris Hawks Jaya (HH Jaya) Algorithm|The HH Jaya algorithm is a proposed solution that integrates the capabilities of Harris Hawks Optimization (HHO) and Jaya Algorithm to optimize communication efficiency in distributed algorithms. This algorithm aims to minimize round complexity while maintaining solution quality or approximation ratios.
7ce14aa1-37c5-5f92-bc16-13a4c04438c7|Ensemble Learning with Multinomial Naive Bayes (MNB), Support Vector Machine (SVM), Passive Aggressive (PA), and Stochastic Gradient Descent (SGD)|This solution proposes an ensemble learning approach that combines the strengths of multiple machine learning algorithms to optimize communication efficiency in distributed algorithms. The ensemble model aims to minimize round complexity while maintaining solution quality or approximation ratios.
21e15576-c7a0-5ad4-9456-39cf85ef742b|Load Balancing with Resource Failure Consideration for Deadline Control (LBFD)|LBFD is a load balancing scheme that considers resource fault index and job deadline to optimize load balance in distributed systems. It uses a proactive fault-tolerant scheduling approach that takes into account the resource fault index and job deadline to minimize the impact of resource failures on load balancing. It sorts jobs according to their deadlines and resources according to their fault indices, ensuring that jobs with earlier deadlines are executed on resources with lower fault indices. The paper presents simulation results showing that LBFD outperforms existing load balancing schemes, such as EGDC, in terms of throughput and fault tolerance.
39700a1f-cc7a-56c3-bf97-aa7024ff2d6c|Distributed Reconfiguration of Colorings|The authors propose a distributed algorithm for recoloring interval and chordal graphs, which involves computing a schedule of colorings to transform an initial coloring into a target coloring while minimizing the number of communication rounds.
3bcfaf81-1f88-5209-be19-71e95a17c446|Interval Decomposition|The authors propose an interval decomposition algorithm for chordal graphs, which involves computing a partition of the graph into components of small diameter.
3c3e12f1-6ca0-5d03-9791-79f0450d62dc|Local Recoloring|The authors propose a local recoloring technique for interval graphs, which involves recoloring a subset of vertices using a constant number of colors.
39971184-6b7e-53ad-84a6-ddbc5fd5526b|Adaptive Graph Decomposition|The authors propose an adaptive graph decomposition technique to handle heterogeneous and irregular graphs. This technique involves decomposing the graph into smaller subgraphs with controlled diameter, allowing for more efficient processing and analysis.
a40d6a6b-7991-53b0-8d97-78d2428b0bb8|Kempe Chain-Based Recoloring|The authors propose a Kempe chain-based recoloring technique to adapt to changes in the graph structure. This technique involves using Kempe chains to recolor the graph, allowing for efficient adaptation to changes in the graph structure.
87b7030c-7b53-5c0b-8869-c4efcb15f44d|Distributed Coloring Algorithm|The authors propose a distributed coloring algorithm to handle heterogeneous and irregular graphs. This algorithm involves coloring the graph in a distributed manner, allowing for efficient processing and analysis.
df560808-b286-51f6-ad8d-f7ad54ddd7bc|Distributed Recoloring Algorithm for Interval and Chordal Graphs|The authors propose a distributed recoloring algorithm for interval and chordal graphs, which efficiently processes dynamic updates in these graph structures. The algorithm uses a combination of graph decompositions, Kempe chains, and recoloring techniques to minimize computational costs and iterations.
aee9fd6e-909a-58a9-a56a-11fbc826d614|Tight Simulation|Tight simulation is a novel pattern matching model that aims to improve the scalability of graph simulation while preserving its important properties. It introduces a new selectivity criterion for finding the candidate vertex and radius out of the query graph, which helps to reduce the number of matches and improve the efficiency of the algorithm.
232c2e51-0fb6-584a-b881-07f4e396737e|Vertex-Centric Distributed Algorithm|The paper proposes a vertex-centric distributed algorithm for graph simulation, which is designed to work efficiently in a distributed computing environment. The algorithm uses a message-passing model to communicate between vertices and employs a BSP framework to ensure scalability.
3683ac8c-70ad-5a95-a722-0d196503dd5c|Distributed Dual Simulation|Distributed dual simulation is a distributed algorithm that optimizes communication efficiency by reducing the number of messages exchanged between vertices during the simulation process. This solution specifically addresses the challenge of optimizing communication efficiency by minimizing the number of communication rounds required for evaluating parent relationships.
a59a6758-64d4-5c2f-a7b7-2b51f063713b|Dual Filter Mechanism|The dual filter mechanism is designed to reduce the number of false positives in graph pattern matching queries on heterogeneous and irregular graphs. It uses a two-stage filtering approach to eliminate non-matching vertices.
bc121f94-412e-5340-ab8a-44f87b628bbe|Ball Creation Mechanism|The ball creation mechanism is designed to efficiently create balls around matching vertices in graph pattern matching queries on heterogeneous and irregular graphs.
b647fc01-4242-5996-a6ce-edfb9647a2b5|Sparsity Optimization|The authors propose a sparsity optimization technique to reduce memory consumption and improve scalability in graph processing. This technique involves dynamically selecting between sparse matrix-vector multiplication (SpMV) and sparse matrix-sparse vector multiplication (SpMSpV) based on the sparsity of the graph. The technique takes advantage of the fact that SpMSpV has lower computation and communication costs than SpMV when the graph is sparse. By dynamically switching between these two operations, the algorithm can reduce memory consumption and improve scalability. The paper shows that this technique can reduce the execution time of the last few iterations of the algorithm by up to 50% (Figure 10).
b5638106-67ec-5f9e-b423-04ce747bb134|Broadcasting-based Implementation|The authors propose a broadcasting-based implementation for the extract and assign operations to reduce memory consumption and improve scalability. This technique involves manually implementing these operations to avoid load balancing issues and reduce memory access. The technique takes advantage of the fact that the extract and assign operations can be implemented in a way that avoids load balancing issues and reduces memory access. By manually implementing these operations, the algorithm can reduce memory consumption and improve scalability. The paper shows that this technique can improve the performance of the algorithm by up to 2x (Figure 6).
9705eeeb-8a0e-50fb-90fe-13443a409d85|Hooking Strategies|The authors propose several novel hooking strategies to reduce memory consumption and improve scalability in graph processing. These strategies involve modifying the tree hooking scheme to reduce the number of iterations required to find connected components. The techniques take advantage of the fact that the tree hooking scheme can be modified to reduce the number of iterations required to find connected components. By using these strategies, the algorithm can reduce memory consumption and improve scalability. The paper shows that these strategies can reduce the number of iterations required to find connected components by up to 46.2% (Figure 4).
f4966039-c18f-51d5-b6da-87fff3e0286c|Optimized MPI Communication|The authors propose an optimized MPI communication approach to minimize communication bottlenecks in the distributed implementation of the FastSV algorithm. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the number of communication rounds and improving overall efficiency.
5240adfd-2917-539c-aeaf-b205bdff2a13|Sparsity-Aware Matrix-Vector Multiplication|The authors propose a sparsity-aware matrix-vector multiplication approach to reduce the computational cost of the FastSV algorithm. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by minimizing the number of communication rounds and improving overall efficiency.
e4bc19a5-286a-593a-92c9-fffbc7b54183|Early Termination|The authors propose an early termination approach to reduce the number of iterations in the FastSV algorithm. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by minimizing the number of communication rounds and improving overall efficiency.
e72e1995-e0ac-509f-add4-c0bfb21c67c8|FastSV Algorithm|The FastSV algorithm is a novel solution proposed by the authors to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. It is a distributed memory parallel algorithm for finding connected components in an undirected graph, which is a fundamental operation in many large-scale applications. The FastSV algorithm simplifies the classic Shiloach-Vishkin algorithm and employs several novel and efficient hooking strategies for faster convergence. It uses a combination of stochastic hooking, aggressive hooking, and shortcutting to reduce the height of trees in the graph, ultimately leading to the identification of connected components. The unique mechanism involved in the FastSV algorithm is its use of a simplified SV algorithm with just two steps: hooking trees conditionally onto other trees and shortcutting. This approach allows for faster convergence and is more suitable for distributed memory platforms. Quantitative results from the paper demonstrate the effectiveness of the FastSV algorithm. For example, it is shown to be on average 2.21 times faster than the previous fastest algorithm, LACC, and can find connected components in a hyperlink graph with 3.27 billion vertices and 124.9 billion edges in just 30 seconds using 262,144 cores of a Cray XC40 supercomputer.
91a0abb9-8a50-53c5-9847-76c320253a9b|Broadcasting-Based Implementation|The broadcasting-based implementation solution is a technique proposed by the authors to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. It involves implementing the extract and assign operations in the FastSV algorithm using a broadcasting-based approach to reduce load imbalance and enhance memory locality. The unique mechanism involved in this solution is its use of a manual implementation of the extract and assign operations to avoid load balancing issues. This approach allows for a reduction in communication overhead and enhances memory locality, ultimately leading to improved performance. Quantitative results from the paper demonstrate the effectiveness of the broadcasting-based implementation solution. For example, it is shown that this approach can reduce the runtime of the FastSV algorithm and improve its overall performance.
105885ed-aa9e-5bbb-876d-99235c77be11|Sparsity-based Optimization for Load Balancing|The authors propose using sparsity to optimize load balancing in distributed systems. They observe that in the last few iterations of the algorithm, only a small fraction of vertices participate in the computation, and thus, using sparse matrix-sparse vector multiplication (SpMSpV) instead of sparse matrix-dense vector multiplication (SpMV) can significantly reduce the computation and communication cost.
b812bdf5-a348-58b4-8150-d88d7759a470|Broadcasting-based Implementation for Load Balancing|The authors propose a broadcasting-based implementation for the extract and assign operations to optimize load balancing in distributed systems. They observe that these operations may cause a load balancing issue when there is too much access on a few locations, and thus, using a broadcasting-based approach can help alleviate this issue.
52f749d5-8838-5228-a712-95030386be44|Distributed Vertex Hashing TRI ST IMPR (DVHT i)|DVHT i is a distributed algorithm that extends the centralized algorithm TRI ST IMPR for triangle counting in graph streams. It uses a vertex hash function to map edges to workers, reducing the communication overhead and improving the load balancing.
c04d6f12-b8aa-59fc-a8ac-899b5a4cc4c3|Distributed Edge Hashing TRI ST IMPR (DEHT i)|DEHT i is a distributed algorithm that extends the centralized algorithm TRI ST IMPR for triangle counting in graph streams. It uses an edge hash function to map edges to workers, reducing the communication overhead and improving the load balancing.
a597e22f-eb8d-59c5-a374-d5217a32ea11|Hierarchically Grouped Distributed Vertex Hashing TRI ST IMPR (DVHT b)|DVHT b is a distributed algorithm that extends the centralized algorithm TRI ST IMPR for triangle counting in graph streams. It uses a vertex hash function to map edges to workers and hierarchically groups the workers to reduce the communication overhead.
4ecdaf86-e8e7-59d2-bb77-3bd08aca4104|Hierarchically Grouped Distributed Edge Hashing TRI ST IMPR (DEHT b)|DEHT b is a distributed algorithm that extends the centralized algorithm TRI ST IMPR for triangle counting in graph streams. It uses an edge hash function to map edges to workers and hierarchically groups the workers to reduce the communication overhead.
caae8083-b495-5151-bed5-324111e512e6|Distributed Edge Hashing (DEHT) and Distributed Vertex Hashing (DVHT)|The authors propose two distributed algorithms, DEHT and DVHT, which utilize edge hashing and vertex hashing, respectively, to optimize communication efficiency in distributed triangle counting.
3c162770-edc9-5bb8-a892-9aebbed1f528|Improved Master-Worker-Aggregator Architecture|The authors propose an improved Master-Worker-Aggregator architecture with multiple masters and hierarchically grouped aggregators to further optimize communication efficiency.
08c1f669-9a41-5f37-945c-45dfe6ef1974|Adaptive Aggregation Method|The authors propose an adaptive aggregation method that dynamically adjusts the aggregation frequency based on the number of workers and the size of the sample set.
96707817-cc1a-5a0f-8a4f-d391fa8430e4|Distributed Edge Hashing TRI ST BASE (DEHT b)|DEHT b is a distributed streaming algorithm that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by utilizing an edge hash function to map edges to workers, reducing communication overhead and improving workload balance.
42055203-6612-5936-ad38-bf8cb52b4f56|Hierarchically Grouped Distributed Vertex Hashing TRI ST IMPR (DVHT i)|DVHT i is a distributed streaming algorithm that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by utilizing a vertex hash function to map vertices to workers and hierarchically grouping aggregators, improving the accuracy and reducing the communication overhead.
9644723b-fcd0-5fd5-8ab9-8198bcf8c157|Distributed Edge Hashing (DEH) Strategy|The authors propose a Distributed Edge Hashing (DEH) strategy to optimize load balance in distributed systems. This strategy involves using an edge hash function to map edges to workers directly, ensuring that each edge is distributed equally to workers. This approach enables good workload balance and reduces communication overhead on the master.
f7e2afae-2fec-57be-91a3-bfef4beca001|Hierarchically Grouped Aggregators (HGA) Architecture|The authors propose a Hierarchically Grouped Aggregators (HGA) architecture to optimize load balance in distributed systems. This architecture involves using multiple masters and hierarchically grouped aggregators to share the load, reducing the computation load on single master and aggregator.
81f323e9-aecd-5e70-b22d-a7db4190bb5d|Adaptive Aggregation Method (AAM)|The authors propose an Adaptive Aggregation Method (AAM) to optimize load balance in distributed systems. This method involves using a dynamic aggregation approach that adapts to different edge distribution strategies, ensuring efficient and balanced processing of edges.
4eb70636-f427-5ece-ab5c-6fba457d6db0|Master Worker Aggregator Architecture|The Master Worker Aggregator Architecture is a distributed framework designed to efficiently process graph dynamics by utilizing a master node to distribute edges across multiple worker nodes. This approach enables the algorithm to adapt to dynamic graph updates while minimizing computational costs and iterations.
ef16f5b5-edaa-5bbe-8e6a-ddcaec8e5c4e|Edge Distribution Strategy|The Edge Distribution Strategy is a technique designed to efficiently distribute edges across multiple workers in a distributed graph processing system. This approach enables the algorithm to adapt to dynamic graph updates while minimizing computational costs and iterations.
6936d85e-e3a4-55b3-9052-322568bf82ab|Aggregation Method|The Aggregation Method is a technique designed to efficiently aggregate the results of multiple workers in a distributed graph processing system. This approach enables the algorithm to adapt to dynamic graph updates while minimizing computational costs and iterations.
45835d44-8f1b-5641-a378-c40c6e3ebbbd|Distributed Semi-Streaming (DSS) Model|The DSS model is a novel approach to memory-efficient scalable graph processing, where each machine only keeps the states of its assigned vertices in memory and treats their adjacency lists and incoming and outgoing messages as local disk streams. The DSS model distributes the vertex states across machines, allowing each machine to process a portion of the graph while keeping only the necessary vertex states in memory. This approach reduces memory consumption and enables the processing of large-scale graphs. The paper demonstrates that the DSS model achieves a memory cost of O( V /n) on each machine, where  V  is the number of vertices and n is the number of machines.
80a72d41-dc29-53f2-9c90-14ff91f7cede|ID Recoding Technique|The ID recoding technique is a method for eliminating the need for expensive external memory join or group by operations in graph processing. The technique involves recoding the IDs of vertices and edges in the graph, allowing for efficient message passing and combination without the need for external memory operations. The paper shows that the ID recoding technique reduces the memory cost of graph processing and improves performance, with results demonstrating a significant reduction in execution time compared to existing approaches.
22d1379c-e6df-5907-a1cc-b4cd731317ec|Streaming Function for Sparse Computation|The streaming function is a technique for adapting disk I/O cost to the workload sparsity in graph processing. The technique involves using a streaming function to skip reading the edges of inactive vertices, reducing disk I/O cost and improving performance. The paper demonstrates that the streaming function improves performance in sparse computation workloads, with results showing a significant reduction in execution time compared to existing approaches.
91d1995b-fa07-5cf1-9dc0-56669cf5290a|Parallel Framework for Computation and Communication|The parallel framework is a method for overlapping computation and communication in graph processing. The framework involves running three units in parallel: a sending unit, a receiving unit, and a computing unit, allowing for efficient overlap of computation and communication. The paper demonstrates that the parallel framework improves performance, with results showing a significant reduction in execution time compared to existing approaches.
52893a2c-b996-544d-b4f8-d779bcfbceb3|Parallel Execution Framework|The parallel execution framework is a method proposed by the authors to optimize communication efficiency in distributed algorithms. This framework involves overlapping computation with communication to reduce the number of communication rounds.
efdc664b-5fd3-5429-a752-a563df8aeb7c|Message Combining|Message combining is a method proposed by the authors to optimize communication efficiency in distributed algorithms. This technique involves combining multiple messages targeted at the same vertex into a single message, reducing the number of messages transmitted between machines.
93cd8c04-94d0-558b-8981-bc71af68e29e|ID Recoding|ID Recoding is a technique used in GraphD to adapt disk IO cost to the workload sparsity, which is particularly useful for handling heterogeneous and irregular graphs. This solution involves recoding the IDs of vertices in the adjacency lists to reduce the number of edges that need to be scanned during computation.
a288c702-d3d4-597d-84bf-ab92fe4c3e16|Streaming Function|The streaming function is a technique used in GraphD to skip scanning unnecessary adjacency lists during computation, which is particularly useful for handling heterogeneous and irregular graphs. This solution involves using a streaming function to skip num items in the adjacency lists, allowing for more efficient computation.
279de6c5-3b73-586a-b7fd-06ff324c48a1|Parallel Framework|The parallel framework is a technique used in GraphD to overlap computation and communication, which is particularly useful for handling heterogeneous and irregular graphs. This solution involves using a parallel framework to run three units in parallel: a sending unit, a receiving unit, and a computing unit.
03c814b2-cf91-5d76-84aa-5bdd427946e0|In-Memory Message Combining|In-Memory Message Combining is a technique used in GraphD to combine messages in memory, which is particularly useful for handling heterogeneous and irregular graphs. This solution involves using an in-memory array to combine messages, reducing the overhead of message transmission.
9e04c507-14e1-5cec-9a6c-373019df4fe6|In-Memory Message Digesting|In-Memory Message Digesting is a technique used in GraphD to digest messages in memory, which is particularly useful for handling heterogeneous and irregular graphs. This solution involves using an in-memory array to digest messages, reducing the overhead of message transmission.
a62aa621-8d05-56e1-9af8-4dc448c7c38e|Dynamic Vertex Migration|NOT GIVEN
aadb38d6-0647-5e4c-88c7-53875bea24f4|Work Stealing|NOT GIVEN
c2ad2cec-2ee8-5cc8-be7d-86a318f3102d|Data Redistribution|NOT GIVEN
519d3b72-11b5-5ea4-87d9-b9f85aece2b6|Task Scheduling|NOT GIVEN
977c5553-5893-5fcd-8516-64cb1d9ffe40|Resource Allocation|NOT GIVEN
12fabfd9-e827-5d5d-aad5-e5ebb1b4b3c1|External Memory Processing|The external memory processing technique is used to eliminate the need for expensive external memory join or group by operations.
c14e509b-0679-5761-8d20-5e1864fb1ca5|Outgoing Message Streams|The outgoing message streams technique is used to reduce the number of messages transmitted between machines.
fbd89a24-44c1-5f17-87bd-8f94070fff67|Incoming Message Stream|The incoming message stream technique is used to reduce the number of messages transmitted between machines.
d92cf546-76b7-5709-b379-67d37a46bf6c|Vertex ID Recoding|The vertex ID recoding technique is used to establish an efficient one-to-one mapping between vertex IDs and their positions in the state array A.
f728d389-d703-56f1-aeec-0b5bdab36807|Edge Streaming|The edge streaming technique is used to reduce the number of edges transmitted between machines.
dee741d8-9bed-5175-9097-9240af6178a6|Graph Organization|The graph organization technique is used to reduce the number of edges transmitted between machines.
5e4e7fde-823d-5413-9962-133eae55969f|Distributed Semi-Streaming Model|The distributed semi-streaming model is used to reduce the number of edges transmitted between machines.
d2ef332b-bb7b-54e6-88a2-39e420edb0b9|Memory-Efficient Message Digesting|The memory-efficient message digesting technique is used to reduce the memory usage of the system.
3b7d56b1-fe09-53c6-9790-1a5225814dc1|Outgoing Message Streams with Message Combining|The outgoing message streams with message combining technique is used to reduce the number of messages transmitted between machines.
3c38405d-d11b-55b0-bdc0-4927522b1536|Incoming Message Stream with Message Combining|The incoming message stream with message combining technique is used to reduce the number of messages transmitted between machines.
cd5fb04c-e636-5daa-951b-31780db1384b|Vertex-Centric Computation|The vertex-centric computation technique is used to reduce the number of edges transmitted between machines.
7be6476b-2028-51b5-88f6-fb15ea277744|Edge-Centric Computation|The edge-centric computation technique is used to reduce the number of edges transmitted between machines.
b3870d9b-5a6f-521e-89a9-6be135609a32|Path-Based Holistic Detection Plan|The authors propose a path-based holistic detection plan for multiple patterns in distributed graph frameworks. This solution specifically addresses the challenge of memory-efficient scalable graph processing by designing a plan that can be easily computed and incorporated with other queries, and is highly parallelizable.
0f2a96bc-76ef-5d49-87df-be572a16db7a|Query Set Aware Sink Vertex Selection|The authors propose a query set aware sink vertex selection strategy, which selects the sink vertex for each query considering the constraints in the query set. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the redundant computation and communication cost among queries.
edc68bb7-4635-5cbf-8b5c-8c6c522f3bdc|Rule Merging with Message Superset|The authors propose a rule merging strategy with a message superset, which combines the transition rules into one with a message superset. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the redundant computation and communication cost among queries.
fd0f0a45-3aaf-57a3-879e-3ccf10800d26|Rule Sharing with Common Join|The authors propose a rule sharing strategy with a common join, which shares the computation and communication cost among queries. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the redundant computation and communication cost among queries.
0d3a2158-66e2-5d58-a13d-9b3458bb61fe|Query Set-Aware Sink Vertex Selection|The authors propose a query set-aware sink vertex selection strategy to optimize communication efficiency in distributed algorithms. This solution involves selecting the sink vertex for each query based on the maximum degree of the vertex, considering the query set as a whole.
52b65cc9-28d8-515f-9e29-06d74968d4e6|Relaxed Vertex Equivalence|The authors propose a relaxed vertex equivalence strategy to optimize communication efficiency in distributed algorithms. This solution involves relaxing the vertex equivalence to enable more rules to be reused, reducing the shared computation among queries.
edc1e32e-1d1d-5473-88b6-1a25256f5dc8|Frequent Path Selection|The authors propose a frequent path selection strategy to select paths that can be shared by multiple queries. This solution specifically addresses the challenge of optimizing GPU memory access for graph processing by selecting paths that minimize the computation cost and maximize the sharing among queries.
b1f490a0-11c5-51bd-bd67-855e6a5d9449|Common Join Rule Sharing|The authors propose a common join rule sharing strategy to share common joins among rules. This solution specifically addresses the challenge of optimizing GPU memory access for graph processing by sharing common joins among rules, which reduces the number of joins and evaluation time.
6a06f413-d66c-519c-b829-19ede366f332|Distributed Graph Coloring with Optimal Number of Colors|The authors propose a distributed graph coloring algorithm that can color a graph with an optimal number of colors in a memory-efficient and scalable manner. The algorithm uses a combination of techniques such as graph decomposition, local coloring, and distributed Lovsz Local Lemma to achieve optimal coloring. The graph decomposition technique is used to divide the graph into smaller subgraphs, which are then colored locally using a randomized algorithm. The distributed Lovsz Local Lemma is used to ensure that the coloring is proper and efficient. The authors show that their algorithm can color a graph with an optimal number of colors in O(log n log log n) rounds, which is a significant improvement over existing algorithms.
84c9d317-3362-5fba-9b23-2f3ff968df7a|Distributed Lovsz Local Lemma (DLLL)|The authors propose using the Distributed Lovsz Local Lemma (DLLL) to optimize communication efficiency in distributed algorithms. Specifically, they utilize the DLLL to solve the k-coloring problem in a distributed setting. The DLLL is a distributed algorithm that uses a probabilistic approach to find a k-coloring of a graph. It works by iteratively applying a local lemma to a set of events, which are defined as the set of possible color assignments to a vertex. The algorithm uses a distributed approach to compute the probabilities of these events and to update the color assignments. The authors show that the DLLL can solve the k-coloring problem in O(log n) rounds, which is a significant improvement over previous algorithms. They also demonstrate that the DLLL can be used to solve other distributed problems, such as the maximum independent set problem.
5ad34173-0dfd-5dc0-9231-64d1fc0f265e|Deg-1 List Coloring|The authors propose using a deg-1 list coloring algorithm to optimize communication efficiency in distributed algorithms. Specifically, they utilize this algorithm to color the vertices of a graph with a limited number of colors. The deg-1 list coloring algorithm works by iteratively assigning colors to vertices based on their degree. The algorithm uses a distributed approach to compute the degree of each vertex and to update the color assignments. The authors show that the deg-1 list coloring algorithm can color a graph with a limited number of colors in O(log n) rounds. They also demonstrate that this algorithm can be used to solve other distributed problems, such as the maximum independent set problem.
074077a3-602e-59f9-b013-0c4d38cef1cc|D-Dense Decomposition|The authors propose using a d-dense decomposition algorithm to optimize communication efficiency in distributed algorithms. Specifically, they utilize this algorithm to decompose a graph into dense and sparse components. The d-dense decomposition algorithm works by iteratively identifying dense components in a graph and decomposing them into smaller subgraphs. The algorithm uses a distributed approach to compute the density of each component and to update the decomposition. The authors show that the d-dense decomposition algorithm can decompose a graph into dense and sparse components in O(1) rounds. They also demonstrate that this algorithm can be used to solve other distributed problems, such as the k-coloring problem.
504572a4-1c92-5f24-9e67-21bdeadf7d68|Local Lemma Application|The authors propose using a local lemma application to optimize communication efficiency in distributed algorithms. Specifically, they utilize this application to solve the k-coloring problem in a distributed setting. The local lemma application works by iteratively applying a local lemma to a set of events, which are defined as the set of possible color assignments to a vertex. The algorithm uses a distributed approach to compute the probabilities of these events and to update the color assignments. The authors show that the local lemma application can solve the k-coloring problem in O(log n) rounds. They also demonstrate that this application can be used to solve other distributed problems, such as the maximum independent set problem.
adce7edc-ebbc-5cc7-81ce-44bb91585a2d|Distributed Randomized Algorithm for Optimal Graph Coloring|The authors propose a distributed randomized algorithm for optimally coloring graphs with a large maximum degree. The algorithm is designed to handle heterogeneous and irregular graph structures by adaptively adjusting the coloring process based on the graph's properties. The algorithm uses a combination of techniques, including a partial random coloring procedure, a distributed Lovsz Local Lemma, and a graph decomposition method. The partial random coloring procedure assigns colors to vertices randomly, while the distributed Lovsz Local Lemma is used to ensure that the coloring is proper. The graph decomposition method is used to divide the graph into smaller subgraphs, making it easier to color. The authors demonstrate the effectiveness of their algorithm by showing that it can color graphs with a large maximum degree in O(log n log log n) rounds with high probability.
13341600-256f-5ad4-a52c-198183488442|Distributed Load Balancing via Randomized Coloring|This solution involves using a randomized coloring algorithm to distribute tasks across nodes in a distributed system, ensuring that each node has a balanced workload.
0bf46ec4-9dbc-5d5e-94d7-5ff69b993733|Distributed Graph Coloring Algorithm|"The authors propose a distributed graph coloring algorithm that efficiently colors graphs with an optimal number of colors in the LOCAL model of computation. The algorithm is designed to work for graphs with maximum degree  and chromatic number close to . The algorithm uses a combination of techniques, including the Lovsz Local Lemma, graph decomposition, and semi-random coloring. The authors also introduce the concept of ""c-reducers"" and ""hollow sets"" to reduce the graph size while preserving its colorability. The authors show that their algorithm can color graphs with maximum degree  and chromatic number close to  in O(log n log log n) rounds, which is a significant improvement over previous results."
e3fcfebf-2607-5266-9deb-720e02d6cfa8|Buffered Distributed Memory Approach|The authors propose a buffered distributed memory approach to address the challenge of memory-efficient scalable graph processing. This approach involves using a user-defined buffer size to control the amount of intermediate data generated during graph processing, thereby reducing memory consumption and improving scalability.
fddbfead-95d9-5b55-88d9-ddb9357be12f|Bloom Filter-based Edge Query|The authors propose using Bloom Filters to optimize edge membership queries in graph processing, reducing memory consumption and improving scalability.
c73c51e7-fa43-5d54-921c-df12192c1152|Communication-Avoiding Bloom Filter|The authors propose a communication-avoiding Bloom Filter approach to reduce the overall communication volume in graph processing, trading off performance with quality.
4b8dcd85-23be-5efe-ac7e-bb22f543d659|Communication-Avoiding Bloom Filter-based Estimation|The authors propose a communication-avoiding Bloom Filter-based estimation approach to reduce the overall communication volume. This approach involves storing remote edges in Bloom Filters and exchanging the bit arrays among process neighbors.
e64a650b-c9b0-5fb0-a03d-e2d9a0b296e7|Adaptive Associative Containers|The authors propose using adaptive associative containers to optimize edge lookups in distributed memory graph processing. This approach involves using different associative containers, such as std::unordered_map and std::multimap, to store remote edges and improve search times. The adaptive associative container approach uses a combination of different associative containers to store remote edges and improve search times. The authors also employ a variant of vertex-based partitioning to balance the edges across processes. The authors report a 20-30 improvement in execution times for graphs with heavy-tailed degree distributions, and a 3-4 scalability improvement for remote computations between 8-32 nodes.
b013ee9b-3b6f-58d5-8c05-e8fe5b6ac54c|C Associative Containers|The authors propose using C associative containers to optimize edge lookups in distributed systems. This approach reduces the number of edge lookups and improves load balance by using efficient data structures.
4d5a40bd-2aab-5d38-b0fa-8ae4bb59862a|Communication-Avoiding Bloom Filter Estimation|The authors propose a communication-avoiding Bloom Filter estimation approach to reduce the overall communication volume and improve memory access efficiency.
017b4d45-547f-5f66-a1f3-b058adb1b693|Bloom Filter-based Communication Avoidance|The authors propose using a Bloom Filter to reduce the overall communication volume by trading off performance with quality. This approach involves storing the remote edges in Bloom Filters and then exchanging the bit arrays of the underlying process-local Bloom Filters in a single round.
756bf570-4885-5526-b9ca-7f2390220ae0|BiGJoin Dataflow Primitive|The BiGJoin dataflow primitive is a solution that addresses the challenge of memory-efficient scalable graph processing by providing a data parallel dataflow computation primitive for evaluating subgraph queries on large static and dynamic graphs in a distributed setting.
92b319d0-6830-5f3f-bca5-a516af223976|Delta BiGJoin Algorithm|The Delta BiGJoin algorithm is a solution that addresses the challenge of memory-efficient scalable graph processing by providing a distributed algorithm for maintaining join queries under insertion-only workloads.
38e33275-d2f9-5abd-92c5-c993e5a92e08|BiGJoin S Algorithm|The BiGJoin S algorithm is a solution that addresses the challenge of memory-efficient scalable graph processing by providing a distributed algorithm that achieves worst-case optimal communication and computation costs and maintains a total memory footprint linear in the number of input edges.
993a5890-7573-59a1-b7ac-6b4fed5860d5|Compact Index|The compact index is a solution that addresses the challenge of memory-efficient scalable graph processing by providing a data structure that reduces memory usage and improves performance.
4c13d9af-f3e3-5850-a5c3-2de78cadcdef|BiGJoin S|BiGJoin S is a distributed algorithm designed to optimize communication efficiency in distributed algorithms by achieving worst-case optimal communication and computation costs while maintaining workload balance across workers on arbitrary input instances.
787ba5ac-e455-5b86-9dbf-cf27b539d5b9|Delta BiGJoin|Delta BiGJoin is a distributed algorithm that optimizes communication efficiency in dynamic graphs by maintaining indices that can be super linear in the size of the inputs.
daf30653-4440-50fa-a72c-b2953a9cc614|BiGJoin|BiGJoin is a distributed algorithm designed to efficiently process and analyze complex, real-world graphs characterized by heterogeneity in vertex and edge attributes, irregular structures, and variations in density, connectivity, and weights.
2f0c452f-efe0-5974-bed6-f07f3ee9ee3a|Workload Balanced Dataflow|The workload balanced dataflow is a distributed algorithm designed to achieve workload balance across workers in a distributed system. It extends the BiGJoin algorithm by introducing a workload balanced dataflow that ensures each worker has a balanced workload.
e4bf6f85-e5e2-5241-86e6-dbae136071da|Delta-BiGJoin|Delta-BiGJoin is a distributed algorithm designed to efficiently process dynamic updates in large graphs by maintaining join queries over dynamic relations. It uses a dataflow primitive to maintain join queries and supports incremental view maintenance.
1d23d915-7c0f-519f-80a3-8b9e8ca5aae4|BiGJoin-S|BiGJoin-S is a distributed algorithm designed to efficiently process static graphs by using a dataflow primitive to extend prefixes of attributes in the global order. It assembles a dataflow fragment that starts from a stream of prefixes of some number j of attributes and produces as output the corresponding stream of prefixes resulting from the extension.
2dea897c-f7e1-570f-b4e8-9e62b1c0b887|Delta-GJ|Delta-GJ is an incremental view maintenance algorithm designed to efficiently process dynamic updates in large graphs by maintaining join queries over dynamic relations. It uses a dataflow primitive to maintain join queries and supports incremental view maintenance.
f87530be-b472-51a8-8c74-c1171fd182d1|Local-based Genetic Algorithm (GA LP)|GA LP is a genetic algorithm that uses local information to generate offspring, aiming to optimize communication efficiency in distributed algorithms. It relies on the Label Propagation (LP) algorithm to detect communities in large-scale networks.
5253da09-fed8-55d7-aafe-2115e4f6673d|Re nement Phase|The Re nement Phase is a local search strategy that refines the results of GA LP, aiming to improve the quality of the solution.
06e8cafb-84e9-5ded-b7f9-48fe5c819e7e|Directed Modularity|Directed Modularity is a measure used to evaluate the quality of the communities in directed networks, aiming to optimize communication efficiency in distributed algorithms.
0a796385-6750-576b-8433-8a4ab35bc3eb|Roulette Wheel Selection|Roulette Wheel Selection is a selection algorithm used in GA LP to select individuals for the recombination process, aiming to optimize communication efficiency in distributed algorithms.
78259f6a-e485-54bb-99d9-2f77884b2c17|Crossover Operator|The Crossover Operator is a genetic operator used in GA LP to generate offspring, aiming to optimize communication efficiency in distributed algorithms.
fa0af304-4b94-547d-a980-6eab55503d97|Genetic Algorithm based on Label Propagation (GA LP)|GA LP is a solution that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by proposing a genetic algorithm that incorporates the Label Propagation (LP) algorithm to detect communities in directed networks. The GA LP algorithm is designed to handle the heterogeneity and irregularity of graph structures by using a local-based search strategy that relies on the LP algorithm to refine the results.
9d65829d-b174-5e63-946d-43ef247ac585|Request-Respond API|The Request-Respond API is a technique used in Pregel to reduce memory consumption and improve scalability in graph processing. It involves allowing workers to request values from other workers, rather than sending messages to all neighbors.
f1c89f41-ec4c-54cb-abe1-ea0dc63eb8e4|Dynamic Repartitioning (DP)|DP is a technique used in GPS to reduce memory consumption and improve scalability in graph processing. It involves dynamically repartitioning the graph across workers during processing.
aad876b7-b367-558c-8caa-42044016158c|Message Combiner|The authors propose a message combiner technique to reduce the number of messages sent over the network. This technique combines multiple messages sent from the same machine to the same vertex into a single message, thereby reducing the overall communication overhead.
b12192f1-9825-53ac-b3ca-c747aa61705c|Mirroring and Request-Respond API|The authors propose using mirroring and request-respond API to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. Mirroring involves creating a mirror of a high-degree vertex in each worker that keeps a partition of the adjacency list of the high-degree vertex. The request-respond API allows workers to request values from other workers and receive the requested values in the next iteration, reducing the number of messages passed.
ebf152bb-7f87-5c97-a5b3-a091f9e443b0|Finishing Computations Serially (FCS)|The authors propose using FCS to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. FCS involves monitoring the size of the active subgraph and sending it to the master for serial computation when the size is small enough, reducing the overhead of distributed computation.
d02b11ed-0be9-5b60-a66e-0cea67d456fd|Edge Cleaning On Demand (ECOD)|ECOD is a technique that delays the operation of edge cleaning and regards the edges as stale edges until they are involved in later computation where they are demanded to be removed.
85454c09-2eed-50fe-9be3-454a979f5fac|Single Pivot (SP)|SP is a heuristic for speeding up the computation of connected components (CCs) by sampling a vertex v and running the cheaper BFS algorithm instead of the CC algorithm from v.
836a1f8d-2e47-5ea9-bfff-54684a9387b2|Search Space Reduction|The authors propose a search space reduction technique to address the challenge of memory-efficient scalable graph processing. This technique aims to aggressively prune away the non-matching part of the background graph, both vertices and edges, to reduce memory consumption and computational costs.
29ad3a6c-31b3-56c3-b583-dab0d51136d7|Redundant Work Elimination|The authors propose a redundant work elimination technique to address the challenge of memory-efficient scalable graph processing. This technique aims to eliminate duplicate computations by maintaining additional state to identify constraint execution results that can be reused across prototypes.
86687de7-52b5-52e9-bf8e-4f9b2ebb9563|Load Balancing through Reshuffling|The authors propose a load balancing technique through reshuffling to address the challenge of memory-efficient scalable graph processing. This technique aims to rebalance the distributed graph by reshuffling vertex to processor assignment to evenly distribute vertices and edges across processing cores.
8886a567-0da5-5119-8fd9-d140c8858857|Prototype Ordering|The authors propose a prototype ordering technique to address the challenge of memory-efficient scalable graph processing. This technique aims to order prototypes in a way that minimizes the number of non-local constraint checks.
e0c417cf-1c00-59eb-8d38-83594841c1a1|Constraint Ordering|The authors propose a constraint ordering technique to address the challenge of memory-efficient scalable graph processing. This technique aims to order constraints in a way that minimizes the number of non-local constraint checks.
0b1a2e84-001d-53d8-bdf9-93f1a2d39a59|Parallel Prototype Search|The authors propose a solution to search multiple prototypes in parallel, leveraging the opportunity to replicate the pruned graph on smaller deployments and search multiple prototypes in parallel.
ef101b8a-b1fe-51ff-97db-f9db2fe02f09|Max Candidate Set Generation|The authors propose a solution to generate the maximum candidate set, which eliminates all the vertices and edges that do not have any chance to participate in a match, regardless of the distance to the search template.
53dc234a-a6f4-5c5f-8b82-eb1cdd52e0c5|Multi-Level Parallelism|The authors propose a multi-level parallelism technique to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This technique aims to offer multiple levels of parallelism, including vertex-level parallelism and search-level parallelism.
93adf0de-a65d-5e72-97f1-f2497c239000|Load Balancing through Reloading on a Smaller Processor Set|This solution involves reloading the problem on a smaller set of nodes to reduce network traffic and improve overall solution efficiency. This approach is specifically designed to address load imbalance issues that arise during the execution of the pattern matching algorithm.
eecb29ff-6078-56e3-af85-82105926ca8e|Iterative Trussness Update Algorithm|The authors propose an iterative algorithm to update trussness values in a graph after edge insertions or deletions. This algorithm aims to minimize the number of iterations required to maintain the maximal k-truss structure.
f515c156-eda4-5a78-984c-dfac7926b91c|Incremental Betweenness Centrality Update Algorithm|The authors propose an incremental algorithm to update betweenness centrality measures in response to edge modifications. This algorithm aims to minimize the computational cost of updating centrality measures.
e05550b7-7102-52e2-9d6c-b45c6fd11e04|Adaptive Trussness Update Algorithm|The authors propose an adaptive algorithm to update trussness values in a graph after edge insertions or deletions. This algorithm aims to minimize the number of iterations required to maintain the maximal k-truss structure.
f80df2c9-38ce-5b29-a0a1-b35a04f04ac3|Slow Passing Fast Consuming (SPFC) Approach|The SPFC approach is a novel method for memory-efficient scalable graph processing that combines slow message passing with fast message consuming. This approach aims to reduce memory consumption by delaying the passing of less important messages and allowing vertices to consume messages quickly, thereby reducing the number of messages stored in memory.
83a94d78-5751-5a1d-9764-f74201d51277|Vertex Mirroring Technique|The vertex mirroring technique is a method for reducing memory consumption by mirroring vertices on neighbor workers, allowing vertices to access the latest state of their neighbors without waiting for messages to be received.
be0fd01d-6260-582e-a086-b53f4e14e6c7|Approximate Computation Approach|The approximate computation approach is a method for reducing memory consumption by eliminating less useful messages and preserving the smartest messages.
3e2e981a-6739-594d-9388-8b7c50dcf49d|Priority-Based Message Passing Scheduler|The priority-based message passing scheduler is a method for reducing memory consumption by prioritizing messages based on their importance.
4bf9c0d3-161d-58ee-af83-6d9a29eecbc2|Two-Phase Termination Check Protocol|The two-phase termination check protocol is a method for reducing memory consumption by removing the global synchronous barrier and allowing vertices to terminate quickly.
8b1c5ed2-3d35-593d-a5c8-2689e0f9e0f2|Priority Setting Strategy|The priority setting strategy is a technique used in the SPFC approach to determine which vertices should have high priority and send messages first. The strategy is based on the idea that vertices with more intermediate states or messages that can lead to more intermediate states should have low priority.
495f079e-bc86-5360-bde2-788f881f70b3|Slow Message Passing Scheduler|The authors propose a slow message passing scheduler as a solution to optimize load balance in distributed systems. This scheduler is designed to reduce the overhead of passing and processing messages, which is a significant contributor to load imbalance in distributed graph processing systems.
d19fa763-d2e6-57bc-8255-281fa9a89f7c|Fast Message Consuming Strategy|The authors propose a fast message consuming strategy to optimize load balance in distributed systems. This strategy involves removing the message store and directly applying received messages to the destination vertex, reducing the overhead of message processing and improving load balance.
b3317387-c44d-533c-9ab1-770c38e6c69f|Biconnected Components Decomposition|The authors propose using biconnected components decomposition to reduce memory consumption and improve scalability in graph processing. This approach involves dividing the graph into smaller subgraphs, which can be processed independently, thereby reducing memory requirements.
d761c02d-6ccb-5ca5-9084-dcdb3d75254f|Incremental Computation within Components|The authors propose an incremental method to update betweenness centrality within each graph component, avoiding redundant recomputation. This approach involves identifying the nodes that need to be updated and performing a partial breadth-first search to update the betweenness centrality values.
4d1b5db1-bf2d-5a6b-8211-1ea9672ec25b|Parallel Implementation of iCENTRAL|The authors propose a parallel implementation of their algorithm, iCENTRAL, which can handle large-scale graph data. This approach involves dividing the graph into smaller subgraphs, which can be processed independently, thereby reducing memory requirements.
42794f76-ff8f-5aca-a0df-7bca4b90566c|iCENTRAL|iCENTRAL is an incremental algorithm for computing betweenness centrality in evolving graphs, which scales to large graphs by requiring space linear to the graph size.
f1006dfa-5c2a-5708-be59-a6b32d90998d|Incremental Betweenness Centrality Computation within Biconnected Components|This solution addresses the challenge of efficient graph dynamics processing by proposing an incremental algorithm for updating betweenness centrality in evolving graphs. The algorithm, called iCENTRAL, decomposes the graph into biconnected components and proves that processing can be localized within the affected components. This approach avoids redundant recomputation within the affected component, making it more efficient than existing methods.
9cbd2f15-58b3-518a-85a2-f2e50742cfd8|Pattern-Aware Graph Exploration|The authors propose a pattern-aware graph exploration approach that directly finds subgraphs of interest while avoiding exploration of unnecessary subgraphs, and simultaneously bypassing expensive computations throughout the mining process.
8e490d8d-07da-567d-a2b0-20c2218283dd|Anti-Edge and Anti-Vertex Abstractions|The authors introduce two novel abstractions, Anti-Edge and Anti-Vertex, that express advanced structural constraints on patterns to be matched, allowing users to easily express complex mining tasks.
4cebb081-978d-5577-9470-30436edda4d7|Domain-Based Support Calculation|The authors propose a domain-based approach for support calculation, which is more memory-efficient than traditional approaches.
bacf9105-483c-54eb-b1ce-3b3ffd49d0de|Exploration Plan Generation|The authors propose a method for generating an exploration plan from a given pattern, which guides the data graph exploration to ensure generated matches are unique.
abec575f-cc67-5df9-9dc7-4bad4ccf8e71|Pattern-Aware Graph Mining|The authors propose a pattern-aware graph mining system, PEREGRINE, which directly explores the subgraphs of interest while avoiding exploration of unnecessary subgraphs and simultaneously bypassing expensive computations throughout the mining process.
39f69912-3dde-5272-bae2-61422155c73f|Symmetry Breaking|The authors propose using symmetry breaking to guide graph exploration and reduce workload imbalance.
885f9fa5-43bb-52fb-8242-aa09947010af|Low-Matching-Order Traversal|The authors propose a low-matching-order traversal strategy to reduce workload imbalance.
ac2f98b6-26ec-5e36-bd5b-8ad762b9c181|Dynamic Load Balancing|The authors propose a dynamic load balancing strategy to reduce workload imbalance.
2e2dfc74-1137-5dfc-a01e-a879dc0e18bd|Early Termination for Existence Queries|The authors propose an early termination strategy for existence queries to reduce unnecessary exploration.
f24d71bc-59d3-568d-8331-31da86520c0f|Dynamic Load Balancing through Matching Order Enforcement|This solution involves enforcing a strict matching order based on vertex degrees to reduce load imbalance in distributed graph mining systems. By following a high-to-low matching order, the system can dynamically prune more explorations from high-degree tasks while enabling those explorations in low-degree tasks.
521a4def-9cfe-534b-8027-66f90bcb4908|Early Pruning for Dynamic Load Balancing|This solution involves pruning tasks as they become free, allowing threads to pick up new tasks dynamically. This approach helps to reduce load imbalance by minimizing the time spent on unnecessary explorations.
eb1b3599-581f-5c7a-be7b-98223aacd4fd|On-the-fly Aggregation for Load Balancing|This solution involves performing on-the-fly aggregation to provide global updates as mining progresses. This approach helps to reduce load imbalance by minimizing the time spent on aggregation.
f88c88b5-31ad-59bd-b5bd-c0d757c11516|Pattern-Aware Matching Engine|The authors propose a pattern-aware matching engine that directly finds canonical subgraphs from a given vertex in the data graph.
469f9458-f91b-5fa7-8918-e53e392d5eeb|Deterministic Low Diameter Decompositions (DLDDs)|The authors propose using Deterministic Low Diameter Decompositions (DLDDs) to address the challenge of memory-efficient scalable graph processing. DLDDs are a type of graph decomposition that partitions the graph into smaller subgraphs with low diameter, allowing for more efficient processing and reduced memory consumption.
1dd7be9c-e2d6-5ebc-87c6-573fa7b72051|High Girth Cycle Decompositions|The authors propose using High Girth Cycle Decompositions to address the challenge of memory-efficient scalable graph processing. High Girth Cycle Decompositions involve decomposing the graph into edge-disjoint cycles with high girth, allowing for more efficient processing and reduced memory consumption.
c4c1f301-46c8-587a-8f12-77b8cb4e248e|Sparse Neighborhood Covers|The authors propose using Sparse Neighborhood Covers to address the challenge of memory-efficient scalable graph processing. Sparse Neighborhood Covers involve partitioning the graph into smaller subgraphs with low diameter and overlap, allowing for more efficient processing and reduced memory consumption.
df796705-e47a-58e2-85b1-e381fb214900|Iterated Path Count Flows|The authors propose using Iterated Path Count Flows to address the challenge of memory-efficient scalable graph processing. Iterated Path Count Flows involve iteratively computing path counts and flows in the graph, allowing for more efficient processing and reduced memory consumption.
23c704eb-c73e-5621-aaf9-c76883d6c0a3|Length-Constrained Cutmatches|The authors propose using Length-Constrained Cutmatches to address the challenge of memory-efficient scalable graph processing. Length-Constrained Cutmatches involve computing a collection of paths between two sets of nodes and certifying that there is no low-congestion way of extending the current collection of paths with a moving cut.
1659aced-684d-584d-a8fd-22e9e76f7026|Batched Multiplicative Weights Framework|The authors propose a batched version of the multiplicative weights framework to optimize communication efficiency in distributed algorithms. This approach involves iteratively updating the weights of the arcs in the graph based on the flow values and capacities, allowing for more efficient communication and reducing the number of rounds required.
0b985d01-ce46-5ab9-b739-03de5f5f2daf|Lightest Path Blockers|The authors propose the use of lightest path blockers to optimize communication efficiency in distributed algorithms. This approach involves computing the lightest path blockers in the graph and using these blockers to update the flow values and capacities.
faea7e0e-4b19-55e8-8c66-1fe4a52a5c75|Distributed Expander Decompositions|The authors propose the use of distributed expander decompositions to optimize communication efficiency in distributed algorithms. This approach involves computing the expander decompositions of the graph and using these decompositions to update the flow values and capacities.
100cc070-f034-5df8-aa66-a16ccc9406db|Deterministic Low Diameter Decompositions|The authors propose a deterministic low diameter decomposition algorithm to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm is designed to handle graphs with varying degrees, weights, and sparsity, and to reduce communication overhead and enhance memory locality.
2b50953b-0746-5605-a732-836f97a6bf60|Batched Multiplicative Weights|The authors propose a batched multiplicative weights algorithm to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm is designed to handle graphs with varying degrees, weights, and sparsity, and to reduce communication overhead and enhance memory locality.
8dc84807-112b-5122-83c9-40e06a98934e|Length-Weight Expanded DAG|The authors propose a length-weight expanded DAG algorithm to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm is designed to handle graphs with varying degrees, weights, and sparsity, and to reduce communication overhead and enhance memory locality.
76f83085-1073-51a9-ac7a-acfade6a80d7|High Girth Cycle Decomposition|The authors propose a high girth cycle decomposition algorithm to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm is designed to handle graphs with varying degrees, weights, and sparsity, and to reduce communication overhead and enhance memory locality.
ea47cb7c-5ee9-56ee-a7db-b4e5342ccae1|Deterministic Integral Blocking Flows via Flow Rounding|This solution addresses the challenge of optimizing load balance in distributed systems by proposing a deterministic algorithm for computing integral blocking flows in a distributed setting. The algorithm uses a flow rounding technique to ensure that the computed flow is integral and feasible.
d9226d62-eeab-5011-b021-39808587fc3c|Deterministic Integral Blocking Flows|This solution addresses the challenge of efficient graph dynamics processing by proposing a novel approach to computing integral blocking flows in dynamic graphs. The authors introduce the concept of deterministic integral blocking flows, which can be computed in a more efficient manner than traditional methods.
e4038b4f-0e40-577b-a2e8-15f004f6744a|High-Girth Cycle Decompositions|This solution addresses the challenge of efficient graph dynamics processing by proposing a novel approach to computing high-girth cycle decompositions in dynamic graphs. The authors introduce the concept of high-girth cycle decompositions, which can be computed in a more efficient manner than traditional methods.
25739369-9f8b-53b3-9fc6-395792aa0edd|Deterministic CONGEST Algorithm for Maximum Independent Set|This solution addresses the challenge of efficient graph dynamics processing by proposing a novel approach to computing maximum independent sets in dynamic graphs. The authors introduce the concept of a deterministic CONGEST algorithm that computes a maximum independent set in a more efficient manner than traditional methods.
f87716fd-8e42-5598-bb58-0de1bdf643c8|Multi-Commodity Length-Constrained Flows and Cutmatches|This solution addresses the challenge of efficient graph dynamics processing by proposing a novel approach to computing multi-commodity length-constrained flows and cutmatches in dynamic graphs. The authors introduce the concept of multi-commodity length-constrained flows and cutmatches, which can be computed in a more efficient manner than traditional methods.
d7165f2b-6b0f-5bf1-88ec-6a0d5db538f3|Deterministic Expander Decompositions|This solution addresses the challenge of efficient graph dynamics processing by proposing a novel approach to computing deterministic expander decompositions in dynamic graphs. The authors introduce the concept of deterministic expander decompositions, which can be computed in a more efficient manner than traditional methods.
15eab978-86cf-5595-9e5b-b544899cd4e6|1-Approximate Distributed Bipartite b-Matching|This solution addresses the challenge of efficient graph dynamics processing by proposing a novel approach to computing 1-approximate distributed bipartite b-matching in dynamic graphs. The authors introduce the concept of 1-approximate distributed bipartite b-matching, which can be computed in a more efficient manner than traditional methods.
32b27183-75e1-5f26-9274-cef7fd6fe738|Overapproximating Directed Acyclic Graph (ODAG)|ODAG is a data structure designed to store and compress embeddings in graph mining algorithms, enabling memory-efficient scalable graph processing. ODAG represents embeddings as sequences of numbers, and stores them in a compact manner by grouping embeddings in one ODAG per pattern. This approach reduces memory consumption by avoiding the need to store each embedding separately. The paper reports that ODAGs can reduce memory cost by several orders of magnitude, even in relatively small graphs such as CiteSeer.
1c72d8ed-0dbe-540c-b5db-c1ce85c10c37|Two-Level Pattern Aggregation|This solution is an optimization technique for pattern-based aggregation in graph mining algorithms, which reduces the number of graph isomorphism checks required. The technique involves computing a quick pattern for every embedding, obtaining a number of quick patterns that is orders of magnitude smaller than the candidate embeddings, and then executing graph isomorphism only for quick patterns. This approach minimizes the required number of graph isomorphism checks. The paper reports that disabling two-level pattern aggregation can increase execution time up to 4 times, demonstrating the effectiveness of this solution.
90ba74df-6b80-5b29-94ca-d3381c621464|Coordination-Free Exploration Strategy|This solution is a technique for avoiding redundant work and minimizing communication costs in distributed graph mining algorithms. The technique involves using a canonicality check to avoid redundant work, and storing embeddings ef ciently using ODAGs. This approach enables coordination-free exploration, reducing communication overhead and improving scalability. The paper reports that the coordination-free exploration strategy enables Arabesque to scale to hundreds of cores over a cluster, obtaining orders of magnitude reduction of running time over centralized baselines.
62b7c0bb-4ce9-5367-991b-92051213e164|Distributed k-core Decomposition Algorithm|The authors propose a distributed k-core decomposition algorithm for large dynamic graphs that are partitioned and distributed across multiple machines. This algorithm addresses the challenge of memory-efficient scalable graph processing by allowing the computation of k-cores in a distributed manner, reducing the memory requirements for processing large graphs.
6659b7a0-601f-51b0-9300-8b09ed3e3ab6|Incremental k-core Maintenance Algorithm|The authors propose an incremental k-core maintenance algorithm for dynamic graphs that updates the k-cores in response to edge insertions or deletions. This algorithm addresses the challenge of memory-efficient scalable graph processing by minimizing the number of nodes that need to be updated during the computation.
f4b5dde2-43fd-5c10-8190-5eb36092b592|Distributed Streaming k-core Decomposition Algorithm|The authors propose a distributed streaming k-core decomposition algorithm for large dynamic graphs that are partitioned and distributed across multiple machines. This algorithm addresses the challenge of memory-efficient scalable graph processing by allowing the computation of k-cores in a streaming manner, reducing the memory requirements for processing large graphs.
dc6d6fe5-ce27-5eb8-b8c2-0cb1d3a5e764|Pruning Strategy for Candidate Nodes|The authors propose a pruning strategy to reduce the number of candidate nodes that need to be updated after an edge insertion or deletion. This strategy is based on Theorem 2, which states that if the number of neighbors of a node w with coreness greater than or equal to k_w is less than k_w, then the coreness of w remains unchanged.
f515f689-2e42-5eab-b292-9e2c29505be2|Distributed k-Core Decomposition Algorithm|The authors propose a distributed k-core decomposition algorithm that computes the coreness of nodes in a graph by iteratively removing edges and updating the coreness values of affected nodes. The algorithm is designed to minimize communication rounds by only updating nodes that are directly affected by the edge removals.
1a7a6205-b18e-5fd2-8197-fab37734ff76|W2W Computing Mode|The authors propose a W2W (Worker-to-Worker) computing mode that allows workers to communicate directly with each other to update coreness values. This mode is used in conjunction with the M2W (Master-to-Worker) mode to minimize communication rounds.
f1aec6b7-827e-506b-96b2-49f8a77fdc17|Local Computing Mode|The authors propose a Local computing mode that allows workers to perform local computations to update coreness values without needing to communicate with other workers.
5e91900a-b257-5d89-a904-4b473c9f8324|Streaming k-core Decomposition Algorithm|The authors propose a streaming k-core decomposition algorithm to handle dynamic graphs with incremental changes. The algorithm is designed to adapt to changes in the graph structure and update the k-core decomposition accordingly.
81175313-2a8a-5812-a34b-ae9330fc5b68|Incremental k-core Maintenance Strategy|The authors propose an incremental k-core maintenance strategy that updates the coreness of nodes in response to edge insertions/deletions, without recomputing the entire graph.
b1a19738-d17b-5ac1-b176-af76dc09b15f|Distributed Random Walk Betweenness Centrality Algorithm|The authors propose a distributed algorithm to compute random walk betweenness centrality in a memory-efficient and scalable manner. The algorithm is designed to work under the CONGEST model, where each edge can only transfer O(log n) bits in each round.
3b1bad79-380e-5187-a81f-cba67f0cb0b8|Random Walk Simulation with Bounded Length Constraint|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by simulating random walks with a bounded length constraint. The authors propose imposing a bounded length constraint l on the random walks, which allows them to reduce the number of communication rounds required to compute the random walk betweenness centrality.
4ea3d3dc-162a-5deb-8a26-7ca97176cfcc|Distributed Random Walk Betweenness Algorithm with O(log n) Random Walks|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a distributed random walk betweenness algorithm that uses O(log n) random walks. The authors show that this algorithm can compute the random walk betweenness centrality with an approximation ratio of 1 - , where  is an arbitrarily small constant.
fe030254-520d-570d-94f6-ec30fd135a6c|Distributed Random Walk Betweenness Algorithm|The authors propose a distributed algorithm to compute random walk betweenness centrality in linear time, which can handle heterogeneous graph structures with varying degrees and weights.
50e250a3-848d-5566-a112-2551d22bec70|Distributed Random Walk Algorithm|The authors propose a distributed algorithm to compute random walk betweenness centrality in linear time. The algorithm involves each node maintaining a parameter for each source, counting the number of times random walks starting at each source pass through it. The algorithm then computes the random walk betweenness using the counts and the node's neighbors' information.
7d972bb2-ad61-5b9f-8197-42768e6cd9f0|Message Passing Interface with Customizable Execution Policies|The authors propose a message passing interface that allows users to specify customizable execution policies for graph processing. This solution addresses the challenge of memory-efficient scalable graph processing by enabling users to control the order of vertex updates and message passing, which can help reduce memory consumption and optimize memory usage.
c4eac5bd-b6ca-586b-a29a-a63aeca49142|Batch Scheduling with Priority-based Vertex Scheduling|The authors propose a batch scheduling approach that schedules a set of vertices to be processed for the next tick at the barrier, allowing for both fast convergence and high scalability. This solution addresses the challenge of memory-efficient scalable graph processing by reducing memory consumption and optimizing memory usage through efficient vertex scheduling.
985a9fc1-956e-56de-a5cd-d4f0fe5c7b24|Low-Overhead Concurrency Control Mechanism|The authors propose a low-overhead concurrency control mechanism for vertex updates, which addresses the challenge of memory-efficient scalable graph processing by reducing memory consumption and optimizing memory usage through efficient concurrency control.
49826096-483c-59ed-a94a-5cffd7f43b04|Asynchronous Execution with Message Passing|The authors propose an asynchronous execution model with message passing to optimize communication efficiency in distributed algorithms. This approach allows vertices to process messages as soon as they are received, rather than waiting for a global synchronization barrier.
a120e120-4a5e-5c82-b27d-9f8c05015ec4|Prioritized Scheduling with Message Selection|The authors propose a prioritized scheduling approach with message selection to optimize communication efficiency in distributed algorithms. This approach schedules vertices based on their priority values, which are updated based on the residual of the newly received message.
43d90e77-223c-52a5-8d5f-5739bf4e45bf|Eager Execution with Message Relaxation|The authors propose an eager execution approach with message relaxation to optimize communication efficiency in distributed algorithms. This approach allows vertices to invoke their Proceed function before all incoming messages have arrived, relaxing the consistency property of the BSP model.
725d70a8-33dc-5a1a-90da-569c9aac80c5|Gauss-Seidel Execution with Message Selection|The authors propose a Gauss-Seidel execution approach with message selection to optimize communication efficiency in distributed algorithms. This approach schedules vertices based on their priority values, which are updated based on the residual of the newly received message.
cd44bd7b-3006-5716-b1f5-55efe5cb7d3f|GRACE Framework|The GRACE framework is a parallel graph processing framework that provides an iterative synchronous programming model for developers. It captures data dependencies using messages passed between neighboring vertices like the BSP model.
7180b01e-4e0e-5df6-a8d2-b813bf3b4540|Asynchronous Execution in BSP|The authors propose an asynchronous execution model within the Bulk Synchronous Parallel (BSP) framework to efficiently process dynamic updates in large graphs. This approach allows for the relaxation of isolation and consistency properties in the BSP model, enabling faster convergence and better performance.
ca1a02bc-bbaa-5a88-9770-1f7cd6383f39|Customizable Execution Interface|The authors propose a customizable execution interface that allows users to specify their own asynchronous execution policies, enabling efficient graph dynamics processing. This interface provides a set of functions that can be used to update vertex scheduling priorities, select messages, and schedule vertices for processing.
5eb3a2ff-b5c7-5d4c-a96f-3986a01cd047|Gauss-Seidel Execution Policy|The authors propose a Gauss-Seidel execution policy, which is a specific asynchronous execution policy that can be used to efficiently process dynamic updates in large graphs. This policy involves scheduling vertices in a specific order, based on their scheduling priorities, to minimize the number of iterations required to converge.
d1b8cd64-dc31-5a47-b953-a1dd5a92499c|Subgraph Generation Method|The authors propose a subgraph generation method to address the challenge of memory-efficient scalable graph processing. This method involves transforming the input graph into a formation where each record conveys part of two-leap information of a vertex. This allows for the detection of all maximal cliques that a vertex involves, reducing memory consumption and optimizing memory usage.
569d96d2-84d6-5cd6-98c3-c6a61840e0bd|Key-Based Clique Enumeration Algorithm|The authors introduce a key-based clique enumeration algorithm that operates on subgraphs generated by the subgraph generation method. This algorithm performs a depth-first search in a key-oriented manner, reducing memory consumption and optimizing memory usage.
cc519c11-e05a-5409-9cca-e5eb48f8b755|MapReduce-Based Distributed Processing|The authors propose a MapReduce-based distributed processing approach to address the challenge of memory-efficient scalable graph processing. This approach involves partitioning the input graph into smaller subgraphs, processing each subgraph in parallel, and combining the results to obtain the final output.
15d8a422-127e-56c2-80e7-4f79695aa29f|Optimized Message Dissemination|The authors propose an optimization technique to reduce the number of messages exchanged among hosts in the one-to-many scenario. They suggest that each host x creates a message containing only those updates that could be interesting for host y, instead of sending all updates to all nodes.
115082a5-c863-58b1-be91-4c3cdb169728|Broadcast-Based Communication|The authors propose the use of broadcast-based communication in the one-to-many scenario, where a single message is sent at each round, containing all the estimates that have changed since the previous one.
ee4efa26-2871-57ee-a39a-92d6b429c0fc|Adaptive Termination Detection|The authors propose a decentralized approach to detect when convergence to the correct coreness values has been reached. They suggest using epidemic protocols for aggregation to compute the last round in which any of the hosts has generated a new estimate.
8943e332-7bf6-5e93-928a-f5e3aca0b013|Node-Host Assignment Policy|The authors propose a simple policy for assigning nodes to hosts in the one-to-many scenario. They suggest assigning each node u to host u mod H, where H is the number of hosts.
bf1960e0-4429-5b5c-b095-a98a06d4f75e|In-Memory Message Organization|This solution addresses the challenge of memory-efficient scalable graph processing by organizing messages in memory, allowing for efficient processing and reducing the need for disk I/O.
0fb36513-aea0-56f9-84d8-1f3d1f020f67|Partial Computation|This solution addresses the challenge of memory-efficient scalable graph processing by only activating a partial set of vertices, reducing memory usage and computational costs.
04db6c25-88d1-5123-9f0e-01cb3bf3b915|Inverted Neighbor List|This solution addresses the challenge of memory-efficient scalable graph processing by using an inverted neighbor list to activate host vertices, reducing memory usage and computational costs.
5e23b8bc-1983-5dce-b1c8-a66142584b0c|Sync via Attributes|This solution addresses the challenge of memory-efficient scalable graph processing by syncing vertex attributes instead of repeatedly sending messages through each edge, reducing communication cost and memory usage.
3e00b7ba-73af-55df-a787-5660faaa7a24|Compute and Sync Model|The authors propose a compute and sync model to optimize communication efficiency in distributed algorithms. This model involves a two-phase approach, where the compute phase focuses on local computation and the sync phase synchronizes the vertex states in all working machines. The compute and sync model reduces communication cost by avoiding unnecessary message transmission and achieving high I/O efficiency. It also supports keeping messages in memory, which significantly speeds up the computation. The authors demonstrate that their system, SCALEG, outperforms existing disk-based systems in terms of communication cost and running time. Specifically, SCALEG achieves a 14.7x and 7.8x speedup over GraphD and GDIR, respectively, on average.
d8326c9f-e3ec-58a9-8e80-e75603aef247|In-Memory Message Management|The authors propose an in-memory message management technique to optimize communication efficiency in distributed algorithms. This technique involves maintaining all messages in memory and bounding the size of communication messages. The in-memory message management technique reduces communication cost by avoiding disk I/O and minimizing the number of messages transmitted between machines. The authors demonstrate that their system, SCALEG, achieves a significant reduction in disk I/O and memory cost compared to existing disk-based systems. Specifically, SCALEG reduces disk I/O by 61% and 46% compared to GraphD for running BFS and PR, respectively.
6eebfabf-945e-59fc-a73e-f5e0540f9554|Adaptive Activation Mechanism|The authors propose an adaptive activation mechanism to boost the activation efficiency of vertices in distributed graph processing.
fffa2b3b-19fd-5dd6-8436-e55e1b54ebb0|Neighborhood Expression|The authors propose a novel approach to efficient graph dynamics processing by introducing the concept of Neighborhood Expression, which allows users to locally access all neighbors for each vertex. This approach enables the system to automatically handle vertex interactions, message reductions, and activation, thereby reducing the computational cost and number of iterations.
b5fbba64-d5c1-5836-81d9-34236f226020|Hybridization Technique|The authors propose a hybridization technique that combines the D-stepping algorithm with the Bellman-Ford algorithm to reduce the number of relaxations and improve performance. This technique involves running the D-stepping algorithm for the initial phases and then switching to the Bellman-Ford algorithm.
bf7561c2-9af1-59d9-8083-e6737b0dbb73|Load Balancing Strategy|The authors propose a load balancing strategy that splits the neighborhood of high-degree vertices across processing nodes to reduce load imbalance. This strategy involves distributing the incident edges of high-degree vertices among other processing nodes.
e9dc18de-41eb-53e1-a703-c8377157e68e|Pruning Technique|The authors propose a pruning technique that reduces the number of relaxations by classifying edges into short and long edges. This technique involves relaxing only a fraction of the input edges and avoiding redundant relaxations.
85536a41-e870-5535-8869-5821285eb8c6|Inner-Outer Short (IOS) Heuristic|The authors propose the IOS heuristic, which further reduces the number of short edge relaxations by classifying short edges into two groups.
4ff308f9-6353-59f4-9adf-a8d699f87f11|Load Balancing Technique|The authors propose a load balancing technique that distributes the load evenly among processors to optimize communication efficiency. This technique involves splitting the neighborhood of heavy degree vertices across processing nodes and distributing the load among threads within a processing node.
c03dbc21-85ed-5c05-a102-854aa0cbf4ee|Hybrid Push-Pull Model|The authors propose a hybrid push-pull model that combines the benefits of both push and pull models to reduce communication overhead and improve memory locality. This solution specifically addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by dynamically switching between push and pull models based on the graph structure and vertex degrees.
94174a9a-cfdf-5cc3-b7ad-19b7104fbacb|Intra-Node Thread-Level Load Balancing|The authors propose an intra-node thread-level load balancing strategy to overcome load imbalance issues in heterogeneous graphs. This solution specifically addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by dynamically distributing vertices among threads based on their degrees.
37b93049-f215-5ca2-aa95-ece199126422|Inter-Node Vertex Splitting|The authors propose an inter-node vertex splitting strategy to reduce communication overhead and improve memory locality in heterogeneous graphs. This solution specifically addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by splitting high-degree vertices across multiple nodes.
324e02cf-db2b-5041-99a3-90bbba46e844|Edge Classification and Direction Optimization|The authors propose an edge classification and direction optimization technique to reduce communication overhead and improve memory locality in heterogeneous graphs. This solution specifically addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by classifying edges into short and long edges and optimizing their direction.
b5f119a0-2a68-5c8e-8632-dbe2d2fbe777|Inter-Node Load Balancing using Vertex Splitting|This solution involves splitting vertices with extreme degrees and distributing their incident edges among other processing nodes to achieve better load balance.
341d3e1b-c902-585c-87c2-64d9a70a27d3|Two-Tiered Load Balancing Strategy|This solution involves employing a combination of intra-node thread-level load balancing and inter-node load balancing using vertex splitting to achieve optimal load balance.
54627825-8920-5b0c-a6ce-485776e04da3|Pruning Heuristic|The pruning heuristic is a solution proposed by the authors to address the challenge of efficient graph dynamics processing. This heuristic involves classifying edges into short and long edges and relaxing only the forward edges in the long edge phase.
c21075ea-078f-51b5-8c50-73f41ec6f486|Push-Pull Decision Heuristic|The push-pull decision heuristic is a solution proposed by the authors to address the challenge of efficient graph dynamics processing. This heuristic involves making a push-pull decision at the end of each bucket to determine whether to use the push model or the pull model.
9f1a6a07-c7d9-58fc-b3c8-033d84ac3f9c|Parallelization of the Modi edDCSC Algorithm|The authors propose a parallel implementation of the Modi edDCSC algorithm to address the challenge of memory-efficient scalable graph processing. The algorithm is designed to find strongly connected components in distributed graphs, which is a crucial step in radiation transport applications.
2b19d180-bc42-5d83-8ae8-87600dbfacd4|Binary Tree Termination Detection|The authors propose a binary tree termination detection algorithm to optimize communication efficiency in distributed algorithms. This solution specifically addresses the challenge by reducing the time complexity of termination detection from O(P) to O(log P), where P is the number of processors.
05cd1434-4b3d-5af5-8e65-800ddce84325|Simultaneous Work on Multiple Problem Instances|The authors propose exploiting additional parallelism on multiple problem instances by working on all graphs simultaneously. This solution specifically addresses the challenge by reducing idle time and improving overall performance.
c1762687-2430-5480-94c9-949a57e67082|Modified DCSC Algorithm|The authors propose a modified version of the DCSC algorithm, which is designed to efficiently detect strongly connected components in distributed graphs. This algorithm is specifically tailored to handle heterogeneous and irregular graphs, which are common in real-world applications.
f4ccbf06-f112-52ff-b4f3-e3ee5c4b15db|Hybridization with Tarjan's Algorithm|The authors propose a hybrid approach that combines the modified DCSC algorithm with Tarjan's algorithm. This approach is designed to handle graphs with many strongly connected components, which can be challenging for the modified DCSC algorithm.
54808596-d987-5d2f-9ee3-ed97b1e59a15|Hybridization of Modi edDCSC with Tarjan's Serial Algorithm|This solution proposes a hybrid approach that combines the Modi edDCSC algorithm with Tarjan's serial algorithm to optimize load balance in distributed systems. The idea is to use Tarjan's algorithm to detect local SCCs on each processor independently, collapse these SCCs into single nodes (supernodes), and then apply the Modi edDCSC algorithm to the remaining graph.
17fa3f69-1d25-5d27-983e-806073c6b79a|Distributed Termination Detection Algorithm|This solution involves using a distributed termination detection algorithm to determine when a trim or mark step is completed. The algorithm uses a binary tree topology to send termination detection messages, allowing for efficient detection of termination conditions.
631b6ea8-dea6-53fc-8557-a76132548733|Modi edDCSC Algorithm|The Modi edDCSC algorithm is a parallel algorithm designed to find strongly connected components in distributed graphs. It addresses the challenge of efficient graph dynamics processing by minimizing computational costs and iterations through a divide-and-conquer approach. The algorithm uses a recursive partitioning approach, where each recursive step selects a random pivot vertex and partitions the graph into three disjoint subgraphs. The algorithm then recursively applies itself to each subgraph, reducing the problem size and minimizing iterations. The use of a trim step at the beginning of each iteration helps to eliminate portions of the graph without strongly connected components, further reducing computational costs. The paper presents experimental results demonstrating the effectiveness of the Modi edDCSC algorithm in finding strongly connected components in large graphs. The results show that the algorithm achieves good scalability and performance on thousands of processors, with execution times much less than those of the numerical computation it precedes.
a960b232-a9aa-5fd8-8619-ef8c53d71422|Hierarchical Embedding of Random Graphs|The authors propose a hierarchical embedding of random graphs to address the challenge of memory-efficient scalable graph processing. This solution involves constructing a hierarchical structure of random graphs, where each level of the hierarchy represents a refinement of the graph partitioning. The authors use this structure to route messages efficiently between different parts of the graph, reducing communication overhead and memory consumption.
40829727-8c5b-5235-aee5-0885ac56e0fe|Pseudo-Random Partitions and Portals|The authors propose the use of pseudo-random partitions and portals to address the challenge of memory-efficient scalable graph processing. This solution involves partitioning the graph into smaller parts using pseudo-random hash functions and adding portals to facilitate communication between different parts of the graph.
527526cf-0bf5-581b-ae33-7b851f9ff685|Distributed Permutation Routing|The authors propose a distributed permutation routing algorithm to address the challenge of memory-efficient scalable graph processing. This solution involves routing messages between different parts of the graph using a hierarchical structure of random graphs.
9b54939d-3cdc-5d66-8289-ddcc294c5788|Hierarchical Routing Scheme|The authors propose a hierarchical routing scheme to address the challenge of memory-efficient scalable graph processing. This solution involves using a hierarchical structure of random graphs to route messages between different parts of the graph.
d884d79f-aa85-58fb-9a4d-7655004d4d04|Low-Congestion Embedding of Random Graphs|The authors propose a low-congestion embedding of random graphs to optimize communication efficiency in distributed algorithms. This solution involves embedding a random graph in the original graph, resulting in a low-congestion embedding that enables efficient routing of messages.
a31d0cef-05fa-55d5-8315-87911315dc61|Adaptive Routing using Hierarchical Portals|The authors propose an adaptive routing algorithm that uses hierarchical portals to efficiently route packets between nodes in a heterogeneous and irregular graph. This solution involves constructing a hierarchical structure of random graphs and using it to route packets between nodes.
86d1feef-4545-59b5-ada3-55e6ca2a0ee5|Virtual Tree Construction|The authors propose a virtual tree construction algorithm that can be used to maintain a balanced tree structure in a distributed system. This algorithm is designed to work in conjunction with the hierarchical embedding of random graphs.
d83d31bc-3b59-5ad8-a3fc-c7cf93abaf38|Portal Construction|The authors propose a portal construction algorithm that can be used to construct portals between different parts of the graph. This algorithm is designed to work in conjunction with the hierarchical embedding of random graphs.
3990e881-8d6d-550a-b399-72cbc282f903|Hierarchical Embedding of Good Expansion Random Graphs|The authors propose a hierarchical embedding of good expansion random graphs on the base graph to optimize GPU memory access for graph processing. This approach involves recursively dividing the graph into smaller parts and embedding a new random graph in each part, resulting in a hierarchical structure that allows for efficient routing and memory access. The use of random walks enables the construction of a low-congestion embedding of an approximate Erds-Rnyi random graph, which is then used to route messages efficiently. The hierarchical structure allows for the efficient routing of messages between different parts of the graph, reducing memory access latency.
56bf179e-e560-5865-a62f-ff8fdad059dc|LACC (Linear Algebraic Connected Components)|LACC is a distributed-memory algorithm for finding connected components in large-scale graphs. It uses linear algebraic primitives to efficiently process the graph and minimize memory consumption.
8fb65065-4cde-5832-8193-81b194088a5b|Adaptive Vector Sparsity|This solution involves adapting the sparsity of vectors used in the LACC algorithm to minimize memory consumption and optimize performance.
5a009185-57e8-5bad-a227-04e7434c3a90|Customized All-to-All Communication|This solution involves optimizing the all-to-all communication pattern in the LACC algorithm to minimize communication overhead and improve scalability.
a7d33150-58b2-5c5d-a47e-0d389e49978a|Load Balancing and Communication Efficiency|This solution involves optimizing the load balancing and communication efficiency of the LACC algorithm to minimize memory consumption and improve performance.
6d01d562-f269-5ce2-822c-b01b5a19460c|Customized All-to-All Communication Scheme|The authors propose a customized all-to-all communication scheme to optimize communication efficiency in distributed algorithms. This scheme is designed to handle imbalanced collective communication patterns inherent in the AS algorithm. The scheme involves broadcasting entries from few low-ranked processes and then removing those processes from all-to-all collective calls. This approach reduces the number of messages sent and words moved, resulting in improved communication efficiency. The paper reports that this scheme makes the shortcut and starcheck operations highly scalable, contributing to the overall performance of the LACC algorithm.
784535c2-2752-5351-bdb7-04f4d25d0907|Hypercube-Based All-to-All Implementation|The authors propose a hypercube-based all-to-all implementation to replace MPI Alltoallv calls, which are not scaling beyond 1024 MPI ranks. This implementation uses a hypercube-based algorithm with log p latency cost, which is more efficient than the pairwise exchange algorithm used in MPI Alltoallv. The paper reports that this implementation improves the scalability of the LACC algorithm, allowing it to scale to 4K nodes (262K cores) on a Cray XC40 supercomputer.
40438fb6-07f2-57bf-b854-70a4504975dd|Adaptive Use of Sparse Vectors|The authors propose an adaptive use of sparse vectors to reduce communication overhead in the LACC algorithm. The algorithm uses sparse vectors to represent the parent and star vectors, which reduces the number of elements that need to be communicated. The algorithm also adapts to the sparsity of the input graph, using dense vectors when the graph is dense and sparse vectors when the graph is sparse. The paper reports that this approach reduces the communication overhead and improves the performance of the LACC algorithm, especially for graphs with a large number of connected components.
45a3e151-0c9c-56c3-9a07-4c053b69a0bf|Load Balancing using Random Permutation|The authors propose using random permutation to load balance the distribution of the adjacency matrix and associated dense vectors in the LACC algorithm. This solution involves randomly permuting the rows and columns of the adjacency matrix to distribute the load evenly across processors.
d9bcaaa2-3244-5b08-a9c0-863643001659|Adaptive All-to-All Communication Scheme|The authors propose an adaptive all-to-all communication scheme to optimize load balance in distributed systems. This scheme dynamically adjusts the communication pattern based on the data distribution, reducing the impact of imbalanced collective communication patterns inherent in the AS algorithm.
d9625bca-a7e2-5ff2-b984-abc6f37f9388|Customized Vector Operations|The authors design customized vector operations to optimize load balance in distributed systems. These operations take advantage of vector sparsity, reducing the amount of data that needs to be communicated and processed.
37a70dbc-c92b-53ed-aa39-e32c5a94ee35|Load Balancing through Random Permutation|The authors propose a load balancing technique through random permutation of rows and columns of the adjacency matrix. This approach ensures a load-balanced distribution of the matrix and associated dense vectors.
d30c3036-14c6-5f85-acf1-ece31c19dee5|Bound-based Pruning Technique|The authors propose a bound-based pruning technique to optimize communication efficiency in distributed algorithms. This technique involves maintaining a top-k candidate result in a priority queue and using the upper and lower bounds of matching scores to prune search spaces. The bounds are updated during iterations, and the technique ensures that only the most promising candidates are explored, reducing the number of communication rounds.
f4813484-be67-5f36-96b9-993059e2097e|Hierarchical Inheritance Relations|The authors propose the use of hierarchical inheritance relations to optimize communication efficiency in distributed algorithms. This approach involves modeling graph queries with hierarchical inheritance relations to improve query quality and reduce the number of communication rounds.
52b3597f-2b28-50a5-963d-dcd1de5f1965|Distributed Implementation|The authors propose a distributed implementation of their algorithm using the GraphX framework. This implementation optimizes communication efficiency by using a global data structure and a user-defined class type to store the necessary information for each vertex.
61a631ed-c8ec-5401-830d-083d0c9c3af0|Hierarchical Inheritance Relations-based Graph Query Algorithm|The authors propose a graph query algorithm that incorporates hierarchical inheritance relations to improve the quality of query results. This algorithm is designed to handle heterogeneous information networks with hierarchical structures, where entities have different types and relations.
db97fdcc-7fe9-5942-88bd-ac1097b529dd|Top-k Selection with Bounds|The authors propose a top-k selection method that uses bounds to efficiently select the top-k candidate nodes. This method is designed to handle the challenge of adaptive algorithms for heterogeneous and irregular graphs.
a0718b4d-ae46-5083-a3f3-b496322279fc|General Graph Query Algorithm with Hierarchical Inheritance Relations|The authors propose a general graph query algorithm that incorporates hierarchical inheritance relations to improve the quality of query results. This algorithm is designed to handle heterogeneous information networks with hierarchical structures, where entities have different types and relations.
10c360de-14a9-506b-947e-a62ab3446d59|Distributed Implementation of Graph Query Algorithm|The authors propose a distributed implementation of the graph query algorithm using the Spark GraphX framework. This implementation is designed to handle large-scale heterogeneous information networks.
52d0c0c2-d7b1-5837-96f3-730e13c70e72|Hierarchical Inheritance-based Graph Query Algorithm|The authors propose a graph query algorithm that incorporates hierarchical inheritance relations to improve query efficiency and effectiveness. This algorithm is designed to handle dynamic updates in large graphs by leveraging the hierarchical structure of the data.
9b702bfa-e9c3-584d-a000-89fabe90ce8b|Star Query Algorithm with Bounding-based Pruning|The authors develop a star query algorithm that utilizes bounding-based pruning to efficiently search for top-k answers in large graphs. This algorithm is designed to handle dynamic updates by quickly identifying and pruning irrelevant nodes.
30e01e38-efd7-5574-b4c1-36a856b18cfa|Reduced Synchronization Optimistic Coloring (RSOC)|The authors propose an improved parallel graph coloring algorithm called Reduced Synchronization Optimistic Coloring (RSOC), which aims to reduce memory consumption and improve scalability in graph processing. RSOC is designed to minimize thread synchronization and reduce the number of conflicts, resulting in fewer iterations and improved performance.
253d4c86-e668-5c89-9b06-4a4c5c3c86e6|Iterative Contraction of Vertices|The authors propose an algorithm that iteratively contracts vertices in a distributed graph, effectively reducing the number of vertices and edges, and thus the communication complexity. This approach is specifically designed to address the challenge of optimizing communication efficiency in distributed algorithms.
7b01fcdf-d344-5939-903d-6b7b5198c051|Distributed 1-Coloring Algorithm|The authors present a distributed 1-coloring algorithm that can be used to color a graph with a small number of colors. This algorithm is designed to optimize communication efficiency by minimizing the number of communication rounds.
3b8796dd-7b10-5e55-8ade-578235a0ef88|Lower Bound Technique|The authors present a lower bound technique that can be used to analyze the communication complexity of distributed algorithms. This technique is specifically designed to address the challenge of optimizing communication efficiency in distributed algorithms.
dca51c21-4819-5630-91c3-33ef4c4ac891|Optimized Graph Layout|The authors propose an optimized graph layout that reduces memory storage requirements per edge. This layout includes storing 4-bit local buffer pointers in the CSC edgelist, which allows for efficient binning operations and reduces memory overhead.
98d05f71-8bb6-5d4a-8e91-3756686cfad2|Two-Level Bucketing|The authors propose a two-level bucketing technique that allows for better performance trade-offs between binning and accumulation threads. This technique involves partitioning vertices into two groups: high-degree vertices and low-degree vertices.
acc2de7c-9099-52ca-9bda-00a9853d4d85|Concurrent Execution Model|The authors propose a concurrent execution model that allows for asynchronous parallel execution of binning and accumulation threads. This model reduces memory overhead by storing only a small fraction of intermediate data.
15e24f50-f7c6-576b-a593-ca0152815452|Active Vertex Set|The authors propose an active vertex set data structure that reduces memory overhead by storing only the active vertices.
96ac5b1e-9cc6-52f9-aaea-d1b0ed4ccdeb|Active Vertex Set Support|The authors propose an active vertex set support approach that maintains a set of active vertices and only processes the vertices that are active.
25ff0cb2-e497-58fa-b47a-5091de98e320|Propagation Blocking with Delta Caching|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by introducing a propagation blocking algorithm that separates vertex-centric computation into two phases: binning and accumulation. The algorithm is further optimized with delta caching, which reduces memory overhead and improves work efficiency.
2eda8b3e-47a9-5719-86e0-bfebcc138c68|Asynchronous Execution with Active Vertex Set|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by introducing an asynchronous execution model that enables concurrent processing of vertices and edges.
89c8f5d3-448c-532a-af3f-b383659d2dfa|Propagation Blocking (PB) Algorithm|The PB algorithm is a technique designed to improve the efficiency of graph dynamics processing by separating vertex-centric computation into two phases: binning and accumulation. This approach reduces the memory access bottleneck and improves parallel performance.
56ba7cf7-604a-5171-bcf2-44acd9d36a44|Multi-Round HCubeJoin|The authors propose a multi-round HCubeJoin approach to reduce communication cost and memory consumption in distributed graph processing. This solution involves dividing the output of a join query into smaller cubes and assigning each cube to a server for processing, minimizing the communication cost with respect to the constraints of the available memory.
34c360f5-e14d-51a7-a723-5cf38956b009|Modified Leapfrog with Aggregation|The authors propose a modified Leapfrog algorithm that supports join with group by and aggregation in a pipeline fashion. This solution involves pushing down group by and aggregation over joins to reduce the intermediate results and memory consumption.
e2006aeb-d8d3-5006-9694-95ffe32f797b|Attribute Ordering|The authors propose an attribute ordering approach to facilitate group by and aggregation computing in Leapfrog. This solution involves sorting the relations following a specific attribute order to minimize the join cost and memory consumption.
200f2100-c4ee-5f47-ace1-07e3338c7e8d|Generalized Hypertree Decomposition (GHD)|The authors propose a GHD approach to decompose a cyclic join query into a join tree. This solution involves representing the pattern graph as a join tree using tree decomposition to reduce the memory consumption and computational cost.
1c492121-629b-5114-b90b-201f49d56b9a|Pipelined Leapfrog with Aggregation|The authors propose a pipelined Leapfrog with aggregation approach to optimize communication efficiency in distributed algorithms. This approach involves pushing down group by and aggregation over joins and processing them in a pipeline fashion, reducing the communication cost by minimizing the number of intermediate results.
25eb9bef-28e2-52e8-bf17-e8d688a7276d|Leapfrog with Aggregation|The authors propose a modified Leapfrog algorithm that supports join with group by and aggregation in a pipeline fashion. This solution specifically addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by reducing the intermediate results and memory usage during join operations.
a33ba149-4a44-56d2-b3b2-b94dd8ffa414|Cost Sharing|The authors propose a cost-sharing approach to optimize load balance in distributed systems. This approach involves sharing the computing cost of common sub-queries among multiple queries, reducing the overall computing cost and improving load balance.
17e6e878-bda6-579b-a094-96d1fc0887b7|Homomorphism Counting|This solution addresses the challenge of efficient graph dynamics processing by proposing a homomorphism counting approach for distributed subgraph counting. The approach is designed to minimize the communication cost and computing cost by reducing the number of matches enumerated.
692de7c3-2825-5da8-b7f2-fdae7339fdec|Parallel Complex Coloring Algorithm|The authors propose a parallel complex coloring algorithm to address the challenge of memory-efficient scalable graph processing. This algorithm is designed to efficiently color the edges of a bipartite graph, which represents the scheduling problem in input-queued switches.
f97befaa-4557-59d7-8dda-94cfe43fe34e|Frame-Based Scheduling Algorithm|The authors propose a frame-based scheduling algorithm that uses the parallel complex coloring algorithm to schedule packets in input-queued switches. This algorithm is designed to efficiently schedule packets in a frame-based manner, reducing memory consumption and optimizing memory usage.
9d9f174d-f9a0-5971-8f5d-c0a6e4b32f49|Complex Coloring Algorithm|The authors propose a complex coloring algorithm to optimize load balance in distributed systems. This algorithm is designed to efficiently color the edges of a bipartite graph, representing the scheduling of packets in a frame-based packet switch. The complex coloring algorithm uses a parallel processing approach, allowing for simultaneous color exchanges on vertices in the bipartite graph. This approach enables the algorithm to efficiently eliminate variables and achieve a proper edge coloring with a minimal number of colors. The authors demonstrate the effectiveness of the complex coloring algorithm through simulations, showing that it can achieve 100% throughput and an acceptable delay in large-scale packet switches.
9f4dbcaf-75b1-5a40-b17c-c7bc99b1a174|Fine-Grained Dynamic Graph Management Scheme|The authors propose a fine-grained dynamic graph management scheme to reduce CPU-GPU data transfer cost. This scheme involves dividing the original static partitions into small pieces, determining the size of each piece, and logically dividing the static partitions into small pieces. The scheme also involves constructing logical partitions for graph processing by gathering the active pieces for the related TGP jobs.
66e0a17c-0156-5915-aef1-e2014833d99c|Memory Management Scheme|The authors propose a memory management scheme to identify the most frequently accessed graph data and buffer it in the global memory of the GPU. The scheme follows two basic rules: assigning a higher priority to pieces with a higher average vertex degree and associating pieces with more active vertices.
14a54548-274f-5283-8bac-9fe7daa11923|Load Balancing Mechanism|The authors propose a load balancing mechanism to dynamically ensure load balancing for the related jobs. The mechanism involves triggering switching operations when load imbalance occurs.
b899a773-78a6-5f53-a904-5a039d6cd4b0|Logical Partitions Load Scheduling Strategy|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a logical partitions load scheduling strategy. This strategy aims to maximize the utilization ratio of each loaded logical partition by first loading the most frequently accessed logical partitions required by more TGP jobs. The strategy also takes into account the importance of unshared logical partitions to maximize the GPU parallelism of the related jobs.
5ab34652-6022-5b37-9cfc-02806082e58c|Value-Based Memory Management Scheme|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a value-based memory management scheme. This scheme aims to identify the most frequently accessed graph data and buffer it in the GPU memory to improve the global memory utilization ratio.
8ca8ff3c-3da0-527c-8478-2e740b06b88d|Switching for Load Balancing Between SMs|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a switching mechanism for load balancing between SMs. This mechanism aims to ensure balanced load between the SMs by assigning unprocessed active vertices from the overloaded job to the underloaded job.
0151dd53-8337-5d62-a1b8-83ea54bd84f9|Message Buffer Queue|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a message buffer queue. This queue aims to accumulate small messages and combine them into larger packets for batched transmission in a more regular manner.
a2ee390a-53f6-5b14-a37e-f62b21e62530|LPS Execution Model|The LPS Execution Model is a novel approach to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. It dynamically profiles the access similarity of graph partitions to be processed by jobs within the next iteration and makes these jobs handle these partitions more regularly within the next iteration based on the profiled similarity.
a210864f-51e4-5ef0-878c-b71aa6793241|Switching Mechanism for Load Balancing|This solution addresses the challenge of optimizing load balance in distributed systems by proposing a switching mechanism for load balancing. The mechanism ensures load balancing between SMs by assigning unprocessed active vertices from overloaded jobs to underloaded jobs.
b5583e51-813b-576b-a482-fd7044fc4ba3|Fine-Grained Graph Division|The authors propose a fine-grained graph division technique to divide the original static partitions into small pieces, allowing for more efficient loading of graph data into the GPU memory. This approach enables the system to load only the active pieces of the graph, reducing the amount of data transferred between the CPU and GPU.
b1811a45-70c0-5b9e-94e2-78ff6d941765|Loading Processing Switching (LPS) Execution Model|The authors propose an LPS execution model to dynamically profile the access similarity of graph partitions and make jobs handle these partitions more regularly. This approach aims to reduce CPU-GPU graph data transfer overhead and improve GPU utilization ratio.
5c9584b8-822b-5c2a-9225-8b226162978d|Load Balancing Approach|This solution addresses the challenge of efficient graph dynamics processing by proposing a load balancing approach. The approach involves dynamically ensuring load balancing for the related jobs by switching operations when load imbalance occurs.
09478f02-6f19-51bd-8c58-15d7f768c8d4|Logical Partitioning Scheme|This solution addresses the challenge of efficient graph dynamics processing by proposing a logical partitioning scheme. The scheme involves constructing logical partitions for graph processing by gathering the active pieces for the related TGP jobs.
af28c1fc-d901-54a1-bfb6-5b4c6826c6a4|C DLPA Algorithm|The C DLPA algorithm is a novel method that combines DLPA with the notion of maximal cliques and utilizes a new updating mechanism that updates each node label by probability of its adjacent nodes. This approach aims to improve the accuracy of community detection and avoid oscillations effectively.
3f2b1c7b-5384-5c9a-b5f1-bd639cd3f3be|Low Congestion Shortcut Framework|The authors propose the Low Congestion Shortcut Framework to address the challenge of memory-efficient scalable graph processing. This framework focuses on designing memory-aware data partitioning strategies and minimizing communication between processors by employing advanced data structures.
57321c20-3c0b-5c11-b936-db217fbeea32|Tree Restricted Shortcuts|The authors propose the use of Tree Restricted Shortcuts to address the challenge of memory-efficient scalable graph processing. This solution focuses on designing memory-aware data partitioning strategies and minimizing communication between processors by employing advanced data structures.
95ceb1df-dce7-5bdd-844a-46fdd00366fc|AggregateSubtree Algorithm|The authors propose the AggregateSubtree Algorithm to address the challenge of memory-efficient scalable graph processing. This solution focuses on designing memory-aware data partitioning strategies and minimizing communication between processors by employing advanced data structures.
ca4ddc94-f725-56d2-b389-6fe0dae5c18f|Low Diameter Decomposition Algorithm|The authors propose the Low Diameter Decomposition Algorithm to address the challenge of memory-efficient scalable graph processing. This solution focuses on designing memory-aware data partitioning strategies and minimizing communication between processors by employing advanced data structures.
9439a0ab-ed6a-5482-9258-b957ef8327d0|ExpectedSPForest Algorithm|The authors propose the ExpectedSPForest Algorithm to address the challenge of memory-efficient scalable graph processing. This solution focuses on designing memory-aware data partitioning strategies and minimizing communication between processors by employing advanced data structures.
c15f1f23-abbc-5327-991a-386b4ab74a98|ExpectedTS Algorithm|The authors propose the ExpectedTS Algorithm to address the challenge of memory-efficient scalable graph processing. This solution focuses on designing memory-aware data partitioning strategies and minimizing communication between processors by employing advanced data structures.
6d3bcd8f-18a0-5d25-9cb2-85af174793c4|Low-Congestion Shortcut Framework|The authors propose a low-congestion shortcut framework to optimize communication efficiency in distributed algorithms. This framework allows for the characterization of how hard it is to solve the part-wise communication problem in the CONGEST model for any given topology G. The framework provides a way to de ne a shortcut quality parameter for any topology, which captures how hard or easy it is to route information for this given network.
e70ed9c7-0441-5234-b662-ef618bbd7927|LDDSubroutine Algorithm|The authors propose the LDDSubroutine algorithm, which is a distributed algorithm that computes a low-diameter decomposition of a graph. The algorithm uses a shortcut framework to compute the low-diameter decomposition, and is designed to work in conjunction with the ExpectedSPForest algorithm.
17e23929-6341-586d-9af9-d42b071eb575|Tree-Restricted Shortcuts|The authors propose the use of tree-restricted shortcuts to improve the performance of graph algorithms on heterogeneous and irregular graphs. Tree-restricted shortcuts are a type of shortcut that is restricted to a specific tree structure, allowing for more efficient communication between parts of the graph.
1eb9a287-de75-5bd7-a7c8-58b751a48bae|Heads-Tails Clustering Algorithm|The authors propose the Heads-Tails Clustering Algorithm to address the challenge of efficient graph dynamics processing. This algorithm is used to compute a low-diameter hierarchical clustering of a graph, which is a sequence of partitions of the graph's vertices that satisfy certain properties.
e057be96-a580-5fa3-918c-375a74bb629a|SSSPTree Algorithm|The authors propose the SSSPTree Algorithm to address the challenge of efficient graph dynamics processing. This algorithm is used to compute an approximate single-source shortest path tree of a graph, which is a tree that approximates the shortest path distances from a source vertex to all other vertices in the graph.
27bb8482-7acb-5f2a-888e-854b2e0d96f4|Iterative MapReduce KCminer|The authors propose an iterative MapReduce-based solution, KCminer, to address the challenge of memory-efficient scalable graph processing. KCminer is designed to mine k-cliques from large networks by iteratively expanding cliques of size 1 to k. Mechanisms: KCminer uses a state space search approach, which allows it to efficiently explore the graph and identify k-cliques. The iterative MapReduce framework enables KCminer to process large graphs by breaking down the computation into smaller tasks that can be executed in parallel. Results: The authors report that KCminer can handle very large networks with millions of nodes and edges, and that it is faster than available serial tools like MACE.
bd23142d-1dfd-5d4e-bad4-aa868f5c2629|Non-Iterative MapReduce KCminer|The authors also propose a non-iterative MapReduce-based solution, KCminer, to address the challenge of memory-efficient scalable graph processing. This solution is designed to mine k-cliques from smaller networks that can fit into the main memory of each machine. Mechanisms: The non-iterative KCminer uses a similar state space search approach as the iterative version but avoids the overhead of launching multiple MapReduce jobs. Instead, it loads the entire graph into memory and uses a single MapReduce job to enumerate k-cliques. Results: The authors report that the non-iterative KCminer is much faster than the iterative version and can handle smaller networks with high performance.
341e0cbb-aba5-57f6-b428-ccba4f39186a|Parallel Shared Memory KCminer|The authors propose a parallel shared memory-based solution, KCminer, to address the challenge of memory-efficient scalable graph processing. This solution is designed to mine k-cliques from large networks using a single multi-processor machine. Mechanisms: The parallel shared memory KCminer uses a similar state space search approach as the MapReduce versions but utilizes multithreading to parallelize the computation. This approach avoids the overhead of communication between machines and enables faster execution. Results: The authors report that the parallel shared memory KCminer can effectively use the power of all cores and is faster than available serial tools like MACE.
3e69092b-3eb7-5795-b1c5-e04904a1336d|KCminer|KCminer is a distributed solution to the k-clique problem that uses a state space search approach and can be executed on both distributed systems like cloud and parallel shared memory systems. KCminer uses a MapReduce framework to process large networks and can handle heterogeneous graph structures with varying degrees, weights, and sparsity. It also tackles issues of random and irregular memory access patterns by using a non-iterative approach that reduces I/O overhead and enhances memory locality. The paper presents various experiments that demonstrate the effectiveness of KCminer, including its ability to process very large networks with millions of nodes and edges, and its scalability on cloud computing platforms.
ebb3ecc9-5a61-5ebe-82eb-dbc9d8c459d3|Triangle Caching|The authors propose a triangle caching technique to reduce redundant computation and improve performance. This solution involves caching locally enumerated triangles to avoid re-enumerating them.
e235d3e9-492c-57a9-ba05-cf3501a826fa|Backtracking-based Framework|The authors propose a backtracking-based framework to reduce memory consumption and improve performance. This solution involves using a backtracking-based approach to enumerate matches of the pattern graph in the local neighborhood of a data vertex.
ea28cc5c-07f9-5951-910d-1f4c42ced0c2|Streaming BENU Framework|The authors propose a Streaming BENU framework to enumerate incremental matches in dynamic graphs. This solution involves using a backtracking-based approach to enumerate matches of the pattern graph in the local neighborhood of a data vertex.
35d8ffc6-2cb7-5442-9f0b-0ea0dee927b4|Incremental Pattern Graphs|The authors propose the concept of incremental pattern graphs to reduce redundant computation and improve performance. This solution involves enumerating incremental pattern graphs at each time step to reduce redundant computation.
afe7b4f8-d21a-5cba-ba30-0bfa4f465695|Common Subexpression Elimination|The authors propose a common subexpression elimination technique to reduce memory consumption and improve performance. This solution involves eliminating common subexpressions in the execution plan.
02a7c942-6b76-5b80-9c9f-ede5b7923fc3|Instruction Reordering|The authors propose an instruction reordering technique to reduce memory consumption and improve performance. This solution involves reordering instructions in the execution plan.
a7066a29-57e1-5bf5-b04e-f90184b58e7b|Vertex Cover-Based Compression|The authors propose a vertex cover-based compression technique to reduce memory consumption and improve performance. This solution involves compressing the subgraph matching results based on a vertex cover of the pattern graph.
6288d34f-f365-5adf-b49a-bac6353a937a|Incremental Execution Plan|The authors propose an incremental execution plan to reduce communication costs by reusing previously computed results. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by minimizing the number of database queries and reducing the amount of data transferred between nodes.
82c31ba4-e08e-53ab-8390-cf189b3f39d9|Delta Enumeration|The authors propose the use of delta enumeration to efficiently enumerate incremental matches in dynamic graphs. This solution specifically addresses the challenge of efficient graph dynamics processing by reducing the computational costs of enumerating incremental matches.
e5221866-b37e-5e70-b528-c0f364c37d57|Primal-Dual Distributed Algorithm|The authors propose a Primal-Dual distributed algorithm for approximating a minimum weight vertex cover in hypergraphs of rank f. This algorithm addresses the challenge of memory-efficient scalable graph processing by utilizing a distributed approach that minimizes communication between processors and optimizes memory usage.
906d34a0-3bab-5e4b-8b67-7e472ad173e6|Distributed Reductions for Covering ILPs|The authors propose distributed reductions that allow computing an f-approximate solution for general covering integer linear programs (ILPs). This solution addresses the challenge of memory-efficient scalable graph processing by reducing the memory requirements for solving ILPs.
10897cdc-d7a4-58a7-a3e1-fc491818eb03|Primal-Dual Schema with Exponential Increase of Dual Variables|The authors propose a Primal-Dual schema to optimize communication efficiency in distributed algorithms. This approach involves exponentially increasing the dual variables to reduce the number of communication rounds while maintaining solution quality.
5e168522-18d4-5927-b102-0fe793846200|Distributed Reduction from Covering ILPs to MWHVC|The authors propose a distributed reduction from covering Integer Linear Programs (ILPs) to the Minimum Weight Hypergraph Vertex Cover (MWHVC) problem. This reduction enables the application of efficient MWHVC algorithms to solve covering ILPs, leading to improved communication efficiency.
ab61245a-a91c-5d2d-aef1-b072db12f3eb|Levels-Based Approach for MWHVC|The authors propose a levels-based approach for the MWHVC problem, which involves assigning levels to vertices and incrementing them based on the dual variables. This approach enables efficient communication and reduces the number of rounds.
8828fd53-9a0d-54de-b5ce-9f4be73c4853|Adaptive Primal-Dual Algorithm for Weighted Vertex Cover|The authors propose an adaptive primal-dual algorithm for the weighted vertex cover problem in hypergraphs, which can handle heterogeneous graph structures with varying degrees, weights, and sparsity. The algorithm uses a primal-dual approach to adaptively adjust the dual variables based on the local information of each vertex, ensuring that the algorithm can efficiently handle irregular memory access patterns and reduce communication overhead.
1105ee21-8e71-56cd-9eeb-6ba488dd658d|Distributed Reduction for General Covering ILPs|The authors propose a distributed reduction technique for general covering integer linear programs (ILPs), which can handle heterogeneous graph structures and irregular network topologies. The technique reduces the ILP to a weighted vertex cover problem in a hypergraph, allowing the authors to apply their adaptive primal-dual algorithm to solve the ILP.
db2a4abc-f616-5bac-b4aa-ae100482e463|Primal-Dual Distributed Algorithm for Load Balancing|The authors propose a Primal-Dual distributed algorithm for load balancing in distributed systems. This algorithm is designed to optimize the load balance by iteratively adjusting the dual variables and incrementing the levels of vertices. The algorithm ensures that the set of tight vertices constitutes a vertex cover, which is essential for achieving load balance.
ae42ef3a-c799-5c32-bae8-f92bde5a1ad0|Reduction of Covering ILPs to Zero-One Covering Programs|The authors propose a reduction of covering Integer Linear Programs (ILPs) to zero-one covering programs. This reduction allows for the computation of approximate solutions to covering ILPs using a distributed algorithm for zero-one covering programs.
03d9b2c5-1a03-59ff-8ee6-1092ecf1256c|Distributed Algorithm for Zero-One Covering Programs|The authors propose a distributed algorithm for zero-one covering programs. This algorithm is designed to compute an approximate solution to the zero-one covering program by iteratively adjusting the dual variables and incrementing the levels of vertices.
e476f56a-6cfe-5bb8-ba80-059061954a27|Reduction of Zero-One Covering Programs to MWHVC|The authors propose a reduction of zero-one covering programs to the Minimum Weight Hypergraph Vertex Cover (MWHVC) problem. This reduction allows for the computation of approximate solutions to zero-one covering programs using a distributed algorithm for MWHVC.
492a3134-2175-5cb0-b421-6da16bceb6da|Primal-Dual Distributed Algorithm for Minimum Weight Hypergraph Vertex Cover (MWHVC)|The authors propose a Primal-Dual distributed algorithm for the Minimum Weight Hypergraph Vertex Cover (MWHVC) problem, which is a generalization of the Minimum Weight Vertex Cover (MWVC) problem. The algorithm is designed to efficiently process dynamic updates in large hypergraphs while minimizing computational costs and iterations.
578037ab-e7b4-5720-9534-3d7ea2f03910|Distributed Reductions for General Covering Integer Linear Programs (ILPs)|The authors propose distributed reductions for general covering ILPs, which can be used to solve the MWHVC problem. The reductions allow for the translation of LP constraints into hyperedges, enabling the use of the Primal-Dual algorithm for MWHVC to solve general covering ILPs.
6ab35a3d-72ba-548e-8989-4c7a331f69e1|Virtualized Scaling Framework|The authors propose a virtualized scaling framework to address the challenge of memory-efficient scalable graph processing. This solution involves applying the scaling framework to a virtual graph, which reduces the task to computing O(log n) iterations of virtual SSSP on a graph with s-radius at most nV^(-1).
72667914-b154-5056-83b6-5a4a4a47e485|Bucketing and Virtual SSSP|The authors propose a bucketing approach to divide the graph into smaller subgraphs, each containing nodes with similar distances from the source node. They then apply a virtual SSSP algorithm to each bucket, which reduces the memory consumption and computational costs associated with processing large-scale graph data.
bde5f82d-347f-5c81-a6f4-e052c283fc7c|Additive Approximation of SSSP|The authors propose an additive approximation algorithm for SSSP, which reduces the memory consumption and computational costs associated with processing large-scale graph data.
f892bb63-d8f2-562a-9863-2e898fb6bcd2|Virtual Node Sampling|The authors propose a virtual node sampling approach to reduce the memory consumption and computational costs associated with processing large-scale graph data.
375a6e33-5064-5f85-a42f-92f36453e1c1|Distributed Scheduling of Algorithms|The authors propose a distributed scheduling approach to optimize the execution of multiple algorithms in parallel, which reduces the memory consumption and computational costs associated with processing large-scale graph data.
414dbb49-a21d-52ca-abcb-344f1187d32e|ShortRange Algorithm|The authors propose a ShortRange algorithm to optimize communication efficiency in distributed algorithms. This solution involves computing h-hop distances using a hybrid of BFS and Bellman-Ford algorithms.
e01f612e-f875-52ae-8264-651d7c726f32|Virtual SSSP|The authors propose virtual SSSP as a solution to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution involves computing SSSP on a virtual graph, allowing for more efficient processing and analysis.
5a3584c4-f7d4-56c2-bb04-a78244c34bfa|Virtualized ShortRange Algorithm|The authors propose a virtualized ShortRange algorithm to optimize load balance in distributed systems. This method involves running the ShortRange algorithm on a virtual graph, where each node is sampled with probability k nV, for some parameter k.
02ffc043-a7f5-530d-bbfd-64cec4f9029f|Scaling Framework|The authors propose a scaling framework to optimize load balance in distributed systems. This method involves reducing the problem to log n iterations of computing SSSP on a graph with non-negative integer weights, where in each iteration, the source-wise s radius is bounded by n 1.
1c13496f-1a92-52b6-891f-e3bbf173e1cd|Virtual SSSP Algorithm|The authors propose a virtual SSSP algorithm to optimize load balance in distributed systems. This method involves running the SSSP algorithm on a virtual graph, where each node is sampled with probability k nV, for some parameter k.
33c50e7d-a9bd-5d7e-9550-143502683f48|Bounded Hop SSSP Algorithm|The authors propose a bounded hop SSSP algorithm to optimize load balance in distributed systems. This method involves running the SSSP algorithm on a graph with bounded hop diameter.
e713deb8-fb42-54b2-9713-e95775a75e47|Bounded Distance SSSP Algorithm|The authors propose a bounded distance SSSP algorithm to optimize load balance in distributed systems. This method involves running the SSSP algorithm on a graph with bounded distance.
61798ca4-bf76-5a42-8cce-af99b57e51e4|Extend Algorithm|The authors propose an Extend algorithm to optimize load balance in distributed systems. This method involves extending the distance computation to non-virtual nodes.
aaeddc8b-c239-59eb-8c98-cae22b71a32c|Pipelining Algorithm|The authors propose a pipelining algorithm to optimize load balance in distributed systems. This method involves pipelining multiple algorithms in parallel to minimize the number of messages sent along each edge.
6562083b-05f7-5616-a4c7-5c4efa1dc28e|In-Situ Computation on Sparse Graph Data|The authors propose an in-situ computation approach that directly operates on sparse graph data stored in crossbar memory arrays, eliminating the need for sparse-to-dense conversions and redundant computations on invalid edges.
961b5735-475a-5101-962e-f9560379bdb1|Graph Data Representation using Coordinate List Format|The authors utilize the coordinate list format to represent large graph data, which avoids wasted storage for non-existing relations.
0e3f3012-970f-5fb8-90d7-ae3c28ca352a|Memory-Aware Data Partitioning using Sub-Shards|The authors employ a memory-aware data partitioning strategy that divides graph data into sub-shards, which are stored in a contiguous manner to increase locality and reduce memory consumption.
fb465cc3-31c8-5b30-a3dc-bcffceb8bfab|Content-Addressable Memory (CAM) Search Operation|The authors utilize the content-addressable memory (CAM) search operation to identify the rows with matches in the crossbar memory arrays, reducing the overhead of searching and processing graph data.
b0ea61f4-a720-52ca-8a4b-8604f9b4fea3|Transposable Crossbar Arrays|The authors employ transposable crossbar arrays that can perform MAC operations selectively on data elements either row-wise or column-wise, enabling efficient processing of graph data.
b7ab9a1b-49f4-5111-a15e-fec755dbc1bf|GaaS X Architecture|The GaaS X architecture is a processing-in-memory accelerator designed to optimize communication efficiency in distributed algorithms by leveraging sparse data representation and reducing redundant computations.
46282997-d3ef-507d-b2ac-d4d901d0e269|Asynchronous Depth-First Traversal (ADFT)|ADFT is a query evaluation strategy that adapts asynchronous depth-first traversal to the property graph setting, allowing for controllable high degrees of parallelism and precise control over memory consumption. ADFT achieves this by breaking down the query into stages, each responsible for visiting one vertex, and using hop engines to select the next vertex to hop to based on filters in the query. This approach reduces the number of active intermediate results and states, minimizing memory utilization. The paper demonstrates that ADFT achieves good scalability and performance on large graphs, with results showing that PGX.D Async can load and query graphs that do not fit in the main memory of a single machine.
005c049e-5844-56b8-86c5-58a63325af34|Dynamic Memory Management for Message Buffers|This solution involves dynamically managing message buffers to control memory consumption during query execution, ensuring that the system does not run out of memory. The approach uses a counter to track the number of messages in the buffer and dynamically adjusts the buffer size based on the available memory, preventing memory overflow and ensuring efficient memory usage. The paper shows that this approach enables PGX.D Async to handle large graphs with limited memory, demonstrating its effectiveness in reducing memory consumption and improving scalability.
b61e704d-f145-5a44-b85b-227c646e77de|Precise Flow Control|Precise Flow Control is a mechanism that ensures query completion within memory bounds, allowing for strict control over memory consumption during query execution. This approach involves tracking the completion of each stage in the query and exchanging special COMPLETED messages between machines to detect termination, ensuring that the system does not run out of memory. The paper demonstrates that Precise Flow Control enables PGX.D Async to complete queries within memory bounds, ensuring efficient memory usage and scalability.
2938f98e-4b5e-5d56-80b6-3429260a4d17|Stage-Based Query Execution|This solution involves breaking down the query into stages, each responsible for visiting one vertex, and executing each stage sequentially to reduce memory utilization. This approach uses hop engines to select the next vertex to hop to based on filters in the query, reducing the number of active intermediate results and states. The paper shows that this approach enables PGX.D Async to achieve good scalability and performance on large graphs, with results demonstrating its effectiveness in reducing memory consumption and improving scalability.
33ffbca3-1692-5f6a-8846-6bd48b0e0d61|Dynamic Memory Management (DMM)|The authors propose a dynamic memory management (DMM) approach to optimize communication efficiency in distributed algorithms. This solution involves dynamically adjusting the memory allocation for message buffers based on the workload and available resources.
989ff786-623d-5b04-949a-387a1c0a75e5|Flow Control Manager (FCM)|The authors propose a flow control manager (FCM) approach to optimize communication efficiency in distributed algorithms. This solution involves using a flow control manager to track the number of unprocessed messages from one machine to another for each stage.
be2236a3-a56e-5962-b2ef-8d28bfd3fd3f|Hop Engine Optimization (HEO)|The authors propose a hop engine optimization (HEO) technique to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. HEO optimizes the hop engine to reduce the number of intermediate results and improve memory locality.
e79bcce9-1dd5-549d-a81c-cd33aa888d04|Query Scheduling (QS)|The authors propose a query scheduling (QS) technique to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. QS optimizes the query execution order to reduce memory overhead and improve performance.
e4b8f212-48db-56b0-8f3a-7bd8e70fac53|Asynchronous Depth-First Traversal|The authors propose an asynchronous depth-first traversal approach to optimize load balance in distributed systems. This approach allows nodes to process tasks independently and asynchronously, reducing the need for synchronization and improving overall system performance.
4bad0c13-63c9-56e8-8fd8-963c5826c261|Flow Control with Finite Memory Requirements|The authors propose a flow control approach with finite memory requirements to optimize load balance in distributed systems. This approach ensures that each node has a finite amount of memory available for processing tasks, preventing memory bottlenecks and improving overall system performance.
d0a78ac4-59d0-5969-99a6-95d2c67f27a2|Kokkos-based Parallel Graph Coloring|The authors propose a Kokkos-based parallel graph coloring algorithm that utilizes the Kokkos performance portability framework to enable memory-efficient and scalable graph processing. This solution specifically addresses the challenge by leveraging Kokkos ability to manage memory and optimize data access patterns, allowing for efficient processing of large-scale graph data.
0678df8d-daaf-51ee-9d72-7b1d9d2b529d|Distributed-Memory Speculative Coloring|The authors propose a distributed-memory speculative coloring algorithm that enables memory-efficient and scalable graph processing by speculatively coloring vertices and iteratively resolving conflicts. This solution specifically addresses the challenge by reducing memory consumption and minimizing communication overhead through the use of speculative coloring and conflict resolution techniques.
d495b61c-e032-52fb-ba28-c2bb7a5cbadc|Vertex-Degree-Based Recoloring|The authors propose a vertex-degree-based recoloring algorithm that prioritizes recoloring lower-degree vertices to reduce memory consumption and minimize communication overhead. This solution specifically addresses the challenge by optimizing recoloring operations and reducing memory consumption through the use of vertex-degree-based prioritization.
0a49d502-1f78-5193-9325-fcde33fb1723|Partial Distance 2 Coloring|The authors propose a partial distance 2 coloring algorithm that enables memory-efficient and scalable graph processing by coloring a subset of vertices in the graph. This solution specifically addresses the challenge by reducing memory consumption and minimizing communication overhead through the use of partial coloring techniques.
1c87e15e-214e-5013-b501-13981d002406|D1-2GL (Distributed 1-Ghost Layer)|D1-2GL is a solution that aims to optimize communication efficiency in distributed algorithms by reducing the number of communication rounds. It achieves this by introducing a second ghost layer, which allows for more efficient conflict resolution and reduces the need for additional communication rounds.
4eb4c938-5037-5f6b-a865-bfa8d8b80944|Prioritizing Distributed Recoloring by Vertex Degrees|This solution proposes a novel approach to distributed recoloring, where vertices are prioritized based on their degrees. This approach aims to reduce the number of communication rounds by minimizing the number of conflicts that need to be resolved.
00696796-32d2-52bc-8491-649dc81105ca|Partial Distance-2 Coloring (PD2)|The authors propose a partial distance-2 coloring algorithm that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm is designed to handle bipartite graphs and is optimized for distributed-memory systems.
84332c15-155c-58dd-a0be-67405e93e356|Hierarchical Sampling Algorithm|The Hierarchical Sampling Algorithm is a solution proposed by the authors to address the challenge of memory-efficient scalable graph processing. This algorithm is designed to efficiently generate a large number of random walks simultaneously for all nodes of the graph, while minimizing memory consumption.  The algorithm works by recursively dividing each large node into small ones, which are then organized as an alias tree in a hierarchical manner. This approach allows the algorithm to select a node from the set of neighbors following the distribution of weights on edges in constant time, thereby reducing memory usage and improving scalability.  The unique mechanism involved in this solution is the use of a hierarchical alias tree, which enables efficient sampling from large nodes while minimizing memory consumption. This approach differs from existing methods, which often rely on flat data structures that can lead to high memory usage and scalability issues.  Quantitative results from the paper demonstrate the effectiveness of the Hierarchical Sampling Algorithm, showing that it can handle large graphs with billions of edges and outperform existing methods in terms of memory efficiency and scalability.
10ed7814-9011-5b3d-a3c7-9a835ea7caf9|Pre-computing Short Walks for Small Nodes|The Pre-computing Short Walks for Small Nodes solution is another approach proposed by the authors to address the challenge of memory-efficient scalable graph processing. This solution is designed to speed up the random walks among small degree nodes by pre-computing short walks for these nodes.  The algorithm works by pre-computing a set of short walks for each small node, which can then be used to accelerate the generation of random walks. This approach reduces the number of random walks that need to be generated, thereby minimizing memory consumption and improving scalability.  The unique mechanism involved in this solution is the use of pre-computed short walks, which enables efficient acceleration of random walks among small degree nodes. This approach differs from existing methods, which often rely on generating random walks from scratch, leading to high memory usage and scalability issues.  Quantitative results from the paper demonstrate the effectiveness of the Pre-computing Short Walks for Small Nodes solution, showing that it can significantly reduce memory consumption and improve scalability for large graphs.
71e1051e-5c66-5a3b-b0de-e5765dfb0c5f|Optimizing the Number of Random Walks in Each Pipeline|The Optimizing the Number of Random Walks in Each Pipeline solution is a third approach proposed by the authors to address the challenge of memory-efficient scalable graph processing. This solution is designed to optimize the number of random walks generated in each pipeline, thereby minimizing memory consumption and improving scalability.  The algorithm works by dynamically adjusting the number of random walks generated in each pipeline, based on the available memory and computational resources. This approach ensures that the algorithm can efficiently generate a large number of random walks while minimizing memory consumption and improving scalability.  The unique mechanism involved in this solution is the use of dynamic adjustment of the number of random walks, which enables efficient optimization of memory usage and scalability. This approach differs from existing methods, which often rely on fixed parameters or heuristics, leading to suboptimal performance.  Quantitative results from the paper demonstrate the effectiveness of the Optimizing the Number of Random Walks in Each Pipeline solution, showing that it can significantly improve memory efficiency and scalability for large graphs.
0b039339-1fed-535e-9450-f6ad5564f9c2|Pre-computing Big Moves|Pre-computing Big Moves is a solution proposed by the authors to optimize communication efficiency in distributed algorithms. This solution is designed to address the challenge of minimizing round complexity while maintaining solution quality or approximation ratios.
efaee213-b4af-51f6-a81e-f60f6daf9962|Optimizing the Number of Random Walks|Optimizing the Number of Random Walks is a solution proposed by the authors to optimize communication efficiency in distributed algorithms. This solution is designed to address the challenge of minimizing round complexity while maintaining solution quality or approximation ratios.
52aca7d0-8438-5d7d-ba4f-54593a61233e|Pre-Computing Short Walks|The Pre-Computing Short Walks solution is designed to address the issue of underutilization of small nodes in the graph. This solution precomputes for each small node a set of short walks to facilitate the acceleration of random walk generation on small nodes.
9671b43f-51f2-5e94-b5ad-3f5b04645edb|Hierarchical Sampling with Alias Tree|The authors propose a hierarchical sampling method using an alias tree to optimize load balance in distributed systems. This method is designed to handle large nodes with high degrees, which can lead to skewness and imbalance in the system.
59c1e347-7b3a-5bc6-9100-43c5dae53a2a|Auto-tuning the Number of Random Walks in Each Pipeline|The authors propose a method to auto-tune the number of random walks in each pipeline to optimize load balance in distributed systems. This method is designed to avoid the underutilization of computing resources and reduce the number of pipelines.
1f46bec8-7959-5e31-b74b-741a3d7494e1|Distributed Algorithm for Fully Approximate PPR|The Distributed Algorithm for Fully Approximate PPR is a technique used to efficiently compute the fully approximate Personalized PageRank (PPR) on large graphs.
5250c6de-5739-521c-b7ab-38902b358769|Spectral Sparsification for Distributed Graph Clustering|The authors propose a solution that utilizes spectral sparsification to reduce the memory requirements for distributed graph clustering. They develop a distributed algorithm that constructs a spectral sparsifier of the input graph, which is a sparse subgraph that approximates the original graph's spectral properties. This allows for efficient clustering of the graph while minimizing memory consumption.
1f6077f0-ce50-5044-bf77-f2fc42aaed1e|Distributed Geometric Clustering via Successive Sampling|The authors propose a solution for distributed geometric clustering that uses a successive sampling approach. They develop a distributed algorithm that iteratively samples points from the input data and grows balls around them until a stopping criterion is met.
842c0590-d3af-566c-be8f-b215b6f5e3b8|Distributed Sampling for Geometric Clustering|The authors propose a distributed algorithm for geometric clustering using distributed sampling, which reduces the communication complexity by sampling points in parallel. The algorithm uses a distributed sampling procedure to sample points in parallel, which reduces the communication complexity by avoiding the need to communicate all points. The authors show that their algorithm achieves a communication complexity of e O(sk) bits in the message passing model and e O(s + k) bits in the blackboard model, which is optimal up to logarithmic factors.
f83699d9-734a-5402-966a-594784fe539e|Successive Sampling for k-Median and k-Means|The authors propose a distributed algorithm for k-median and k-means clustering using successive sampling, which reduces the communication complexity by sampling points in rounds. The algorithm uses a successive sampling procedure to sample points in rounds, which reduces the communication complexity by avoiding the need to communicate all points. The authors show that their algorithm achieves a communication complexity of e O(sk) bits in the message passing model and e O(s + k) bits in the blackboard model, which is optimal up to logarithmic factors.
0cb12683-aac8-580f-963a-72a661bf985d|Parallel Guessing for k-Center|The authors propose a distributed algorithm for k-center clustering using parallel guessing, which reduces the communication complexity by guessing the centers in parallel. The algorithm uses a parallel guessing procedure to guess the centers in parallel, which reduces the communication complexity by avoiding the need to communicate all points. The authors show that their algorithm achieves a communication complexity of e O(sk) bits in the message passing model and e O(s + k) bits in the blackboard model, which is optimal up to logarithmic factors.
f553019a-6aca-5223-a31e-0a183143ce1d|Spectral Sparsification for Heterogeneous Graphs|The authors propose a spectral sparsification technique to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This technique involves constructing a chain of coarse sparsifiers, which are used to approximate the original graph's Laplacian matrix. The chain is constructed recursively, with each step involving the sampling of rows of the matrix based on their leverage scores. This approach enables the efficient processing of heterogeneous graphs with varying degrees, weights, and sparsity.
573a3fe0-46ce-534a-b09b-6499c01e78aa|Distributed Spectral Clustering for Irregular Graphs|The authors propose a distributed spectral clustering algorithm that can handle irregular graph structures. This algorithm involves the construction of a spectral sparsifier of the graph, which is then used for clustering. The algorithm is designed to work in a distributed setting, where the graph is partitioned across multiple sites.
bb4d3506-28e7-5b6d-8153-d17924d4cdaa|Spectral Sparsification for Efficient Graph Dynamics Processing|The authors propose a spectral sparsification technique to efficiently process dynamic updates in large graphs. This method involves constructing a chain of coarse sparsifiers, which are used to approximate the graph's Laplacian matrix. The chain is constructed recursively, with each step involving the sampling of rows of the incidence matrix based on their leverage scores. This approach enables the efficient computation of trussness values and adaptation to graph topology changes due to edge/vertex insertions/deletions.
e7aee898-df03-5d04-88ce-59a1935fb5a0|Distributed Spectral Clustering for Efficient Graph Dynamics Processing|The authors propose a distributed spectral clustering algorithm for efficient graph dynamics processing. This method involves constructing a spectral sparsifier of the graph, which is then used for clustering. The algorithm is designed to work in a distributed setting, where the graph is partitioned across multiple sites. Each site computes a local spectral sparsifier, which is then combined to obtain a global sparsifier.
e1f38b62-d65a-5b9a-92ab-cafd564ba7fb|MapReduce Divide kMedian|MapReduce Divide kMedian is a partitioning-based parallelization of any arbitrary sequential clustering algorithm. It addresses the challenge of memory-efficient scalable graph processing by partitioning points into sets of size n, computing centers for each partition in parallel, and then combining the centers into a single set for clustering.
3e3d67b2-71bd-52f4-8190-7aa06ca084d1|Iterative Sample|Iterative Sample is a sampling algorithm that repeatedly adds new points to the sample until the number of remaining points decreases below a certain threshold. It addresses the challenge of memory-efficient scalable graph processing by reducing the size of the input data while maintaining a representative sample.
5db1b7d9-06e0-5139-a408-ae12be2787db|MapReduce kMedian|MapReduce kMedian is a clustering algorithm that uses Iterative Sample as a subroutine to reduce the size of the input data. It addresses the challenge of memory-efficient scalable graph processing by using a combination of sampling and clustering techniques to efficiently process large-scale graph data.
4f73576d-265f-53e7-9ed6-2a9a6693c107|Iterative Sampling|The authors propose an iterative sampling technique to optimize communication efficiency in distributed algorithms. This method involves repeatedly sampling a subset of points from the input data and refining the sample until a desired level of accuracy is achieved.
2cc1273f-20ad-5873-b973-dd39b0b3481f|MapReduce-Based Algorithm|The authors propose a MapReduce-based algorithm that leverages the iterative sampling technique to optimize communication efficiency in distributed algorithms. This algorithm uses a combination of mapping and reducing operations to process the input data in parallel and minimize the number of communication rounds.
eb2349bf-18c8-53cd-9ae8-d6a7c57d3414|Divide-and-Conquer-Based Algorithm|The authors propose a divide-and-conquer-based algorithm that leverages the iterative sampling technique to optimize communication efficiency in distributed algorithms. This algorithm uses a combination of partitioning and merging operations to process the input data in parallel and minimize the number of communication rounds.
0454e54c-a41e-58ec-ab36-20597c76f4d2|MapReduce kCenter|The authors propose a MapReduce-based algorithm for the k-center problem, which is designed to handle heterogeneous and irregular graphs. This algorithm uses the iterative sampling algorithm as a subroutine and applies a k-center clustering algorithm to the sampled points.
002f5082-d0e6-5013-b76f-16fa1dd276a0|Partitioning-based Parallelization|The authors propose a partitioning-based parallelization approach to handle heterogeneous and irregular graphs. This approach partitions the graph into smaller subgraphs, processes each subgraph in parallel, and combines the results to obtain the final solution.
7af96d22-396a-5af5-bd7e-d833732060e4|Fair Composition Technique|The authors propose the fair composition technique as a solution to optimize communication efficiency in distributed algorithms. This technique combines self-stabilizing algorithms for sub-problems into a self-stabilizing solution for a more complex problem.
666f162a-0b78-5d18-9369-d363c50b150b|Randomized Self-Stabilizing Algorithm for Locally Distinct Labels|The authors propose a randomized self-stabilizing algorithm to assign locally distinct labels within distance two in an anonymous network. This algorithm is designed to optimize communication efficiency by reducing the number of communication rounds required to achieve locally distinct labels.
4dfacb39-79d4-57af-a92e-fc9eecc6046c|Maximum Matching Algorithm for Bipartite Graphs|The authors propose a maximum matching algorithm for bipartite graphs, which is designed to optimize communication efficiency by reducing the number of communication rounds required to find a maximum matching.
ecd09bb1-2830-5e1e-ba46-8ac337dfc0ab|Contextual Resource Optimization|This solution addresses the challenge of memory-efficient scalable graph processing by optimizing the accessibility of required resources in distributed systems. It focuses on contextual resource optimization, which considers not only the resources of nodes themselves but also the resources in their contexts.
0cc35d7b-3937-5a9b-a573-ab2e9cb224ba|Network Layer-Oriented Task Allocation|This solution addresses the challenge of memory-efficient scalable graph processing by considering the network layers in distributed systems. It focuses on network layer-oriented task allocation, which takes into account not only the contextual nodes but also the contextual layers.
9f800429-e877-5ba0-868d-e192cba5a80c|Redundancy-Based Approaches|This solution addresses the challenge of memory-efficient scalable graph processing by implementing redundancy in resources to improve reliability. It focuses on redundancy-based approaches, which mainly implement redundancy of some elements so that the overall task execution is successful even if some parts of the elements fail.
2f529bb5-6395-59dc-b232-969690106bb3|Trust or Reputation-Based Approaches|This solution addresses the challenge of memory-efficient scalable graph processing by measuring the reliability of nodes for tasks using trust or reputation mechanisms. It focuses on trust or reputation-based approaches, which can measure the reliability of nodes for tasks.
d4b2f0be-8e62-5ad7-94b1-2862fa39761e|Game Theory-Based Approaches|This solution addresses the challenge of memory-efficient scalable graph processing by using game theory to achieve coordination among heterogeneous nodes for task allocation. It focuses on game theory-based approaches, which can adapt to dynamic situations and have good robustness.
9972ec0b-53f3-56e2-972a-414e3feb3b2f|Graph Theory-Based Approaches|This solution addresses the challenge of memory-efficient scalable graph processing by using graph theory to model and analyze the coordination relations among heterogeneous nodes. It focuses on graph theory-based approaches, which can provide a globally optimal solution.
acae2c7d-05e1-5ed9-ac96-11a0de9b0089|Locality Sensitive Task Allocation|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by considering the locality of nodes in the system. The authors propose a locality sensitive task allocation method that takes into account the distances between nodes in the network.
fab00f0f-037b-578f-ab35-d05edb594a71|Diffusion Mechanism for Load Balancing|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by using a diffusion mechanism for load balancing in distributed systems. It allows for efficient load balancing and resource utilization by considering the information of neighboring nodes.
120faaa2-386c-5724-aa5c-6272c3a36efd|Hybrid Control Model|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by using a hybrid control model in distributed systems. It combines centralized and distributed control models to optimize task allocation and resource utilization.
19ddc461-45af-57b8-9dec-646b1c86513b|Locality-Based Task Allocation|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by using a locality-based task allocation approach in distributed systems. It considers the locality of nodes in the network structure to optimize task allocation and resource utilization.
77fb04a7-dc0a-5b14-88df-8adf01332d30|Game Theory-Based Approach|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by using a game theory-based approach in distributed systems. It models the task allocation problem as a game among nodes to optimize task allocation and resource utilization.
8014d6cf-76e7-501a-b185-9322f412b875|Graph Theory-Based Approach|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by using a graph theory-based approach in distributed systems. It models the task allocation problem as a graph problem to optimize task allocation and resource utilization.
e05aa5ff-8220-560b-a1ad-3731fa994ea9|Redundancy-Based Approach|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by using a redundancy-based approach in distributed systems. It implements redundancy of some elements to optimize task allocation and resource utilization.
aa6e7c4f-0a8a-5a67-b2f0-075e9799a887|Non-Redundancy-Based Approach|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by using a non-redundancy-based approach in distributed systems. It optimizes task allocation and resource utilization without implementing redundancy of elements.
3785e479-6d57-588b-b8b9-a9d1b10136d3|Trust or Reputation-Based Approach|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by using a trust or reputation-based approach in distributed systems. It optimizes task allocation and resource utilization by considering the trust or reputation of nodes.
04e55fb5-9349-53e2-8838-a380171943ad|Load Balancing based on Nash Equilibrium|This solution addresses the challenge of optimizing load balance in distributed systems by formulating the load balancing problem as a non-cooperative game among users, where each user attempts to minimize the expected response time of their own jobs. The solution derives a new distributed load balancing algorithm based on the structure of the Nash equilibrium.
5e362147-7eee-5a89-93f7-b51fb10ad2ff|Redundancy-based Reliability-oriented Task Allocation|This solution addresses the challenge of optimizing load balance in distributed systems by implementing redundancy of resources to improve reliability. The solution involves allocating tasks to nodes with redundant resources to ensure that the tasks can be executed successfully even if some resources fail.
8fe315d2-0cae-56a0-8897-ec0e4f2a4aa9|Trust or Reputation-based Reliability-oriented Task Allocation|This solution addresses the challenge of optimizing load balance in distributed systems by measuring the reliability of nodes for tasks based on their trust or reputation. The solution involves allocating tasks to nodes with high trust or reputation values to ensure that the tasks can be executed successfully.
acb7fd86-f4ba-55fc-b1c9-6a1682eda2c9|Network Layer-oriented Task Allocation|This solution addresses the challenge of optimizing load balance in distributed systems by considering the network layers of nodes, where each network layer is composed of the same type of links and the involved nodes. The solution involves allocating tasks to nodes based on their network layers to optimize the resource access time.
3533f2d3-2cd3-5fba-8180-810e90d5c2a8|Trust-Based Task Allocation|This solution addresses the challenge of efficient graph dynamics processing by using trust-based mechanisms to allocate tasks to nodes. The authors propose a trust-based task allocation algorithm that takes into account the past behaviors of nodes in the resource negotiation of task execution.
773b3449-4677-5c79-91aa-fb567ae0d3a3|Agent-Based Task Allocation|This solution addresses the challenge of efficient graph dynamics processing by using agent-based approaches to allocate tasks to nodes. The authors propose an agent-based decentralized task allocation mechanism that uses autonomous agents to perform task allocation and load balancing.
bbaa5eae-303f-54b7-8595-8b8edc486201|Game Theory-Based Task Allocation|This solution addresses the challenge of efficient graph dynamics processing by using game theory-based approaches to allocate tasks to nodes. The authors propose a game theory-based task allocation algorithm that takes into account the strategic interactions between nodes.
52ee9c90-752b-5aae-a2a7-20ff4db06323|Graph Theory-Based Task Allocation|This solution addresses the challenge of efficient graph dynamics processing by using graph theory-based approaches to allocate tasks to nodes. The authors propose a graph theory-based task allocation algorithm that takes into account the structural properties of the graph.
9aef0bdb-9bd6-536d-84a7-1b5719af94fd|Coalition-Based Task Allocation|This solution addresses the challenge of efficient graph dynamics processing by using coalition-based approaches to allocate tasks to nodes. The authors propose a coalition-based task allocation algorithm that takes into account the cooperation between nodes.
68330d7e-bc45-5e5e-b5d2-b00bd414663b|Diffusion-Based Task Allocation|This solution addresses the challenge of efficient graph dynamics processing by using diffusion-based approaches to allocate tasks to nodes. The authors propose a diffusion-based task allocation algorithm that takes into account the diffusion of tasks between nodes.
98b3543a-5a67-52aa-97b7-934443e17586|Learning-Based Task Allocation|This solution addresses the challenge of efficient graph dynamics processing by using learning-based approaches to allocate tasks to nodes. The authors propose a learning-based task allocation algorithm that takes into account the learning and adaptation of nodes.
7ee022a9-3514-5963-b2a1-5b7af309618d|Graph Partitioning with 2-Level Hierarchical Partitioning|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a 2-level hierarchical partitioning strategy for graph data. The first level partitions the graph into smaller subgraphs that can fit in memory, and the second level further partitions each subgraph to optimize cache locality.
10b47270-cf8e-5f40-be29-9e6db1d829c2|Selective Scheduling with Parallel Sliding Windows|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a selective scheduling approach that uses parallel sliding windows to process graph data. The approach allows for efficient processing of large graphs by minimizing memory access overhead and optimizing computation.
0ea5321e-6914-5530-ab60-3c2da2c64fe8|FlashGraph with Semi-External Memory Processing|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a semi-external memory processing approach that uses flash storage to store graph data. The approach allows for efficient processing of large graphs by minimizing memory access overhead and optimizing computation.
9f66b8b7-cb05-520e-8da1-924ba8d13934|PathGraph with Path-Centric Processing|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a path-centric processing approach that models large graphs using collections of tree-based partitions. The approach allows for efficient processing of large graphs by minimizing memory access overhead and optimizing computation.
2b84c82f-bae9-5d34-9fd4-085235fc42cb|Hybrid Computing Model|The authors propose a hybrid computing model that combines the advantages of synchronous and asynchronous computing models to optimize communication efficiency in distributed algorithms. This model uses asynchronous messaging to enable vertices to be computed using new contiguous vertex values, reducing the need for global synchronization and minimizing round complexity.
3a45afcc-b9c7-5a6a-836e-2b4443558694|Dynamic Repartition|The authors mention dynamic repartition as a strategy to improve load balance in distributed systems. This involves repartitioning the graph during algorithm execution to adjust to the execution state of the existing algorithm.
e176b9bf-8da2-51b8-b1b7-c2182b2c9e21|Vertex-Centric Flow Graph Processing Model|The authors propose a vertex-centric flow graph processing model, which is designed to reduce disk-based graph I/O and improve the situation of system I/O bottleneck.
df9008a6-3d07-51b7-871f-a6d74b469889|Selective Scheduling|The authors propose selective scheduling as a strategy to accelerate the convergence of some vertices in the graph, especially those vertices that change significantly in two adjacent iterations.
b0ff4b94-a794-5b02-b300-22fe896e469c|Partitioning-based Join Strategy|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a partitioning-based join strategy for distributed SPARQL query processing. The strategy optimizes join processing by partitioning local partial matches and then joining them in a way that minimizes the number of involved vertices and edges.
91bde2bb-3722-583d-9862-b59d941ab2d7|Distributed Assembly|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a distributed assembly approach for distributed SPARQL query processing. The approach optimizes the assembly process by distributing the local partial matches across multiple sites and then assembling them in parallel.
8176e061-cd34-5cc4-b342-64ff004683fb|Partial Evaluation and Assembly Framework|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a partial evaluation and assembly framework for distributed SPARQL query processing. The framework optimizes the query processing by evaluating a query on each graph fragment in parallel to find local partial matches, and then assembling them to compute the final query result.
e241e7f8-7838-5ac1-86e9-83900e0b9a1b|Divide and Conquer Approach|The authors propose a divide and conquer approach to optimize communication efficiency in distributed algorithms. This approach involves dividing the search space into smaller sub-problems and solving them in parallel, which reduces the number of communication rounds required.
2ed8f5ce-8e74-5fff-8a91-8176ac2ba361|Optimal Partitioning of Local Partial Matches|The authors propose an algorithm to find the optimal partitioning of local partial matches, which reduces the number of communication rounds required for assembly.
896a3205-ca96-59fe-ac11-5702ac332e6f|Optimal Partitioning Algorithm|The authors propose an optimal partitioning algorithm to optimize load balance in distributed systems. This algorithm finds the optimal partitioning of local partial matches to minimize the cost of assembly.
ecff0481-055a-5d38-8a9e-9438540bd68e|Partitioning-Based Join Strategy|The authors propose a partitioning-based join strategy to optimize load balance in distributed systems. This strategy uses the partitioning information to optimize the join order and reduce the cost of assembly.
66f868ad-63dd-5c2f-8eaf-870860e5450e|Centralized Assembly Algorithm|The authors propose a centralized assembly algorithm to assemble local partial matches and compute crossing matches. This algorithm iteratively joins local partial matches to form the final query result.
0e0f403c-c850-5b68-a54a-ac35bff5330f|Distributed Assembly Algorithm|The authors propose a distributed assembly algorithm to assemble local partial matches and compute crossing matches in parallel. This algorithm divides the search space among multiple sites, which then assemble local partial matches in parallel.
01ead8c2-dabc-5731-83a6-02e1a950ff73|Cost Model for Optimal Partitioning|The authors propose a cost model to determine the optimal partitioning of the query graph, which minimizes computational costs and iterations.
3beb45ee-d009-5f3b-b3cb-6fd73e79fb9b|I/O-Efficient Core Graph Decomposition|This solution addresses the challenge of memory-efficient scalable graph processing by proposing an I/O-efficient algorithm for core graph decomposition. The algorithm is designed to handle massive networks and reduce memory consumption by minimizing the number of I/O operations.
fb682e4a-910e-5421-9573-f1a0fc618adb|Truss Decomposition in Massive Networks|This solution addresses the challenge of memory-efficient scalable graph processing by proposing an algorithm for truss decomposition in massive networks. The algorithm is designed to reduce memory consumption and optimize memory usage by employing a novel approach to truss decomposition.
54e1666c-3055-5328-9a2c-4eaef8a6b662|External Hash Join with Buffer and Batch|This solution addresses the challenge of memory-efficient scalable graph processing by implementing an external hash join with buffer and batch to save memory. The algorithm buffers the data in two streams, sorts it according to the join key, and then joins the data in batches to reduce memory consumption.
757b308a-3251-597f-9955-a978951b32e5|CliqueJoin|CliqueJoin is a distributed subgraph matching algorithm that extends the state-of-the-art algorithm CliqueJoin to handle labelled subgraph matching and improve its performance by migrating from MapReduce to Timely data flow system.
28623b2e-dbeb-54ea-a12b-e034b7dfbf91|Predictable Inter-Cube Communication|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a predictable inter-cube communication mechanism. The authors introduce a batched communication approach that reorders vertex processing to enable batched messages between cubes, reducing the overhead of inter-cube communication.
a942c8e3-0686-5306-b29c-4b4247e7c6e5|Decoupled Intra-Cube Data Movements|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a decoupled intra-cube data movement mechanism. The authors introduce a heterogeneous core design, where processing units (PUs) and auxiliary units (AUs) are used to handle different access types, eliminating interference and reducing memory traffic.
98f4cd82-68d2-5cda-87a9-91ba44202ca1|Batched and Overlapped Inter-Cube Communication|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a batched and overlapped inter-cube communication mechanism. The authors reorder the vertex processing order to enable batched communication, which reduces the number of communication rounds and improves overall efficiency.
ab10055f-12d4-5346-8d62-0adc89611cfd|Streamlined Inter-Cube Communication|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a streamlined inter-cube communication mechanism. The authors use heterogeneous cores for different access types, which reduces the number of communication rounds and improves overall efficiency.
28368d89-bf20-5efb-a5ad-158310cd42bb|Batched Communication|The authors propose a batched communication mechanism that reduces the overhead of inter-node communication by aggregating multiple messages into a single batch.
acd1451d-6210-54a4-8627-077b501fae41|Rounded Execution|The authors propose a rounded execution mechanism that synchronizes the processing across different nodes at the end of each iteration.
a2d13e79-0610-5f9b-89fc-60b163d319f8|Intra-Cube Data Movements|The authors propose an intra-cube data movement approach that decouples intra-cube data movements from inter-cube communication, reducing the overhead of data movements within each cube.
673bed62-756f-545c-bd53-4b29836abedf|Type-based Partitioning with METIS|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a two-level load-balanced partitioning of the input graph to workers, first by type and then by topology using METIS. This approach ensures that each worker processes a single vertex type, eliminating the evaluation of all vertices in a partition if its type does not match the vertex type specified in that hop of the query.
4d8a7574-5500-565a-8d04-cf3c04f84637|Interval Compute with Time-Aware Query Optimization|This solution addresses the challenge of memory-efficient scalable graph processing by proposing an interval-centric computing model that leverages concise information about the graph to allow accurate selection of a distributed query execution plan from several choices.
76315cad-3229-5656-93dd-dfff9ca4f29a|Memory Optimizations using Interning and Property Value Encoding|This solution addresses the challenge of memory-efficient scalable graph processing by proposing memory optimizations using interning and property value encoding.
c2e75e1a-c45d-5551-ade7-6f388ede327d|Message Optimization|The authors propose a message optimization technique to reduce the redundancy in messages sent during query execution. They construct a result tree, where vertices and edges that match at a previous hop are higher up in the tree, and subsequent vertex and edge matches are its descendants. This approach minimizes the number of messages sent, reducing communication overhead.
d51be35b-a7f9-5cfb-a508-127d0af1d52e|Type-Based Partitioning|The authors propose a type-based partitioning technique to balance the load across workers in a superstep. The technique partitions the graph by vertex type, ensuring that each worker processes a balanced number of vertices.
28d83be8-32f5-5654-8664-3855318f8aab|Type-based Partitioning|The authors propose a type-based partitioning approach to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This approach involves partitioning the graph into sub-partitions based on vertex types, which helps to reduce the network messaging cost between vertices of the same type.
73b1e969-8faf-5652-bc09-59336f823029|Interval Centric Computing|The authors propose an interval-centric computing approach to process temporal graph queries.
2c95fbd5-8f81-5754-bc16-2cbf8fada158|Temporal Path Query Model|The authors propose a temporal path query model to express a wide variety of requirements over temporal property graphs.
8720c23a-0661-588a-8904-91146c43d9ad|Distributed Query Engine|The authors propose a distributed query engine to process temporal graph queries.
c60d09ca-1eaf-5aef-b171-057dca361650|Query Planning and Optimization|The authors propose a query planning and optimization approach to select the most efficient query plan.
aa8d766b-3727-507a-8635-8bd601495a8b|Memory Optimizations|The authors propose memory optimizations to reduce the memory usage in distributed graph processing.
d03ca60f-cd4c-5332-b899-682a5f935294|Weak Scaling|The authors propose a weak scaling approach to evaluate the performance of the distributed query engine.
dbd647d9-ae79-5755-a0a4-263649b987b3|Type-Based Partitioning with METIS|This solution addresses the challenge of optimizing load balance in distributed systems by employing a two-level partitioning approach. Initially, vertices are grouped by type to form typed partitions, which are then further divided into sub-partitions using the METIS algorithm. This technique ensures that each worker has a balanced load of vertices and edges, reducing skewness and improving overall system performance.
b9567fee-14d8-5919-aafb-4df785613bd2|Cost Model-Based Query Plan Selection|This solution addresses the challenge of optimizing load balance in distributed systems by using a cost model to select the optimal query plan. The cost model estimates the execution time of different query plans based on graph statistics and selects the plan with the lowest estimated time.
366a4ed5-fdbc-54d1-a92c-58c0eb8bba2c|Message Optimization using Result Trees|This solution addresses the challenge of optimizing load balance in distributed systems by reducing the size of messages sent between workers. The approach constructs a result tree, where vertices and edges that match at a previous hop are higher up in the tree, and subsequent vertex and edge matches are its descendants.
dc11e46e-c73a-5a80-9ef2-bebba4402078|Type-based Graph Partitioning|The authors propose a type-based graph partitioning approach, which partitions the graph into sub-partitions based on vertex types. This approach helps to eliminate the evaluation of vertices that do not match the query predicate, reducing the computational cost of query processing.
87ac1c44-6219-593b-881d-c16469cd2b53|Distributed Greedy Approximation to Maximum Weighted Independent Set (DistGreedy)|DistGreedy is a distributed algorithm designed to solve the Maximum Weighted Independent Set (MWIS) problem in wireless networks with fading channels. The algorithm aims to achieve a balance between throughput and delay performance while addressing the challenge of memory-efficient scalable graph processing.
572bd8d0-dd5e-5f02-9222-eaa536e5e6f6|Distributed Greedy Approximation (DistGreedy) Algorithm|The DistGreedy algorithm is a distributed solution that addresses the challenge of optimizing communication efficiency in distributed algorithms by minimizing the number of communication rounds. It achieves this by iteratively selecting a maximal independent set of vertices in a graph, where each vertex represents a link in the network. The algorithm operates in a distributed manner, with each node making local decisions based on its neighbors weights, and it terminates after a certain number of intervals.
7dd0becd-ffb1-5b6b-b45f-cc75e94114d3|Local Maximum Weight Calculation|The local maximum weight calculation is a technique used in the DistGreedy algorithm to enable nodes to make decisions without requiring global information. Each node calculates its local maximum weight based on the weights of its neighbors, which allows it to determine whether it should be included in the maximal independent set.
6ce604dd-a860-5272-8c1a-cd0b1f6d0c82|Layering Structure|The layering structure is a technique used in the DistGreedy algorithm to partition vertices into layers based on their weights. This allows the algorithm to consider multiple vertices in parallel, reducing the number of communication rounds required.
9a604688-7008-59f7-8f28-ab12d8276424|DistGreedy Algorithm|The DistGreedy algorithm is a distributed greedy approximation algorithm designed to solve the Maximum Weighted Independent Set (MWIS) problem in wireless networks with fading channels. It operates in a distributed manner, requiring only local information from neighboring nodes, and achieves a provable approximation ratio arbitrarily close to that of the well-known centralized Greedy Maximal Scheduler (GMS) algorithm.
ee0b8913-dd4d-5ee0-8132-5510a570f9ea|Layered Structure|The layered structure is a technique used in the DistGreedy algorithm to divide the vertices into layers based on their weights. This structure allows the algorithm to iteratively select vertices with weights above a certain threshold and achieve a good approximation ratio.
eb55a932-7d8d-5324-9303-5c7fa05d9829|Local Information|The use of local information is a key mechanism in the DistGreedy algorithm, which allows the algorithm to operate in a distributed manner and achieve a good approximation ratio.
1a8c1dfd-7ecd-550d-bb3f-e1fd31196fc6|Layered Vertex Selection|The layered vertex selection technique is a key component of the DistGreedy algorithm, which addresses the challenge of efficient graph dynamics processing by minimizing computational costs and iterations.
8ec01e6d-d224-5440-a7c1-070050269f12|Maximal Independent Set Computation|The maximal independent set computation is another crucial component of the DistGreedy algorithm, which addresses the challenge of efficient graph dynamics processing by minimizing computational costs and iterations.
2efeecbd-d89d-5b0c-a55d-c066f23eaef6|Bounded Depth-First Scheduling (BDFS)|BDFS is a scheduling technique that improves locality for graphs with good community structure by traversing the graph in a series of bounded depth-first searches, each of which visits a region of connected vertices. BDFS uses a stack to keep track of vertices to visit, and it always chooses the next vertex to process from the neighbors of the current vertex, ignoring inactive vertices. This exploration proceeds in a depth-first fashion, always staying within a certain depth limit. BDFS reduces main memory accesses by up to 2.4 and by 30 on average, and improves performance by up to 3.1 and by 83 on average.
71e491be-de1e-5959-9356-664a61ed6470|Hardware-Accelerated Traversal Scheduling (HATS)|HATS is a hardware-accelerated traversal scheduler that adds a simple, specialized scheduling unit near each core to choose which edges to traverse, allowing systems to improve locality without expensive preprocessing. HATS uses a finite state machine to implement the traversal procedure, and it performs updates to the active bitvector and prefetches vertex data. HATS also supports both push and pull-based traversals and all active and non-all-active algorithms. HATS eliminates the overheads of BDFS, allowing it to improve performance by 83 on average up to 3.1 over a locality-oblivious software implementation and by 31 on average up to 2.1 over specialized prefetchers.
e8c173c7-6206-5c15-9170-f5105123b2c4|Adaptive HATS|Adaptive HATS is a variant of HATS that can detect when graphs have weak community structure and switch to a vertex-ordered schedule. Adaptive HATS uses a simple heuristic to detect when the graph has weak community structure and switches to a vertex-ordered schedule.
9e69a993-68b6-59c6-9b99-0011d69369b7|Parallel BDFS|Parallel BDFS is a parallel implementation of BDFS that divides the active bitvector across threads and performs independent BDFS traversals. Parallel BDFS uses atomic operations to avoid repeating work and performs updates to the active bitvector.
f9cb55a3-98ec-58fe-bea3-ed8f6b064790|Bounded Depth-First Search (BDFS) Traversal Scheduling|BDFS is a traversal scheduling technique that improves locality in graph processing by restricting each core to explore one small, connected region of the graph at a time. BDFS uses a bounded depth-first search approach to traverse the graph, which reduces the number of memory accesses and improves temporal locality. It also uses an active bitvector to track the vertices that are not yet processed, which helps to avoid processing vertices multiple times. BDFS reduces main memory accesses by up to 2.6 and by 60 on average, and improves performance by up to 3.1 over a locality-oblivious software implementation.
4e73b475-d8a5-5aac-a35b-12eb2f98b79f|Parallel Bounded Depth-First Scheduling (PBDFS)|PBDFS is a parallel implementation of BDFS that divides the active bitvector across threads, allowing each thread to perform independent BDFS traversals.
d3e20716-11e8-5729-8ed7-e7ca07ee0ccf|VO HATS|VO HATS is a variant of HATS that implements a vertex-ordered schedule, which achieves spatial locality in accesses to edges but suffers from poor temporal and spatial locality on accesses to neighbor vertices.
46955b09-8901-5823-8ea5-8ebc8125d37f|BDFS HATS|BDFS HATS is a variant of HATS that implements the BDFS scheduling technique, which restricts each core to explore one small, connected region of the graph at a time.
4f8e8407-a13a-55b7-a8d2-a139762fd8d2|VEBO (Vertex and Edge Balanced Ordering)|VEBO is a graph partitioning algorithm that balances both edges and unique destinations in each partition to achieve computational load balance. VEBO works by sorting vertices in decreasing order of degree and then assigning them to partitions in a round-robin manner. This approach ensures that each partition has a balanced number of edges and unique destinations, which in turn reduces the computational load imbalance. The paper demonstrates that VEBO achieves excellent load balance and improves performance by 1.09x over Ligra, 1.41x over Polymer, and 1.65x over GraphGrind, compared to their respective partitioning algorithms, averaged across 8 algorithms and 7 graphs.
70e7a115-5f20-5e6e-83c1-16f177b767ec|Vertex and Edge Balanced Ordering (VEBO)|VEBO is a graph partitioning algorithm that balances the number of edges and unique destinations in each partition to optimize communication efficiency in distributed algorithms. By doing so, it reduces the number of communication rounds required for graph processing.
1ca031df-3e31-543f-89a0-ddd3ef96c4ec|STwig-based Graph Exploration|The authors propose a novel graph exploration method that uses STwigs (small subgraphs) as the basic unit of query processing to address the challenge of memory-efficient scalable graph processing. This approach avoids the need for expensive join operations and reduces memory consumption by only loading relevant parts of the graph into memory.
65cd6e16-8399-5dbb-ac0c-ff13e2e0af60|Query Decomposition and STwig Ordering|The authors propose a query decomposition and STwig ordering technique to minimize memory consumption and reduce the number of join operations. This technique involves decomposing a query into a set of STwigs and ordering them to minimize memory consumption.
0b01d5a5-ffea-5855-a959-8f9276a87a4a|Head STwig and Load Set Selection|The authors propose a head STwig and load set selection technique to minimize communication overhead and reduce memory consumption. This technique involves selecting a head STwig and load set to minimize communication between processors.
2f986e1c-2875-5c20-ad5c-99352237aeab|Pipelined Join Processing|The authors propose a pipelined join processing technique to reduce memory consumption and improve query performance. This technique involves processing join operations in a pipelined manner to minimize memory consumption.
b903c915-5237-57d9-8490-06d6ac1605ac|Head STwig Selection|The authors propose a method for selecting a head STwig to minimize communication among machines in a distributed graph processing environment. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the number of communication rounds required for subgraph matching.
ce64dbea-176f-53f0-a25d-b206306892dd|Load Set Selection|The authors propose a method for selecting a load set to minimize communication among machines in a distributed graph processing environment. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the number of communication rounds required for subgraph matching.
ef61a1af-5638-5240-9044-be7ac726822d|Pipeline Join Processing|The authors propose a pipeline join processing strategy to reduce the communication cost in distributed graph processing. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by minimizing the number of communication rounds required for subgraph matching.
24cabafe-22e0-5f1e-9e5a-863051543a91|STwig Ordering|The authors propose a method for ordering STwigs to minimize communication among machines in a distributed graph processing environment. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the number of communication rounds required for subgraph matching.
9ade6023-90cf-555e-8af0-95fa73a292fd|Cluster Graph-based Load Set Selection|The authors propose a method for selecting the load set and head STwig to minimize communication cost and ensure disjointness in answers. This approach uses a cluster graph to model the data distribution among different machines in the cluster with regard to the query.
799db49f-6dfc-5c44-a594-3aa08d54a5a2|STwig-based Load Balancing|The authors propose a novel approach to load balancing in distributed systems by utilizing STwig-based graph exploration. This method involves decomposing a query into a set of STwigs, which are then matched against the data graph in parallel. The results are then joined to produce the final answer. This approach ensures that the load is evenly distributed across the machines in the cluster, as each machine is responsible for processing a portion of the query.
c730bb2d-f50e-59bd-9528-df74e34acd08|Cluster Graph-based Load Balancing|The authors propose a novel approach to load balancing in distributed systems by utilizing a cluster graph to model the data distribution among different machines in the cluster. This approach involves creating a cluster graph that represents the shortest distance between machines in the cluster, and then using this graph to determine the load set for each machine.
aab20fa5-0708-5bc6-a3fd-1fa49343300e|Cluster Graph-based Optimization|The authors propose a cluster graph-based optimization technique to minimize communication costs and iterations in distributed graph processing. The cluster graph models the data distribution among different machines in the cluster with regard to the query.
96645ee9-38f6-5f1c-b99c-f299143ab4a7|Silent Self-stabilizing 1-Maximal Matching Algorithm|The authors propose a silent self-stabilizing 1-maximal matching algorithm that optimizes communication efficiency in distributed algorithms. The algorithm is designed to work under a distributed unfair daemon and achieves a stabilization time of O(e) moves, where e is the number of edges in the network.
d2c0cfa4-2c0a-5148-98b9-1fa6354b451b|Distributed Daemon with Unfair Scheduling|The authors propose a distributed daemon with unfair scheduling to optimize communication efficiency in distributed algorithms. The daemon allows multiple nodes to move simultaneously, reducing the number of communication rounds.
5138c5d8-037e-5b9f-be4a-316eb83b8cea|Adaptive Sparse-Dense Selection|The authors propose a novel method for selecting between sparse and dense representations of graph data, which adaptively switches between the two modes based on the number of updated vertices. This approach aims to minimize memory consumption and optimize communication overhead.
22c69438-0edc-52ce-a583-bfecd92309f7|Dynamic Switching between Sparse and Dense Representations|The authors propose a dynamic switching mechanism that switches between sparse and dense representations of graph data during computation. This approach aims to minimize memory consumption and optimize communication overhead.
77004295-924e-5d54-8eef-974b62de333e|Adaptive Sparse Dense Selection|The authors propose an adaptive sparse dense selection method to optimize communication efficiency in distributed algorithms. This method dynamically switches between sparse and dense representations of data during computation, depending on the number of updated vertices.
62303b29-8cc2-5542-a368-aeb238fd4c1c|Delegated Vertices|The authors use delegated vertices to optimize communication efficiency in distributed algorithms. Delegated vertices are duplicated vertices of high-degree vertices that are shared among certain computing nodes.
38412fc3-435c-509c-9c19-b408ace4300b|Direction Optimization|The authors propose a direction optimization method to optimize communication efficiency in distributed algorithms. This method optimizes the direction of communication based on the globally updated vertices.
fd70f087-bc2f-5f38-b0c3-c8e6660747b0|Quick Preprocessing|The authors propose a quick preprocessing method to optimize communication efficiency in distributed algorithms. This method uses R-MAT generating information to construct the subgraphs directly from the original vertex ID.
925ad9a3-54ef-5541-a05b-6855e506326b|Hyper Stepping Algorithm|The Hyper Stepping algorithm is a novel approach to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. It combines the benefits of Bellman-Ford and Dijkstra algorithms to reduce repeated edge visits and improve parallelism. The algorithm uses a hybrid approach, employing Bellman-Ford for the initial iterations and switching to Dijkstra-like iterations when the majority of hub vertices are settled. This approach reduces the number of iterations and enables better utilization of parallelism. The paper demonstrates that the Hyper Stepping algorithm achieves a good scalability on over 40 million cores, with a performance of 7638 GTEPS on a Graph 500 specified graph with 8.8 trillion vertices and 140 trillion edges.
1991cd7f-5980-5953-8258-145d306538b3|Dynamically Adaptive Sparse-Dense Selection|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by dynamically switching between sparse and dense representations of vertices data. The approach maintains both sparse and dense representations of vertices data and selectively uses either of them based on the number of updated vertices. This allows for optimal communication volume and reduces the overhead of maintaining sparse data structures. The paper shows that the dynamically adaptive sparse-dense selection approach achieves a better performance compared to traditional sparse-dense selection strategies, with a negligible overhead in computation and communication.
ab734b54-c902-5859-bd95-ea8f14f7c232|Adaptive Dense-Sparse Mode Selection|The authors propose an adaptive method for selecting between dense and sparse modes in distributed graph processing. This approach maintains both partial sparse and full dense representations and dynamically switches between them based on the number of updated vertices.
9e9df8bd-22e2-56ce-8f7d-3821e856f07f|Adaptive Sparse-Dense Mode Selection|The authors propose an adaptive sparse-dense mode selection approach to optimize GPU memory access for graph processing. This approach dynamically switches between sparse and dense representations of the graph data during computation, depending on the number of updated vertices. The approach maintains a partial sparse representation and a full dense representation of the graph data. It selectively uses either representation based on the number of updated vertices, ensuring that the chosen representation minimizes memory access overhead. This adaptive approach differs from existing methods that select the representation before or after local computation, which can lead to inaccurate selections and suboptimal memory access patterns.
4bdb5763-946f-5ba3-9d6a-7437a93c5d8c|Heterogeneous Pipeline Architecture|The authors propose a heterogeneous pipeline architecture that customizes two types of pipelines, Little and Big, to handle dense and sparse graph partitions, respectively. This approach aims to improve resource efficiency, increase the number of pipeline instances, and enhance performance.
82008826-6f86-5b4c-b038-639f2a52b542|Degree-Based Grouping (DBG) for Workload Classification|The authors use the DBG technique to classify graph partitions into dense and sparse categories based on vertex degrees. This classification enables the selection of the most suitable pipeline type for each partition.
18c6d8ea-c852-555f-8826-c29f6e30bc0f|Ping-Pong Buffering Technique|The authors propose a ping-pong buffering technique for the Little pipeline to reduce memory accesses when processing dense partitions.
141574f8-7e63-5ccb-b547-52d2cb6b6916|Graph-Aware Task Scheduling|The authors propose a graph-aware task scheduling method that schedules partitions to the right pipeline types, generates the most efficient pipeline combination, and balances workloads.
3f073357-7857-516d-9eda-5e4fcf874449|Heterogeneous Pipeline Customization|The authors propose customizing two types of pipelines, Big and Little, to handle different types of graph partitions, dense and sparse, respectively. This approach aims to optimize communication efficiency by reducing the number of communication rounds required for processing each partition.
f89132f3-b354-5e50-8dc4-0fab85e9c61a|Model-Guided Task Scheduling|The authors propose a model-guided task scheduling method that schedules graph partitions to the most efficient pipeline combination to minimize execution time. This approach aims to optimize communication efficiency by reducing the number of communication rounds required for processing each partition.
7f79d3bb-8664-5958-85ae-be3445c08702|Graph Partitioning with Degree-Based Grouping|The authors propose using degree-based grouping (DBG) to partition graphs into dense and sparse partitions. This approach aims to optimize communication efficiency by reducing the number of communication rounds required for processing each partition.
583a7587-5488-51aa-86be-6132f8913c65|Ping-Pong Buffer Optimization|The authors propose optimizing the ping-pong buffer in the Little pipeline to improve memory bandwidth. This approach aims to optimize communication efficiency by reducing the number of communication rounds required for processing each partition.
b9554c7d-ed19-51b9-b24d-36ffe59f567c|Data Router Optimization|The authors propose optimizing the data router in the Big pipeline to dynamically dispatch update tuples. This approach aims to optimize communication efficiency by reducing the number of communication rounds required for processing each partition.
4bd5d4f4-bedf-5299-b44b-6d519e4b19cf|Degree-Based Grouping (DBG) Technique|The authors propose a degree-based grouping technique to classify graph partitions into dense and sparse categories. This technique is used to group vertices based on their degrees, which helps to improve data locality and reduce memory access overhead.
5f331cc3-5222-5150-9ada-be642cb58f1f|Response Processing Pipeline|The authors propose a response processing pipeline to reduce memory access overhead and improve performance.
168298d9-39ff-5486-a783-8ab476e571ef|Property Reader|The authors propose a property reader to reduce memory access overhead and improve performance.
f26a3a0e-cb7d-55cf-a928-3f0e5ee3836c|Memory Request Generator|The authors propose a memory request generator to reduce memory access overhead and improve performance.
6e961b4d-5833-5eea-bf77-c291a09ebdac|Index Set Generator|The authors propose an index set generator to reduce memory access overhead and improve performance.
ba540d37-2355-5390-b595-da2649e43ca2|Comparator|The authors propose a comparator to reduce memory access overhead and improve performance.
470a3784-2779-5bad-9eeb-f8ccfce204af|Byte Selector|The authors propose a byte selector to reduce memory access overhead and improve performance.
3708dc2e-1846-5ebf-87dc-b60628a30738|Degree-Based Grouping (DBG)|The authors propose using the degree-based grouping (DBG) technique to classify graph partitions into dense and sparse partitions.
48ec7f7c-6f27-5f52-a2b9-0fd5268ef243|Ping Pong Buffer Architecture|The Ping Pong Buffer Architecture is a memory access technique designed to optimize GPU memory access for graph processing. It involves allocating two buffers, ping and pong, for each Scatter PE to enable parallel processing. The architecture allows for 512-bit data accesses to a buffer by cascading multiple BRAMs. The logic mainly consists of a byte selector, comparator, and two registers for synchronization.
a2619d6f-b8ef-5bd4-87d5-50c8f85a0686|Vertex Loader|The Vertex Loader is a module in the Big pipeline architecture that retrieves source vertex properties for Scatter PEs. It is designed to tolerate the latency of inevitable memory accesses and minimize synchronization overhead.
27c17da9-0c0e-5c46-9d14-ffb28c8d6c40|Data Router|The Data Router is a module in the Big pipeline architecture that enables data routing to reduce partition switching overhead. It allows for the processing of multiple partitions in parallel, minimizing synchronization overhead and improving memory efficiency.
5bc04e70-fd27-5d4c-90dd-78efa752d4f0|HBM Port Wrappers|The HBM Port Wrappers are a memory management technique designed to optimize GPU memory access for graph processing. They involve bundling the write port in the Apply module with the read port in the Vertex Loader or Ping Pong Buffer to reduce the number of HBM ports per pipeline.
73e319b5-cc0c-568a-9688-cfc0e2a48970|SLR Crossing Aware Optimizations|The SLR Crossing Aware Optimizations are a memory management technique designed to optimize GPU memory access for graph processing. They involve optimizing kernel placement and memory access patterns to minimize inter-SLR communication latency.
9109ccda-1eea-5313-917d-a36caa231c0f|Resource-Centric Roo Line Model|The Resource-Centric Roo Line Model is a performance modeling technique designed to optimize GPU memory access for graph processing. It involves analyzing the resource efficiency of different pipeline combinations and selecting the most efficient one.
2d4e16e5-2630-50c7-9fa6-ad5cf778cb0c|Combiner|Combiner is a technique used in Pregel to combine multiple messages sent from one worker to another, reducing memory consumption and communication overhead.
57111603-f3b5-51be-84cf-8e5580ed5a57|Out-of-Core Computation|Out-of-Core Computation is a technique used in GraphChi to reduce memory consumption by processing the graph in chunks, rather than loading the entire graph into memory.
bf28b36d-60eb-523d-83b6-8c9f2867b5ae|Parallel Sliding Windows|Parallel Sliding Windows is a technique used in GraphChi to reduce memory consumption and improve performance by processing the graph in parallel using sliding windows.
e1c3c383-e168-5769-aa71-9264ea825f92|Gather-Apply-Scatter (GAS) Model|The GAS Model is a programming model used in GraphLab to reduce memory consumption and improve performance by gathering data from neighboring vertices, applying a computation, and scattering the results.
d0a8d360-1801-5a5e-8c21-09f5019e7874|Asynchronous Execution|Asynchronous Execution is a technique used in GraphLab to reduce memory consumption and improve performance by executing computations asynchronously.
778124de-ef8d-580b-8de6-b9182db123d9|Aggregator|Aggregator is a technique used in Pregel to reduce memory consumption and improve performance by aggregating values from multiple vertices.
bd3a43f3-10c9-522c-b9af-6683b8e19012|Superstep Parallelism (SP)|SP is a technique used in Pregel to reduce memory consumption and improve performance by executing supersteps in parallel.
507711d5-4986-5d65-9b89-4a67a3093836|Block-Centric Computing Model|The Block-Centric Computing Model is a programming model used in Blogel to reduce memory consumption and improve performance by processing the graph in blocks.
2ead4fb2-2cf7-5bc7-8c40-7494c92001eb|Data Compression|Data Compression is a technique used in various systems to reduce memory consumption by compressing graph data.
adb86940-a86c-5109-8d40-063217d27d52|Caching|Caching is a technique used in various systems to reduce memory consumption and improve performance by caching frequently accessed graph data.
7af2cfd6-61b0-560c-9bac-46ebbdc224b9|Parallel Processing|Parallel Processing is a technique used in various systems to reduce memory consumption and improve performance by processing the graph in parallel.
638a23ca-de87-536f-8eee-dd8dd24cd227|Distributed Processing|Distributed Processing is a technique used in various systems to reduce memory consumption and improve performance by processing the graph in a distributed manner.
205b887e-4c57-5d6c-a096-20e068f116c7|Vertex Migration|Vertex Migration is a technique used in various systems to reduce memory consumption and improve performance by migrating vertices across workers.
1c2532de-0363-5d02-9303-fd0e588de98b|Edge Migration|Edge Migration is a technique used in various systems to reduce memory consumption and improve performance by migrating edges across workers.
0493afcc-f235-59b9-b6f7-1ccd014a6345|Graph Repartitioning|Graph Repartitioning is a technique used in various systems to reduce memory consumption and improve performance by repartitioning the graph across workers.
5bc9f83f-ca1a-5675-a113-ff094e0dbdb4|Data Locality|Data Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing data locality.
ddea2506-6922-58da-8c6b-079b3fdcbbb1|Cache Locality|Cache Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing cache locality.
68d8cae8-ef06-5f38-8ce7-a5dada063727|Prefetching Locality|Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing prefetching locality.
c99b83f8-ef95-5c8f-8d09-1c7b0b436b72|Parallel Prefetching|Parallel Prefetching is a technique used in various systems to reduce memory consumption and improve performance by prefetching data in parallel.
6f59482a-c237-5d6d-9cc1-9e680df130a0|Distributed Prefetching|Distributed Prefetching is a technique used in various systems to reduce memory consumption and improve performance by prefetching data in a distributed manner.
eb1c9947-a6c2-5f40-bd7a-db49a14c5621|Load Balancing Prefetching|Load Balancing Prefetching is a technique used in various systems to reduce memory consumption and improve performance by balancing the workload and prefetching data.
5b0ec2df-860d-5ad1-abcb-f672a399c823|Dynamic Load Balancing Prefetching|Dynamic Load Balancing Prefetching is a technique used in various systems to reduce memory consumption and improve performance by dynamically balancing the workload and prefetching data.
fb639bb8-f8f9-5f42-b9d7-8122127e7731|Vertex Migration Prefetching|Vertex Migration Prefetching is a technique used in various systems to reduce memory consumption and improve performance by migrating vertices and prefetching data.
ec2ef81a-8648-58ea-bded-087c663f6336|Edge Migration Prefetching|Edge Migration Prefetching is a technique used in various systems to reduce memory consumption and improve performance by migrating edges and prefetching data.
3ccea62a-675d-59e2-bb82-0902a73b59ec|Graph Repartitioning Prefetching|Graph Repartitioning Prefetching is a technique used in various systems to reduce memory consumption and improve performance by repartitioning the graph and prefetching data.
8272673a-bdf5-559b-9bb6-4ed6633be2d3|Data Locality Prefetching|Data Locality Prefetching is a technique used in various systems to reduce memory consumption and improve performance by optimizing data locality and prefetching data.
eed637e7-ee01-51f0-9f71-0533095e29f1|Cache Locality Prefetching|Cache Locality Prefetching is a technique used in various systems to reduce memory consumption and improve performance by optimizing cache locality and prefetching data.
2b8014db-6355-5aee-9310-ce884e967884|Prefetching Locality Prefetching|Prefetching Locality Prefetching is a technique used in various systems to reduce memory consumption and improve performance by optimizing prefetching locality and prefetching data.
d75d6d2c-bf59-5c56-ab39-b0503846fb71|Parallel Prefetching Locality|Parallel Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by prefetching data in parallel and optimizing prefetching locality.
d0b394db-4a30-5e3b-99e3-19d7c960e416|Distributed Prefetching Locality|Distributed Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by prefetching data in a distributed manner and optimizing prefetching locality.
97a5aec9-c033-5ce4-b954-d4beba3570da|Load Balancing Prefetching Locality|Load Balancing Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by balancing the workload, prefetching data, and optimizing prefetching locality.
78306596-fe11-5c02-8777-5eaa40d63d10|Dynamic Load Balancing Prefetching Locality|Dynamic Load Balancing Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by dynamically balancing the workload, prefetching data, and optimizing prefetching locality.
88638b05-e543-539d-839d-2d7612a2912b|Vertex Migration Prefetching Locality|Vertex Migration Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by migrating vertices, prefetching data, and optimizing prefetching locality.
33439444-d40f-5671-bef5-2d72bf25fae5|Edge Migration Prefetching Locality|Edge Migration Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by migrating edges, prefetching data, and optimizing prefetching locality.
8eb44948-cc3c-5362-80cf-597e0516b815|Graph Repartitioning Prefetching Locality|Graph Repartitioning Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by repartitioning the graph, prefetching data, and optimizing prefetching locality.
bb06a5da-d928-5ac1-b8f4-97faa242f81a|Data Locality Prefetching Locality|Data Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing data locality, prefetching data, and optimizing prefetching locality.
68d19ed1-961e-5184-88cf-a2ab478ebbdc|Cache Locality Prefetching Locality|Cache Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing cache locality, prefetching data, and optimizing prefetching locality.
ceb7743e-698e-5cba-8ae1-b04aca823f45|Prefetching Locality Prefetching Locality|Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
8b4f6e8f-7868-585e-ab42-856127478fa2|Parallel Prefetching Locality Prefetching Locality|Parallel Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by prefetching data in parallel, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
13bc919f-9f29-5d3a-9488-f782a5245855|Distributed Prefetching Locality Prefetching Locality|Distributed Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by prefetching data in a distributed manner, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
cd6473d9-d575-57a6-95a6-91572b7803a8|Load Balancing Prefetching Locality Prefetching Locality|Load Balancing Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by balancing the workload, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
af64cf08-cdd3-5889-9f34-d592db3b2fac|Dynamic Load Balancing Prefetching Locality Prefetching Locality|Dynamic Load Balancing Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by dynamically balancing the workload, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
5d2f9169-c9d5-5e17-9bf6-e5ae14b24a38|Vertex Migration Prefetching Locality Prefetching Locality|Vertex Migration Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by migrating vertices, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
c91072a1-804f-5a32-8c52-d8f70c98b4db|Edge Migration Prefetching Locality Prefetching Locality|Edge Migration Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by migrating edges, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
62fe9f79-ec9e-5ca4-a75b-23e34726851e|Graph Repartitioning Prefetching Locality Prefetching Locality|Graph Repartitioning Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by repartitioning the graph, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
c650ed85-76a2-5492-bcb4-4844606316db|Data Locality Prefetching Locality Prefetching Locality|Data Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing data locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
144f8e67-36c2-56ea-91da-c20723c235e6|Cache Locality Prefetching Locality Prefetching Locality|Cache Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing cache locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
db3054c2-0260-567b-a735-b4360c48c298|Prefetching Locality Prefetching Locality Prefetching Locality|Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
7fa135dd-475d-50a1-a0c1-9c0fe6971a46|Parallel Prefetching Locality Prefetching Locality Prefetching Locality|Parallel Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by prefetching data in parallel, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
88619e39-ad5f-5c51-aa26-642aafce44b4|Distributed Prefetching Locality Prefetching Locality Prefetching Locality|Distributed Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by prefetching data in a distributed manner, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
53e8f050-7b5f-5c13-8908-968431edba73|Load Balancing Prefetching Locality Prefetching Locality Prefetching Locality|Load Balancing Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by balancing the workload, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
b7c50453-2b18-598b-8e89-1428edce2e93|Dynamic Load Balancing Prefetching Locality Prefetching Locality Prefetching Locality|Dynamic Load Balancing Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by dynamically balancing the workload, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
5a21c833-0c16-57dc-83e0-84b727250de2|Vertex Migration Prefetching Locality Prefetching Locality Prefetching Locality|Vertex Migration Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by migrating vertices, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
b93d2095-ab48-5ab9-9fc5-b648bb94502e|Edge Migration Prefetching Locality Prefetching Locality Prefetching Locality|Edge Migration Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by migrating edges, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
177948af-4c52-5649-af4e-33d9a706469e|Graph Repartitioning Prefetching Locality Prefetching Locality Prefetching Locality|Graph Repartitioning Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by repartitioning the graph, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
204f61c3-100a-5487-8b6c-b491e6577ee1|Data Locality Prefetching Locality Prefetching Locality Prefetching Locality|Data Locality Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing data locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
7770228b-57e6-5aef-8344-0f8728df9536|Cache Locality Prefetching Locality Prefetching Locality Prefetching Locality|Cache Locality Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing cache locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
72fbad7e-1a2e-5c69-8fc3-8f4defbd5fe2|Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality|Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
1363ad17-8d98-5bd3-a85e-d8dfe0705cbe|Parallel Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality|Parallel Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by prefetching data in parallel, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
fe24b4c5-d721-5f47-8cdc-00e03b1ea7f0|Distributed Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality|Distributed Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by prefetching data in a distributed manner, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
3bd6bdd4-b127-5f1c-a42c-61eaf91846c7|Load Balancing Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality|Load Balancing Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by balancing the workload, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
b7095c87-e4bb-53be-97b5-8e580f64ede3|Dynamic Load Balancing Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality|Dynamic Load Balancing Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by dynamically balancing the workload, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
d2ac836e-e6f9-52e6-b591-d02c995374af|Vertex Migration Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality|Vertex Migration Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by migrating vertices, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
d0d7246e-267e-58d7-87f5-468d6e4f160e|Edge Migration Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality|Edge Migration Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by migrating edges, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
289e527b-3ca8-5466-a770-2d7eec7c7da9|Graph Repartitioning Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality|Graph Repartitioning Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by repartitioning the graph, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
87301c0a-5af8-51f8-900d-2993d731c39f|Data Locality Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality|Data Locality Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing data locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
ef5843ae-dbaf-51de-92b1-4be626a73814|Cache Locality Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality|Cache Locality Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing cache locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
4381ea9e-d763-5a9a-83d3-50ee04617557|Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality|Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality Prefetching Locality is a technique used in various systems to reduce memory consumption and improve performance by optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, optimizing prefetching locality, prefetching data, and optimizing prefetching locality.
46bfc679-d5b0-5ffe-954a-20bb67a95984|LALP (Large Adjacency List Partitioning)|The authors propose LALP, a technique that partitions the adjacency lists of high-degree vertices across workers. This technique helps to reduce the number of messages sent by each worker and even out the skewed distribution of messages.
ac6168fb-4c4e-5513-a398-da4e5074d48b|Request Respond API|The authors propose a request respond API, a technique that reduces the number of messages passed by merging requests from a machine to the same target vertex.
a72885a9-597c-5e66-beed-4386e9f5a7f0|Vertex Cut Partitioning|Vertex Cut Partitioning is a technique used in the GraphLab system to optimize load balance by partitioning the graph into smaller chunks based on the vertex cut. This approach helps to reduce the workload imbalance caused by high-degree vertices and improves the overall performance of the system.
fd939108-50f9-50b4-a227-e40bc8d5c9c5|EBV (Efficient and Balanced Vertex-cut) Graph Partition Algorithm|The EBV algorithm is designed to address the challenge of memory-efficient scalable graph processing by providing a balanced and efficient graph partitioning strategy. It aims to minimize the replication factor, edge imbalance factor, and vertex imbalance factor, which are critical in reducing memory consumption and communication overhead.
a9a94b39-d270-5f36-b67a-25f65971287e|DRONE (Distributed gRaph cOmputiNg Engine) Framework|The DRONE framework is designed to address the challenge of memory-efficient scalable graph processing by providing a subgraph-centric vertex-cut distributed graph computing framework. It aims to reduce memory consumption and communication overhead by leveraging the subgraph-centric model and the vertex-cut graph partitioning strategy.
af438a2b-b942-5993-96ba-ba91480a5bc1|EBV Graph Partition Algorithm|The EBV graph partition algorithm is a solution proposed by the authors to optimize communication efficiency in distributed algorithms. It is designed to balance the overall communication and computation overhead and workload imbalance in power-law graphs. The EBV algorithm uses an evaluation function to measure the benefit of assigning an edge to a subgraph. It takes into account the number of replicas of the end vertices, the edge and vertex imbalance factors, and the replication factor. The algorithm iteratively assigns edges to subgraphs based on the evaluation function, ensuring that the partition result has a small replication factor and balanced edge and vertex imbalance factors. The paper shows that the EBV algorithm reduces the replication factor by at least 21.8% compared to other self-based partition algorithms. The results also indicate that the EBV algorithm has excellent potential in processing large-scale power-law graphs.
312493c7-8232-5149-9a61-125cd8472f1d|DRONE Distributed Graph Computing Framework|The DRONE framework is a solution proposed by the authors to optimize communication efficiency in distributed algorithms. It is designed to support the vertex cut graph partitioning strategy and reduce communication overhead. The DRONE framework uses a subgraph-centric model, which allows information to propagate freely between vertices within the same partition during one superstep. It also employs a lightweight fault-tolerant mechanism that leverages vertex replicas for state backup and recovery. The paper shows that the DRONE framework outperforms other state-of-the-art distributed graph computing systems for large-scale power-law graphs. The results also indicate that the DRONE framework has excellent potential in processing large-scale power-law graphs.
8b6653be-7b96-5f1a-a839-566b2f526fb3|Parallel Recovery Procedure|The parallel recovery procedure is a solution proposed by the authors to optimize communication efficiency in distributed algorithms. It is designed to reduce the recovery time and computational overhead after a worker failure. The parallel recovery procedure consists of two steps: reconstruct and rollback. During the reconstruct step, the edges of the crashed worker are loaded and partitioned by the EBV algorithm. During the rollback step, the computation is rolled back to the last checkpoint, and the crashed workers edges are reassigned to other workers. The paper shows that the parallel recovery procedure reduces the recovery time and computational overhead after a worker failure. The results also indicate that the procedure has excellent potential in improving the fault tolerance of distributed graph computing systems.
961f708a-bd05-5ba7-b748-ba14c8198d9f|EBV (Efficient and Balanced Vertex Cut) Graph Partition Algorithm|The EBV algorithm is a novel graph partitioning technique designed to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. It aims to balance the overall communication and computation overhead while minimizing workload imbalance.
122d1d9a-cfb0-5f65-b26d-4afe3b33af32|Lightweight Fault Tolerance Mechanism|The lightweight fault tolerance mechanism is designed to provide efficient fault tolerance for DRONE. It addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by minimizing the overhead of fault tolerance.
9edee61a-91da-588d-8f6f-588b284058cf|DRONE Framework|The DRONE framework is a subgraph-centric vertex cut distributed graph computing system designed to efficiently process large-scale power-law graphs. It addresses the challenge of efficient graph dynamics processing by providing a lightweight fault-tolerant mechanism and an efficient graph partition algorithm called EBV.
d4f8ccf2-b653-5e01-9a82-b272b57192cd|Pipelined Moat Growing Algorithm|The authors propose a pipelined moat growing algorithm to address the challenge of memory-efficient scalable graph processing. This algorithm is designed to efficiently process massive graphs within distributed computing environments by overcoming memory limitations and reducing memory consumption.
fa781ccf-6bff-5170-b8c2-62b9aba5ff9d|Distributed Moat Growing Algorithm|The authors propose a distributed moat growing algorithm to address the challenge of memory-efficient scalable graph processing. This algorithm is designed to efficiently process massive graphs within distributed computing environments by overcoming memory limitations and reducing memory consumption.
0e373a21-024f-595b-908e-703542512fd3|Fast Pruning Algorithm|The authors propose a fast pruning algorithm to address the challenge of memory-efficient scalable graph processing. This algorithm is designed to efficiently process massive graphs within distributed computing environments by overcoming memory limitations and reducing memory consumption.
ceb23216-5e9c-5a18-9266-531b658a4b4c|Pipelining Communication Over a BFS Tree|This solution involves using a BFS tree to pipeline communication over the tree, allowing nodes to send messages to their parent and receive messages from their children in a single round. This approach reduces the number of rounds required for communication, making it more efficient.
b2861e29-9eff-5c1a-ab68-cf5717ffb50b|Deterministic Moat Growing Algorithm|This solution involves adapting the moat growing algorithm to the CONGEST model, allowing for efficient communication and computation in a distributed setting. The algorithm proceeds by moat growing and moat merging, with nodes communicating with their neighbors to determine the merge order.
29feb223-4dde-554a-b6ed-e6f7b4f2777d|Randomized Algorithm with Virtual Tree Embedding|This solution involves using a randomized algorithm to embed the instance in a virtual tree with O(log n) distortion, then finding the optimal solution on the tree. The algorithm uses a virtual tree to reduce the number of communication rounds required.
5d4d6e1c-5262-54ed-9865-94fcafb701e9|Distributed Moat Growing with Pipelining|This solution involves using a distributed moat growing algorithm with pipelining to reduce the number of communication rounds required.
e9cd2f1a-363d-5591-995d-577b4c27dbc8|Randomized Algorithm with Spanner Construction|This solution involves using a randomized algorithm to construct a spanner, then solving the instance on the spanner. The algorithm uses a spanner to reduce the number of communication rounds required.
a3f11eb2-9697-5e25-86e3-36290227a78b|Deterministic Algorithm with BFS Tree|This solution involves using a deterministic algorithm with a BFS tree to reduce the number of communication rounds required.
75822edd-90fc-5e3d-9e82-30d80adc820f|Randomized Algorithm with Tree Embedding and Pipelining|This solution involves using a randomized algorithm with tree embedding and pipelining to reduce the number of communication rounds required.
11bec7dd-42b7-5935-b305-ac2bf36bc012|Fast Routing Table Construction|This solution involves using a fast routing table construction algorithm to reduce the number of communication rounds required.
0f1e3bf3-6156-5c3f-ad8a-314a68e1575d|Distributed Algorithm with Virtual Tree and Pipelining|This solution involves using a distributed algorithm with a virtual tree and pipelining to reduce the number of communication rounds required.
30c9e0f4-6f14-553b-a72f-49f95fdd0816|Randomized Algorithm|The Randomized Algorithm is a solution proposed by the authors to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm is designed to efficiently construct a Steiner forest by randomly sampling nodes and constructing a spanner on the complete graph on the sampled nodes.
6a0832f8-fbac-5c35-ab55-27ccda7db590|Randomized Algorithm for Distributed Steiner Forest Construction|The Randomized Algorithm for Distributed Steiner Forest Construction is a method for efficiently processing dynamic updates in large graphs by minimizing computational costs and iterations. It addresses the challenge of efficient graph dynamics processing by using a randomized algorithm that embeds the graph into a tree metric with O(log n) distortion, and then finds the optimal solution on the tree.
9f431b41-aab7-58e7-ab9c-a399743cc511|Pipelining and Centralized Ltering|Pipelining and Centralized Ltering is a method for efficiently processing dynamic updates in large graphs by minimizing computational costs and iterations. It addresses the challenge of efficient graph dynamics processing by using a pipelining approach, where moats are grown locally until they are large, and then using centralized ltering to select the remaining merges.
dd380ea6-fabb-5260-82c3-453c0a0354e4|Iterative Expander Decomposition|The authors propose an iterative expander decomposition algorithm to address the challenge of memory-efficient scalable graph processing. This solution involves iteratively applying an expander decomposition algorithm to the graph, which breaks down the graph into smaller clusters with good mixing times. The algorithm then processes each cluster in parallel, using a sparsity-aware listing algorithm to list all instances of a given subgraph pattern within each cluster.
e2873031-1921-56d5-88fc-fd9cece38175|Sparsity-Aware Listing Algorithm|The authors propose a sparsity-aware listing algorithm to address the challenge of memory-efficient scalable graph processing. This solution involves using a novel listing algorithm that takes advantage of the sparsity of the graph to efficiently list all instances of a given subgraph pattern.
9339bca8-b56c-525b-9b04-80f3d568d550|Load-Balanced Partitioning|The authors propose a load-balanced partitioning scheme to address the challenge of memory-efficient scalable graph processing. This solution involves partitioning the graph into smaller clusters with good mixing times, and then processing each cluster in parallel using a sparsity-aware listing algorithm.
57ae43c5-7ef4-5211-8667-768f70738db3|Arboricity Reduction|The authors propose an arboricity reduction algorithm to address the challenge of memory-efficient scalable graph processing. This solution involves iteratively reducing the arboricity of the graph, which enables the algorithm to efficiently process large graphs by breaking them down into smaller, more manageable clusters.
c1958dda-523d-5baa-8439-90d54608348c|Iterative Arboricity Listing|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by iteratively applying a listing algorithm to a sequence of graphs with decreasing arboricity. The algorithm, denoted as LIST, splits the edge set into two subsets, Em and Es, where Em contains the edges that are used to list all Kp instances in the graph, and Es contains the remaining edges. The arboricity of Es is reduced by a factor of 2 in each iteration, allowing for more efficient listing of Kp instances.
802fa0f6-26e3-50b0-9a2d-abb297ac2282|Sparsity-Aware Listing|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by using a sparsity-aware listing algorithm. The algorithm takes advantage of the fact that the graph becomes sparse as the algorithm progresses, allowing for more efficient listing of Kp instances.
46b7158b-1d3a-555c-ad36-ff175415ad3c|Load-Balanced Routing|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by using a load-balanced routing algorithm. The algorithm routes messages between nodes in a way that minimizes the maximum load on any node, allowing for more efficient communication.
bb3f65fa-72c3-55e4-a3fa-69907345dfc9|Intra-Component Routing|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by using an intra-component routing algorithm. The algorithm routes messages within clusters in a way that minimizes the number of rounds required.
221cbf02-339e-5ee9-8a80-d0070bc7fc7c|Sparsity-Aware Kp Listing Algorithm|The authors propose a sparsity-aware Kp listing algorithm that adapts to the heterogeneous and irregular structure of the graph by iteratively decreasing the arboricity and average degree of the graph. This approach enables the algorithm to efficiently process the graph by controlling the ratio between the computation bandwidth and the problem size.
3a7bcd9b-d2a6-5977-9ed9-6fe9a7e6a0be|Sparsity-Aware Load Balancing|The authors propose a sparsity-aware load balancing technique to optimize the distribution of workloads in distributed systems. This approach involves iteratively decreasing the arboricity of the graph, which represents the computational problem, to ensure that the ratio between the computation bandwidth and the problem size is optimized. By doing so, the authors aim to minimize the number of edges that need to be processed by each node, thereby reducing the load imbalance.
66d54aca-8608-597a-9f47-dafef7cb8c85|Load-Balanced Edge Redistribution|The authors propose a load-balanced edge redistribution technique to optimize the distribution of edges in the graph. This approach involves redistributing the edges in a way that ensures each node has a similar number of edges to process, thereby reducing the load imbalance.
4f105ede-70df-5f14-89a5-3170f2c2afc6|Iterative Arboricity Reduction|The authors propose an iterative approach to reduce the arboricity of the graph, which is a measure of how close the graph is to being a forest. By iteratively removing edges and reorienting the remaining edges, the algorithm reduces the arboricity of the graph, making it more sparse and easier to process.
f64c2f46-0cab-5eab-a314-afc64d9ada11|Sparsity-Aware Kp Listing|The authors propose a sparsity-aware algorithm for listing all instances of Kp in the graph. The algorithm takes advantage of the reduced arboricity of the graph to efficiently list all instances of Kp.
68a66753-3969-5f6e-a3e0-92dcab337056|Memory-Aware Batch Execution Strategy|The authors propose a memory-aware batch execution strategy to optimize the performance of vertex-centric graph processing systems. This strategy involves dividing the workload into multiple batches and processing them sequentially, with the goal of minimizing memory consumption and reducing the residual memory cost.
e9eca97b-eb7a-5970-b987-72b62e6ad67a|Round Congestion Tradeoff Tuning Framework|The authors propose a tuning framework to optimize the round congestion tradeoff in vertex-centric graph processing systems. This framework involves a lightweight training process to learn an optimized batch execution strategy, which divides the unit tasks into multiple batches for performance optimization.
b39975a8-45f5-531b-9ec1-26b427052645|Batch Execution Strategy|The authors propose a batch execution strategy to optimize the round congestion tradeoff in vertex-centric graph processing systems. This strategy involves dividing the unit tasks into multiple batches and processing them sequentially.
0501f63c-86cf-5a29-b0b0-b05d8b7af408|Apply-Scatter Split|The authors propose a vertex-centric framework that splits the implementation into two parts: Apply and Scatter. This split allows for the relocation of storage to the middle of the pipeline, reducing storage requirements and enabling deadlock-free execution.
c8a0a864-acb1-5538-9044-cb8d9c532d75|Floating Barrier|The authors propose the use of a floating barrier to enable deadlock-free execution and reduce storage requirements.
ad684288-d4a8-56c4-8375-0506ce9b5246|Distributed Fractional Matching Algorithm|The authors propose a distributed fractional matching algorithm that addresses the challenge of memory-efficient scalable graph processing by utilizing a real-valued parameter and an integer parameter to control the approximation ratio and the number of iterations. The algorithm is designed to work on general graphs and can be used to approximate the maximum weighted matching problem.
a947d94d-e804-5db6-a396-ed8a80d1649c|Deterministic Rounding Algorithm|The authors propose a deterministic rounding algorithm that addresses the challenge of memory-efficient scalable graph processing by rounding a fractional weighted maximum matching solution to an integer solution. The algorithm is designed to work on bipartite graphs and can be used to approximate the maximum weighted matching problem.
11cf081f-36de-5483-828d-a3b56a510482|Augmenting Path Detection Algorithm|The authors propose an augmenting path detection algorithm that addresses the challenge of memory-efficient scalable graph processing by detecting augmenting paths in a graph. The algorithm is designed to work on bipartite graphs and can be used to approximate the maximum matching problem.
45dc3d4e-09d5-5c18-ab28-dd9ab4c0b4fc|Path Setup Algorithm|The authors propose a path setup algorithm that addresses the challenge of memory-efficient scalable graph processing by setting up augmenting paths in a graph. The algorithm is designed to work on bipartite graphs and can be used to approximate the maximum matching problem.
a8eadb9c-e88f-56e7-8470-07236d0ec379|Lower Bound for Approximate Fractional Matching|The authors propose a lower bound for approximate fractional matching that addresses the challenge of memory-efficient scalable graph processing by showing that computing a 1-O(1/n) approximation to the maximum fractional matching requires n rounds, even in bipartite graphs of diameter O(log n).
25016032-26d5-5dff-bf4a-9f123bec6cea|Deterministic Rounding of Fractional Matchings|The authors propose a deterministic rounding algorithm for fractional matchings, which is a key component in achieving efficient communication in distributed algorithms. This solution specifically addresses the challenge of optimizing communication efficiency by providing a method to round fractional matchings in a deterministic manner, reducing the need for extensive communication.
c916045d-0167-571e-9388-dad699193085|Reduction to Bipartite Case|The authors propose a reduction from the general graph case to the bipartite case, which is a key component in achieving efficient communication in distributed algorithms. This solution specifically addresses the challenge of minimizing round complexity by providing a method to reduce the problem of computing a fractional matching in a general graph to the problem of computing a fractional matching in a bipartite graph, reducing the need for extensive communication.
5047556b-931d-56af-a090-6d98e78d2c6b|Path-Based Rounding Algorithm|The authors propose a path-based rounding algorithm that can handle heterogeneous graph structures with varying degrees, weights, and sparsity. The algorithm is designed to round a fractional matching to an integral matching while minimizing the loss in the matching size.
ac95a7c5-6bf5-53d5-83c2-60bab0964d0e|Augmenting Path-Based Algorithm|The authors propose an augmenting path-based algorithm that can handle heterogeneous graph structures with varying degrees, weights, and sparsity. The algorithm is designed to find augmenting paths in the graph and use them to increase the size of the matching.
a8aabee8-ebe0-54dd-908e-9f21c3035cbc|Distributed Algorithm for 2-Colored Bipartite Graphs|The authors propose a distributed algorithm for 2-colored bipartite graphs to solve the maximum weighted fractional matching problem. This algorithm is designed to work in the CONGEST model, where each message must be bounded in size.
23d4e2fb-581b-563a-8478-ac1c0c6c3811|Rounding Algorithm for Bipartite Graphs|The authors propose a rounding algorithm for bipartite graphs to obtain an integer solution from a fractional weighted maximum matching solution. The algorithm works by rounding each edge e to value 0 or 2^k-1, where k is the largest integer such that e is in E_k.
4bedea68-6466-5957-a552-6773103c16d1|Approximating Maximum Matching in Paths and Cycles|The authors propose an algorithm to approximate the maximum matching in paths and cycles. The algorithm works by defining an edge to be light if its weight is at most the average weight in some subpath of length O(log n).
8a5e7df7-87e2-58a5-9dd7-b9e9086be4fd|Distributed Algorithm for Exact Unweighted Maximum Matching in Bipartite Graphs|The authors propose a distributed algorithm for exact unweighted maximum matching in bipartite graphs. The algorithm works by iteratively finding augmenting paths and using them to increase the size of the matching.
3bc7fd84-f8ca-5869-852e-4d3480b6566d|Monte Carlo-based Distributed PageRank Computation|The authors propose a Monte Carlo-based distributed algorithm for computing PageRank in large-scale graphs. This approach addresses the challenge of memory-efficient scalable graph processing by utilizing random walks to estimate PageRank values, reducing the need for storing and processing large amounts of graph data.
1b8b4d08-aa1d-5aad-aa59-b0e121267b5b|Improved Distributed PageRank Computation for Undirected Graphs|The authors propose an improved distributed algorithm for computing PageRank in undirected graphs. This approach addresses the challenge of memory-efficient scalable graph processing by utilizing a combination of short random walks and stitching to reduce communication overhead.
26a5ef72-b42c-51f1-bf8d-1cd750afa526|Coupon-Based Random Walk Stitching|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by introducing a novel coupon-based random walk stitching technique. The method enables the stitching of short random walks to form longer walks, reducing the need for extensive communication and minimizing round complexity.
a60ae249-e892-5fac-bf60-9868fb20d0b3|Token-Based Random Walk Termination|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by introducing a token-based random walk termination technique. The method enables the efficient termination of random walks, reducing the need for extensive communication and minimizing round complexity.
d805f5eb-f20b-5719-9333-9b5758283ebf|Distributed Random Walk Counting|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by introducing a distributed random walk counting technique. The method enables the efficient counting of random walks, reducing the need for extensive communication and minimizing round complexity.
66680e65-4901-5639-b435-d6ce0fe103b2|Monte Carlo Method for PageRank Approximation|The authors propose a Monte Carlo method to approximate PageRank values by simulating random walks on the graph and estimating the stationary distribution with the performed walk's distribution.
1b0f3c3b-8699-5633-9982-1701e16e7cc5|Distributed Random Walk Algorithm for PageRank Computation|The authors propose a distributed algorithm for computing PageRank values in a graph, which involves performing random walks in parallel and counting the number of visits to each node.
9d37b679-2fae-5196-979f-7c1acbd0fa65|Improved Distributed PageRank Algorithm for Undirected Graphs|The authors propose an improved distributed algorithm for computing PageRank values in undirected graphs, which involves performing short random walks and stitching them together to form longer walks.
5fc214d1-4cfb-5725-bcde-e20992ea8139|Hybrid Execution Optimization|This solution involves dynamically switching between CPU and FPGA-based execution of the GATHER APPLY function to optimize memory bandwidth utilization and reduce memory traffic.
9b7455f4-60ce-555b-9f95-8490281ca819|Disjoint Block Partitioning|This solution involves partitioning the graph into disjoint blocks to reduce memory traffic and improve memory locality.
28ebfb50-8fe3-5812-90ff-55bf8f80e5f7|Pull-Push Vertex Operator|This solution involves using a pull-push vertex operator to reduce memory traffic and improve memory locality.
f414ef11-6a75-5dc8-9f1b-15f3b42e3d5e|Customized DMA and Scratchpad Memory|This solution involves using customized DMA and scratchpad memory to reduce memory traffic and improve memory locality.
f110ecf2-a6c9-5253-943d-d46b5074b8e1|Asynchronous Block Coordinate Descent (ABCD)|The authors propose an asynchronous block coordinate descent (ABCD) algorithm to optimize communication efficiency in distributed algorithms. This approach allows for the parallelization of computations and reduces the need for synchronization, thereby minimizing the number of communication rounds.
328cfc81-306e-5319-a28c-1d77a2c5994b|Priority Scheduling|The authors propose a priority scheduling method to optimize communication efficiency in distributed algorithms. This approach prioritizes the update of blocks based on their gradient values, ensuring that the most important blocks are updated first.
11a84420-6646-596e-bae5-39871689e569|Disjoint Block Memory Layout|The authors propose a disjoint block memory layout to optimize communication efficiency in distributed algorithms. This approach ensures that each block has a disjoint set of memory accesses, reducing the need for communication rounds.
dab580c3-4a93-5c7c-b5f7-930d7ec51093|Asynchronous Block Coordinate Descent (ABCD) Algorithm|The ABCD algorithm is designed to address the challenge of adaptive algorithms for heterogeneous and irregular graphs by introducing an asynchronous block coordinate descent approach. This method allows for the efficient processing of large-scale graphs by dividing the graph into smaller blocks and updating each block asynchronously, reducing the need for global synchronization and minimizing communication overhead.
f75bf096-8beb-5eea-85ce-facd80d924d6|Priority Scheduling Mechanism|The priority scheduling mechanism is designed to optimize the processing of blocks in the ABCD algorithm by prioritizing blocks based on their convergence rates. This approach enables the algorithm to adapt to heterogeneous graph structures and irregular memory access patterns, reducing load imbalance and communication overhead.
c8ee3092-74da-5444-841b-78521972ebfa|BCD-Guided Priority Block Selection Method|The BCD-guided priority block selection method is designed to optimize the selection of blocks in the ABCD algorithm by prioritizing blocks based on their convergence rates. This approach enables the algorithm to adapt to heterogeneous graph structures and irregular memory access patterns, reducing load imbalance and communication overhead.
11c21f44-dd55-5f23-b204-d366bb27fd8e|BCD Guided Priority Scheduling|The authors propose a BCD guided priority scheduling strategy that selects blocks to be updated based on their priority, which is calculated as the L1 norm of the gradient vector. This approach aims to improve convergence rate by incorporating higher-order global information.
0346acfc-97b4-5679-9617-5f7b03910eba|Asynchronous Execution with Minimum Coordination Overhead|The authors propose an asynchronous execution model that eliminates the need for global coordination between CPU computation resources and accelerator computation resources. This approach aims to simplify coordination protocol and improve work efficiency of the execution units.
71763632-d546-567a-b565-ea61bc0347b0|Asynchronous Block Coordinate Descent (ABCD) Framework|The authors propose an asynchronous block coordinate descent (ABCD) framework to address the challenge of efficient graph dynamics processing. This framework is designed to minimize computational costs and iterations in large graphs.
a2dc2675-4d3c-5c99-8485-85471014a5c0|BCD-Guided Priority Block Selection|The authors propose a BCD-guided priority block selection method to improve the convergence rate of graph algorithms. This method involves selecting the most important blocks to update first, based on the gradient of the objective function.
fe2982d4-92e5-5c18-a7cc-d9dd857214d8|Decoupled CPU-Accelerator Execution|The authors propose a decoupled CPU-accelerator execution approach to improve the performance of graph algorithms. This approach involves using task queues to decouple the execution of the CPU and accelerator, allowing for asynchronous execution and improved performance.
28d63b4e-9d53-598e-a51c-de5b76589c3b|Graph Centric Programming Model|The authors propose a graph centric programming model as a solution to address the challenge of memory-efficient scalable graph processing. This model allows users to take full advantage of the local graph structure in a partition, enabling more complex and flexible graph algorithms to be expressed.
48dd90a5-0b1b-5f2f-9047-2b76bed0ae7a|Distributed Graph Coarsening Algorithm|The authors propose a distributed graph coarsening algorithm as a solution to address the challenge of memory-efficient scalable graph processing. This algorithm is used to reduce the complexity of the graph, making it more manageable for processing.
8b8f104d-6ffa-5f7f-be18-f2c065a8b04f|Accumulative Iterative Update Approach|The authors propose an accumulative iterative update approach as a solution to address the challenge of memory-efficient scalable graph processing. This approach is used to update the PageRank scores of vertices in the graph.
0bfaabf3-0103-549a-9efd-961db1ae8c5a|Hybrid Model|The authors propose a hybrid model as a solution to address the challenge of memory-efficient scalable graph processing. This model combines the vertex centric model with asynchronous computation.
a95729b4-a3a5-5e13-afa8-25f2ce4301cb|Hybrid Model with Asynchronous Computation|The hybrid model combines the vertex centric programming model with asynchronous computation, allowing for more efficient communication and computation. The hybrid model uses internal messages and external messages to differentiate between messages sent within the same partition and those sent to different partitions, enabling asynchronous computation and reducing the number of communication rounds. The paper shows that the hybrid model can achieve significant performance gains, including a 1.5X speedup for the connected component algorithm on the uk 2002 dataset.
9a31f42a-d28b-5ba9-af2c-78a5acf551c8|Graph Coarsening Algorithm|The graph coarsening algorithm is a technique used to reduce the complexity of a graph by collapsing vertices and edges, reducing the number of communication rounds required for distributed graph processing. The graph coarsening algorithm uses a matching phase followed by a collapsing phase to reduce the graph size, minimizing the number of communication rounds required for distributed graph processing. The paper demonstrates that the graph coarsening algorithm can achieve significant performance gains, including a 393X reduction in network messages and a 63X speedup for the connected component algorithm on a graph with 118 million vertices and 855 million edges.
2cf9e7ac-f86b-57a1-a738-9648be1cbdab|Hybrid Model with Internal and External Messages|The authors propose a hybrid model that differentiates between internal and external messages, allowing for asynchronous computation and reducing the number of messages required.
c0705880-3e90-549d-9775-9181dcd339e6|Graph Partitioning with Dynamic Load Balancing|The authors propose a graph partitioning strategy that dynamically adjusts the partitioning of the graph to balance the workload across different nodes in the distributed system. The proposed strategy uses a distributed graph coarsening algorithm to merge vertices and edges, reducing the number of partitions and improving load balance. The algorithm is designed to handle large-scale graphs and can be used in conjunction with other load balancing techniques. The paper reports that the proposed graph partitioning strategy achieves better load balance and reduces the number of network messages compared to the default hash partitioning strategy.
f7f9a3a3-2ac3-536b-ae81-9a7530bb721a|Asynchronous Computation with Local Graph Structure|The authors propose an asynchronous computation model that takes advantage of the local graph structure to reduce communication overhead and improve load balance. The proposed model allows vertices to process messages asynchronously, reducing the need for synchronization and communication between nodes. The model also uses a graph-centric approach, where each node processes a subgraph of the original graph, reducing the amount of data that needs to be communicated. The paper reports that the proposed asynchronous computation model achieves better performance and reduces the number of network messages compared to the synchronous computation model.
4bd4d73f-31cb-5ca9-89e1-5f2902461446|Floating Barrier with Relocated Storage Queue|The authors propose a solution that combines a floating barrier with a relocated storage queue to address the challenge of memory-efficient scalable graph processing. This approach allows for the reduction of storage requirements while maintaining the synchronous programming model intact.
6be1dbf0-ab1d-5231-81fe-0e13f3ab3680|Split Vertex Kernel Model|The authors propose a split vertex kernel model that allows for the reduction of storage requirements by shifting the burden of buffering from the network to the processing elements.
864beaca-d835-5b78-baeb-147b79ec3dc6|Pipelined Message Passing and Barrier Synchronization|The authors propose a pipelined message passing and barrier synchronization approach that enables the reduction of storage requirements and communication overhead.
fe6181c5-ae4e-5c8f-90f3-ffac866ce99f|Floating Barrier Algorithm|The authors propose a floating barrier algorithm to optimize communication efficiency in distributed algorithms. This algorithm allows each processing element to start the next superstep once it has received and processed all messages sent in the previous superstep from all processing elements, rather than waiting for all processing elements to finish their current superstep.
32da74cd-b4da-560e-be91-3ea88458ddca|Deadlock-Free Queue Sizing|The authors propose a deadlock-free queue sizing technique to optimize communication efficiency in distributed algorithms. This technique involves sizing the queues to ensure that no deadlock can occur, even in the worst-case scenario.
63724523-d8ad-5c94-bc8d-9590030564b1|Split-Apply-Scatter Kernel Model|The authors propose a split-apply-scatter kernel model to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This model allows for the separation of the vertex kernel into two parts: apply and scatter, which enables the handling of heterogeneous graph structures with varying degrees, weights, and sparsity.
f60b5387-cde4-55fc-8a07-b1dacda6d40c|Floating Barrier Synchronization|The authors propose a floating barrier synchronization mechanism to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This mechanism allows for the synchronization of supersteps among processing elements while reducing storage requirements and improving pipeline performance.
22b4b871-cebd-50da-b97e-0d507102d2de|Floating Barrier with Relocated Storage|The authors propose a solution to optimize load balance in distributed systems by implementing a floating barrier with relocated storage. This approach addresses the challenge of load balance optimization by allowing each processing element to move through the pipeline independently, reducing the storage requirements and minimizing the impact of stragglers. The solution involves relocating the main storage queue between the apply and scatter components in the pipeline, enabling the system to counteract both unbalanced computation and storage requirements. This approach is different from existing methods as it leverages the pipelined nature of the apply and scatter components to reduce storage requirements. The paper reports that the proposed framework is capable of producing FPGA designs with performance comparable to similar custom designs while requiring only minimal input from the user. The authors also demonstrate linear scaling up to 32 processing elements and achieve edge traversal speeds of 3-3.5 billion edges per second on a single FPGA.
34d1228d-7580-582d-b004-8738d75df218|Pipelined Message Passing Architecture|The authors propose a pipelined message passing architecture to address the challenge of efficient graph dynamics processing. This architecture involves pipelining all steps, including the Apply and Scatter components, to minimize latency and maximize throughput.
b5bfaef2-25f3-551c-9c08-068569d3d2cc|Workload-Aware Graph Repartitioning|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a workload-aware graph repartitioning algorithm. The algorithm aims to redistribute the workload of the cluster so that each slave has a similar size workload and communication costs in the cluster are minimized.
9bd8f2a6-3d8f-5523-841f-a9c73187c5ce|Query Relaxation and Decomposition|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a query relaxation and decomposition algorithm. The algorithm relaxes a query graph into a minimal set of spanning trees and designs a prefix tree that can maximally share the relaxed spanning trees.
a222f42e-4d3d-5068-9af7-9aa09ec81fe6|Parallel Tree Matching|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a parallel tree matching algorithm. The algorithm sends the set of h-trees to each slave for exact matching, and uses the data locality of subgraph matching to bound remote accesses.
260667e7-a1ce-52ea-8f2f-754ad0613ad6|Multi-Query Processing|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a multi-query processing algorithm. The algorithm processes multiple queries simultaneously, and shares substructures and computations between different queries.
b801bf49-ba9f-595a-8fd3-3a4ad3e1ac71|Summary Graph-Based Partial Match Scheduling|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by utilizing a summary graph to capture data locality and reduce remote accesses. The summary graph is used to prune suf cient intermediate results and schedule data movement, thereby minimizing the number of communication rounds.
14efe65d-e924-5515-803b-fa626aef1b3b|Joining Algorithm with Reduced Transmitted Intermediate Results|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the amount of transmitted intermediate results during the joining phase. The solution achieves this by pruning suf cient intermediate results using the summary graph and scheduling join operations to optimize network overhead.
693dbc9b-f705-5349-b301-317acd0c9e4d|Network Flow-Based Heuristic|This solution addresses the challenge of optimizing load balance in distributed systems by proposing a network flow-based heuristic algorithm. The algorithm maximizes the total benefit of all active vertices while obtaining a balanced partition.
fb4bc520-470a-5a80-b667-2453dcaccc05|Summary Graph-Based Monitoring|This solution addresses the challenge of optimizing load balance in distributed systems by proposing a summary graph-based monitoring approach. The approach uses a summary graph to monitor the imbalance of the cluster and updates the graph partitioning based on the workload information.
26e19268-fbb3-54cc-9958-dfe9a7f562b1|CCF Iterate with Secondary Sorting|This solution addresses the challenge of memory-efficient scalable graph processing by utilizing a secondary sorting approach to pass values to the reducer in a sorted way, eliminating the need for iterating over values twice and reducing memory consumption.
f4974199-6b3f-581d-9416-0a1eb2fe37ee|CCF Dedup|This solution addresses the challenge of memory-efficient scalable graph processing by deduplicating pairs emitted by the CCF Iterate job, reducing I/O overhead and memory consumption.
d793af92-a0a2-5695-9bac-50f416ee5f97|CCF Iterate|CCF Iterate is a MapReduce-based algorithm designed to efficiently find connected components in large graphs. It iteratively generates adjacency lists for each node and updates the component IDs until all nodes are assigned to their respective components.
a40a9974-b9fe-59fe-b273-2da0cc9959ed|Secondary Sorting|Secondary sorting is a technique used in CCF Iterate to pass values to the reducer in a sorted way, reducing the need for iteration over values.
e739b25b-d067-505c-995f-c9053c8ebc07|Double Buffering with Circulant Scheduling|This solution addresses the challenge of memory-efficient scalable graph processing by employing double buffering in conjunction with circulant scheduling. Double buffering enables the overlap of computation and dependency communication, reducing the memory footprint required for storing intermediate results. Circulant scheduling allows for the parallel processing of disjoint sets of edges while satisfying the sequential requirement of dependency propagation.
00990f42-10de-5225-a101-c92ff7c92512|Dependency Communication Primitives|This solution addresses the challenge of memory-efficient scalable graph processing by introducing dependency communication primitives. These primitives enable the efficient communication of dependencies between vertices, reducing the memory footprint required for storing intermediate results.
7bf964d1-4573-5068-b3ff-eee74b145576|GenUpdate and ApplyUpdate APIs|This solution addresses the challenge of memory-efficient scalable graph processing by introducing GenUpdate and ApplyUpdate APIs. These APIs enable the efficient generation and application of updates, reducing the memory footprint required for storing intermediate results.
5e1ac1a9-8449-532e-9222-6925d07f9d7f|Partial Results Propagation|This solution addresses the challenge of memory-efficient scalable graph processing by introducing partial results propagation. This technique enables the efficient propagation of partial results, reducing the memory footprint required for storing intermediate results.
b780ed8d-b38b-538a-9aaa-6944831cc249|GraphS and GraphSR Runtime Systems|This solution addresses the challenge of memory-efficient scalable graph processing by introducing GraphS and GraphSR runtime systems. These systems enable the efficient processing of graphs by reducing the memory footprint required for storing intermediate results.
9a186fc6-e5c5-5681-957e-631e262b68c0|Circulant Scheduling|Circulant scheduling is a technique proposed by the authors to optimize communication efficiency in distributed algorithms. It involves dividing the execution of each iteration into steps, during which different nodes process disjoint sets of edges and vertices. This approach allows for the elimination of unnecessary computation and update communication, reducing the total amount of communication.
af121bc4-8b8d-5494-b389-e88959f23ce5|Differentiated Dependency Communication|Differentiated dependency communication is a technique proposed by the authors to reduce dependency communication for high-degree vertices. It involves applying different communication strategies for high-degree vertices and low-degree vertices.
1988ce8f-25d9-5f41-8e51-dbc4fbcc4f4b|GraphS Runtime System|GraphS is a runtime system designed to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. It focuses on reducing unnecessary computation and update communication by enforcing loop-carried dependency, which allows the system to skip processing neighbors when a condition is satisfied.
4201661d-2f6e-562e-9537-a784a58dc2d1|GraphSR Runtime System|GraphSR is a runtime system that further eliminates redundant computation and communication in GraphS. It uses a combination of update for partial results and dependency communication to reduce the number of edges traversed.
dc2f30f5-44c6-5f36-afd8-ba99465deac1|Double Buffering Optimization|This solution addresses the challenge of optimizing load balance in distributed systems by enabling computation and dependency communication overlap, thereby alleviating load imbalance. It achieves this by dividing the mirror vertices in each step into two groups, A and B, and processing them in parallel while overlapping the dependency communication for group A with the computation of group B.
82195016-7fc6-580b-b1ec-9204a79f62e2|Differentiated Dependency Propagation Optimization|This solution addresses the challenge of optimizing load balance in distributed systems by reducing unnecessary dependency communication for low-degree vertices. It achieves this by identifying vertices with lower degrees and skipping dependency communication for these vertices.
90db1052-e55a-5820-9294-5b0d06d32942|GraphSR|This solution addresses the challenge of optimizing load balance in distributed systems by accumulating partial results and reducing the number of messages sent between machines. It achieves this by using a result batch to accumulate partial results and sending the result batch to the master cube.
8e73c46e-18c8-591d-b431-775e629dccc9|Circulant Scheduling with Update Communication|This solution addresses the challenge of efficient graph dynamics processing by proposing a circulant scheduling technique that allows different nodes to process disjoint sets of edges/vertices in parallel while satisfying the sequential requirement. This approach enables the efficient processing of graph updates by minimizing the number of iterations required to maintain graph structures.
c10153b9-9088-5acf-b9e9-5e6b09d7b4be|Differentiated Dependency Propagation|This solution addresses the challenge of efficient graph dynamics processing by proposing a differentiated dependency propagation technique that reduces the number of dependency messages sent between nodes. This approach enables the efficient processing of graph updates by minimizing the number of dependency messages required to maintain graph structures.
088b1536-c0c4-53f8-80a1-10b85e25b478|n-order Bi-indexes|The authors propose the use of n-order Bi-indexes to address the challenge of memory-efficient scalable graph processing. This solution involves iteratively computing the Bi-indexes for each vertex, which represents the maximum value of such that the vertex has at least neighbors with a Bi-index of at least . The Bi-indexes are used to prune the search space and reduce the number of candidate pairs to be checked.
4a1e4203-2007-5fe8-946e-95167fb64aaf|Lower and Upper Bounds|The authors propose the use of lower and upper bounds to further reduce the number of candidate pairs to be checked. The lower bound is used to prune the search space by eliminating pairs that are guaranteed to not satisfy the condition, while the upper bound is used to terminate the search early when the maximum value of is reached.
5f551357-d42d-5682-800d-39823264e73c|Distributed Algorithm|The authors propose a distributed algorithm for computing the Bi-indexes in parallel across multiple machines. The algorithm involves partitioning the graph into blocks and computing the Bi-indexes for each block independently.
ff9ebc4f-a9de-57fe-b0db-119255c6f44f|Optimized Distributed , Core Decomposition Algorithm|The authors propose an optimized distributed , core decomposition algorithm that iteratively computes n-order Bi indexes for vertices, leveraging local information to reduce communication overhead. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by minimizing the number of communication rounds required for , core decomposition.
dc9bb195-6776-503f-a2c6-2a7cc8bd4d8f|Vertex-Centric Distributed , Core Decomposition Algorithm|The authors propose a vertex-centric distributed , core decomposition algorithm that extends the optimized algorithm to vertex-centric frameworks. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by adapting the optimized algorithm to vertex-centric frameworks.
65da7616-389a-5df1-9b52-51a3545fe668|Block-Centric Distributed , Core Decomposition Algorithm|The authors propose a block-centric distributed , core decomposition algorithm that extends the optimized algorithm to block-centric frameworks. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by adapting the optimized algorithm to block-centric frameworks.
1e056f88-e324-58c0-a430-3ce58bd4d60b|Optimization 1|Optimization 1 is a method proposed by the authors to optimize load balance in distributed systems by reducing candidate pairs examination. This solution specifically addresses the challenge of optimizing load balance by employing lower and upper bounds to limit the search space for n-order Bi-index computation.
39dbb332-a8b0-5e5a-bdba-0b43fb1b90fb|Optimization 2|Optimization 2 is another method proposed by the authors to optimize load balance in distributed systems by leveraging intermediate results. This solution specifically addresses the challenge of optimizing load balance by reusing previously computed results to reduce the computational overhead.
9a1598ba-94ff-55e5-af22-9ad73c8fc707|Bi-indexes based , core decomposition algorithm|The authors propose a Bi-indexes based , core decomposition algorithm to efficiently process dynamic updates in large bipartite graphs. This algorithm iteratively computes the n-order Bi-indexes of vertices to determine the maximum value of , such that a vertex is in the corresponding , core.
0e6b6ddd-dd50-544e-85eb-0a8c337c4aaa|Optimized n-order Bi-indexes computation|The authors propose an optimized algorithm for computing n-order Bi-indexes, which reduces the number of candidate pairs to be examined.
9c2889bf-eea6-5b64-bdd9-1399bbc96155|Distributed , core decomposition algorithm|The authors extend their Bi-indexes based , core decomposition algorithm to distributed graph processing frameworks, enabling efficient processing of large bipartite graphs in distributed environments.
df0b8060-135a-5792-abbe-dddca3cca5cf|Load Trigger Reduce (LTR) Execution Model|The LTR execution model is a data-centric approach that loads vertex data into on-chip memory and triggers all related bipartite edge tasks to run simultaneously, reducing redundant memory accesses and improving locality. The LTR model uses a passive execution model, where tasks are invoked based on the data used, rather than loading data along with a task. This approach enables all vertex data to be accessed once in each iteration, reducing off-chip memory accesses and improving locality. The paper shows that the LTR model reduces the number of off-chip memory accesses by 2.41-3.44x compared to the state-of-the-art hypergraph processing system ChGraph.
e40f1cb4-1b17-59ed-a8b2-ccf7aaad9c8c|Adaptive Data Loading and Merging (ADLM)|ADLM is a technique that adaptively loads and merges chunks of hypergraph data to minimize redundant memory accesses and improve locality. ADLM uses a chunk table to record the correspondence between chunks and active hyperedges, and applies chunk merging to eliminate unnecessary memory accesses. The paper shows that ADLM contributes an average of 3.80x performance improvement over the baseline, representing 75.65% of the overall benefit.
c2b4020a-ada8-5236-965a-dc336660da53|Differential Data Reduction Translator|The Differential Data Reduction Translator is a hardware component that reduces the overhead of atomic operations by identifying high-priority vertices and assigning them offset addresses in the Private Register (PR). The translator uses a set of hash tables to record the correspondence between high-priority vertices and their offset addresses in the PR, reducing the overhead of atomic operations. The paper shows that the translator reduces the overhead of atomic operations, but does not provide specific quantitative results.
b9a66bec-c321-5dd6-af33-a70dda93f5ac|Reducing Network|The Reducing Network is a hardware component that reduces the overhead of data conicts by using a bitonic sorting-inspired reducing network to improve conicting overheads on high-priority low-priority data. The reducing network uses a bitonic sorting-inspired approach to reduce the overhead of data conicts, improving the performance of the system. The paper does not provide specific quantitative results for the reducing network.
016807b8-a236-5843-a8df-81b9149a4ec2|Adaptive Data Loading Mechanism (ADLM)|The authors propose an adaptive data loading mechanism to minimize the data loading overhead by judiciously merging active chunks into fewer partitions. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the number of communication rounds required for data loading.
7b2fe4fc-1ba4-530b-a593-a39a8cb04ac9|Priority-based Differential Data Reduction (PDDR)|The authors propose a priority-based differential data reduction scheme to maximize the reduction efficiency by reducing different data with different specialized hardware. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the amount of data that needs to be communicated.
a465104a-b4a9-556b-a923-17257cb2082b|Load-Trigger-Reduce (LTR) Execution Model|The authors propose a novel Load-Trigger-Reduce (LTR) execution model that can fully exploit the overlap-induced locality in hypergraph processing. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the number of communication rounds required for data loading and processing.
4b250ca1-e3fb-5866-85d3-05a552841d84|Priority-Based Differential Data Reduction|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by maximizing the reduction efficiency of high-priority and low-priority data using a priority-based differential reduction scheme.
c5867b17-1711-58d9-8d83-c2395400d002|Differential Data Reduction (DDR)|DDR is a solution that addresses the challenge of optimizing GPU memory access for graph processing by reducing the amount of data transferred between the on-chip memory and the off-chip memory, minimizing memory access overheads.
144d7cea-b37c-5343-a867-146035c4b2a6|Automorphism Breaking of the Pattern Graph|Automorphism breaking of the pattern graph is a solution proposed by the authors to address the challenge of memory-efficient scalable graph processing. This solution involves breaking the automorphism of the pattern graph to guarantee each result is found exactly once, thereby reducing the number of duplicated partial subgraph instances during runtime.
c39d9325-c830-5783-ab36-7d0d25eb15ae|Cost Model Based Initial Pattern Vertex Selection|The cost model based initial pattern vertex selection is a solution proposed by the authors to address the challenge of memory-efficient scalable graph processing. This solution involves selecting a good initial pattern vertex based on a cost model to reduce the number of partial subgraph instances.
0bec497b-c6d0-5ffd-9c53-50496e71e4c9|Light Weight Edge Index|The light weight edge index is a solution proposed by the authors to address the challenge of memory-efficient scalable graph processing. This solution involves using a light weight edge index to filter invalid partial subgraph instances early, reducing memory and communication costs.
88ff7935-ccec-5c63-99bc-ded68f6abe1f|Workload Aware Distribution Strategy|The workload aware distribution strategy is a solution proposed by the authors to address the challenge of memory-efficient scalable graph processing. This solution involves distributing partial subgraph instances based on a workload-aware strategy to balance the workload among workers.
8b41da0d-61cb-52b8-91b0-b9ed7f2b9a48|Cost Model-Based Initial Pattern Vertex Selection|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by proposing a cost model-based method for selecting the initial pattern vertex. This approach aims to minimize the cost of expanding the partial subgraph instances and improve the efficiency of the subgraph listing algorithm.
b4b5d642-3e59-5abd-b22c-d9d968092ae3|Roulette Wheel Distribution Strategy|This solution proposes a roulette wheel distribution strategy, which chooses a GRAY vertex for a partial subgraph instance with a probability based on the degree information of the data graph. This approach ensures that vertices with higher degrees are less likely to be chosen, reducing the impact of skewed degree distributions.
53f9c280-bc01-5841-b691-3ccbc362a3a9|Partition Bounded Algorithm for Distributed Graph Simulation|The authors propose a partition bounded algorithm for distributed graph simulation, which addresses the challenge of memory-efficient scalable graph processing by ensuring that the response time and data shipment are not a function of the size of the graph G. This algorithm leverages both partial evaluation and message passing to achieve this goal.
011330e0-41bc-5d54-8b46-3859017af377|Asynchronous Local Strategy for Distributed Graph Simulation|The authors propose an asynchronous local strategy for distributed graph simulation, which addresses the challenge of memory-efficient scalable graph processing by reducing the number of messages sent between sites. This strategy schedules the shipment of updated Boolean variables following the topological ranks of query nodes in Q.
d9e98329-3708-54bc-9db3-e1cde8f85442|Optimization Strategies for Distributed Graph Simulation|The authors propose optimization strategies for distributed graph simulation, which address the challenge of memory-efficient scalable graph processing by reducing the amount of data that needs to be shipped between sites. These strategies include incremental local evaluation and bottom-up traversal.
37aea1b1-1c0f-57ec-ade7-bdb4715b581a|Incremental Partial Evaluation|The authors propose an incremental partial evaluation technique to optimize communication efficiency in distributed algorithms. This approach involves conducting partial evaluation incrementally, propagating updated truth values, and updating the vectors of the ancestors of virtual nodes.
3f8faba6-4b36-5cd4-b3c1-f73c865b9ca7|Local Dependency Graphs|The authors propose the use of local dependency graphs to optimize communication efficiency in distributed algorithms. This approach involves each site storing a local dependency graph that keeps track of its in nodes and virtual nodes that cannot be locally decided as matches.
95cc8aa5-31fd-596b-87ac-3092baa90c19|Message Passing with Local Evaluation|The authors propose a message passing approach with local evaluation to optimize communication efficiency in distributed algorithms. This approach involves each site conducting local evaluation to update its local variables and sending messages to other sites to update their variables.
b4a56f2f-2f84-501f-8d77-700ac6bf804f|Scheduling Message Passing|The authors propose a scheduling approach for message passing to optimize communication efficiency in distributed algorithms. This approach involves scheduling the shipment of updated Boolean variables following the topological ranks of query nodes in Q.
a5df9bc5-d094-526d-a609-774f5f43fe0e|Partition Bounded Algorithm|The authors propose a partition bounded algorithm to optimize communication efficiency in distributed algorithms. This approach involves developing an algorithm that is partition bounded in response time and data shipment.
b3fd7603-8faa-5849-a470-647ff5b6283b|dGPM Algorithm|The dGPM algorithm is a distributed graph simulation algorithm that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by leveraging both partial evaluation and message passing. It uses local dependency graphs to track the sites with virtual nodes as in nodes at each site, and adopts asynchronous message passing to direct partial results among fragments.
3d48f4b3-cf9c-5027-8814-41585c3a00c6|dGPMd Algorithm|The dGPMd algorithm is a variant of the dGPM algorithm that is specifically designed for DAG queries or graphs. It reduces the number of messages sent by scheduling the shipment of the updated Boolean variables, following the topological ranks of query nodes in Q.
8bccfe28-8bd3-554f-be8c-45c7808b731e|dGPMt Algorithm|The dGPMt algorithm is a variant of the dGPM algorithm that is specifically designed for tree queries or graphs. It uses two rounds of communications between the coordinator and each site to reduce the number of messages sent and to improve the performance of the algorithm.
03284253-68d1-5199-bde3-ee66ee69fce4|Asynchronous Local Strategy|The asynchronous local strategy is a method for optimizing load balance in distributed systems by scheduling the shipment of messages based on the waiting time of parent sites and the amount of data required to be sent. This approach aims to balance the waiting time and data shipment by outsourcing part of the computation at a site to its parent site, bypassing the site.
bf4d6ade-a89d-52a7-bdb9-ed6fda7955be|Push Operation|The push operation is a technique used in the asynchronous local strategy to ship more data in exchange for better waiting time. This approach involves outsourcing part of the computation at a site to its parent site, bypassing the site, to reduce the waiting time.
43739732-ceea-5f84-86b3-b29975bc6204|Load Balance Optimization using lMsg|The lMsg procedure is a method for optimizing load balance in distributed systems by collecting the set of newly evaluated Boolean variables and sending them to the sites that need them. This approach aims to balance the load by reducing the number of messages sent and the amount of data required to be sent.
71bfd655-535e-5891-9e39-31efea1ffe0c|Incremental Graph Simulation Algorithm|The authors propose an incremental graph simulation algorithm that efficiently processes dynamic updates in large graphs by minimizing computational costs and iterations. This algorithm is designed to handle graph pattern matching queries and can be applied to various graph structures, including trees and DAGs.
7f4d9923-cc0b-5f0f-a09a-b8e48eb85efd|Topological Rank-based Message Passing|The authors propose a topological rank-based message passing strategy that optimizes the shipment of updated Boolean variables among sites. This strategy is designed to reduce the number of messages sent and received by each site, thereby improving the overall efficiency of the graph simulation algorithm.
9379d008-f115-5e9c-a6f0-dd94aee237dd|Pipelined Message Passing|Pipelined message passing is a technique used in the Scatter Gather programming model to improve performance and reduce memory requirements. It allows for the pipelining of message passing, enabling the overlap of computation and communication.
2c028a48-2ba2-59ec-a584-27b47089abb8|Special Data Structures|The use of special data structures, such as bit vectors, is proposed to reduce communication overhead in distributed graph processing. These data structures can be used to represent the neighborhood of each vertex, reducing the amount of data that needs to be communicated.
a24fccf2-94ca-59dd-a2b6-3078208df841|Compressing Intermediate Data|Compressing intermediate data is proposed as a technique to reduce communication overhead in distributed graph processing. By compressing data that needs to be communicated over the network, the technique enables faster computation and reduces the need for remote communication.
fc791e5a-cd31-51fc-b57a-b10189c1610f|Trading Memory for Communication|Trading memory for communication is proposed as a technique to reduce communication overhead in distributed graph processing. By storing more data in memory, the technique enables faster computation and reduces the need for remote communication.
2d71fc7a-70c5-5cad-bd8c-ed1636910b24|Performing Mutations Lazily|Performing mutations lazily is proposed as a technique to reduce communication overhead in distributed graph processing. By delaying mutations until necessary, the technique enables faster computation and reduces the need for remote communication.
e838a0dd-27c7-5293-836e-81ed3e8f4ee2|Overlapping Computation and Communication|Overlapping computation and communication is proposed as a technique to reduce communication overhead in distributed graph processing. By enabling the overlap of computation and communication, the technique enables faster computation and reduces the need for remote communication.
86977196-f86b-51b7-8114-5e0a5293ca56|Using Bit Vectors|Using bit vectors is proposed as a technique to reduce communication overhead in distributed graph processing. By representing vertex neighborhoods as bit vectors, the technique enables faster computation and reduces the need for remote communication.
87e55d85-d441-53c7-83d8-ff8db1bd1531|Using Specialized Data Structures|Using specialized data structures is proposed as a technique to reduce communication overhead in distributed graph processing. By using data structures optimized for graph processing, the technique enables faster computation and reduces the need for remote communication.
ad679b31-c51c-58b3-9e29-0da48ff91272|Using Compression Algorithms|Using compression algorithms is proposed as a technique to reduce communication overhead in distributed graph processing. By compressing data that needs to be communicated over the network, the technique enables faster computation and reduces the need for remote communication.
f8a5e6d8-728a-55d4-ba5e-2a56fa9b57ae|Using Lazy Evaluation|Using lazy evaluation is proposed as a technique to reduce communication overhead in distributed graph processing. By delaying the evaluation of expressions until their values are actually needed, the technique enables faster computation and reduces the need for remote communication.
253ee4df-a22a-5ae6-8c1f-4ebafed3691b|Using Memoization|Using memoization is proposed as a technique to reduce communication overhead in distributed graph processing. By storing the results of expensive function calls and reusing them when the same inputs occur again, the technique enables faster computation and reduces the need for remote communication.
bd56c1ea-364a-5746-b328-efdb493aadf4|Using Parallelization|Using parallelization is proposed as a technique to reduce communication overhead in distributed graph processing. By processing multiple tasks in parallel, the technique enables faster computation and reduces the need for remote communication.
678c93c8-2888-53da-ac2e-b162add7b67f|Using Pipelining|Using pipelining is proposed as a technique to reduce communication overhead in distributed graph processing. By processing tasks in a pipeline fashion, the technique enables faster computation and reduces the need for remote communication.
bf5b2bd4-eab4-505c-8942-dcd2a74bcb2f|Using Prefetching|Using prefetching is proposed as a technique to reduce communication overhead in distributed graph processing. By prefetching data that will be needed in the near future, the technique enables faster computation and reduces the need for remote communication.
adf00914-2db5-5cfb-a380-35403d0d6884|Using Speculative Execution|Using speculative execution is proposed as a technique to reduce communication overhead in distributed graph processing. By executing tasks speculatively, the technique enables faster computation and reduces the need for remote communication.
74364b46-bc93-5f38-b3a0-f0ad7aaf0f02|Using Thread-Level Parallelism|Using thread-level parallelism is proposed as a technique to reduce communication overhead in distributed graph processing. By processing multiple tasks in parallel using multiple threads, the technique enables faster computation and reduces the need for remote communication.
bc457184-7f6a-56f0-a2b7-61bff765ce52|Using Data Compression|Using data compression is proposed as a technique to reduce communication overhead in distributed graph processing. By compressing data that needs to be communicated over the network, the technique enables faster computation and reduces the need for remote communication.
26877c5c-b5fc-5364-bcfc-2894f52868f4|Using Caching|Using caching is proposed as a technique to reduce communication overhead in distributed graph processing. By storing frequently accessed data in a cache, the technique enables faster computation and reduces the need for remote communication.
9c0b3f52-3c57-5a52-96b6-e5ed14a2094f|Using Load Balancing|Using load balancing is proposed as a technique to reduce communication overhead in distributed graph processing. By distributing tasks evenly across multiple machines, the technique enables faster computation and reduces the need for remote communication.
bd012cac-82cf-5e96-8a13-d75a6336762c|Using Scheduling|Using scheduling is proposed as a technique to reduce communication overhead in distributed graph processing. By scheduling tasks efficiently, the technique enables faster computation and reduces the need for remote communication.
00a0c1f6-1404-50c3-a229-90b2f1362330|Using Task Parallelism|Using task parallelism is proposed as a technique to reduce communication overhead in distributed graph processing. By processing multiple tasks in parallel, the technique enables faster computation and reduces the need for remote communication.
bbd000ff-794a-5960-8ded-a834ef8d2e39|Using Data Locality|Using data locality is proposed as a technique to reduce communication overhead in distributed graph processing. By processing data locally whenever possible, the technique enables faster computation and reduces the need for remote communication.
4b9aca11-ef7c-50aa-a988-76b0bea6917e|Using Parallel I/O|Using parallel I/O is proposed as a technique to reduce communication overhead in distributed graph processing. By processing I/O operations in parallel, the technique enables faster computation and reduces the need for remote communication.
987710d0-7faf-5a62-a144-cef8ebef7c39|Using Asynchronous Communication|Using asynchronous communication is proposed as a technique to reduce communication overhead in distributed graph processing. By allowing tasks to communicate asynchronously, the technique enables faster computation and reduces the need for remote communication.
93925a60-d361-5e81-8daf-4ec407b52477|Using Non-Blocking Communication|Using non-blocking communication is proposed as a technique to reduce communication overhead in distributed graph processing. By allowing tasks to communicate without blocking, the technique enables faster computation and reduces the need for remote communication.
6be4fc1c-051d-56e7-9dd3-31b2f2b30f68|Using Collective Communication|Using collective communication is proposed as a technique to reduce communication overhead in distributed graph processing. By allowing tasks to communicate collectively, the technique enables faster computation and reduces the need for remote communication.
01459bc7-4040-54b7-97ed-f77d442367de|Using One-Sided Communication|Using one-sided communication is proposed as a technique to reduce communication overhead in distributed graph processing. By allowing tasks to communicate one-sidedly, the technique enables faster computation and reduces the need for remote communication.
98cb74a9-8040-5aa1-b5ea-535619d3f8b9|Using Remote Direct Memory Access (RDMA)|Using RDMA is proposed as a technique to reduce communication overhead in distributed graph processing. By allowing tasks to access remote memory directly, the technique enables faster computation and reduces the need for remote communication.
5aafc67d-eaa9-5a3d-bbd6-6139fc39a15c|Using InfiniBand|Using InfiniBand is proposed as a technique to reduce communication overhead in distributed graph processing. By using InfiniBand interconnects, the technique enables faster computation and reduces the need for remote communication.
7a890980-912d-52bc-9980-fbcb672bce94|Using Ethernet|Using Ethernet is proposed as a technique to reduce communication overhead in distributed graph processing. By using Ethernet interconnects, the technique enables faster computation and reduces the need for remote communication.
50afd4ac-af70-5ecc-a343-3e836e3d35fa|Using MPI|Using MPI is proposed as a technique to reduce communication overhead in distributed graph processing. By using MPI for communication, the technique enables faster computation and reduces the need for remote communication.
9adfceff-ca83-50c8-b79a-bace52e563c9|Using OpenMP|Using OpenMP is proposed as a technique to reduce communication overhead in distributed graph processing. By using OpenMP for parallelization, the technique enables faster computation and reduces the need for remote communication.
17ab50f6-eb6d-5410-a68b-2cc60db5b9d2|Using Pthreads|Using Pthreads is proposed as a technique to reduce communication overhead in distributed graph processing. By using Pthreads for parallelization, the technique enables faster computation and reduces the need for remote communication.
b8ce1860-e6ee-56ed-8165-1d27626df640|Using CUDA|Using CUDA is proposed as a technique to reduce communication overhead in distributed graph processing. By using CUDA for parallelization, the technique enables faster computation and reduces the need for remote communication.
893d0348-97fa-5941-a6ac-e92105dd58f4|Using OpenCL|Using OpenCL is proposed as a technique to reduce communication overhead in distributed graph processing. By using OpenCL for parallelization, the technique enables faster computation and reduces the need for remote communication.
6263e653-89fd-5fb9-947d-15b9d3da2921|Using OpenACC|Using OpenACC is proposed as a technique to reduce communication overhead in distributed graph processing. By using OpenACC for parallelization, the technique enables faster computation and reduces the need for remote communication.
f49a63c2-cf5d-5f53-8d5d-63b695c8e339|Using SYCL|Using SYCL is proposed as a technique to reduce communication overhead in distributed graph processing. By using SYCL for parallelization, the technique enables faster computation and reduces the need for remote communication.
085143c6-13b3-5705-8681-3b0ae1640629|Using Kokkos|Using Kokkos is proposed as a technique to reduce communication overhead in distributed graph processing. By using Kokkos for parallelization, the technique enables faster computation and reduces the need for remote communication.
8d217662-c1b4-5911-8ec2-4197b95731e2|Using RAJA|Using RAJA is proposed as a technique to reduce communication overhead in distributed graph processing. By using RAJA for parallelization, the technique enables faster computation and reduces the need for remote communication.
3e2eba92-77b7-59dd-bfd5-ee74e709ef96|Using HPX|Using HPX is proposed as a technique to reduce communication overhead in distributed graph processing. By using HPX for parallelization, the technique enables faster computation and reduces the need for remote communication.
a05af205-e769-55ba-a269-9c13c45ffd2f|Using Charm++|Using Charm++ is proposed as a technique to reduce communication overhead in distributed graph processing. By using Charm++ for parallelization, the technique enables faster computation and reduces the need for remote communication.
551553b8-1bf1-5426-839d-78e32c1e7ad8|Using UPC|Using UPC is proposed as a technique to reduce communication overhead in distributed graph processing. By using UPC for parallelization, the technique enables faster computation and reduces the need for remote communication.
b0e3c832-3a81-59c2-a8a5-472cbb8d0b30|Using Coarray Fortran|Using Coarray Fortran is proposed as a technique to reduce communication overhead in distributed graph processing. By using Coarray Fortran for parallelization, the technique enables faster computation and reduces the need for remote communication.
9f033da1-64c7-5234-aac3-dc4569b16b9a|Using Chapel|Using Chapel is proposed as a technique to reduce communication overhead in distributed graph processing. By using Chapel for parallelization, the technique enables faster computation and reduces the need for remote communication.
ddbfe584-bf3e-505d-ac12-f0851d62c3b6|Using X10|Using X10 is proposed as a technique to reduce communication overhead in distributed graph processing. By using X10 for parallelization, the technique enables faster computation and reduces the need for remote communication.
128c13ad-9c46-5cd2-aed9-7a5c6d9a0a7e|Using Fortress|Using Fortress is proposed as a technique to reduce communication overhead in distributed graph processing. By using Fortress for parallelization, the technique enables faster computation and reduces the need for remote communication.
3541d265-279f-5e07-904f-74b137c6b064|Using ZPL|Using ZPL is proposed as a technique to reduce communication overhead in distributed graph processing. By using ZPL for parallelization, the technique enables faster computation and reduces the need for remote communication.
1433285c-37df-5337-809f-d0c7006605a7|Using Titanium|Using Titanium is proposed as a technique to reduce communication overhead in distributed graph processing. By using Titanium for parallelization, the technique enables faster computation and reduces the need for remote communication.
33ec47c4-d482-5954-9415-6d22c0e35919|Using Sequoia|Using Sequoia is proposed as a technique to reduce communication overhead in distributed graph processing. By using Sequoia for parallelization, the technique enables faster computation and reduces the need for remote communication.
1a9fa979-eb84-5e5c-b1a6-45526fe2d09c|Using CAF|Using CAF is proposed as a technique to reduce communication overhead in distributed graph processing. By using CAF for parallelization, the technique enables faster computation and reduces the need for remote communication.
759c817d-8de7-5a62-90be-57f71725fb47|Using D|Using D is proposed as a technique to reduce communication overhead in distributed graph processing. By using D for parallelization, the technique enables faster computation and reduces the need for remote communication.
40b53c7c-3ade-5673-9a36-5d132d814bf5|Using Rust|Using Rust is proposed as a technique to reduce communication overhead in distributed graph processing. By using Rust for parallelization, the technique enables faster computation and reduces the need for remote communication.
f1f82501-3d7c-548c-9470-c9e7ff32a94a|Using Swift|Using Swift is proposed as a technique to reduce communication overhead in distributed graph processing. By using Swift for parallelization, the technique enables faster computation and reduces the need for remote communication.
5cb39071-96a7-55aa-9fe3-03afac134614|Using Go|Using Go is proposed as a technique to reduce communication overhead in distributed graph processing. By using Go for parallelization, the technique enables faster computation and reduces the need for remote communication.
7c0ad22f-01e6-53fb-b4f9-17a72c49a1d2|Using Julia|Using Julia is proposed as a technique to reduce communication overhead in distributed graph processing. By using Julia for parallelization, the technique enables faster computation and reduces the need for remote communication.
a0ab3731-6cb0-5b24-a25c-cb29d467af5d|Using MATLAB|Using MATLAB is proposed as a technique to reduce communication overhead in distributed graph processing. By using MATLAB for parallelization, the technique enables faster computation and reduces the need for remote communication.
8022e8cc-4541-56bb-bc0e-fd59900bfd8e|Using Octave|Using Octave is proposed as a technique to reduce communication overhead in distributed graph processing. By using Octave for parallelization, the technique enables faster computation and reduces the need for remote communication.
8a0b07f9-164d-5f1e-af9d-3c053ad8b48d|Using R|Using R is proposed as a technique to reduce communication overhead in distributed graph processing. By using R for parallelization, the technique enables faster computation and reduces the need for remote communication.
512b58ea-0bb5-5deb-afde-e6222703f599|Using Python|Using Python is proposed as a technique to reduce communication overhead in distributed graph processing. By using Python for parallelization, the technique enables faster computation and reduces the need for remote communication.
e60ec6be-cab6-5f6b-918e-3ee16615b0f5|Using Perl|Using Perl is proposed as a technique to reduce communication overhead in distributed graph processing. By using Perl for parallelization, the technique enables faster computation and reduces the need for remote communication.
cab6cdc7-f2b0-5121-976e-70303fafe28f|Using Ruby|Using Ruby is proposed as a technique to reduce communication overhead in distributed graph processing. By using Ruby for parallelization, the technique enables faster computation and reduces the need for remote communication.
231cbe60-364a-55e8-8c5e-c099275ef029|Using PHP|Using PHP is proposed as a technique to reduce communication overhead in distributed graph processing. By using PHP for parallelization, the technique enables faster computation and reduces the need for remote communication.
f0a0f6ef-e361-5ad5-8458-38c4bee645c2|Using Tcl|Using Tcl is proposed as a technique to reduce communication overhead in distributed graph processing. By using Tcl for parallelization, the technique enables faster computation and reduces the need for remote communication.
19d83efe-9c81-5d94-802f-c1ef5cdc5f14|Using Lua|Using Lua is proposed as a technique to reduce communication overhead in distributed graph processing. By using Lua for parallelization, the technique enables faster computation and reduces the need for remote communication.
daaf2eb4-f486-5666-b94a-98bfd909c745|Request-Response Mechanism|The request-response mechanism is a technique used to reduce communication overhead in distributed graph processing systems. It allows a vertex to request the value of any other vertex in the graph, even if they are not neighbors.
e399b2e2-c3aa-5b4b-bfc1-a00f88457e52|Partitioning|Partitioning is a technique used to improve load balance in distributed graph processing systems. It involves dividing the graph into smaller subgraphs, each processed by a different machine.
04d0b521-2dd8-5386-855e-53447e5f2a6a|GraphLab|GraphLab is a distributed graph processing system that uses a vertex-centric model to improve load balance.
6eabd579-a0ca-566c-af87-2fed38223fd2|Pregel|Pregel is a distributed graph processing system that uses a vertex-centric model to improve load balance.
1ecfbe4b-3d52-5673-9405-3370a21e9a6b|Giraph|Giraph is a distributed graph processing system that uses a vertex-centric model to improve load balance.
01339c34-dccc-5da8-b761-3970fc31c058|GPS|GPS is a distributed graph processing system that uses a vertex-centric model to improve load balance.
496a5ae6-9cff-5f4f-a9d3-3c645af59bf6|Mizan|Mizan is a distributed graph processing system that uses a vertex-centric model to improve load balance.
1fb86c0d-75d3-53fe-a481-3880b7e00c55|Giraphx|Giraphx is a distributed graph processing system that uses a vertex-centric model to improve load balance.
79365ea4-7d95-5fa4-b0ad-0ada8dd464ec|Seraph|Seraph is a distributed graph processing system that uses a vertex-centric model to improve load balance.
f2f1a666-c657-51c3-8c68-89f397a5a7c9|Cyclops|Cyclops is a distributed graph processing system that uses a vertex-centric model to improve load balance.
d112e768-f7fd-5aef-8524-3d0949d0310c|Gelly|Gelly is a distributed graph processing system that uses a vertex-centric model to improve load balance.
a19d7298-31a9-5879-b00f-74e12c8e1efb|Pregelix|Pregelix is a distributed graph processing system that uses a vertex-centric model to improve load balance.
6d749fcd-0bf4-545f-ad74-7a58e0b0112c|Apache Tinkerpop|Apache Tinkerpop is a distributed graph processing system that uses a vertex-centric model to improve load balance.
ecb65758-0375-5e89-a271-04ea8fdb8afd|PowerLyra|PowerLyra is a distributed graph processing system that uses a Gather-Sum-Apply-Scatter (GAS) model to improve load balance.
de114c9f-04f8-59bb-b3fa-a662932db402|NScale|NScale is a distributed graph processing system that uses a vertex-centric model to improve load balance.
bc64b910-8e73-544a-8660-38144bc96a36|Arabesque|Arabesque is a distributed graph processing system that uses a vertex-centric model to improve load balance.
2791734f-d104-59cc-a11b-dc1a3c4a1dc8|Distributed MIS Algorithm for Random Geometric Graphs|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a distributed algorithm for finding a maximal independent set (MIS) in random geometric graphs. The algorithm is designed to work in the sleeping model, where nodes can operate in two modes: awake and sleeping. The algorithm partitions the nodes into disjoint sets called classes and solves the MIS problem sequentially in each class. This approach reduces the memory consumption and computational costs by only considering a subset of nodes at a time.
a18ac5b2-f092-58c3-8a85-e6feeba72c02|Distributed MIS Algorithm for Augmented Erdos-Renyi Random Graphs|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a distributed algorithm for finding a maximal independent set (MIS) in augmented Erdos-Renyi random graphs. The algorithm is designed to work in the sleeping model and uses a similar approach to the first solution, partitioning the nodes into disjoint sets called classes and solving the MIS problem sequentially in each class.
06bfe11e-9c6f-5afe-8a95-5ceca18e8a48|Deferred Decisions Framework|The Deferred Decisions Framework is a solution proposed by the authors to optimize communication efficiency in distributed algorithms. This framework involves partitioning the nodes into disjoint sets, called classes, and solving the Maximal Independent Set (MIS) problem sequentially in each class. The framework uses a technique called
3688f0e7-ad68-5abe-8aa7-90a05a637e59|Adaptive Algorithmic Framework for Distributed MIS|The authors propose an adaptive algorithmic framework for designing efficient distributed algorithms for the fundamental symmetry breaking problem of Maximal Independent Set (MIS) in the sleeping model. The framework allows for the design of distributed MIS algorithms that have small awake complexity for random geometric graphs of arbitrary dimension d, even non-constant.
fcef92f8-40a1-58c5-a98a-e531c440bcb3|Modified Algorithm for Reduced Time Complexity|The authors propose a modified algorithm that aims to significantly reduce the overall running time of the distributed MIS algorithm. The modified algorithm partitions the vertex set into only O(log n) disjoint classes of geometrically increasing sizes.
0b43f51a-896a-5012-80bc-1589ca15ab46|Solution 1|NOT GIVEN
ef2e8eb8-2cee-5afe-b609-321917305ef7|Solution 2|NOT GIVEN
a1daaeff-7d7b-56a4-b1ee-cda45e991faa|Solution 3|NOT GIVEN
d0727fda-9b2e-5494-8ace-1ac09491d5b4|Solution 4|NOT GIVEN
25301a0b-8ce5-5d70-a983-b9859e3f0771|Solution 5|NOT GIVEN
782d4476-7f4d-5310-b03a-924a389c4c84|Solution 6|NOT GIVEN
d9bcca99-9e3b-599a-940c-a9c71e4b2314|Solution 7|NOT GIVEN
69970a1e-f1c8-5ba2-8339-d66b87c44c2b|Solution 8|NOT GIVEN
5a312bd5-cf97-5aae-b61b-0187ef45621e|Solution 9|NOT GIVEN
bd388e2c-75ec-532b-a730-2db9673afb83|Solution 10|NOT GIVEN
8d81cf71-15d4-576a-8f1b-f70315662880|Biconnected Component Decomposition (BCD) based Filtering|This solution addresses the challenge of memory-efficient scalable graph processing by employing a BCD-based filtering technique to reduce the number of shortest path computations required for closeness centrality calculation. The BCD is used to identify the components of the graph that are affected by edge insertions or deletions, allowing the algorithm to focus on updating the centrality scores of only the relevant vertices.
605d4894-1bdf-59d4-8db4-2a714bc221e8|Level-based Work Filtering|This solution addresses the challenge of memory-efficient scalable graph processing by employing a level-based work filtering technique to reduce the number of shortest path computations required for closeness centrality calculation. The technique filters out vertices that are not affected by edge insertions or deletions, reducing the number of shortest path computations required.
b3e882ef-4446-54e0-98cc-3e8418f532c7|Identical Vertex Set-based Filtering|This solution addresses the challenge of memory-efficient scalable graph processing by employing an identical vertex set-based filtering technique to reduce the number of shortest path computations required for closeness centrality calculation. The technique identifies sets of identical vertices and updates their centrality scores simultaneously, reducing the number of shortest path computations required.
11f1a391-5861-59b1-a6dc-75bda2dfdf5f|Pipelined Parallelism|This solution addresses the challenge of memory-efficient scalable graph processing by employing pipelined parallelism to overlap the computation of centrality scores with the filtering of vertices. This approach reduces the memory requirements and improves scalability by allowing multiple filters to compute simultaneously on different iterations of the work.
38fec9b3-a379-5e36-84a6-3e9e3219990c|Shared Memory-awareness|This solution addresses the challenge of memory-efficient scalable graph processing by employing shared memory-awareness to reduce memory consumption and improve scalability. The approach uses a shared memory-aware layout to minimize memory consumption and improve scalability.
fa26765f-6fb1-5778-ad99-fa279c611bf6|Replicated Parallelism|The authors propose using replicated parallelism to speed up the computation of closeness centrality scores. This approach involves replicating the ComputeCC filter to process different parts of the graph concurrently.
ccddedb4-1974-5fee-b4b9-8b8ae491ec4b|NUMA-Aware Graph Decomposition|The authors propose using a NUMA-aware graph decomposition approach to reduce communication overhead. This approach involves decomposing the graph into smaller subgraphs, each processed by a different NUMA domain.
806e89fe-08e5-5e4c-a95a-bc4042499e18|Level-Based Work Filtering|The authors propose using a level-based work filtering approach to reduce the number of SSSPs required for closeness centrality computation. This approach involves filtering out vertices that do not need to be updated based on their level in the graph.
c4377944-f3e3-5546-b7ee-18987c813738|Special Vertex Utilization|The authors propose using a special vertex utilization approach to reduce the number of SSSPs required for closeness centrality computation. This approach involves identifying special vertices that can be used to reduce the number of SSSPs.
016083ad-4b93-5902-aa2a-ac8a77c719df|Biconnected Component Decomposition (BCD)|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by decomposing the graph into biconnected components. This decomposition helps to identify vertices whose centrality scores need to be updated after an edge insertion or deletion.
8fda7736-4c5d-5101-aa50-07af776a9d20|Identical Vertex Utilization|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by identifying identical vertices in the graph. Identical vertices have the same centrality score, so only one of them needs to be updated.
3e4062a9-48bb-5d72-8a41-e2b6d9779073|Pipelined Parallelism with Work Filtering|The authors propose a pipelined parallelism approach to optimize load balance in distributed systems. This solution involves decoupling the work filtering and computation phases, allowing for simultaneous processing of updates and filtering of work. The work filtering phase is responsible for identifying the updates that need to be processed, while the computation phase performs the actual updates.
07140daf-a4f3-513f-9d37-9e5836a85747|Biconnected Component Decomposition|The authors propose a biconnected component decomposition approach to reduce the number of SSSPs in the incremental centrality computation. This solution involves decomposing the graph into biconnected components and updating the centrality scores only for the affected components.
74952cac-bbb3-5acc-a57e-42d5f842bab8|Pipelined Parallelism for Incremental Centrality Computation|This solution addresses the challenge of efficient graph dynamics processing by employing pipelined parallelism to overlap computation and communication blocks in incremental centrality computation. It involves using a distributed memory framework that leverages replicated and pipelined parallelism to speed up the maintenance of closeness centrality scores in dynamic networks.
3cfc2268-4a11-5d34-a33a-bed28fd67c66|Level-Based Work Filtering for Incremental Centrality Computation|This solution addresses the challenge of efficient graph dynamics processing by reducing the number of single-source shortest path (SSSP) computations required for incremental centrality updates. It involves using level-based filtering to identify vertices whose centrality scores do not change after an edge insertion or deletion.
45769200-438d-5d5a-8c27-1fe96abd7783|Special Vertex Utilization for Incremental Centrality Computation|This solution addresses the challenge of efficient graph dynamics processing by exploiting special vertices in the graph to reduce the number of SSSP computations required for incremental centrality updates. It involves using articulation vertices and identical vertices to update the centrality scores of affected vertices.
74e68792-0595-512d-83e2-a07f47b7a0b6|Biconnected Component Decomposition for Incremental Centrality Computation|This solution addresses the challenge of efficient graph dynamics processing by using a biconnected component decomposition to identify the affected vertices after an edge insertion or deletion. It involves partitioning the edges of the graph into biconnected components and updating the centrality scores of affected vertices.
d4cb5495-53d8-530b-a7ea-9f4ca99c9340|Trade-off Low Arb Coloring Algorithm|This solution proposes a trade-off between solution accuracy and communication rounds for distributed graph coloring algorithms. The algorithm uses a combination of random color selection and iterative refinement to reduce the number of communication rounds required, while also allowing for a trade-off between solution accuracy and communication rounds.
a9cb34c4-ba6a-5fd9-90e9-8fb2c315f7bf|H Partition Algorithm|The H Partition Algorithm is a deterministic distributed algorithm that computes an acyclic orientation of the edges in a graph with arboricity , such that each node has out-degree at most 2. This algorithm is specifically designed to address the challenge of adaptive algorithms for heterogeneous and irregular graphs by providing a mechanism to efficiently process and analyze complex graph structures.
f05f3c6c-0a7b-5be1-b1ee-740a2c14dc47|Randomized O() Partial Coloring Algorithm|The Randomized O() Partial Coloring Algorithm is a randomized distributed algorithm that partially colors the graph using O() colors in O(log n log ) rounds. This algorithm is specifically designed to address the challenge of adaptive algorithms for heterogeneous and irregular graphs by providing a mechanism to efficiently color the graph while minimizing the number of colors used.
62d4edbc-b197-5ec3-83a9-274753347620|Randomized Distributed Graph Coloring Algorithm|The authors propose a randomized distributed graph coloring algorithm that efficiently colors a graph with arboricity , using a number of colors that depends on the arboricity of the graph. The algorithm consists of two steps: a partial coloring step that uses O(log n) rounds and O( log ) colors, and a deterministic coloring step that uses O(log n) rounds and O() extra colors.
c0ee3b0c-b449-5c71-9d15-bf7df5bc7a00|KokkosKernels-based On-Node Parallel Coloring|The authors propose using KokkosKernels, a performance-portable library, to implement on-node parallel coloring algorithms for graph coloring. This solution specifically addresses the challenge of memory-efficient scalable graph processing by leveraging KokkosKernels' ability to optimize memory access patterns and reduce memory usage.
dac3e48c-4f22-5888-8baa-57a196f8a0ea|Trilinos-based Distributed Memory Graph Coloring|The authors propose using Trilinos, a distributed memory parallel framework, to implement distributed memory graph coloring algorithms. This solution specifically addresses the challenge of memory-efficient scalable graph processing by leveraging Trilinos' ability to manage distributed memory and optimize communication between processors.
158babbf-2ae8-544b-85d6-9eb22e26507a|Two-Ghost-Layer Approach for Reduced Communication|The authors propose a two-ghost-layer approach to reduce communication overhead in distributed memory graph coloring. This solution specifically addresses the challenge of memory-efficient scalable graph processing by minimizing communication between processors.
e98ea591-4d6a-5b58-a363-da3f3297ba19|Speculative and Iterative Coloring Approach|The authors propose a speculative and iterative coloring approach for graph coloring. This solution specifically addresses the challenge of memory-efficient scalable graph processing by allowing for more efficient processing of large graphs.
dbd8c621-5179-5317-be7e-5d8e56b14f07|Two-Ghost-Layer Coloring|The authors propose a novel method to reduce communication in distributed graph coloring by introducing a second ghost layer of vertices. This approach aims to minimize the number of communication rounds required for coloring.
477a908c-7d91-5967-b607-d411d3bdff3b|Speculative Coloring with Conflict Resolution|The authors propose a speculative coloring approach that allows for concurrent coloring of vertices and resolves conflicts through a random conflict resolution scheme.
dc6434d6-f83d-559c-bc93-ef51fa1d7a9e|Distance-2 Coloring with Optimized Conflict Detection|The authors propose a distance-2 coloring algorithm that optimizes conflict detection by examining only the ghost vertices adjacencies.
9e3de840-11a5-58ad-8128-84354e1a5b7c|Kokkos-based Parallel Coloring|The authors propose a Kokkos-based parallel coloring approach to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution focuses on utilizing the Kokkos performance portability framework to develop parallel coloring algorithms that can efficiently handle complex graph structures.
f7f30594-c76a-58ae-b4ae-765f471af0af|Distance-2 Coloring|The authors propose a distance-2 coloring approach to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution focuses on developing a distributed memory algorithm for distance-2 coloring, which can efficiently handle complex graph structures.
bd38082e-862b-5a3d-8a84-b8b7081ec4c0|KokkosKernels-based GPU Coloring|The authors propose using KokkosKernels, a performance-portable programming model, to optimize GPU memory access for graph coloring. They leverage KokkosKernels' ability to provide parallelization for both multicore CPUs and GPUs, allowing for efficient on-node parallelism.
5ab4c7c4-561b-51c3-b94f-57c88d528bf6|Speculative and Iterative Coloring|The authors propose a speculative and iterative coloring approach to optimize GPU memory access for graph coloring. This approach involves coloring as many vertices as possible in parallel and then iteratively fixing conflicts in the resulting pseudo-coloring until no conflicts remain.
e8ecfdfe-3a9a-53f8-a247-7ac386353722|Block-Centric Walk Management|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a block-centric method to manage walk data. Instead of using a vertex-centric approach, which incurs high memory cost, the authors implement each walk pool as a fixed-length buffer, storing at most 1024 walks by default, to avoid dynamic memory allocation. This approach reduces the memory overhead of managing all walk states, allowing the system to support massive concurrent walks.
2c9495b1-2ab7-54ef-98e6-b3b5ce2507f6|Asynchronous Walk Updating|This solution addresses the challenge of memory-efficient scalable graph processing by proposing an asynchronous walk updating scheme. The authors use a re-entry method to allow walks to move as many steps as possible within the currently loaded subgraph without extra subgraph accesses. This approach increases the walk updating rate and reduces the completion time of all walks.
7064e831-48f4-580f-bfd4-0694694c75d7|State-Aware Graph Loading|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a state-aware graph loading scheme. The authors use the states of walks to optimize the process of graph loading and computing, loading the graph block with the most walks from disk into memory to improve I/O utilization.
01b11bc4-c8db-5d79-8deb-29898bc05e65|Disk-Based Walk Management|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a disk-based walk management scheme. The authors use asynchronous batched I/Os to write walk states back to disk, supporting running massive random walks in parallel on huge graphs.
75d86cf7-ba62-593d-8817-114fcfac4e45|Probabilistic Approach to Balance Walk Progress|The authors propose a probabilistic approach to balance walk progress and address the straggler issues in distributed algorithms. This solution specifically addresses the challenge by introducing a probabilistic method to balance the progress of each walk, ensuring that all walks make progress at a similar rate.
f21669e6-db1a-53e9-93a5-70e8a6eda0c0|Block-Centric Walk Indexing|The authors propose a block-centric walk indexing scheme to optimize communication efficiency in distributed algorithms. This solution specifically addresses the challenge by reducing the memory overhead of managing all walk states and improving the walk updating rate.
228ca3ff-af69-53b3-9332-4045bd7d3ae3|State-Aware Graph Loading with Block Caching|This solution involves developing a state-aware graph loading process that loads graph blocks based on the states of walks. The approach also employs a block caching scheme to keep multiple blocks in memory, which helps to reduce the number of disk I/Os.
bd28fd3f-7f64-58b7-a6fe-1a9e7bde889d|Exclusive Pre x Sum|The Exclusive Pre x Sum solution addresses the challenge of memory-efficient scalable graph processing by implementing a pre x sum procedure to improve data access time and thread concurrency during frontier propagation steps in the BFS algorithm. This solution stores frontier data structures in shared memory and handles them through a pre x sum procedure, which is implemented through shuf e instructions to sensibly impact the overall BFS visit performance.
a685abdc-2b07-5134-a7e9-d7009f4e77a5|Dynamic Virtual Warp|The Dynamic Virtual Warp solution addresses the challenge of memory-efficient scalable graph processing by dynamically adjusting the warp size to balance the workload and reduce thread divergence during frontier propagation steps in the BFS algorithm.
ef16600f-18b5-51f6-93f9-66f9789efcc2|Edge Discover|The Edge Discover solution addresses the challenge of memory-efficient scalable graph processing by assigning threads to edges instead of vertices, reducing thread divergence and improving workload balance during frontier propagation steps in the BFS algorithm.
ddcc49f2-b152-5bfd-a2c8-4f48d2a0f977|Dynamic Parallelism|The Dynamic Parallelism solution addresses the challenge of memory-efficient scalable graph processing by dynamically creating child kernels to handle high-degree vertices, reducing thread divergence and improving workload balance during frontier propagation steps in the BFS algorithm.
0d216ad8-9821-5a89-b43b-4d3ae55e9f86|Duplicate Detection and Correction|The Duplicate Detection and Correction solution addresses the challenge of memory-efficient scalable graph processing by implementing a hash table-based approach to detect and correct duplicate vertices during frontier propagation steps in the BFS algorithm.
266384af-8af9-5ac8-b731-728efaf6bdce|Coalesced Read/Write Memory Accesses|The Coalesced Read/Write Memory Accesses solution addresses the challenge of memory-efficient scalable graph processing by implementing a technique to induce coalescence in global memory accesses, reducing memory accesses and improving thread concurrency during frontier propagation steps in the BFS algorithm.
25cc6d29-57d3-566b-8f0a-fc94acdb93c7|Dynamic Virtual Warps|The authors propose the use of dynamic virtual warps to optimize communication efficiency in distributed algorithms. This solution involves dynamically adjusting the size of virtual warps based on the graph characteristics to minimize thread divergence and improve workload balancing.
b202fc37-2071-5cfa-9591-38e29a9b0f1a|Coalesced Read Write Memory Accesses|The authors propose the use of coalesced read write memory accesses to optimize load balance in distributed systems. This solution involves using coalesced memory accesses to reduce the overhead of memory accesses, allowing for more efficient handling of workload imbalance.
881d8d72-28c4-5a0e-90ad-091e69f1f1b5|Distributed Edge Connectivity Algorithm|
693d6c1f-e5ce-5b39-84d0-8049c228b501|Graph Contraction Algorithm|The authors propose a graph contraction algorithm that reduces the size of the graph while preserving non-trivial min-cuts. The algorithm is designed to work in the CONGEST model.
96a0d949-f923-5dcc-a293-47f15c7c9a79|Distributed Min-Cut Algorithm|The authors propose a distributed algorithm for finding the minimum cut in a graph. The algorithm is designed to work in the CONGEST model.
0081c538-10a8-550f-9876-26abd150ca6f|Tripartition Algorithm|The authors propose a tripartition algorithm that partitions the edge set of a graph into three parts: Eh, Es, and Er. The algorithm is designed to work in the CONGEST model.
1b9571d7-220d-5ec4-9074-81db0c6bb0a2|Tree Packing Algorithm|The authors propose a tree packing algorithm that packs trees into a graph. The algorithm is designed to work in the CONGEST model.
e2b7cc37-0abb-5be1-a517-738ad3530117|Distributed MST Algorithm|The authors propose a distributed algorithm for finding the minimum spanning tree (MST) of a graph. The algorithm is designed to work in the CONGEST model.
262905f7-5581-5a03-b5c6-d795f737e991|Distributed Connectivity Certificate Algorithm|The authors propose a distributed algorithm for finding a connectivity certificate of a graph. The algorithm is designed to work in the CONGEST model.
d99b3ff8-5e35-5a83-8f4b-37aeed31bf07|Distributed Trussness Algorithm|The authors propose a distributed algorithm for computing trussness in a graph. The algorithm is designed to work in the CONGEST model.
6859f0fd-e5bd-5b83-8c13-050f34406dcf|Distributed k-Core Algorithm|The authors propose a distributed algorithm for computing the k-core of a graph. The algorithm is designed to work in the CONGEST model.
8d4cb012-7bbe-511e-912f-c79ee0472024|Distributed Community Detection Algorithm|The authors propose a distributed algorithm for detecting communities in a graph. The algorithm is designed to work in the CONGEST model.
3703d027-8be0-5d11-97c9-f7f217c40a84|Distributed Graph Partitioning Algorithm|The authors propose a distributed algorithm for partitioning a graph. The algorithm is designed to work in the CONGEST model.
707bd20d-6e7f-5814-9065-cd45caf3e9ab|Distributed Graph Sparsification Algorithm|The authors propose a distributed algorithm for sparsifying a graph. The algorithm is designed to work in the CONGEST model.
cdb37da2-256a-5a38-b3dc-524cebfa61bb|Distributed Graph Sampling Algorithm|The authors propose a distributed algorithm for sampling a graph. The algorithm is designed to work in the CONGEST model.
18951595-42f1-5836-af46-70b7b496b157|Distributed Graph Sketching Algorithm|The authors propose a distributed algorithm for sketching a graph. The algorithm is designed to work in the CONGEST model.
ae349709-0e00-51cb-8ff1-5eda914b1f4e|Distributed Graph Embedding Algorithm|The authors propose a distributed algorithm for embedding a graph. The algorithm is designed to work in the CONGEST model.
8e060068-b27d-573c-8c6d-b1f59ba35bb2|Distributed Graph Clustering Algorithm|The authors propose a distributed algorithm for clustering a graph. The algorithm is designed to work in the CONGEST model.
1cd807a7-b1eb-5d2d-90fb-cc77424792c2|Distributed Graph Classification Algorithm|The authors propose a distributed algorithm for classifying a graph. The algorithm is designed to work in the CONGEST model.
52dd1907-80cc-532c-9873-73f5fbc20368|Dynamic Message Aggregation|The authors propose a dynamic message aggregation technique to reduce the number of messages sent between processors, thereby decreasing memory consumption and communication overhead. This technique aggregates multiple small messages designated for the same receiver into a single message, reducing the startup overhead and memory requirements.
e6b587be-44e2-54d0-be57-bf9d224425bf|Indirect Communication Protocol|The authors propose an indirect communication protocol to reduce communication overhead and memory consumption. This protocol allows processors to communicate with each other indirectly, reducing the number of messages sent and received.
bfb171f5-161b-54b5-9c01-8b9d04a94460|Contraction-Based Triangle Counting|The authors propose a contraction-based triangle counting algorithm to reduce memory consumption and communication overhead. This algorithm contracts the graph by removing non-cut edges, reducing the number of edges and vertices that need to be processed.
4c366010-807d-5c46-a1eb-782c7aec067b|Approximate Triangle Counting using Membership Query Data Structures|The authors propose an approximate triangle counting algorithm using membership query data structures to reduce memory consumption and communication overhead. This algorithm uses a membership query data structure to approximate the number of triangles in the graph.
cb502dfd-31b0-56b1-b8ec-09669a5315d1|Message Aggregation with Dynamic Buffering|The authors propose a message aggregation technique that reduces the number of messages sent between processors, thereby optimizing communication efficiency. This technique uses a dynamic buffer to aggregate multiple small messages into a single message, reducing the startup overhead and total communication volume.
d7e7b9e3-3f3b-5472-b9fc-9de5ea0d345e|Indirect Message Delivery|The authors propose an indirect message delivery technique that reduces the number of messages sent between processors. This technique uses a grid-based approach to route messages through intermediate processors, reducing the number of direct messages sent.
408e8c25-82d5-5601-99e6-53f280985274|Contraction-Based Communication Reduction|The authors propose a contraction-based approach to reduce the communication volume. This approach involves contracting edges in the graph, reducing the number of messages sent between processors.
57243d4e-0c52-598e-9fd8-5cdf6e8e2275|Hybrid Parallelism with Multi-Threading|The authors propose a hybrid parallelism approach that combines MPI parallelism with multi-threading. This approach reduces the communication volume and improves the running time of the algorithm.
53c548a2-51bd-507d-8217-7dbc80bd804e|Hybrid Parallelism|The authors propose a hybrid parallelism approach to combine distributed memory parallelism with shared memory parallelism to enhance the performance of triangle counting algorithms. This solution specifically addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by reducing the communication overhead and enhancing memory locality.
bc0004bf-0397-5a38-87a0-e29ca914e85b|Graph Contraction|The authors propose a graph contraction technique to optimize load balance in distributed systems. This technique involves contracting the graph by removing non-cut edges, reducing the communication volume and improving the load balance.
edfdad06-a3c4-5f34-a983-18b4f5c032e1|Dynamic Trussness Update Algorithm|The authors propose an algorithm for efficiently updating trussness values in a graph after edge insertions or deletions. This algorithm specifically addresses the challenge of efficient graph dynamics processing by minimizing the number of iterations required to maintain maximal k-trusses under updates. The algorithm uses a novel approach based on the concept of
2c5144b7-551c-52a8-9970-18baead80c03|Batch Graph Update Algorithm|The authors propose an algorithm for batch processing vertex and edge updates while preserving structural integrity. This algorithm addresses the challenge of efficient graph dynamics processing by enabling efficient and adaptive updates to graph structures. The algorithm uses a novel approach based on the concept of
84c0a72c-6399-5b3f-8f9b-4533ef53af12|Task Fragmentation|Task fragmentation is a load balancing technique designed to deal with skew in the input graphs, which improves the performance of QFrag by up to four times. Task fragmentation focuses on identifying the few heavy trees that cause most of the work and redistributes them to other workers. This approach is different from existing load balancing techniques, which often rely on random partitioning or simple hashing. The paper shows that task fragmentation consistently outperforms embarrassingly parallel execution, with low overhead when the workload is not skewed and high gains when it is.
b832f567-69dc-5630-b11d-3937edcedca8|Replicating the Input Graph|QFrag replicates the input graph at every worker, allowing for efficient local graph exploration and reducing the need for communication between workers. By replicating the input graph, QFrag can reuse decades of research in sequential algorithms for subgraph isomorphism, which are optimized for local graph access. The paper shows that QFrags approach leads to very efficient local graph exploration, even with larger graphs and analytical queries, and outperforms other systems that distribute the input graph.
e4a31b66-7ead-544c-8777-6cf4179499a2|Parallelizing Subgraph Isomorphism Algorithms|QFrag parallelizes state-of-the-art algorithms for subgraph isomorphism, such as TurboISO, to scale out the execution of complex analytical queries. QFrag uses a task fragmentation approach to deal with skew in the input graphs and employs a push-based approach to distribute the work among workers. The paper shows that QFrags parallelization approach leads to significant performance gains, with the task fragmentation policy able to mitigate skew and improve performance by up to four times.
2f321a08-3d88-5efa-8bfa-c38976842820|Load Balancing through Task Fragmentation|This solution involves using task fragmentation to balance the load among workers in a distributed system. By breaking down tasks into smaller subtasks, the system can distribute the workload more evenly, reducing the need for communication and minimizing the impact of stragglers.
e7ee4be1-0459-5f9d-864f-02a98be99f28|Load Balancing via Subgraph Isomorphism|QFrag uses a task fragmentation approach to deal with skew in the input graphs, which improves its performance by up to 4 compared to a naive approach. QFrag runs independent parallel instances of a sequential graph matching algorithm with load balancing. It replicates the input graph at each worker and uses graph exploration to distribute the work. The paper shows that QFrag outperforms other state-of-the-art distributed graph search systems in running complex analytical queries.
9c1fb4b9-5c79-5903-bc54-1e9823292ecb|Subgraph Isomorphism Algorithm|QFrag uses a tree-based subgraph isomorphism algorithm, which is a state-of-the-art sequential algorithm for subgraph isomorphism. The algorithm starts by transforming the query graph into a spanning tree and matching this tree, and then enumerates partial embeddings based on the candidate tree. The evaluation shows that QFrag is able to run complex analytical queries that other systems are unable to handle, and outperforms other systems by orders of magnitude.
a46d6213-0657-5f58-b459-ca72b5a7719f|Lightweight Graph Partitioning|The authors propose a lightweight graph partitioning approach to divide the input graph into smaller partitions, each of which can fit in the on-chip RAMs of the FPGA. This approach enables efficient on-chip buffering of vertex data and increases parallelism.
5327b52a-df81-577b-bb93-8f274e4e5736|Optimized Data Layout|The authors propose an optimized data layout that reduces non-sequential external memory accesses by sorting the edges in each shard based on their destination vertices.
585852c0-5929-5693-8823-150fda992ff5|Update Combining and Filtering|The authors propose an update combining and filtering scheme to reduce the data communication between the FPGA and external memory.
63fb3d0f-860e-50be-aebf-8fcf99a65642|Partition Skipping|The authors propose a partition skipping scheme to reduce redundant edge traversals for non-stationary graph algorithms.
19534b76-6ee6-590c-9cae-6a5a7c827209|Data Layout Optimization|The authors propose an optimized data layout to reduce non-sequential external memory accesses. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by minimizing the number of non-sequential memory accesses.
fd2a08fe-66eb-5956-b418-b3ebb7575261|Edge-Centric Graph Processing Framework|The authors propose an edge-centric graph processing framework, HitGraph, to accelerate general graph algorithms on FPGAs. This framework is designed to handle heterogeneous graph structures and irregular memory access patterns.
55b8c57d-42bf-5d2a-b410-88eb87fe1e16|Update Merging and Filtering|The authors propose an update merging and filtering scheme to reduce the data communication between the FPGA and external memory.
d54fc3c3-817a-5d68-b4f9-397e15d96986|Partition Skipping Optimization|The authors propose a partition skipping optimization to reduce the number of edge traversals in the scatter phase. This optimization is specifically designed to address the challenge of load balance optimization in distributed systems by minimizing the number of redundant edge traversals.
ad9de59d-39b4-57dd-b5ee-4683089d5c64|Update Combining and Filtering Optimization|The authors propose an update combining and filtering optimization to reduce the number of updates written into the external memory. This optimization is specifically designed to address the challenge of load balance optimization in distributed systems by minimizing the number of updates.
6cb72587-d8ee-5eb4-9bcf-0f9261d70653|Vertex Buffering|This solution addresses the challenge of optimizing GPU memory access for graph processing by reducing the number of memory accesses to vertex data. The authors propose a vertex buffering scheme that buffers vertex data in on-chip RAMs, reducing memory access overhead.
5e02ff57-f6e0-518b-bbec-3f2d78c1ebb2|Scalable Graph Processing Class (SGC)|The authors propose a new class of MapReduce algorithms, called Scalable Graph Processing Class (SGC), which is designed to address the challenge of memory-efficient scalable graph processing. SGC relaxes some constraints in the existing MapReduce classes to make it suitable for scalable graph processing.
0326a228-6ea5-5603-b870-02d88142e5bd|NE Join|The authors propose a new graph operator called NE join, which is designed to propagate information from nodes to their adjacent edges in a graph. NE join is a key component of the SGC class and can be used to design a wide range of graph algorithms.
be6923f5-24e9-516d-857d-e60bbd6f7554|EN Join|The authors propose a new graph operator called EN join, which is designed to aggregate information from adjacent edges to nodes in a graph. EN join is a key component of the SGC class and can be used to design a wide range of graph algorithms.
ef738814-3841-578b-bee1-e8e448b1a16c|Graph Keyword Search Algorithm|The authors propose a new algorithm for graph keyword search, which is designed to find all rooted trees in a graph that contain a set of keywords. The algorithm uses the NE join and EN join operators to propagate keyword information from nodes to their adjacent edges and aggregate the information from edges to nodes.
d7ec263a-461e-52d2-b304-0287ae98186d|Connected Component Computation Algorithm|The authors propose a new algorithm for connected component computation, which is designed to find all connected components in a graph. The algorithm uses the NE join and EN join operators to propagate information from nodes to their adjacent edges and aggregate the information from edges to nodes.
6abbf8cb-69d2-53b2-8439-56aa4176f125|Minimum Spanning Forest Computation Algorithm|The authors propose a new algorithm for minimum spanning forest computation, which is designed to find the minimum spanning forest of a graph. The algorithm uses the NE join and EN join operators to propagate information from nodes to their adjacent edges and aggregate the information from edges to nodes.
913b8258-9407-5b6f-aaef-00e6047e839d|NE Join and EN Join|The authors propose two graph join operators, NE Join and EN Join, which are designed to solve a large range of graph problems in SGC. These operators aim to minimize communication rounds by propagating information from nodes to edges and aggregating information from edges to nodes.
5d828792-12e6-5e33-a75e-6a2335ab858a|HashToMin and HashGToMin|The authors propose two MapReduce algorithms, HashToMin and HashGToMin, which are designed to solve the connected component problem in SGC. These algorithms aim to minimize communication rounds by using a hash function to assign nodes to machines.
36e916e4-38fb-55b1-a4cf-3f448465759f|MSF Computation|The authors propose a new algorithm for minimum spanning forest (MSF) computation in SGC, which aims to minimize communication rounds by using a combination of NE Join and EN Join.
ffc16cfc-017d-517f-a4a3-e12a1f3acdc8|Two-Graph-Operator-Based Algorithm|The authors propose a two-graph-operator-based algorithm for connected component computation, which is designed to handle heterogeneous and irregular graphs. The algorithm uses the NE join and EN join operators to propagate information between nodes and edges.
ed22b291-fdd0-5200-9387-02d72ad81849|Minimum Spanning Forest (MSF) Computation Algorithm|The authors propose an algorithm for MSF computation, which is designed to handle heterogeneous and irregular graphs. The algorithm uses the NE join and EN join operators to propagate information between nodes and edges.
9b835569-d702-5e73-9988-bc5d1d3c4649|Graph Join Operators (NE Join and EN Join)|The authors propose two graph join operators, NE Join and EN Join, which are designed to solve a large range of graph problems in a scalable and efficient manner. These operators are used to propagate information from nodes to edges and aggregate information from edges to nodes.
e555bc41-b007-557d-85eb-c9fbd58f5d5b|Uni ed Graph Processing System|The authors propose a uni ed graph processing system that uses a single node table and edge table to represent the graph. This system is designed to be self-contained, allowing for more complex graph processing tasks to be designed by chaining several graph queries together.
c60c9749-c4ce-5b3e-9ac7-a290b5b4a2a6|NE Join and EN Join Operators|The authors introduce two graph join operators, NE join and EN join, which are designed to efficiently propagate information between nodes and edges in a graph. These operators are used to solve a wide range of graph problems, including PageRank, breadth-first search, and graph keyword search.
d0f86421-854d-536d-84b2-388d2a607a81|Minimum Spanning Forest (MSF) Computation|The authors propose an algorithm for computing the Minimum Spanning Forest (MSF) of a graph in a distributed environment. The algorithm uses the SGC class and the NE join and EN join operators to efficiently compute the MSF.
7af5a90a-c206-5f79-ba0c-b7f7f8b0a2e7|Connected Component (CC) Computation|The authors propose an algorithm for computing the Connected Component (CC) of a graph in a distributed environment. The algorithm uses the SGC class and the NE join and EN join operators to efficiently compute the CC.
15e396dd-c175-5187-aac5-648f2c7c5d66|Radar Push Algorithm|The Radar Push algorithm is a distributed algorithm designed to estimate PageRank values in large graphs efficiently. It addresses the challenge of memory-efficient scalable graph processing by reducing the bandwidth requirement and minimizing the number of communication rounds.
06173a8a-df25-5f78-88ed-fafa3b4aacec|MRP Algorithm|The MRP algorithm is an extension of the Radar Push algorithm, designed to estimate PageRank values in large graphs with improved round complexity. It addresses the challenge of memory-efficient scalable graph processing by reducing the number of communication rounds and minimizing the bandwidth requirement.
ea08d896-3c76-54cb-b842-a83b4b4d8f34|Multi-phase Radar Push (MRP)|MRP is a distributed algorithm designed to optimize communication efficiency in distributed PageRank computation. It addresses the challenge by reducing the round complexity to O(log log n) while maintaining a reasonable bandwidth requirement of O(log n^3).
37456b02-0bc9-52ff-9b5a-413a73a890a1|Radar Push (RP)|RP is a distributed algorithm designed to optimize communication efficiency in distributed PageRank computation. It addresses the challenge by reducing the round complexity to O(log n) while maintaining a reasonable bandwidth requirement of O(log n^2).
cf947963-a2a5-5a6d-b48e-ddc0f6b832d0|Improved PageRank Algorithm (IPRA)|IPRA is a distributed algorithm designed to optimize communication efficiency in distributed PageRank computation. It addresses the challenge by reducing the round complexity to O(log n) while maintaining a reasonable bandwidth requirement of O(log n^3).
7c75393a-1fa2-5e22-8480-fd00f76c7058|Local Sparsification Technique|The authors propose a local sparsification technique to address the challenge of memory-efficient scalable graph processing. This technique involves designing a locally sparse graph G generated by the local sparsification procedure, which allows each node to simulate r O p logn rounds of the MIS algorithm if it knows its r hop neighborhood in G. The sparsification is such that the whole r neighborhood is small, has size at most n for a desirably small constant 0, and the maximum degree in G is bounded to 2O log n.
4000410f-5bd2-5864-91e3-89086a3c78e0|Sparsified MIS Algorithm|The Sparsified MIS Algorithm is a solution proposed by the authors to optimize communication efficiency in distributed algorithms. This algorithm works in phases, each consisting of P p logn 10 iterations and one additional round. At the beginning of each phase, each node sends its pt v to its neighbors, and then node v sets dt v P u N v pt u . The algorithm then proceeds in iterations, with each node deciding whether to join the MIS based on its pt v value and the values of its neighbors.
df0d48a7-b419-5c25-9d94-cdc21a3bbb9c|Beeping MIS Algorithm|The Beeping MIS Algorithm is a solution proposed by the authors to optimize communication efficiency in distributed algorithms. This algorithm works by having each node beep with probability pt v and then deciding whether to join the MIS based on the beeps received from its neighbors.
71792e19-7a29-596f-91bf-8ad98a9e0df4|Local Sparsification Technique for MIS|The authors propose a local sparsification technique for the Maximal Independent Set (MIS) problem, which is a fundamental problem in distributed graph algorithms. The technique involves simulating O(log n) rounds of the MIS algorithm in O(log log n) rounds of the congested clique model, with high probability. This is achieved by making each node learn its 2-hop neighborhood in O(1) rounds and then applying this procedure recursively on the G2i powers of the base graph G. The local sparsification step involves selecting a subset of nodes that are likely to be part of the MIS, and then simulating the MIS algorithm on this subset. The recursive application of the MIS algorithm allows the authors to reduce the number of rounds required to solve the problem. The authors show that their technique can solve the MIS problem in O(log log n) rounds of the congested clique model, with high probability. This is a significant improvement over existing algorithms, which require O(log n) rounds.
d9807db3-8321-5fa2-b967-b3e0671a97fe|Phase-Based Approach|The Phase-Based Approach is a solution proposed by the authors to address the challenge of efficient graph dynamics processing. This approach is used in conjunction with the Sparsified MIS Algorithm to efficiently process dynamic updates in the graph.
1a419e48-053e-5d6a-8fc1-ae0de7e0c07c|Beep Vector Simulation|The Beep Vector Simulation is a solution proposed by the authors to address the challenge of efficient graph dynamics processing. This technique is used in conjunction with the Local Sparsification Technique to simulate the behavior of other nodes in the graph.
1a8ba913-a165-51e8-b863-9a00d4b5e806|Balanced, Practical Pregel Algorithms (BPPA)|BPPA is a solution that focuses on designing vertex-centric graph algorithms that achieve good load balancing and linear cost at each superstep, while also ensuring the algorithm terminates after a reasonable number of supersteps.
60a7013d-df7d-5b83-a021-403b2690eb78|Task-based Vertex Pulling API|The authors propose a task-based vertex pulling API that allows users to write distributed subgraph mining algorithms. This API enables tasks to request vertices and edges for subsequent mining, reducing memory consumption by only loading necessary data.
7768d486-594c-5f0a-b6b0-0c9c8950c9ea|Vertex Cache Design|The authors propose a novel vertex cache design that supports highly concurrent vertex accesses by tasks. This design minimizes memory consumption by only caching requested vertices and evicting unused vertices.
39b7cf55-fde4-576f-be3c-30f7f9dc3e64|Lightweight Task Scheduling|The authors propose a lightweight task scheduling approach that ensures high task throughput while keeping memory consumption bounded. This approach minimizes CPU occupancy due to task scheduling.
85d105af-da3e-53cb-9dc7-198f893d15bc|Bounded Memory Consumption|The authors propose a design that ensures bounded memory consumption by only keeping a pool of tasks in memory at any time, using a local vertex table to keep a partition of vertices, and using a remote vertex cache with a bounded capacity.
88b49747-6747-5aa7-ac94-de0473e4479a|Task-Based Vertex Pulling|The authors propose a task-based vertex pulling approach to optimize communication efficiency in distributed algorithms. This approach involves dividing the mining problem into independent tasks, each represented by a different tree branch in the set enumeration tree. Each task is associated with a subgraph that it constructs and mines upon, allowing for concurrent processing and minimizing the need for communication.
a60b5115-b87f-5abf-90d0-af8b7efe7923|Aggregator-Based Result Aggregation|The authors propose an aggregator-based result aggregation approach to optimize communication efficiency in distributed algorithms. This approach involves using an aggregator thread to synchronize aggregated values periodically at a user-specified frequency.
2b3cf987-ed56-555c-a64a-9dff507fad82|Work Stealing among Machines|This solution involves machines about to become idle stealing tasks from busy ones to balance workloads. The main thread of each worker synchronizes task processing progress, and machines that are about to become idle prefetch tasks from heavily loaded ones for processing.
106318ed-df66-50dc-8ff0-ff97c42bdba5|Task Spilling and Refilling|This solution involves spilling tasks from task queues to disk when they become full and refilling them when there are insufficient tasks to keep CPU cores busy. This approach helps to minimize the number of disk-buffered tasks and prioritize partially processed tasks over new tasks.
212e7147-90f7-5d8a-a826-bcb0fde3cf5c|Load Balancing through Task Generation|This solution involves generating tasks from vertices in the local vertex table on demand when memory permits, rather than generating all tasks at the beginning. This approach helps to balance workloads among machines by dynamically generating tasks based on available memory.
69ed6eec-409c-5625-a68c-b832c677538a|Extendable Embedding Abstraction|The authors propose an extendable embedding abstraction to enable fine-grained task scheduling and low-cost data reuse, which directly addresses the challenge of memory-efficient scalable graph processing. This abstraction allows for the decomposition of subgraph enumeration into smaller tasks, enabling the efficient use of memory and reducing communication overhead.
895c8574-c857-58ad-a8cd-9b6d5a06e66a|Static Data Cache|The authors propose a static data cache to reduce communication overhead and improve performance in graph processing. The cache is designed to store frequently accessed graph data, reducing the need for remote data accesses and minimizing communication overhead.
f9d189e8-2f58-55e0-87bf-acc6ab531d7c|Horizontal Data Sharing|The authors propose a horizontal data sharing mechanism to enable the efficient sharing of data between extendable embeddings in the same chunk. This mechanism reduces the need for redundant data accesses and minimizes communication overhead.
35b2a2bb-7e78-5406-9449-faa24290aa20|BFS-DFS Hybrid Exploration|The authors propose a BFS-DFS hybrid exploration strategy to optimize memory usage and reduce communication overhead in graph processing. This strategy enables the efficient use of memory and reduces the need for redundant computation.
435e6dc8-07ec-55e0-a684-a3d99e29f5ab|Hierarchical Data Representation for Vertical Data Reuse|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by enabling the sharing of intermediate results between parent and child extendable embeddings. The hierarchical data representation allows for the storage of intermediate results in an extendable embedding, which can be directly accessed and copied by its children, reducing the need for redundant computation and communication.
e194a0e9-c129-5680-b746-8103ef90807a|Static Data Cache with No Replacement|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the number of remote data accesses. The static data cache stores frequently accessed graph data, eliminating the need for redundant fetches and reducing communication overhead.
e8a0fc98-14ff-5407-b006-47b298dc4d01|Horizontal Data Sharing among Extendable Embeddings|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by sharing edge lists among extendable embeddings in the same chunk. This approach reduces the number of remote data accesses and communication overhead.
42a88007-e42d-5d6c-aeb8-acb61ba44d62|BFS-DFS Hybrid Exploration with Fixed-Size Chunks|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by balancing the trade-off between memory consumption and parallelism. The BFS-DFS hybrid exploration approach generates sufficient concurrent tasks for communication-computation overlapping with bounded memory consumption.
31a9c5e7-a364-5f57-8417-a4a221b1b15f|Hybrid BFS-DFS Exploration with Fixed-Size Extendable Embedding Chunks|This solution addresses the challenge of optimizing load balance in distributed systems by proposing a hybrid exploration strategy that combines the benefits of Breadth-First Search (BFS) and Depth-First Search (DFS) for graph pattern mining. The approach involves dividing the graph into fixed-size chunks, exploring each chunk in a BFS manner, and then performing DFS within each chunk. This strategy enables the system to generate a large number of concurrent tasks, which can be distributed evenly across nodes, thereby achieving better load balance.
00c4f1eb-92b8-5967-ba12-6b37ecc607ed|Static Software Graph Data Cache with No Replacement|This solution addresses the challenge of optimizing load balance in distributed systems by proposing a static software graph data cache that stores frequently accessed graph data. The cache is shared across all chunks and nodes, and its size is typically 5-15% of the graph size per node. The cache is filled at the beginning of the execution and remains unchanged throughout.
300a4d47-09b6-5990-9c2d-ad60691b412f|Horizontal Data Sharing among Extendable Embeddings in the Same Chunk|This solution addresses the challenge of optimizing load balance in distributed systems by proposing a horizontal data sharing strategy among extendable embeddings in the same chunk. The approach involves maintaining a hash table that stores the active edge lists requested by extendable embeddings in a chunk, and sharing these edge lists among embeddings in the same chunk.
16182161-15d2-5d42-8827-4098805b9f04|NUMA-Aware Support for Distributed Graph Pattern Mining|This solution addresses the challenge of optimizing load balance in distributed systems by proposing a NUMA-aware support for distributed graph pattern mining. The approach involves dividing the graph partition of a socket node into sub-partitions, and running the BFS-DFS hybrid exploration independently on each socket based on the local sub-partition.
393cb514-ec88-5d23-b7c0-2abd950abc20|Hybrid BFS-DFS Exploration|The authors propose a hybrid BFS-DFS exploration strategy to address the challenge of efficient graph dynamics processing. This strategy combines the benefits of both BFS and DFS exploration, allowing for efficient computation and communication overlapping.
3bc0714b-c04a-54f4-8121-878d56b5ad2a|BDG Partitioning|BDG partitioning is a novel graph partitioning strategy that aims to reduce memory consumption and improve scalability in graph processing systems. It partitions the graph into blocks based on the vertex degree distribution, ensuring that each block has a similar number of vertices and edges. This approach helps to minimize the number of vertices that need to be pulled from remote machines, reducing communication overhead and memory usage.
86efd4ac-c2a9-5e8a-96d3-a2cdac139c1b|Task Pipeline Design|The task pipeline design is a novel approach to processing graph mining tasks in a distributed environment. It streamlines task processing by allowing CPU computation, network communication, and disk I/O to be processed asynchronously, hiding the overheads of network and disk I/O within the cost of CPU computation.
0559f490-bc94-5e16-ad23-9f82e2f6751a|RCV Cache|RCV Cache is a cache-based approach to storing remote candidate vertices, reducing the number of times they need to be pulled from remote machines. It uses a reference count mechanism to update the cache and minimize memory usage.
2fc40392-0dbf-5302-8212-927e86e12382|Process-Level Cache|Process-Level Cache is a cache-based approach to storing vertex data, reducing the number of times they need to be pulled from remote machines. It uses a shared cache mechanism to store vertex data, minimizing memory usage.
5f579c44-1711-54c3-97ab-c1e9e4239a13|Task Stealing|Task Stealing is a dynamic load balancing approach that steals tasks from other workers when a worker is idle, reducing the number of tasks that need to be processed.
7ba52f64-d6d4-50b1-811a-0e45d886510e|Locality-Sensitive Hashing (LSH) based Task Priority Queue|The authors propose an LSH-based task priority queue to optimize communication efficiency in distributed algorithms. This design orders inactive tasks that share common remote candidates to be near each other, reducing the number of communication rounds required.
dec5cf17-bda1-5e55-8d83-29628681bb05|Remote Candidate Vertex (RCV) Cache|The authors propose an RCV cache to optimize communication efficiency in distributed algorithms. This design stores remote candidate vertices in a local cache, reducing the number of communication rounds required to access these vertices.
61a91440-a72a-558d-813f-88f48ee1667b|LSH-based Task Priority Queue|The LSH-based task priority queue is a technique used to order tasks in the task pipeline based on their locality-sensitive hashing (LSH) keys. This approach aims to reduce the communication overhead and improve the locality of the task pipeline.
c28d967a-1562-51ba-a863-7be8e39e34d7|Color-Coded BFS|The authors propose a color-coded BFS algorithm to detect cycles in a graph. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the memory consumption and computational costs associated with graph traversal.
b301ed6c-6820-58e2-837a-ddc47e3e587c|Sublinear Time Protocol for C2k|The authors propose a sublinear time protocol for detecting C2k cycles in a graph. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the computational costs and communication overhead associated with cycle detection.
8659e942-99dd-5aa6-9704-01b927a1e9dc|Lower Bound on Deterministic Triangle Detection|The authors propose a lower bound on deterministic triangle detection in the CONGEST model. This solution specifically addresses the challenge of memory-efficient scalable graph processing by providing a theoretical limit on the computational costs and communication overhead associated with triangle detection.
d6be1794-449f-57d7-b4c8-35a4d47da11c|Lower Bound on Randomized Triangle Detection|The authors propose a lower bound on randomized triangle detection in the CONGEST model. This solution specifically addresses the challenge of memory-efficient scalable graph processing by providing a theoretical limit on the computational costs and communication overhead associated with triangle detection.
964c30d0-a98e-5e8d-abb0-89ec7666fda1|Pipelining|The authors propose a pipelining technique to optimize communication efficiency in distributed algorithms. This method involves breaking down the computation into smaller tasks and executing them in a pipeline fashion, reducing the overall communication overhead.
df44dde9-32ae-50d0-b0cf-1472a895409e|Nash-Williams Decomposition|The authors propose using the Nash-Williams decomposition to optimize communication efficiency in distributed algorithms. This method involves decomposing the graph into smaller components and executing the algorithm on each component separately, reducing the overall communication overhead.
503d3e6a-d70a-58c3-b74a-26c7ee4d1144|Pipelined Decomposition|The authors propose a pipelined decomposition technique to handle low-arboricity graphs. This technique is specifically designed to handle heterogeneous and irregular graphs by decomposing the graph into layers and processing each layer in parallel.
75841821-b7b5-50f6-84b7-ccc7cad425b5|Adaptive Algorithm for C2k Detection|The authors propose an adaptive algorithm for detecting C2k cycles in graphs. This algorithm is specifically designed to handle heterogeneous and irregular graphs by using a combination of color-coding and pipelined decomposition techniques.
86a33ae5-448b-543c-850e-d6dc6321b96c|Pipelined Algorithm|The authors propose a pipelined algorithm to detect even cycles in a graph. This algorithm uses a combination of color-coding and pipelining to detect cycles.
f6b3d891-3d9f-5384-b264-e47f8de04a69|Decomposition-Based Algorithm|The authors propose a decomposition-based algorithm to detect even cycles in a graph. This algorithm uses a decomposition of the graph into smaller subgraphs to detect cycles.
406d2a74-7eac-528a-91bb-e4bb4031d076|Bandwidth Lower Bound|The authors propose a bandwidth lower bound technique to show that detecting triangles in a graph requires at least log n bits of communication.
0e74e995-7264-5403-8745-95638e1a1cf3|Information-Theoretic Lower Bound|The authors propose an information-theoretic lower bound technique to show that detecting triangles in a graph requires at least log n bits of communication.
c687a1b5-a260-566d-9dad-8aa84acc9a6f|Subgraph Detection Algorithm|The authors propose a subgraph detection algorithm to detect subgraphs in a graph. This algorithm uses a combination of color-coding and pipelining to detect subgraphs.
05fcf46c-8bef-52a6-a933-5fd64c800dd0|Lower Bound for Subgraph Detection|The authors propose a lower bound technique to show that detecting subgraphs in a graph requires at least n^{2-1/k} rounds, where k is the size of the subgraph.
02ce949d-4bc2-5a1c-9efe-de263678249c|Distributed Algorithm for Subgraph Detection|The authors propose a distributed algorithm for subgraph detection in a graph. This algorithm uses a combination of color-coding and pipelining to detect subgraphs.
dcce33ff-a6a5-5563-85f3-4efa2a4863c6|Distributed CONGEST Approximation of Weighted Vertex Covers and Matchings|The authors propose a distributed algorithm for approximating the minimum weighted vertex cover and maximum weighted matching problems in the CONGEST model. The algorithm is designed to work efficiently in a distributed computing environment with limited memory and communication constraints.
03100ec4-7b52-5cc3-b8e6-8a5b77e40045|SUPPORTED CONGEST Model Algorithm|The authors propose an algorithm for the SUPPORTED CONGEST model, which is an extension of the classic CONGEST model. The algorithm is designed to work efficiently in a distributed computing environment with limited memory and communication constraints.
aa003385-99a5-5e8d-bc51-0b8c3c5de4e0|Diameter Reduction|The authors propose a diameter reduction technique to reduce the problem of approximating the minimum weighted vertex cover to the case of approximating the same problem on graphs of small diameter.
58c3cc9a-bc58-5ae0-80d0-d278a84069bf|Bipartite Double Cover|The authors propose a bipartite double cover technique to reduce the problem of approximating the minimum weighted vertex cover to the case of approximating the same problem on bipartite graphs.
f2f40aec-910c-57ea-9d83-f2a373df6cc6|Weighted Set Cover|The authors propose a weighted set cover technique to achieve a good approximation ratio for the minimum weighted vertex cover problem.
6db375e1-e5ba-56f8-b2cb-3a239e3c11d8|Deterministic Distributed Vertex Coloring|The authors propose a deterministic distributed vertex coloring algorithm that achieves a 2-approximation in O(log log n) rounds, significantly improving upon the previous best known deterministic algorithm.
4662e1fc-445f-5237-af9c-fb1c727edb77|Distributed Approximation of Weighted Vertex Covers and Matchings|The authors present a distributed algorithm for approximating the minimum weighted vertex cover and maximum weighted matching problems in the CONGEST model.
db9e5708-24a9-5190-b3a7-a58a07008956|Polylogarithmic Time Deterministic Network Decomposition|The authors propose a polylogarithmic time deterministic network decomposition algorithm that can be used to improve the efficiency of distributed algorithms.
ddfe3870-b83f-5e26-be8f-609cc9546cbf|Distributed Local Approximation Algorithms for Maximum Matching|The authors present a distributed local approximation algorithm for the maximum matching problem that achieves a 1-approximation in polylogarithmic time.
3b2b25a4-913a-5136-a495-1cd3f3ab073c|Clustering Approach for Reducing Problem Size|The authors propose a clustering approach to reduce the problem size by dividing the graph into smaller subgraphs. This approach is designed to handle irregular graph structures and reduce the communication overhead.
d6ad839e-956c-5a95-b55c-631223b21d42|Bipartite Double Cover for Handling Heterogeneous Graphs|The authors propose a bipartite double cover approach to handle heterogeneous graph structures with varying degrees, weights, and sparsity. This approach is designed to reduce the problem to a bipartite graph, which can be solved more efficiently.
80adebe1-49c5-5c7c-a837-126b4999c377|Fractional Matching Algorithm for Handling Weighted Graphs|The authors propose a fractional matching algorithm to handle weighted graphs with varying degrees, weights, and sparsity. This approach is designed to find a maximum weighted matching in the graph.
d3bc04ae-d6c5-5b84-a272-e87041d50e38|Cache Tiling|The authors propose a cache tiling approach to improve cache locality during the candidate generation stage of the similarity search. This involves dividing the inverted index into smaller blocks that can fit in the system cache, reducing latency during candidate generation.
7c5854b2-9100-59e2-84f9-d4ff8fe2a8f8|Mask-Based Hash Tables|The authors propose the use of mask-based hash tables to reduce the amount of memory required for storing query object values and meta-data during search. This allows for more efficient use of cache memory and reduces the number of cache misses.
772f86f5-cc05-5835-aa70-b77ba60602a9|Dynamic Task Partitioning|The authors propose a dynamic task partitioning approach to assign a small set of objects to a thread to process as soon as it has finished processing its previous assigned set. This approach helps to prevent loss of cache locality and improves the overall performance of the similarity search algorithm.
e7f980db-91d9-59df-b1c1-41ce2a973dbc|Query Vector Mask Hashing|The authors propose a query vector mask hashing approach to efficiently compute a hash key by using the mask 1 h to truncate the feature ID, where the ID is its position in the pre-defined global feature processing order, to the 0, h-1 domain.
753be57e-af8a-5d4b-b74c-882e5577357e|Cache Tiling with Dynamic Task Partitioning|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by employing cache tiling and dynamic task partitioning. The authors propose dividing the inverted index into blocks that fit in the system cache, reducing latency during candidate generation. Additionally, they use dynamic task partitioning to assign small sets of objects to threads, allowing for efficient processing and minimizing communication overhead.
9df23a5e-6a13-5237-8d00-aa388cc574e3|Bulk Synchronous Parallelism|The authors propose a bulk synchronous parallelism approach to process queries in bulk, forcing threads to read from the same subset of query vectors, which should be located in sequential memory blocks.
8bba5e09-5caa-5053-9c35-c790b4629897|Mask-Based Hash Table|The authors propose using a mask-based hash table to store query object values and meta data during search, allowing them to persist in the cache during candidate verification. Mechanisms/Techniques: The hash table array is initialized with negative values, and for each feature in the query vector, a hash key is computed by truncating the feature ID to the 0, h-1 domain using a mask. Offsets into the sparse query vector are stored at locations in the hash table corresponding to feature hash keys. Results: The authors report that this technique leads to few collisions in practice and allows for O(1) access times for most lookups.
04f35e1c-5c34-58fc-bdc5-482d9dda90d3|pL2AP|pL2AP is a multi-core parallel algorithm designed to efficiently solve the All-Pairs similarity search problem in high-dimensional sparse datasets. It employs cache tiling optimizations, combined with fine-grained dynamically balanced parallel tasks, to improve cache locality during similarity search. It uses a mask-based hash table to store query object values and meta-data, allowing for O(1) access times and reducing memory requirements. pL2AP achieves 1.5x-238x speedup over existing parallel baselines and 2x-34x speedup over the fastest serial method on datasets with hundreds of millions of non-zeros.
98150f94-6c31-53ef-aafa-35cac11bb486|Anchored Coreness-based Algorithm|The authors propose an anchored coreness-based algorithm to address the challenge of memory-efficient scalable graph processing. This algorithm employs in H index and out H index to compute the anchored corenesses in a distributed way, which helps to reduce memory consumption and optimize memory usage.
75a9357b-9fbe-51c8-be9f-b61775cf7ca6|Skyline Coreness-based Algorithm|The authors propose a skyline coreness-based algorithm to address the challenge of memory-efficient scalable graph processing. This algorithm uses a newly designed index, called D index, for D core decomposition through skyline coreness computing.
6e8085d0-d68e-56bf-8466-a3169c8cf111|Distributed Graph Processing Frameworks|The authors propose the use of distributed graph processing frameworks, such as vertex-centric and block-centric frameworks, to address the challenge of memory-efficient scalable graph processing.
5a1337d3-4c38-5afd-bd9b-69a8fd862b4e|Optimization Strategies|The authors propose three optimization strategies to improve the efficiency of the skyline coreness-based algorithm.
7369845a-45a1-55ef-ad45-0d4c8948e572|Anchored Coreness-based Distributed Algorithm|The authors propose an anchored coreness-based distributed algorithm to optimize communication efficiency in distributed D-core decomposition. This algorithm employs in-H index and out-H index to compute the anchored corenesses in a distributed way, reducing the need for extensive communication.
4fb6c9fe-581b-5b2a-aaab-22b03c0dd159|Skyline Coreness-based Distributed Algorithm|The authors propose a skyline coreness-based distributed algorithm to optimize communication efficiency in distributed D-core decomposition. This algorithm uses a novel concept of skyline coreness and a D-index to compute the skyline corenesses, reducing the need for extensive communication.
74a453c2-1ebd-54e1-9f78-0466184b636d|Block Centric Extension|The authors propose a block centric extension to the anchored coreness-based and skyline coreness-based algorithms to optimize communication efficiency in distributed D-core decomposition. This extension allows for local computation within blocks, reducing the need for extensive communication.
d1f64ef4-7093-51f5-a5f1-79f04c0428cc|Optimization 1: Pruning Disqualified Pairs|The authors propose an optimization technique to prune disqualified pairs in the skyline coreness-based algorithm, reducing the need for extensive communication.
2f5574db-d021-50c2-9284-169f278f023d|Optimization 2: Fast Computation of n-order D-index|The authors propose an optimization technique to fast compute the n-order D-index in the skyline coreness-based algorithm, reducing the need for extensive communication.
462dfb85-2cd4-5547-9dbf-356ccd868766|Anchored Coreness Based Algorithm|The authors propose an anchored coreness based algorithm to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm is designed to compute the anchored corenesses of vertices in a distributed manner, which can handle heterogeneous graph structures with varying degrees, weights, and sparsity. The algorithm uses a novel concept of anchored coreness, which is defined as the maximum value of k such that a vertex is contained in the (k, k) core. The algorithm iteratively computes the anchored corenesses of vertices using a distributed vertex-centric approach, which reduces communication overhead and enhances memory locality. The authors demonstrate the effectiveness of the anchored coreness based algorithm through experiments on large real-world graphs, showing that it outperforms existing peeling-based algorithms by up to 3 orders of magnitude in terms of running time and communication overhead.
32d44c5b-fc0f-59e2-ae76-4945da7f5648|Skyline Coreness Based Algorithm|The authors propose a skyline coreness based algorithm to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm is designed to compute the skyline corenesses of vertices in a distributed manner, which can handle heterogeneous graph structures with varying degrees, weights, and sparsity. The algorithm uses a novel concept of skyline coreness, which is defined as the pair (k, l) such that a vertex is contained in the (k, l) core and there is no other pair (k', l') such that k' > k and l' > l. The algorithm iteratively computes the skyline corenesses of vertices using a distributed vertex-centric approach, which reduces communication overhead and enhances memory locality. The authors demonstrate the effectiveness of the skyline coreness based algorithm through experiments on large real-world graphs, showing that it outperforms existing peeling-based algorithms by up to 3 orders of magnitude in terms of running time and communication overhead.
39cd916f-fb4e-5e59-9feb-55b933ea5249|D-Index Based Algorithm|The authors propose a D-index based algorithm to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm is designed to compute the D-index of vertices in a distributed manner, which can handle heterogeneous graph structures with varying degrees, weights, and sparsity. The algorithm uses a novel concept of D-index, which is defined as the set of pairs (k, l) such that a vertex is contained in the (k, l) core. The algorithm iteratively computes the D-index of vertices using a distributed vertex-centric approach, which reduces communication overhead and enhances memory locality. The authors demonstrate the effectiveness of the D-index based algorithm through experiments on large real-world graphs, showing that it outperforms existing peeling-based algorithms by up to 3 orders of magnitude in terms of running time and communication overhead.
6f6638ee-e49f-5154-bb41-c80b91349ec5|D-Index-based Algorithm|The D-index-based algorithm is designed to efficiently compute the D-index of all vertices in a directed graph, which is a crucial step in D-core decomposition. This algorithm addresses the challenge of efficient graph dynamics processing by providing a distributed vertex-centric approach to compute all feasible D-indexes simultaneously.
a6ff2a28-c935-5e14-b128-df3fc43576d8|Clique Compression|Clique compression is a technique used to reduce memory consumption by compressing the matches of large cliques in the graph. This is achieved by representing the matches in a compressed way and maintaining the compressed matches in further joins.
a7488332-babb-50ad-b9f4-f13458de144a|Star-Clique Preserved (SCP) Graph Storage|SCP graph storage is a technique used to reduce memory consumption by storing the graph in a way that supports both clique and star as join units. This allows for a better choice of join units, reducing the number of execution rounds and intermediate results.
482b4e23-df9e-5562-b6b5-b8b632b4ed50|Optimal Bushy Join Plan|Optimal bushy join plan is a technique used to reduce memory consumption by finding the optimal join plan for the query. This is achieved by considering all possible join plans and selecting the one that minimizes memory consumption.
bdf841ee-550c-5d97-8ae2-17be1edff08e|Star-Clique Preserved (SCP) Storage Mechanism|The SCP storage mechanism is a novel approach to storing data graphs in a distributed environment, allowing for both star and clique structures to be used as join units. This mechanism enables the algorithm to make a better choice between star and clique, reducing the number of execution rounds and intermediate results.
e59ada1d-8fee-57e5-b42e-4b29afa6564f|SCP Graph Storage Mechanism|The authors propose a novel graph storage mechanism called SCP (Star-Clique Preserved) to support both clique and star as join units. This mechanism is designed to handle heterogeneous graph structures with varying degrees, weights, and sparsity. The SCP storage mechanism introduces a small number of extra edges to the local graph, which helps to reduce the number of execution rounds and intermediate results. This is achieved by leveraging the power law random PR graph model for analysis and using a compact SCP graph storage mechanism that supports both clique and star as join units. The authors demonstrate that the SCP storage mechanism can significantly reduce the number of extra edges introduced by the local graph, leading to improved performance and scalability.
2871210c-07b0-5d88-a5c5-5d368d6e7a77|Dynamic Programming Algorithm for Optimal Bushy Join Plan|The dynamic programming algorithm is designed to optimize load balance in distributed systems by computing the optimal bushy join plan. This approach enables the efficient distribution of workloads across nodes, reducing the impact of workload imbalance and skewness.
266f6a83-a9a8-516c-94a6-0a314c6e3441|Clique Compression Technique|The clique compression technique is designed to optimize load balance in distributed systems by compressing large cliques in the data graph. This approach enables the efficient distribution of workloads across nodes, reducing the impact of workload imbalance and skewness.
f197af89-3577-5291-81d4-5ca0d0206080|Hierarchy of Sparse Neighborhood Covers|The authors propose a novel approach to optimize communication efficiency in distributed algorithms by utilizing a hierarchy of sparse neighborhood covers. This technique enables the construction of communication-efficient fragments, which are crucial for achieving time and message optimality in distributed MST algorithms.
341628f1-d68e-5058-8597-f29ce93846f6|Boruvka-style Merging Strategy|The authors propose a Boruvka-style merging strategy to optimize communication efficiency in distributed algorithms. This approach involves merging fragments in a manner that respects locality, using a local leader to ensure that MOE edges do not have to travel too far.
074d1677-548e-5634-8101-8c6d7e116ac9|Deterministic Singularly-Optimal Algorithm|The authors propose a deterministic singularly-optimal algorithm to optimize communication efficiency in distributed algorithms. This approach involves using a hierarchy of sparse neighborhood covers and a Boruvka-style merging strategy to achieve time and message optimality.
2c4111b8-2341-5ae0-8b1e-f07ce14e71b0|Round- and Message-Optimal Distributed Graph Algorithms|The authors propose a round- and message-optimal distributed graph algorithm to optimize communication efficiency in distributed algorithms. This approach involves using a hierarchy of sparse neighborhood covers and a Boruvka-style merging strategy to achieve time and message optimality.
7e08cbcc-c5fc-5d95-9a12-629419aa2034|Subgraph-Centric Graph Processing|The authors propose a subgraph-centric graph processing approach, where the focus is on processing subgraphs of interest rather than individual vertices. This approach is designed to reduce memory consumption and improve scalability by minimizing the amount of data that needs to be processed and stored.
3dd96350-b3f6-5ed7-ab37-67d460a9591d|In-Memory Subgraph Representation|The authors propose an in-memory subgraph representation technique that enables efficient graph computations over large-scale graphs. This technique is designed to minimize memory consumption and improve scalability by representing subgraphs as lists of vertices and edges, rather than as adjacency lists or matrices.
35494ab4-ea01-5765-b51c-2e492c26e132|Distributed Execution Engine|The authors propose a distributed execution engine that executes user computation on extracted subgraphs in parallel, reducing computational costs and improving scalability. This engine is designed to minimize communication overhead and improve memory efficiency by executing computation on subgraphs in memory.
0051e11b-90b2-510a-8798-98b471b21447|Subgraph-Centric Programming Framework|The authors propose a subgraph-centric programming framework, NSCALESPARK, which optimizes communication efficiency in distributed algorithms by allowing users to specify computations against entire subgraphs or multi-hop neighborhoods. This approach reduces the need for extensive communication and message passing between nodes.
0caa46e3-1146-5092-a802-3e9fc654cef2|Graph Extractor and Packing (GEP) Module|The GEP module is designed to extract and pack subgraphs of interest in a way that minimizes communication rounds and reduces the need for message passing. The module uses a cost-based optimizer to determine the optimal packing strategy.
5e27fc4c-c4c8-52bd-9a13-e0d0733cee7c|In-Memory Graph Data Structure|The authors propose an in-memory graph data structure that enables efficient graph computations over large-scale graphs. The data structure is designed to minimize memory footprint and reduce the need for communication rounds.
d3a04de5-7f9a-592b-b644-ae992bdf6cc2|NSCALESPARK|NSCALESPARK is a subgraph-centric graph programming framework that allows users to specify computations to be executed against a large number of multi-hop neighborhoods or subgraphs of the data graph.
fdca5757-5c90-5589-accb-759b0936f066|Subgraph Extraction Query|The subgraph extraction query allows users to specify the subgraphs of interest and the graph computation to be executed on them using the NSCALESPARK user API.
df8154ab-3a56-5a20-81db-2f110cc604a1|Graph Extraction and Packing (GEP) Module|The GEP module extracts relevant subgraphs of interest and uses a cost-based optimizer for data replication and placement that minimizes the number of machines needed, while attempting to balance load across machines to guard against the straggler effect.
9159053a-a4b2-5239-a811-c583923ea392|Cost-based Optimizer for Data Replication and Placement|The authors propose a cost-based optimizer for data replication and placement to minimize the number of machines needed while balancing load across machines. This optimizer is part of the Graph Extraction and Packing (GEP) module in the NSCALESPARK framework.
3aaa8a0c-91a2-5baf-a906-57921d048d0e|Graph Library|The graph library is a mechanism that provides the data structures for holding the subgraphs in memory as well as the bitmap implementations required for the subgraph-centric programming framework.
b5928a54-77e6-59ec-9af4-7dc1a70f3a69|Pattern-Aware Hardware Accelerator|The authors propose a pattern-aware hardware accelerator for graph pattern mining (GPM), which provides a specialized execution unit for set operations and a connectivity map (c-map) to memoize reusable neighborhood information. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing memory accesses and computation through pattern-aware optimization.
0233114e-52c8-513a-8d0d-afaea54692c4|Pattern-Aware Hardware Accelerator for Graph Pattern Mining|The authors propose a pattern-aware hardware accelerator for graph pattern mining, which provides a novel solution to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This accelerator is designed to efficiently process complex graph patterns and adapt to varying graph structures.
6bfdf155-28d0-5f2f-ab71-9d6a9f7b2945|dMaximalCliques Algorithm|The dMaximalCliques algorithm is a distributed algorithm designed to enumerate all maximal cliques in a graph. It addresses the challenge of memory-efficient scalable graph processing by reducing the scale of the graph through a locality property of the clique, which allows the problem to be solved in a distributed fashion.
78fbf405-10e8-51f5-a741-7da9f75f8583|Graph Partitioning Method|The graph partitioning method is used to divide the graph into smaller subgraphs, which are then processed in parallel by multiple computing nodes. This method addresses the challenge of memory-efficient scalable graph processing by reducing the memory consumption and computational costs associated with processing large-scale graphs.
7cd69c34-dab6-5050-9689-bafc9c298e36|Filtering Process|The filtering process is used to remove repeated maximal cliques and cliques contained by other cliques. This process addresses the challenge of memory-efficient scalable graph processing by reducing the memory consumption associated with storing and processing large-scale graph data.
88e942f6-bf53-5d92-9ed1-c6c56f47e8ef|Pre-deletion|Pre-deletion is a technique used to reduce the scale of the subgraph Gv by considering one edge only once, to the extent possible. This is done by pruning the edges that have already been considered when processing a vertex, thereby reducing the number of edges that need to be processed.
bb8b1085-14ef-56fc-988e-ab245e10645e|Filtering|Filtering is a technique used to remove repeated maximal cliques and cliques contained by other cliques from the results. This is done by checking the size of the clique and the number of cliques of the same size.
20057ec3-96a4-5e97-be82-148d5263e1aa|Random Partition Method|The random partition method is a technique used in the dMaximalCliques algorithm to divide the graph into smaller subgraphs. It addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by reducing the repeated occurrences of vertices in different subgraphs.
de516546-7d29-5e80-8f62-64b55aa09feb|Pruning Mechanism|The pruning mechanism is a technique used in the dMaximalCliques algorithm to reduce the scale of the subgraphs. It addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by removing unnecessary edges and vertices from the subgraphs.
b0caa368-c8cc-5019-905f-5d6c1bad7eb5|BFS-based Partition Method|The authors also propose a BFS-based partition method, which groups a vertex and its neighbors in the same Si. This approach helps in reducing the repeated occurrences of a vertex v in different subgraphs GSi, thereby optimizing load balance in distributed systems.
71ed38c8-609d-5fce-99ca-3969b56c610f|dMaximalCliques|dMaximalCliques is a distributed algorithm designed to efficiently enumerate all maximal cliques in a graph, addressing the challenge of efficient graph dynamics processing by minimizing computational costs and iterations.
077d37ac-66ce-5102-8573-0978a12ffec4|Distributed CSR Format|The authors propose a distributed CSR format to efficiently store the input graph in a distributed environment. This format ensures that forward and reverse edges are stored with the same process, allowing for constant-time access to incoming and outgoing edges.
8f33b35e-79d3-5ff8-aaea-6bbd2ab707f8|Communication Aggregation|The authors propose a communication aggregation optimization to reduce the number of remote communications in the generated MPI code. This optimization is applied to Green Marl programs with a specific pattern of foreach loops iterating over edges or vertices.
a7de8617-6913-5ad2-962c-119212051fd3|Distributed Map Implementation|The authors propose a distributed map implementation to store and retrieve key-value pairs in a distributed environment. This implementation uses a hash function to map each key to the beginning of a linked list, and a table of dummy nodes to store the linked lists.
3d78b666-3a1c-5712-90f1-a3d11762be5d|CommonNbrs Iterator|The authors propose a translation scheme for the CommonNbrs iterator in Green Marl, which iterates over the common neighbors of two nodes. This scheme involves getting the neighbors of the two nodes, populating the common neighbors in a vector, and iterating over the vector.
6650314d-a722-5310-bdd9-e1b1c27a0833|BFS Iterator|The authors propose a translation scheme for the BFS iterator in Green Marl, which iterates over all the reachable nodes of a source node in BFS order. This scheme involves maintaining a local vector of nodes to visit, and iterating over the nodes in the local vector.
cb5e23bf-c958-55e5-86f0-ec0793edfe19|Communication Aggregation Optimization|This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by aggregating remote writes into one or more arrays, reducing the number of remote communications.
58bc13e3-f87b-5711-abdc-51deefe72f19|Bounded Incremental Evaluation (IncEval)|IncEval is a technique used in the GRAPE system to reduce redundant local computations in iterative graph computations. It reuses partial results from the last superstep to speed up the computation.
23cc0c24-41cb-5ef9-8980-2bf07ed8119f|Query Preserving Compression|Query preserving compression is a technique used in GRAPE to reduce memory usage by compressing graph fragments while preserving the query results.
64bcefd9-2015-5818-996d-7c3b35cabd87|Dynamic Grouping|Dynamic grouping is a technique used in GRAPE to reduce memory usage by dynamically grouping nodes and edges in the graph.
11cf7cd9-cdaa-5a18-a604-6cd5cd611b76|Edge Cut Partition Strategy|Edge cut partition strategy is a technique used in GRAPE to reduce communication overhead by partitioning the graph along edges.
9649fd9c-011f-54a9-82e1-12236d0a2680|Neighborhood Index|Neighborhood index is a technique used in GRAPE to reduce memory usage by indexing the neighborhood of nodes in the graph.
dee4dac8-93fe-549c-a831-c85fdeee07b2|Bounded Incremental Evaluation|The authors propose using bounded incremental evaluation to reduce the cost of iterative computations in distributed algorithms. This approach involves using incremental algorithms that can be expressed as a function of the size of changes in the input and output, rather than the size of the entire input.
ab45e216-c232-59e6-bacf-34adb02cecdf|Data Partitioned Parallelism|The authors propose using data partitioned parallelism to reduce communication costs in distributed algorithms. This approach involves partitioning the data into smaller fragments and processing each fragment in parallel.
ec3e5c17-77cc-5904-8db7-2d10f1d89264|Message Composition|The authors propose using message composition to reduce communication costs in distributed algorithms. This approach involves composing messages from variables with changed values and deducing their designations by referencing the graph partition.
8d7c08bd-e641-5836-9b3a-609a1008fda4|Partial Evaluation and Incremental Computation|The authors propose a solution that combines partial evaluation and incremental computation to parallelize sequential graph algorithms. This approach allows for the adaptation of algorithms to heterogeneous and irregular graphs by breaking down the computation into smaller, more manageable pieces.
265cd909-4258-5b8a-81d1-56a93cfedbe9|Graph Fragmentation and Data Partitioning|The authors propose a solution that involves fragmenting the graph into smaller subgraphs and partitioning the data across multiple processors. This approach enables the algorithm to adapt to heterogeneous and irregular graphs by processing each subgraph independently.
786771d9-30c9-5c08-b983-cd1fad7a7df4|Partial Evaluation|This solution addresses the challenge of efficient graph dynamics processing by employing partial evaluation to minimize the cost of computations. It involves using a sequential algorithm to compute the partial results for each fragment of the graph, and then combining these partial results to obtain the final output.
228407d7-3b76-5295-972b-c896b075f9a5|Assemble Partial Results|This solution addresses the challenge of efficient graph dynamics processing by employing a function to assemble partial results from each fragment of the graph. It involves combining the partial results to obtain the final output.
87c8caf6-7c0d-51f0-ad8b-236a8adbfc36|Load Balancer|This solution addresses the challenge of efficient graph dynamics processing by employing a load balancer to minimize the cost of computations. It involves computing an assignment of work units to physical workers to minimize both computational cost and communication cost.
8793a8ec-d11a-5f7d-8c41-6cf63cc5f567|Indexing|This solution addresses the challenge of efficient graph dynamics processing by employing indexing to minimize the cost of computations. It involves creating an index on the graph to speed up query processing.
47387990-9abd-5fda-91ea-9dbca9fa0dcf|Fault Tolerance|This solution addresses the challenge of efficient graph dynamics processing by employing fault tolerance to minimize the cost of computations. It involves using an arbitrator mechanism to recover from both worker failures and coordinator failures.
d5f3fc9b-20df-597e-8bbf-7cee0da92bac|Consistency Control|This solution addresses the challenge of efficient graph dynamics processing by employing consistency control to minimize the cost of computations. It involves using a consistency control strategy to ensure the consistency of the graph.
10c1495b-7db1-5126-9691-c2d8f6a1e6e5|Communication Optimization Techniques|NOT GIVEN
8a835854-2a10-5e5f-906b-694aad84ea01|Random Walk Sampling with Connector Nodes|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a novel random walk sampling technique that utilizes connector nodes to reduce the number of communication rounds. The technique involves generating short random walks from each node, which are then stitched together to form longer walks. Connector nodes are used to connect these short walks, allowing the algorithm to efficiently sample nodes in the network.
a88b7f3d-2d79-58d9-9383-7d8d861e77fd|Path Verification with Breakpoints|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a novel path verification technique that utilizes breakpoints to minimize communication rounds. The technique involves dividing the network into left and right subtrees, with breakpoints used to connect these subtrees. The algorithm then verifies the path by communicating only along the edges of the path.
7f292823-bcf7-5130-87e3-d8b90c80724f|Distributed Random Walk Algorithm with Reservoir Sampling|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a novel distributed random walk algorithm that utilizes reservoir sampling to minimize communication rounds. The algorithm involves generating random walks from each node, which are then sampled using reservoir sampling to reduce the number of communication rounds.
8517b7e1-4c36-517e-ac23-6537fc84ecd2|Adaptive Random Walk Algorithm|The authors propose an adaptive random walk algorithm that can efficiently handle heterogeneous and irregular graphs by dynamically adjusting the walk length and using a reservoir sampling technique to avoid congestion. The algorithm uses a combination of short and long walks to adapt to the graph structure, and the reservoir sampling technique allows it to handle varying degrees and weights in the graph. This approach is different from existing methods that typically use fixed walk lengths or do not account for graph heterogeneity. The paper shows that the adaptive random walk algorithm can achieve a significant reduction in communication overhead and improve memory locality, leading to better performance in distributed and GPU-based systems.
9b59943b-3fe3-5017-a542-af9641c04a67|Decentralized Estimation of Mixing Time|The authors propose a decentralized algorithm for estimating the mixing time of a graph, which is essential for handling heterogeneous and irregular graphs. The algorithm uses a combination of random walks and a decentralized approach to estimate the mixing time, which allows it to adapt to the graph structure and handle variations in density and connectivity. The paper shows that the decentralized estimation algorithm can achieve accurate estimates of the mixing time with high probability, which is essential for optimizing graph processing algorithms.
274088a7-0e1a-53ca-bd18-f43589f86582|Random Spanning Tree Algorithm|The authors propose a random spanning tree algorithm that can efficiently handle heterogeneous and irregular graphs by using a combination of random walks and a decentralized approach. The algorithm uses a combination of random walks and a decentralized approach to construct a random spanning tree, which allows it to adapt to the graph structure and handle variations in density and connectivity. The paper shows that the random spanning tree algorithm can achieve a significant reduction in communication overhead and improve memory locality, leading to better performance in distributed and GPU-based systems.
8dd1f13e-ba9a-52fe-ae0e-8e602e21ec47|Path Verification Algorithm|The authors propose a path verification algorithm to optimize load balance in distributed systems. This algorithm is designed to verify the correctness of a given path in a distributed network, while minimizing the number of rounds required to complete the verification process. The algorithm uses a novel approach to path verification, which involves dividing the path into smaller segments and verifying each segment independently. This approach helps to reduce the number of rounds required to complete the verification process and ensures uniform task allocation. The authors demonstrate the effectiveness of their algorithm through theoretical analysis, showing that it can achieve a significant reduction in the number of rounds required to complete the verification process, compared to existing approaches.
6fdc53ac-925e-5e0d-8317-425bedc4106d|Mixing Time Estimation Algorithm|The authors propose a mixing time estimation algorithm to optimize load balance in distributed systems. This algorithm is designed to estimate the mixing time of a Markov chain in a distributed network, while minimizing the number of rounds required to complete the estimation process. The algorithm uses a novel approach to estimating mixing times, which involves using a distributed random walk algorithm to sample nodes and edges in the network. This approach helps to ensure uniform task allocation and reduces the number of rounds required to complete the estimation process. The authors demonstrate the effectiveness of their algorithm through theoretical analysis, showing that it can achieve a significant reduction in the number of rounds required to complete the estimation process, compared to existing approaches.
8b7267af-3b1c-5aa3-a371-3bae241df998|Multi-Level 2-Hop (ML2hop) Indexing|The authors propose a novel distributed indexing scheme called Multi-Level 2-Hop (ML2hop) to address the challenge of memory-efficient scalable graph processing. ML2hop is designed to reduce memory consumption and optimize memory usage in handling large-scale graph data.
9619d85f-43f5-53d0-8dba-8006f59eb728|Boundary Graph Indexing|The authors propose a boundary graph indexing approach to address the challenge of memory-efficient scalable graph processing. This approach involves constructing a boundary graph that collects and stores the reachability information of pairs of boundary vertices at each partition.
ca265912-9952-56f7-8dbd-761fe3f1d80b|Pruned Labeling (PVL) Indexing|The authors propose a pruned labeling (PVL) indexing approach to address the challenge of memory-efficient scalable graph processing. PVL involves building two label sets for each vertex and proposes several pruning strategies to enhance query efficiency.
c74faaec-a697-5abb-b110-0ed43bc7b1f6|Distributed Set Reachability (DSR) Indexing|The authors propose a distributed set reachability (DSR) indexing approach to address the challenge of memory-efficient scalable graph processing. DSR involves constructing a partition-specific boundary graph as a static reachability index and storing it in each partition.
e8c64175-2d82-5b69-ace8-67a443097a8b|Bi-Directional Query Algorithm (MLQA)|The authors propose a bi-directional query algorithm, called MLQA, which utilizes the ML2hop index to optimize communication efficiency in distributed algorithms. This solution specifically addresses the challenge by reducing the number of communication rounds required for set reachability queries in distributed environments.
159fbf76-2326-5abb-ab69-f4c168ac1357|Multi-Level 2-Hop Labeling Index (ML2hop)|The authors propose a novel distributed indexing scheme called Multi-Level 2-Hop Labeling Index (ML2hop) to efficiently handle set reachability queries in a distributed environment. This solution specifically addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by providing a multi-level index structure that can be efficiently constructed in Pregel-like systems to restrict message exchange among all different partitions within one single round.
4a6d0ea4-f0dd-5082-a37e-25549e6d4459|Incremental Index Maintenance|The authors propose an incremental index maintenance approach to efficiently update the ML2hop index under edge insertions. This approach updates the inner paths in each subgraph based on the inner 2-hop index of the affected vertices.
f10e6381-575f-5c1d-9f58-ada6aa1fd542|Nearly Most Balanced Sparse Cut Computation|The authors propose a distributed algorithm for computing a nearly most balanced sparse cut in a graph, which is a cut that separates the graph into two components with a small number of edges between them.
66ddf9e8-0f95-552b-ac47-55253e5657b1|Hierarchical Structure for Distributed Computation|The authors propose a hierarchical structure for distributed computation, which is used to reduce the communication overhead and enhance memory locality.
3f4ca742-23bc-5e9b-a070-cb4ca46b469d|Low Diameter Decomposition|The authors propose a low diameter decomposition algorithm that is used to decompose the graph into components of low diameter. This algorithm is designed to address the challenge of optimizing load balance in distributed systems by reducing the diameter of the graph, which can help to improve communication efficiency.
14209a3d-88cf-5947-b72f-55d2e6aac2fb|Expander Decomposition Algorithm|The authors propose an expander decomposition algorithm that can be used to decompose a graph into smaller components with high conductance.
8fda06b9-5bbb-5373-ab48-4e9c9f1c0a1c|Distributed Routing Algorithm|The authors propose a distributed routing algorithm that can be used to efficiently route messages in a graph.
e5da1060-66fa-56eb-a13b-bd1d4a31775b|Pre-partitioned Triangle Enumeration (PTE)|PTE is a distributed algorithm designed to enumerate triangles in massive graphs while minimizing memory consumption and optimizing memory usage. PTE employs a pre-partitioning strategy to divide the graph into smaller subgraphs, which are then processed in parallel. This approach reduces memory consumption by avoiding the need to store the entire graph in memory. Additionally, PTE uses a scheduling function to minimize the number of edge sets that need to be retained in memory at a time, further reducing memory usage. The paper reports that PTE outperforms existing algorithms in terms of memory efficiency, with a reduction in memory consumption of up to 245 times on the ClueWeb12 dataset.
8925eb61-72f6-5705-86aa-212534c80c85|Color-coded Edge Partitioning|This solution involves partitioning edges based on their vertex colors to minimize memory consumption and optimize memory usage. By partitioning edges based on their vertex colors, the algorithm can reduce the number of edge sets that need to be retained in memory at a time. This approach also enables the algorithm to process triangles in parallel, further reducing memory consumption. The paper reports that this solution reduces memory consumption by up to 175 times on the ClueWeb09 dataset.
850727e2-d6ca-58ae-a1ee-3084a5094a9d|Scheduling Function for Edge Sets|This solution involves using a scheduling function to determine the order in which edge sets are processed, with the goal of minimizing memory consumption and optimizing memory usage. The scheduling function takes into account the number of edge sets that need to be retained in memory at a time and schedules them accordingly. This approach enables the algorithm to minimize memory consumption by avoiding the need to store unnecessary edge sets in memory. The paper reports that this solution reduces memory consumption by up to 245 times on the ClueWeb12 dataset.
78e516bf-e3b3-5a5d-8712-f2cae63ac930|Pre-Partitioned Triangle Enumeration (PTE)|PTE is a distributed algorithm designed to optimize communication efficiency in triangle enumeration by minimizing the amount of shuffled data, total work, and network read. PTE achieves this by pre-partitioning the graph into sets of edges, storing them in a distributed storage, and then enumerating triangles in each subproblem using a single map step. This approach reduces the amount of shuffled data from O(E^3/2M) to O(E), where E is the number of edges in the graph. The paper demonstrates that PTE outperforms the state-of-the-art distributed algorithm, CTTP, by up to 47% in terms of running time and successfully enumerates more than 3 trillion triangles in the ClueWeb12 graph, which contains 6.3 billion vertices and 72 billion edges.
a5d29f1e-b55a-596e-95c7-0179ea1199c0|Color-Directed Edge Processing (CDEP)|CDEP is a technique used in PTE to remove redundant operations by considering the color direction of edges. CDEP works by intersecting the neighbor sets of two vertices to find triangles, instead of considering all possible edges. This approach eliminates unnecessary computations and reduces the total work. The paper shows that CDEP improves the performance of PTE by reducing the number of redundant operations.
72760582-18fa-5f09-a077-7f6217d652db|Scheduling Function for Triangle Enumeration (SFTE)|SFTE is a scheduling function used in PTE to minimize network read by carefully scheduling triangle computations in subproblems. SFTE works by restricting the number of edge sets read in each subproblem, reducing the amount of network read from O(E^3/2M) to O(E). The paper demonstrates that SFTE improves the performance of PTE by reducing the amount of network read.
d1441930-dd88-550f-a66d-734b35198c3c|Color Direction-based Redundancy Removal|This solution removes redundant operations in triangle enumeration by considering the color direction of edges.
1760903b-dc04-561e-9bce-5436a7897651|Scheduling Function-based Network Read Minimization|This solution minimizes network read by carefully scheduling triangle computations in subproblems.
f237cad2-de47-5f40-b618-78630b736947|PTE (Pre-partitioned Triangle Enumeration)|PTE is a distributed algorithm designed to efficiently enumerate triangles in massive graphs by minimizing the amount of shuffled data, total work, and network read. PTE exploits pre-partitioning to decrease the shuffled data, considers color direction to remove redundant operations, and carefully schedules triangle computations in subproblems to shrink the amount of network read. PTE outperforms the state-of-the-art scalable distributed algorithm by up to 47% and successfully enumerates more than 3 trillion triangles in the ClueWeb12 graph with 6.3 billion vertices and 72 billion edges.
5674db25-0ac7-5ee2-b1b0-e6dffb32e36b|Bulk Synchronous Parallel (BSP) Style Lazy Updating Scheme|The authors propose a BSP style lazy updating scheme to reduce network communication overhead while maintaining vertex position accuracy. This approach updates vertex information only when vertices on the same machine are changed, rather than updating per vertex or per iteration.
92c662ad-503a-582b-9ad2-d85b10115874|Score Metric-Based Label Propagation (LP) Algorithm|The authors propose an LP algorithm that uses a score metric to identify vertices that should be relocated, rather than processing all vertices. This approach reduces the computation volume and communication overhead.
5b53d899-7eff-5464-a5ef-2dab2728d430|Stabilization Process with Higher Connectivity to Remote Vertices (HCRV)|The authors propose a stabilization process that changes the graph topology based on the most required vertices in each partition, rather than on their lower VP score. This approach prevents the algorithm from becoming trapped in local optima.
89679332-b714-53b4-8aa7-6e033cdb8257|Lazy Update BSP Communication Paradigm|The authors propose a lazy update BSP communication paradigm to optimize communication efficiency in distributed algorithms. This approach involves updating vertex information only when vertices on the same machine are changed, reducing the frequency of network communication.
f048af49-6fa2-57cf-93e9-76cc031b925f|Score Metric with VP Score|The authors propose a score metric with VP Score to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution focuses on calculating the appropriateness of a vertex in its current partition based on the number of local and remote edges.
2ebea322-04e0-5fe4-b1e7-a4a24743f29c|Stabilization Process with HCRV|The authors propose a stabilization process with HCRV to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution focuses on relocating vertices in an order that provides each partition with almost the same number of vertices and preventing the number of edges in a particular partition from becoming too large.
aa610e85-23c3-545a-9511-15bdd8969050|Edge Balanced Partitioning Process|The authors propose an edge balanced partitioning process to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution focuses on maintaining the near-perfect edge balance by prioritizing edge balance after the QCLP and stabilization process.
0bab613b-2f57-5a41-93bf-e40e2aec6696|Score-Based Approach with VP Score|The authors propose a score-based approach that uses a vertex partition score (VP Score) to indicate the appropriateness of a vertex in its current partition. This approach prioritizes vertex balance and reduces the number of candidate vertices in every iteration of the LP process.
19dcf07d-600e-5439-96a4-cf5dea6b07c2|Quick Convergence Label Propagation (QCLP) Algorithm|The authors propose a novel QCLP algorithm to address the challenge of efficient graph dynamics processing. This algorithm is designed to reduce computation and increase the quality of edge cuts by implementing the LP process only for limited vertices based on the VP Score.
4b6b9bf7-01bb-511f-a9e5-a0550b3b09cf|Stabilization Process|The authors propose a stabilization process to address the challenge of efficient graph dynamics processing. This process is designed to prevent the algorithm from becoming trapped in local optima by relocating highly connected vertices.
940c736c-8fa7-5cce-8a17-41019dc70d09|Bloom Filter Optimization|The authors propose using a Bloom filter to reduce the number of intermediate results generated during the join operation, thereby decreasing memory usage.
eb597ad9-ae67-555a-b904-da94392fd8b4|Shuffle Selection Optimization|The authors propose selecting the join operand with fewer distinct keys to reduce shuffling, thereby minimizing memory usage and communication overhead.
4da8d437-73a1-5003-8171-ce5c1c27fac5|Triangle Structure-Based Structure Similarity Calculation|The authors propose using a triangle structure to calculate structure similarity, reducing the number of adjacent nodes that need to be considered.
7da65b04-d969-5df1-8530-3f110c46059e|Distributed Network Clustering Algorithm (CASS)|The authors propose a distributed network clustering algorithm (CASS) that uses structure similarity to calculate the weights of edges, reducing memory usage and communication overhead.
d1b42c50-4f6f-545e-9ead-51895c5d79cc|Structure Similarity Reuse|The authors propose reusing the calculated structure similarity to minimize the number of communication rounds. The authors store the calculated structure similarity and reuse it when necessary, reducing the number of communication rounds. This approach is different from existing methods as it uses a caching mechanism to minimize communication overhead. The paper shows that the structure similarity reuse reduces the running time by approximately 50% for the DBLP dataset.
1a6a227d-4082-598c-8625-c519f326d631|Structure Similarity Calculation Optimization|The authors propose using a triangle structure to calculate structure similarity, which in turn reduces memory usage and execution time.
7f0320b2-c394-50dc-b4a4-d0527f871840|Network Reconstructing Optimization|The authors propose reconstructing the network by pruning edges and assigning labels to nodes, which in turn reduces memory usage and execution time.
a990ceca-7ca4-5a1f-950b-d9369ef34aa5|Triangle Structure-Based Join Operations|The authors propose using triangle structures to calculate structure similarity between nodes, which is a key component of their clustering algorithm. This approach involves two join operations to find triangles in the graph, allowing for the efficient calculation of structure similarity.
0ff64e56-00ff-5436-934e-5ec1f44adc6a|Bloom Filter-Based Optimization|The authors propose using a Bloom filter to optimize the join operation, reducing the number of intermediate results and improving performance.
2a090447-b3ef-55c6-b3d0-995d0f06a41f|Shuffle Selection-Based Optimization|The authors propose using shuffle selection to optimize the join operation, reducing the number of intermediate results and improving performance.
22fdb68c-8a57-55b2-87c4-1a4ff783dd5a|Distributed Network Clustering Algorithm|The authors propose a distributed network clustering algorithm that can efficiently analyze large-scale networks.
6b77d535-afb8-59f8-a736-53c0ab87d3bc|GraphX-based Graph Parallel Computation|The authors propose using GraphX, a graph abstraction library on top of Spark, to implement graph parallel computation for SPARQL querying of RDF data. This approach combines graph parallel and data parallel computation in a single system, allowing for seamless combination of graph and record-centric views of data without data movement or duplication.
6ef67a3d-715b-510e-a2f9-1d11ae5f3902|Property Graph Representation of RDF|The authors propose a property graph representation of RDF data, which combines the graph structure with vertex and edge properties. This representation enables the use of graph parallel computation and data parallel computation in a single system.
b166393c-63e7-5801-9c24-03761a2a02b2|BGP Matching using GraphX|The authors propose using GraphX to implement BGP matching, a key component of SPARQL querying. This approach enables the efficient processing of large RDF graphs using graph parallel computation.
4a00483d-03a2-5a14-8910-08ff92cf4136|Data Parallel Computation using Spark|The authors propose using Spark, a general-purpose in-memory cluster computing system, to implement data parallel computation for SPARQL querying of RDF data. This approach enables the efficient processing of large RDF graphs using data parallel computation.
742c9c8c-bc8f-51e8-b067-052908fe3889|Round Elimination Technique|The round elimination technique is a method used to prove lower bounds on the time required to solve locally checkable problems in the distributed setting. It involves iteratively relaxing the problem by adding allowed configurations to the lists collecting the allowed configurations for the nodes or the hyperedges, at the cost of making the problem potentially easier to solve.
871d934b-3b38-57fc-b442-80de5c47a305|Automatic Round Elimination Framework|The automatic round elimination framework is a general outline for proving lower bounds in the distributed setting. It provides a systematic way to apply the round elimination technique to a wide range of problems.
30f03b84-bf4b-515d-9521-234e512818a1|Decomposing a Problem|Decomposing a problem involves breaking down a complex problem into smaller sub-problems, each of which can be solved more efficiently.
0d98cff1-b2b2-52db-928c-5e9c1cf410ce|Relaxing a Problem|Relaxing a problem involves adding further configurations to the lists collecting the allowed configurations for the nodes or the hyperedges, at the cost of making the problem potentially easier to solve.
6ee52b53-cb57-595d-9479-88f41c6b0b88|Label Merging|Label merging involves combining multiple labels into a single label, in order to reduce the number of labels in the problem.
7575572b-3543-5212-ba58-7bd0e0dcd24a|Hypergraph MIS Algorithm|The authors propose a hypergraph MIS algorithm that can handle heterogeneous graph structures with varying degrees, weights, and sparsity. The algorithm is designed to overcome load imbalance, reduce communication overhead, and enhance memory locality in distributed systems.
226c1106-6675-51df-b479-56ad3581b2c4|Round Elimination Framework|The authors propose a round elimination framework that can be used to prove lower bounds for hypergraph MIS. The framework is designed to handle heterogeneous graph structures and can be used to prove lower bounds for other graph problems as well.
fc9af478-c15f-55fd-b18e-e94ea559c53b|O(2 log log r) O(log n) Algorithm|The authors propose an algorithm that can solve hypergraph MIS in O(2 log log r) O(log n) rounds. The algorithm is designed to handle heterogeneous graph structures with varying degrees, weights, and sparsity.
f994e490-683e-5b72-82aa-918b0c7b4abe|Hypergraph-Based Load Balancing|The authors propose a novel approach to load balancing in distributed systems by utilizing hypergraphs to model the system and its workloads. This method involves representing the system as a hypergraph, where nodes represent processors or nodes, and hyperedges represent tasks or workloads. The authors then apply hypergraph-based algorithms to optimize the distribution of workloads across the nodes, ensuring a more balanced and efficient allocation of resources.
23d9e781-9079-5a3f-a5e0-aef715134488|Distributed Load Balancing via Hypergraph MIS|The authors propose a distributed algorithm for load balancing in hypergraphs, which is based on the concept of maximal independent sets (MIS). The algorithm works by iteratively computing MIS in the hypergraph, which represents the system and its workloads. The MIS is then used to allocate tasks to nodes, ensuring a balanced distribution of workloads.
cd35159f-158c-58f8-935f-3ae03ea7eea0|Round Elimination-Based Load Balancing|The authors propose a novel approach to load balancing in distributed systems, which is based on the round elimination technique. This technique involves iteratively eliminating rounds of communication between nodes, while maintaining the correctness of the load balancing algorithm. The authors apply this technique to hypergraph-based load balancing, enabling a more efficient and scalable algorithm.
58778028-f0e4-5ec4-bca2-094cb98a3200|Hypergraph Maximal Matching Algorithm|The authors propose a deterministic algorithm for solving the maximal matching problem on hypergraphs, which is another fundamental problem in graph dynamics processing. The algorithm is designed to work efficiently in the LOCAL model of distributed computing.
f9dd4ccc-a249-5c36-b749-6f5808c75132|Fixed Point Relaxation|The authors propose a fixed point relaxation technique for proving lower bounds on the time required to solve locally checkable problems on trees or hypertrees. The technique is designed to work efficiently in the LOCAL model of distributed computing.
6e864196-f50d-5fab-840e-aa2d7e02aa12|Hypergraph Coloring Algorithm|The authors propose a deterministic algorithm for solving the coloring problem on hypergraphs, which is a fundamental problem in graph dynamics processing. The algorithm is designed to work efficiently in the LOCAL model of distributed computing.
4a836704-5c3a-5f9f-835e-08b1abcfb9fb|Unique Maximum Coloring Algorithm|The authors propose a deterministic algorithm for solving the unique maximum coloring problem on hypergraphs, which is a fundamental problem in graph dynamics processing. The algorithm is designed to work efficiently in the LOCAL model of distributed computing.
6f070030-ea89-54e5-88a6-f1526501cd68|Distributed Lower Bounds|The authors propose a technique for proving lower bounds on the time required to solve locally checkable problems on trees or hypertrees in the LOCAL model of distributed computing.
ad93de32-aa0f-559f-bd2f-4585151f024c|Hypergraph Maximal Matching Lower Bound|The authors propose a lower bound on the time required to solve the maximal matching problem on hypergraphs in the LOCAL model of distributed computing.
b4e80683-2482-5fd7-8fbe-47f0643d082f|Hypergraph MIS Lower Bound|The authors propose a lower bound on the time required to solve the MIS problem on hypergraphs in the LOCAL model of distributed computing.
8f952d72-dc3c-5e9e-8a6e-5f313cb30473|Hypergraph Coloring Lower Bound|The authors propose a lower bound on the time required to solve the coloring problem on hypergraphs in the LOCAL model of distributed computing.
62c4ab3a-9b60-5628-90f2-9cd932dea763|Unique Maximum Coloring Lower Bound|The authors propose a lower bound on the time required to solve the unique maximum coloring problem on hypergraphs in the LOCAL model of distributed computing.
72507226-9837-58cb-a3f9-2f599123b884|Distributed Algorithm for Hypergraph MIS|The authors propose a distributed algorithm for solving the MIS problem on hypergraphs in the LOCAL model of distributed computing.
e933a771-2f6b-5925-802d-32b5a8fcdc9e|Distributed Algorithm for Hypergraph Maximal Matching|The authors propose a distributed algorithm for solving the maximal matching problem on hypergraphs in the LOCAL model of distributed computing.
ab68eb02-0a0a-5bf1-8d15-00cb8c0f9614|Distributed Algorithm for Hypergraph Coloring|The authors propose a distributed algorithm for solving the coloring problem on hypergraphs in the LOCAL model of distributed computing.
a76ddcdc-58b1-5ee9-988f-b40a2a7fe3cd|Distributed Algorithm for Unique Maximum Coloring|The authors propose a distributed algorithm for solving the unique maximum coloring problem on hypergraphs in the LOCAL model of distributed computing.
6fb437b6-e438-571e-950f-58863203c649|Round Elimination Technique for Hypergraph MIS|The authors propose a round elimination technique for proving lower bounds on the time required to solve the MIS problem on hypergraphs in the LOCAL model of distributed computing.
f1e67476-53bb-5091-b85f-a962a3f43c2b|Round Elimination Technique for Hypergraph Maximal Matching|The authors propose a round elimination technique for proving lower bounds on the time required to solve the maximal matching problem on hypergraphs in the LOCAL model of distributed computing.
5b4f3aac-b218-5109-b08e-8ab1c50ae03a|Round Elimination Technique for Hypergraph Coloring|The authors propose a round elimination technique for proving lower bounds on the time required to solve the coloring problem on hypergraphs in the LOCAL model of distributed computing.
c2712066-7c02-55cb-9557-7428986829a2|Round Elimination Technique for Unique Maximum Coloring|The authors propose a round elimination technique for proving lower bounds on the time required to solve the unique maximum coloring problem on hypergraphs in the LOCAL model of distributed computing.
360dd879-7a03-5ae2-a8fc-4e9bfbb8a447|Distributed Algorithm for Hypergraph MIS with Unique Maximum Coloring|The authors propose a distributed algorithm for solving the MIS problem on hypergraphs with unique maximum coloring in the LOCAL model of distributed computing.
fc327ead-15bc-5319-88b3-11b7683dc98f|Distributed Algorithm for Hypergraph Maximal Matching with Unique Maximum Coloring|The authors propose a distributed algorithm for solving the maximal matching problem on hypergraphs with unique maximum coloring in the LOCAL model of distributed computing.
c3296b4f-d49f-5231-bc62-bcac1f56541d|Distributed Algorithm for Hypergraph Coloring with Unique Maximum Coloring|The authors propose a distributed algorithm for solving the coloring problem on hypergraphs with unique maximum coloring in the LOCAL model of distributed computing.
0158cd21-9f4c-5b01-9827-4c76e0d0fc88|Distributed Algorithm for Unique Maximum Coloring with Hypergraph MIS|The authors propose a distributed algorithm for solving the unique maximum coloring problem on hypergraphs with MIS in the LOCAL model of distributed computing.
d6f5ee10-5e7a-5e6c-9415-1ec714d01956|Distributed Algorithm for Unique Maximum Coloring with Hypergraph Maximal Matching|The authors propose a distributed algorithm for solving the unique maximum coloring problem on hypergraphs with maximal matching in the LOCAL model of distributed computing.
62b7de08-8b3e-514e-b010-69a74c0767f5|Distributed Algorithm for Unique Maximum Coloring with Hypergraph Coloring|The authors propose a distributed algorithm for solving the unique maximum coloring problem on hypergraphs with coloring in the LOCAL model of distributed computing.
94ae7276-b83d-5623-92dd-8cc61c0bb7fb|Distributed Algorithm for Hypergraph MIS with Coloring|The authors propose a distributed algorithm for solving the MIS problem on hypergraphs with coloring in the LOCAL model of distributed computing.
b1d9bcf3-97c7-5c3a-9bc8-c7e407dcefdf|Distributed Algorithm for Hypergraph Maximal Matching with Coloring|The authors propose a distributed algorithm for solving the maximal matching problem on hypergraphs with coloring in the LOCAL model of distributed computing.
b4e6c28b-0282-55dd-9814-1cec2910bb1c|Distributed Algorithm for Hypergraph Coloring with MIS|The authors propose a distributed algorithm for solving the coloring problem on hypergraphs with MIS in the LOCAL model of distributed computing.
b2df3794-3f11-511c-8eb8-694d6d309bd6|Distributed Algorithm for Hypergraph Coloring with Maximal Matching|The authors propose a distributed algorithm for solving the coloring problem on hypergraphs with maximal matching in the LOCAL model of distributed computing.
931181aa-1e4d-5913-ab75-05e17b7477f6|Distributed Algorithm for Unique Maximum Coloring with Hypergraph MIS and Coloring|The authors propose a distributed algorithm for solving the unique maximum coloring problem on hypergraphs with MIS and coloring in the LOCAL model of distributed computing.
7c996bd9-a3cf-510f-9e93-cae37a5aac4c|Distributed Algorithm for Unique Maximum Coloring with Hypergraph Maximal Matching and Coloring|The authors propose a distributed algorithm for solving the unique maximum coloring problem on hypergraphs with maximal matching and coloring in the LOCAL model of distributed computing.
0e1a3897-6784-5e32-b921-91618276169c|Distributed Algorithm for Hypergraph MIS with Unique Maximum Coloring, Coloring, and Maximal Matching|The authors propose a distributed algorithm for solving the MIS problem on hypergraphs with unique maximum coloring, coloring, and maximal matching in the LOCAL model of distributed computing.
84f82a35-4048-53e7-9926-e7558c6cd150|Distributed Algorithm for Hypergraph Maximal Matching with Unique Maximum Coloring, Coloring, and MIS|The authors propose a distributed algorithm for solving the maximal matching problem on hypergraphs with unique maximum coloring, coloring, and MIS in the LOCAL model of distributed computing.
176d5997-b946-543a-bfd3-1f0afc6162c5|Distributed Algorithm for Hypergraph Coloring with Unique Maximum Coloring, Coloring, and Maximal Matching|The authors propose a distributed algorithm for solving the coloring problem on hypergraphs with unique maximum coloring, coloring, and maximal matching in the LOCAL model of distributed computing.
95dcc822-3847-5224-9073-6ec8bc6a30c7|Distributed Algorithm for Unique Maximum Coloring with Hypergraph MIS, Coloring, and Maximal Matching|The authors propose a distributed algorithm for solving the unique maximum coloring problem on hypergraphs with MIS, coloring, and maximal matching in the LOCAL model of distributed computing.
3cc2d01c-a1e5-5488-9433-4a53cd20549e|Distributed Algorithm for Hypergraph MIS with Unique Maximum Coloring, Coloring, Maximal Matching, and Hypergraph Coloring|The authors propose a distributed algorithm for solving the MIS problem on hypergraphs with unique maximum coloring, coloring, maximal matching, and hypergraph coloring in the LOCAL model of distributed computing.
2925799a-229a-50c3-876a-36106a3419b8|Distributed Algorithm for Hypergraph Maximal Matching with Unique Maximum Coloring, Coloring, Maximal Matching, and Hypergraph Coloring|The authors propose a distributed algorithm for solving the maximal matching problem on hypergraphs with unique maximum coloring, coloring, maximal matching, and hypergraph coloring in the LOCAL model of distributed computing.
2e21bcf2-e13c-5fa3-a773-ad6528113dc5|Distributed Algorithm for Hypergraph Coloring with Unique Maximum Coloring, Coloring, Maximal Matching, and Hypergraph Coloring|The authors propose a distributed algorithm for solving the coloring problem on hypergraphs with unique maximum coloring, coloring, maximal matching, and hypergraph coloring in the LOCAL model of distributed computing.
8e521365-5b87-5963-8cbe-9b2968e0ac6a|Distributed Algorithm for Unique Maximum Coloring with Hypergraph MIS, Coloring, Maximal Matching, and Hypergraph Coloring|The authors propose a distributed algorithm for solving the unique maximum coloring problem on hypergraphs with MIS, coloring, maximal matching, and hypergraph coloring in the LOCAL model of distributed computing.
bc003dcc-588b-5bbb-9880-fccb04f05cce|Distributed Algorithm for Hypergraph MIS with Unique Maximum Coloring, Coloring, Maximal Matching, Hypergraph Coloring, and Hypergraph Maximal Matching|The authors propose a distributed algorithm for solving the MIS problem on hypergraphs with unique maximum coloring, coloring, maximal matching, hypergraph coloring, and hypergraph maximal matching in the LOCAL model of distributed computing.
6927b75a-f557-571b-8028-8dcc47bc085e|Distributed Algorithm for Hypergraph Maximal Matching with Unique Maximum Coloring, Coloring, Maximal Matching, Hypergraph Coloring, and Hypergraph Maximal Matching|The authors propose a distributed algorithm for solving the maximal matching problem on hypergraphs with unique maximum coloring, coloring, maximal matching, hypergraph coloring, and hypergraph maximal matching in the LOCAL model of distributed computing.
4ea8767e-7cb8-55ec-bc72-43ffc4942d19|Distributed Algorithm for Hypergraph Coloring with Unique Maximum Coloring, Coloring, Maximal Matching, Hypergraph Coloring, and Hypergraph Maximal Matching|The authors propose a distributed algorithm for solving the coloring problem on hypergraphs with unique maximum coloring, coloring, maximal matching, hypergraph coloring, and hypergraph maximal matching in the LOCAL model of distributed computing.
bd4d0c16-4e51-50de-aec3-2d875bf70e0f|Distributed Algorithm for Unique Maximum Coloring with Hypergraph MIS, Coloring, Maximal Matching, Hypergraph Coloring, and Hypergraph Maximal Matching|The authors propose a distributed algorithm for solving the unique maximum coloring problem on hypergraphs with MIS, coloring, maximal matching, hypergraph coloring, and hypergraph maximal matching in the LOCAL model of distributed computing.
57c92dcc-12c6-53cd-b651-c5bc34a6b777|Hashing-Based 2D Graph Partitioning|The authors propose a hashing-based 2D graph partitioning scheme to address the challenge of memory-efficient scalable graph processing. This solution involves partitioning the graph into smaller subgraphs using a hashing function, which ensures that each subgraph can fit in the GPU memory. The hashing function is designed to distribute the vertices and edges evenly across the partitions, reducing memory consumption and communication overhead.
ebf02375-f197-5ee8-b037-a424b7a21e14|Degree-Aware Resource Allocation|The authors propose a degree-aware resource allocation mechanism to address the challenge of memory-efficient scalable graph processing. This solution involves assigning different computing and shared memory resources to vertices with dissimilar degrees, which helps to reduce memory consumption and improve performance.
75fa1742-a752-57a5-a8dd-473ca56a5916|Atomic Operation-Based Dynamic Workload Assignment|The authors propose an atomic operation-based dynamic workload assignment mechanism to address the challenge of memory-efficient scalable graph processing. This solution involves using atomic operations to dynamically assign workloads to threads, which helps to reduce memory consumption and improve performance.
3448f787-b639-55eb-8b13-ca41f9412333|Interleaved Hash Table Layout|The authors propose an interleaved hash table layout to address the challenge of memory-efficient scalable graph processing. This solution involves storing the hash buckets of each hash table in an interleaved manner, which helps to reduce memory consumption and improve performance.
64f227b0-79cf-5fbb-ab8b-fa0ae894d6c4|Linear Search with Coalesced Memory Access|The authors propose a linear search with coalesced memory access to address the challenge of memory-efficient scalable graph processing. This solution involves using linear search to find the common elements in the hash table, which helps to reduce memory consumption and improve performance.
f522dab0-c437-536e-9a0d-7c95e5caa281|Hashing-Based 2D Partitioning|The authors propose a hashing-based 2D partitioning scheme to optimize communication efficiency in distributed algorithms. This solution specifically addresses the challenge by reducing the number of communication rounds required for triangle counting.
9b796cc6-8021-52e8-8aba-737e9d0446ad|Collision and Workload Imbalance Co-Optimization|The authors propose a co-optimization technique to address both collision and workload imbalance in distributed algorithms. This solution specifically addresses the challenge by reducing the number of collisions and balancing the workload across vertices.
3759ff2b-a313-5898-91f0-5146c1e30be1|Virtual Combination for Workload Balancing|The authors propose a virtual combination method to balance the intra-vertex workload in distributed algorithms. This solution specifically addresses the challenge by reducing the number of idling threads and balancing the workload across vertices.
5f222c29-3554-57e6-bd27-a9e5ebeb27b1|Vertex-Centric Hashing-Based Triangle Counting|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by proposing a vertex-centric hashing-based triangle counting algorithm. The algorithm constructs a hash table for the neighbor list of each vertex and then searches for triangles by intersecting the hash table with the neighbor lists of other vertices.
1dd31236-201e-5393-888b-24945271b017|Graph and Workload Collaborative Partitioning|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by proposing a graph and workload collaborative partitioning scheme. The scheme partitions the graph into smaller subgraphs and distributes the workloads across multiple GPUs.
165f8389-72e8-5116-be40-2a3d912265db|Caching Hash Table Elements in Shared Memory|The authors propose caching the first few items of each bucket in shared memory to reduce global memory access latency. This solution involves storing hashTable i:len in shared memory and interleaving the hash buckets of each hashTable to cache the first few items of each bucket in shared memory.
4f5f524f-9811-56b2-b02c-c7f397943eca|Path Decomposition Algorithm|The authors propose a path decomposition algorithm to address the challenge of memory-efficient scalable graph processing. This algorithm partitions the graph into edge-disjoint short paths, allowing for efficient processing and reducing memory consumption.
6398e662-e24b-54e1-8908-854bca83d1cb|Weak Orientation Algorithm|The authors propose a weak orientation algorithm to address the challenge of memory-efficient scalable graph processing. This algorithm orients the edges of the graph such that each node has a sufficient number of outgoing edges, enabling efficient contraction and reducing memory consumption.
4a0c3113-fe7a-51c1-a69f-00b572cc2928|Half Path Decomposition Algorithm|The authors propose a half path decomposition algorithm to address the challenge of memory-efficient scalable graph processing. This algorithm partitions the graph into edge-disjoint half paths, allowing for efficient processing and reducing memory consumption.
0d3e5bb1-59cd-5307-acab-fc5ed5ac7454|Distributed Algorithm for Degree Splitting|The authors propose a distributed algorithm for degree splitting to address the challenge of memory-efficient scalable graph processing. This algorithm partitions the graph into edge-disjoint subgraphs, allowing for efficient processing and reducing memory consumption.
e49dfd43-7a66-59ba-ac25-d81007f59144|Path Decomposition Technique|The authors propose a path decomposition technique to optimize communication efficiency in distributed algorithms. This technique involves partitioning the edge set of a graph into edge-disjoint short paths, allowing each node to be the start or end of at most paths. By doing so, the authors aim to reduce the number of communication rounds required for distributed algorithms.
a3301bfe-4190-5863-86a8-ad2ab0beaf26|Iterative Contraction Technique|The authors propose an iterative contraction technique to optimize communication efficiency in distributed algorithms. This technique involves iteratively contracting edges in a graph, reducing the degree of nodes and making it possible to obtain a strong orientation with a small discrepancy.
92c8faad-be8e-5abb-973a-a1df71f9a33f|Edge Contraction Algorithm|The authors propose an edge contraction algorithm to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm contracts edges in a graph to reduce the degree of nodes.
4bf64af4-ca28-58d8-919c-671b73e109eb|Distributed Degree Splitting Algorithm|The authors propose a distributed degree splitting algorithm to optimize load balance in distributed systems. This algorithm partitions the edge set of a given graph into edge-disjoint short paths, ensuring that each node is the start or end of at most paths. By orienting each path in the path decomposition, the algorithm achieves a strong orientation, which is used to optimize load balance.
e89eeebf-5862-5129-930e-c816953034ff|Contraction Algorithm|The authors propose a contraction algorithm to optimize load balance in distributed systems. This algorithm merges nodes and reduces the graph size, making it more efficient to compute the orientation.
b79519db-2c22-5a46-a5b2-8aea541683da|Path Decomposition with Weak Orientation|This solution addresses the challenge of efficient graph dynamics processing by proposing a path decomposition algorithm that utilizes weak orientation to minimize computational costs and iterations. The algorithm iteratively applies a weak orientation algorithm to reduce the maximum degree of the graph, resulting in a low-degree path decomposition that can be efficiently processed.
7fad42ce-02b6-57b4-9f1f-53552cc285c7|Edge Contraction with Orientation|This solution addresses the challenge of efficient graph dynamics processing by proposing an edge contraction algorithm that utilizes orientation to minimize computational costs and iterations. The algorithm contracts edges in parallel while maintaining the orientation of the graph, resulting in a reduced graph that can be efficiently processed.
69209374-e17d-5266-b80a-864076a3a79c|Distributed Degree Splitting|This solution addresses the challenge of efficient graph dynamics processing by proposing a distributed degree splitting algorithm that minimizes computational costs and iterations. The algorithm splits the graph into smaller subgraphs and processes them in parallel, resulting in a significant reduction in computational costs and iterations.
94f1a5d8-16df-5a6c-afcd-9dc6bd0ec048|Non-Overlapping Partitioning Scheme|The authors propose a non-overlapping partitioning scheme to divide the input network into a set of P subgraphs Gi, where each subgraph Gi is constructed by partitioning the set of nodes V into P mutually disjoint subsets Vks, for 0 k P 1, such that, S k Vk V . Node set Vi, along with Nv for all v Vi, constitutes the disjoint non-overlapping portion of the partition Gi.
b229f3f1-cab1-5c20-bfc4-9b5384c1d108|Surrogate Approach|The authors propose a surrogate approach to eliminate redundant messages and reduce communication overhead. In this approach, processor i sends Nv to processor j, and processor j scans Nv to find all nodes u Nv such that u Vj. For all such nodes u, processor j counts triangles incident on edge u, v by performing the operation Nv Nu.
14d6f94d-5e58-5773-9e7b-03b972e8cc4f|Dynamic Load Balancing Scheme|The authors propose a dynamic load balancing scheme to reduce idle time of processors and improve performance. In this scheme, the algorithm divides the total computation into several tasks and assigns them dynamically. The coordinator assigns tasks, receives notifications, and reassigns tasks to idle workers.
06d00fe5-66c8-50d4-ae21-c85cd73308e1|Surrogate Approach for Message Reduction|The authors propose a surrogate approach to reduce message redundancy in the communication between processors. This approach eliminates the need for redundant messages by allowing processors to count triangles incident on edges without exchanging messages.
a62680fa-0d51-57a3-8484-b2f053556663|Dynamic Task Granularity Adjustment|This solution involves dynamically adjusting the size of tasks assigned to processors based on the remaining workload and the number of processors available. The goal is to minimize idle time and ensure that all processors are utilized efficiently.
7f554eb7-8eab-5ff0-9d7d-7a546d286bf0|Surrogate Approach for Redundant Message Elimination|This solution involves eliminating redundant messages exchanged between processors by using a surrogate approach. The surrogate approach allows processors to count triangles incident on a node without exchanging redundant messages.
cb41163b-e364-5b98-84d1-70da296cc9e5|Estimation Function for f(v)|This solution involves using an estimation function to estimate the computational cost of counting triangles for each node.
d6c55219-6732-5f39-839c-94242c65d1f1|Core Number Ordering|The authors propose using core number ordering to reduce the complexity of maximal clique enumeration (MCE) in common real-world graphs. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the computational cost of MCE, which is a crucial step in graph analysis.
a5681de1-4fc2-5b15-b430-bda9ff5f1445|Distributed Maximal Clique Enumeration|The authors propose a distributed algorithm for MCE that can be implemented in a shared-nothing architecture. This solution specifically addresses the challenge of memory-efficient scalable graph processing by distributing the computation of maximal cliques across multiple machines.
0ab81c60-de52-5895-ae94-108f8007d01f|Update Maintenance|The authors propose an algorithm for incrementally updating the set of maximal cliques when the underlying graph is updated. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the computational cost of updating the maximal cliques.
bccc70e1-40c6-53ca-b35a-1a300580751f|Data Distribution with Vertex Ordering|The authors propose a data distribution method that utilizes vertex ordering to reduce the amount of data being communicated in the distributed algorithm. By ordering the vertices based on their core number, degree, or degeneracy, the algorithm can minimize the number of edges that need to be distributed, resulting in improved communication efficiency.
fe74431b-6314-5b68-abe2-ecc7e6273f82|LocalMCE with Set Intersection Optimization|The authors propose an optimization technique for the LocalMCE algorithm that reduces the cost of set intersections. By intersecting smaller sets first, the algorithm can minimize the number of operations required, resulting in improved communication efficiency.
c5edf7ee-ee17-5230-b1f4-9facd94cd25c|Degeneracy Ordering|The authors propose using degeneracy ordering to optimize load balance in distributed systems. This involves ordering the vertices of the graph by their degeneracy numbers, which helps to reduce the size of the subgraphs that need to be processed by each worker. The degeneracy ordering technique is based on the concept of graph degeneracy, which is a measure of the minimum degree of a vertex in a subgraph. By ordering the vertices by their degeneracy numbers, the authors can ensure that the subgraphs processed by each worker are more balanced, which helps to optimize load balance. The authors report that using degeneracy ordering reduces the running time of the algorithm by a factor of 1.60 on average when the number of machines is doubled.
548e6699-0b31-5eba-b83b-f53771e06c30|Core Number Ordering for Maximal Clique Enumeration|This solution addresses the challenge of efficient graph dynamics processing by proposing a novel ordering of vertices based on their core numbers for maximal clique enumeration. The core number ordering is used to reduce the computational cost of MCE by minimizing the number of iterations and set intersections.
0d57bf79-2bbb-5a99-8a76-c48b600af717|Distributed Maximal Clique Computation with Prefix Trees|This solution addresses the challenge of efficient graph dynamics processing by proposing a distributed algorithm for maximal clique computation using prefix trees. The algorithm is designed to minimize computational costs and iterations by leveraging the properties of prefix trees.
51b12735-d12b-57d8-9318-d61ede00e6c0|LocalMCE Algorithm for Maximal Clique Enumeration|This solution addresses the challenge of efficient graph dynamics processing by proposing a novel algorithm for maximal clique enumeration, called LocalMCE. The algorithm is designed to minimize computational costs and iterations by leveraging local properties of the graph.
b3286a68-8706-5210-8f78-48bd07d3eba0|Update Maintenance for Maximal Cliques|This solution addresses the challenge of efficient graph dynamics processing by proposing an algorithm for update maintenance of maximal cliques. The algorithm is designed to minimize computational costs and iterations by leveraging the properties of prefix trees.
e860dae1-ca38-56ce-a323-f63acd6baffd|Distributed Graph Simulation with Bounded Response Time and Data Shipment|This solution proposes a distributed algorithm for graph simulation with bounded response time and data shipment. The algorithm uses a dependency graph to track linked vertices across different machines, allowing for local graph simulation with minimal data shipment.
e824228f-b380-5435-80b6-29b9e0c149b3|Subgraph-Centric Distributed Graph Pattern Matching|This solution proposes a subgraph-centric approach for distributed graph pattern matching, which considers the fragment of vertices located at the same machine as one computing unit. This approach reduces communication costs by avoiding delays in exchanging information between vertices of the same subgraph.
315388a8-09d5-5a4b-99aa-b5f4d66cbf3f|Asynchronous Vertex-Centric Distributed Graph Pattern Matching|This solution proposes an asynchronous vertex-centric approach for distributed graph pattern matching, which allows for concurrent processing of vertices and reduces communication costs.
4b4eb59a-dd18-5ed3-81a6-82f06933bde4|Distributed Algorithm for Strict Simulation|This solution proposes a distributed algorithm for strict simulation, which scales to large graphs by using a subgraph-centric approach and avoiding communication between subgraphs with well-defined diameters.
dbc12434-9fd0-5280-9484-cb674bdbf616|Distributed Algorithm for Graph Simulation with Cubic Time Complexity|This solution proposes a distributed algorithm for graph simulation with cubic time complexity, which uses a subgraph-centric approach and avoids communication between subgraphs with well-defined diameters.
310cf548-2052-518e-833f-d8914676007a|Distributed Algorithm for Subgraph Isomorphism with Bounded Response Time and Data Shipment|This solution proposes a distributed algorithm for subgraph isomorphism with bounded response time and data shipment, which uses a dependency graph to track linked vertices across different machines.
33af8bb3-9baa-53a2-9cdc-51db053f8ea0|Distributed Algorithm for Graph Pattern Matching with Bounded Response Time and Data Shipment|This solution proposes a distributed algorithm for graph pattern matching with bounded response time and data shipment, which uses a dependency graph to track linked vertices across different machines.
39774000-448d-5269-96c9-087704e80773|Distributed Algorithm for Subgraph Matching with Bounded Response Time and Data Shipment|This solution proposes a distributed algorithm for subgraph matching with bounded response time and data shipment, which uses a dependency graph to track linked vertices across different machines.
d03810bd-fd03-5465-ac8e-2536c2578177|Distributed Algorithm for Subgraph Isomorphism with B|
5fa31481-1343-5d95-860e-4025bf2b9796|Dynamic Load Balancing through Task Fragmentation|This solution involves fragmenting tasks into smaller subtasks to achieve better load balance in distributed systems. The authors propose a task fragmentation approach to divide the tasks into smaller, more manageable pieces, which can be executed in parallel across different nodes. This approach helps to reduce the load imbalance by distributing the tasks more evenly across the nodes.
880e28f5-fc20-5a51-a5d3-49f16923e4aa|Load Balancing through Scheduling Algorithm|This solution proposes a scheduling algorithm that assigns tasks to nodes based on their available resources and computational capabilities. The algorithm takes into account the task requirements and node resources to ensure optimal load balance.
279aa3d2-7898-55d0-b905-6ebfac57104e|Load Balancing through Data Redistribution|This solution involves redistributing data across nodes to achieve better load balance. The authors propose a data redistribution approach that takes into account the computational requirements of each task and the available resources of each node.
55f2ed87-9f60-5e35-9fd4-1106251a7b8f|Distributed Subgraph Isomorphism|The authors propose a distributed algorithm for evaluating subgraph isomorphism on large graphs. This solution specifically addresses the challenge of efficient graph dynamics processing by distributing the data graph across multiple machines and processing the query in parallel.
61343e80-7bbb-58b6-be3d-74a1fec7de9d|Graph Simulation with Neighborhood Indexes|The authors propose a graph simulation algorithm that uses neighborhood indexes to accelerate the access to data graph vertices. This solution specifically addresses the challenge of efficient graph dynamics processing by reducing the number of iterations required to evaluate graph simulation.
dde8b9d6-95f3-5d55-b39d-2a65fa2f440a|Subgraph Centric Programming Model|The authors propose a subgraph centric programming model for distributed graph processing. This solution specifically addresses the challenge of efficient graph dynamics processing by allowing for more efficient processing of graph updates.
ba37e94c-4a1c-55be-bd3c-a11ba9b408d2|Asynchronous Vertex-Centric Algorithm|The authors propose an asynchronous vertex-centric algorithm for evaluating subgraph isomorphism on large graphs. This solution specifically addresses the challenge of efficient graph dynamics processing by allowing for more efficient processing of graph updates.
1508a426-5c6d-592a-a9af-435b3b9e5ec2|Distributed Keyword Search (DKS) Algorithm|The DKS algorithm is a novel parallel algorithm for relationship queries on large graphs, which addresses the challenge of memory-efficient scalable graph processing by utilizing a graph parallel computing paradigm.
5afc9653-cbd3-50a1-a864-858c16c872ce|Early Exit from BFS Traversal|The early exit from BFS traversal is a technique used in the DKS algorithm to stop the BFS exploration when the exit criterion is satisfied, without missing the optimal answer.
6e95a6e9-fdb4-5715-860e-b5e1e3c89d29|Liltered Local Tree|The filtered local tree is a data structure used in the DKS algorithm to store the top-K partial answers of each keyword set in the local tree.
1f928699-6459-567b-b5ad-d656f03f93a1|Deep Message Propagation|The deep message propagation is a technique used in the DKS algorithm to propagate the deep messages recursively to cover cases where the answer tree is not fully contained in the local tree.
9e0e7611-0295-5e84-93d6-996a1706c6b7|Local Tree Filtering|The authors propose a local tree filtering technique to optimize communication efficiency in distributed algorithms. This solution involves filtering the local tree of a node to remove branches that are not part of the top-K partial answers.
8db23c41-ed46-55ad-9275-9d4b7a825b96|Aggregator-Based Approach|The authors propose an aggregator-based approach to optimize communication efficiency in distributed algorithms. This solution involves using aggregators to remove duplicate answers, identify global top-K answers, and calculate the largest path lengths of all keyword sets.
0024a18e-de17-5784-bae5-831fe3bace89|Shifted Shortest Path Approach|
3e4bd648-998d-57f2-a017-2261efcae6a9|Randomized Distributed Algorithm|The authors propose a randomized distributed algorithm for constructing a strong network decomposition. The algorithm involves each vertex sampling a radius from a geometric or exponential distribution and broadcasting this value to its neighbors. The algorithm then uses these values to determine the clusters in the graph.
586e7d72-4912-5bab-ab9f-2ff169ae74b9|Network Decomposition|The authors propose using network decomposition to efficiently process graph dynamics. Network decomposition is a partition of the graph into clusters with small diameter. The authors show that network decomposition can be used to solve other graph problems, such as finding a maximal independent set.
74e9d1e7-a4ff-5de9-8507-ba30739415e2|Shifted Shortest Path Approach with Improved Number of Blocks|The authors propose an improved version of the shifted shortest path approach that reduces the number of blocks in the network decomposition. The approach involves using a different distribution for the random values assigned to vertices.
e6be6bb8-ee9d-5bbb-8133-f7d3e1fcdbb6|CONGESTED CLIQUE Algorithm for Triangle Enumeration|The authors propose a CONGESTED CLIQUE algorithm for triangle enumeration, which is designed to address the challenge of memory-efficient scalable graph processing by minimizing communication between processors. The algorithm works by first partitioning the vertex set V into V1 Vn^1/3 locally, without communication. Then, it allocates the less than n triads T n j1, j2, j3 1 j1 j2 j3 n^1/3 to the vertices in Vin in a specific way. Each vertex v V that is assigned a triad j1, j2, j3 is responsible for learning the set of all edges E Vj1, Vj2. The unique mechanism involved in this solution is the use of a specific allocation strategy for triads, which is designed to minimize communication between processors. This strategy is different from existing approaches because it takes into account the degree of vertices in the subgraph and allocates triads accordingly. Quantitative results from the paper show that the algorithm can enumerate all triangles in O(n^1/3) rounds.
e6f3c573-8c56-5afc-a641-23d8dd39dff1|Distributed Nibble Algorithm|The authors propose a Distributed Nibble algorithm, which is designed to address the challenge of memory-efficient scalable graph processing by employing advanced data structures to ensure both efficiency and scalability in managing graph-structured data. The algorithm works by first generating Kb c log m Vol V 2b number of b tokens at the root x. Then, it executes a random walk starting at vi Sg b, and calculates Vol vi t 1..j for all indices j such that Vol vi t 1..j Vol vi t 1..j 1 Vol vi t 1..j. The unique mechanism involved in this solution is the use of a random walk starting at vi Sg b, which is designed to employ advanced data structures to ensure both efficiency and scalability in managing graph-structured data. This mechanism is different from existing approaches because it takes into account the degree of vertices in the subgraph and executes a random walk accordingly. Quantitative results from the paper show that the algorithm can find a sparse cut in O(log^9 m) rounds, with a success probability of 1 - 1/poly(m).
40b13241-afc1-5919-8f3c-004c3f9577a8|Local Triangle Enumeration Algorithm|The authors propose a local triangle enumeration algorithm that can handle heterogeneous and irregular graphs by identifying triangles in a localized manner.
01eab559-ce97-58e9-876a-afa0c12f1f6b|Low Degree Subroutine|The Low Degree subroutine is a technique used in the Distributed Nibble algorithm to partition the edge set of a subgraph into two parts: E_s and E_r. E_s has arboricity at most n, and E_r is the remaining edge set. The subroutine uses a combination of graph partitioning and routing algorithms to achieve load balance. It first identifies a set of vertices with low degree and then partitions the edge set into two parts based on the conductance of the subgraphs. The subroutine achieves a load balance of O(n^(-1/2)) in O(n^1/2) rounds, which is a significant improvement over previous algorithms.
95bf43d7-cfb0-5c3e-b706-7ff52a28fef4|Low Conductance Subroutine|The authors propose a low conductance subroutine that can be used to find a sparse cut in a subgraph with low conductance. This subroutine is designed to efficiently process dynamic updates in large graphs by minimizing computational costs and iterations.
2550325d-b898-51e0-b992-6f9c330b669f|High Diameter Subroutine|The authors propose a high diameter subroutine that can be used to find a sparse cut in a subgraph with high diameter. This subroutine is designed to efficiently process dynamic updates in large graphs by minimizing computational costs and iterations.
c01f5ecd-cd1e-5c7d-8399-9502d2f5da7f|Accumulator-Based Interface for User-Defined Aggregation|The authors propose an accumulator-based interface for user-defined aggregation, which allows for more efficient memory usage by storing only the partially aggregated values of groups, rather than the groups themselves.
d89f518a-cb22-522c-b7e6-13a4b32d27f1|Partial Aggregation with Bounded Memory|The authors propose a partial aggregation strategy that uses bounded memory to reduce memory consumption. This approach processes a bounded number of chunks of input records in parallel, reducing the memory required for aggregation.
f4ddb81f-e8ed-5dc8-a704-052a97424ef0|Aggregation Tree|The authors propose an aggregation tree approach to reduce network traffic and memory consumption. This approach performs partial aggregation within each rack before transmitting data across the network.
b8a3e068-21be-5f68-986a-b8a7cf28531d|DryadLINQ Optimization|The authors propose a DryadLINQ optimization approach that extends the DryadLINQ programming interface and set of optimizations. This approach enables the system to automatically generate optimized execution plans for user-defined aggregation.
f35867ec-b864-5e69-ba37-d382c3052337|Partial Aggregation with Accumulator Interface|This solution proposes using an accumulator-based interface for user-defined aggregation in distributed algorithms, which allows for partial aggregation to be performed locally before sending the results to other nodes for final aggregation. This approach reduces the amount of data that needs to be communicated between nodes, thereby optimizing communication efficiency.
1639ea13-9f8b-5b63-9f5e-3d4510d2746d|Decomposable Functions for Distributed Aggregation|This solution introduces the concept of decomposable functions, which can be broken down into smaller sub-functions that can be executed in parallel. This approach enables distributed aggregation to be performed more efficiently, reducing the number of communication rounds required.
b1e1c548-b513-524c-979a-6defb0aef230|Aggregation Tree Optimization|This solution proposes optimizing the aggregation tree structure to reduce the number of communication rounds required for distributed aggregation. The aggregation tree is optimized by minimizing the number of nodes and edges, reducing the amount of data that needs to be communicated between nodes.
39e02bed-e5a6-56eb-a922-71351b7eb392|Bounded-Size Strategies for Distributed Aggregation|This solution proposes using bounded-size strategies for distributed aggregation, which limit the amount of data that can be aggregated at each node. This approach reduces the number of communication rounds required by limiting the amount of data that needs to be communicated between nodes.
987fc485-43b8-5ccd-b767-88d814cd04f6|Iterative Parallel Greedy Coloring Algorithm|The authors propose an iterative parallel greedy coloring algorithm that addresses the challenge of memory-efficient scalable graph processing by utilizing speculation and iteration to minimize memory consumption and optimize memory usage.
0580c8d1-2154-5196-b63b-5e10f2786b87|Dataflow-Based Coloring Algorithm|The authors propose a dataflow-based coloring algorithm that addresses the challenge of memory-efficient scalable graph processing by utilizing fine-grained synchronization mechanisms to minimize memory consumption and optimize memory usage.
245eed07-439a-5a83-b880-e204bda54bd8|Dataflow Algorithm for Coloring|This solution addresses the challenge of efficient graph dynamics processing by proposing a dataflow algorithm for coloring that is specifically designed for massively multithreaded architectures like the Cray XMT. The algorithm uses a dataflow approach to process the graph, allowing for efficient and adaptive processing of dynamic updates.
9bc57e02-575e-5752-abfc-48d5c984b87d|Delta Optimization|The authors propose a delta optimization technique that allows the system to share information between vertices only when it is relevant for each vertex's neighbors, thus reducing communication overhead and memory usage.
d3c53f45-2323-54d4-88b6-88fa0910d546|Functional Interface|The authors propose a functional interface that decomposes Pregel's compute into a set of smaller functions, which can help reduce memory usage and improve scalability.
a5a8ca8d-4141-53ea-830f-1a43f3db5da1|Edge Pruning Optimization|The Edge Pruning Optimization solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the number of edges in the graph, thereby decreasing memory consumption. This is achieved by iteratively removing nodes from the graph and pruning edges that do not contribute to the discovery of connected components.
e21bd9db-04d9-55c9-99c7-6be322561931|Oblivious Seed Optimization|The Oblivious Seed Optimization solution addresses the challenge of memory-efficient scalable graph processing by reducing the number of messages exchanged between nodes. This is achieved by propagating the seed to all vertices belonging to the connected component without requiring additional information.
e01cff59-b58d-536b-aa6e-0b9bbd2e7703|Finish Computation Sequentially Optimization|The Finish Computation Sequentially Optimization solution addresses the challenge of memory-efficient scalable graph processing by reducing the number of MapReduce iterations. This is achieved by monitoring the size of the active subgraph and performing the remaining processing serially when the size goes below a given threshold.
e3bfa995-f54d-577c-a1a7-ccb682fe546f|Edge Pruning (EP)|The Edge Pruning (EP) solution is a technique used to reduce the number of messages exchanged between nodes in a distributed algorithm. It works by identifying and removing unnecessary edges in the graph, thereby decreasing the amount of communication required between nodes.
13ceb7dc-1b12-5730-941f-75eefffacf7e|Oblivious Seed (OS)|The Oblivious Seed (OS) solution is a technique used to reduce the number of seed identifications in a distributed algorithm. It works by identifying and propagating the smallest node identifier seen so far, without considering the actual seed value.
618177ad-3eaa-5bf9-bc88-ffcc0f8161fa|Finish Computation Sequentially (FCS)|The Finish Computation Sequentially (FCS) solution is a technique used to reduce the number of MapReduce iterations in a distributed algorithm. It works by gathering all nodes that still require processing into a single machine, allowing for sequential computation and reducing the number of iterations required.
0c6b0543-2b04-5fd3-a736-916d0f2fa44d|CRACKER Algorithm|The CRACKER algorithm is a distributed iterative algorithm designed to identify connected components in large graphs. It works by iteratively growing a tree for each connected component, removing nodes from the graph and excluding them from further computation. This approach reduces the total computation time and volume of messages exchanged.
25de11dc-5fbb-579d-b7b1-88c192255bc7|Oblivious Seed (OS) Optimization|The Oblivious Seed (OS) optimization is a technique designed to minimize the creation of high-degree vertices in the graph, which can lead to load imbalance in distributed systems. This is achieved by disabling the seed propagation mechanism for nodes with high degrees, thereby preventing them from becoming bottlenecks in the computation.
0ce9a7ae-d47a-52d1-bc6e-9e8da9072993|Edge Pruning (EP) Optimization|The Edge Pruning (EP) optimization is a technique designed to reduce the number of edges in the graph, which can lead to improved load balance in distributed systems. This is achieved by removing edges that are not necessary for the computation, thereby reducing the amount of data that needs to be processed.
b9c8ee9f-9e51-5293-86f5-dbf1e1cb4b53|Finish Computation Sequentially (FCS) Optimization|The Finish Computation Sequentially (FCS) optimization is a technique designed to reduce the number of iterations required to complete the computation, which can lead to improved load balance in distributed systems. This is achieved by gathering all the nodes that still require processing into a single machine, thereby reducing the number of iterations required to complete the computation.
474a80d2-a616-5d79-8172-7e01d8b3358d|Distributed 2-Approximation Algorithm for Weighted Vertex Cover|The authors propose a distributed 2-approximation algorithm for Weighted Vertex Cover, which addresses the challenge of memory-efficient scalable graph processing by reducing the number of communication rounds required to achieve a good approximation ratio.
a79f7fea-754c-5dc8-af03-cd50d9254c2c|Distributed 2-Approximation Algorithm for Fractional Packing with  = 2|The authors propose a distributed 2-approximation algorithm for Fractional Packing with  = 2, which addresses the challenge of memory-efficient scalable graph processing by reducing the number of communication rounds required to achieve a good approximation ratio.
e7fd78cb-15a6-597b-89ad-386b147ecf58|Distributed Algorithm for Submodular Cost Covering|The authors propose a distributed algorithm for Submodular Cost Covering, which addresses the challenge of memory-efficient scalable graph processing by reducing the number of communication rounds required to achieve a good approximation ratio.
8d63b28d-fa0e-50ec-a617-898c4c6d7f8c|Distributed Algorithm for Max Weighted c-Matching in Hypergraphs|The authors propose a distributed algorithm for Max Weighted c-Matching in Hypergraphs, which addresses the challenge of memory-efficient scalable graph processing by reducing the number of communication rounds required to achieve a good approximation ratio.
d9ae19d3-571a-5dc7-b25f-d39ed049e285|Distributed Fractional Packing Algorithm|The authors propose a distributed fractional packing algorithm that can handle heterogeneous graph structures with varying degrees, weights, and sparsity. This algorithm is designed to overcome load imbalance, reduce communication overhead, and enhance memory locality in distributed systems.
da967ae3-a7e0-5dd6-8623-0d19d2732020|Distributed 2-Approximation Algorithm for Max Weighted c-Matching|The authors propose a distributed 2-approximation algorithm for Max Weighted c-Matching that can handle heterogeneous graph structures with varying degrees, weights, and sparsity. This algorithm is designed to overcome load imbalance, reduce communication overhead, and enhance memory locality in distributed systems.
0b13b681-9e94-5cbd-984a-a48e3db03f72|Distributed Fractional Packing and Maximum Weighted b-Matching via Tail Recursive Duality|This solution addresses the challenge of optimizing load balance in distributed systems by proposing a distributed algorithm for fractional packing and maximum weighted b-matching problems. The algorithm uses a tail recursive duality approach to achieve a 2-approximation for the maximum weighted b-matching problem in O(log n) rounds.
afd45f21-059a-58f7-8174-592a8d2edef1|Distributed Algorithm for Fractional Packing with General |This solution addresses the challenge of optimizing load balance in distributed systems by proposing a distributed algorithm for fractional packing problems with general . The algorithm uses a randomized approach to achieve a 2-approximation in O(log m) rounds.
04b253dc-6b23-571b-baa7-1e8300724a41|Distributed Algorithm for Fractional Packing with  = 2|This solution proposes a distributed algorithm for approximating the Fractional Packing problem with  = 2, achieving a 2-approximation ratio in O(log m) rounds in expectation and with high probability.
313272dc-3812-5937-9803-deaed92825e3|Distributed Algorithm for Max Weighted c-Matching|This solution proposes a distributed algorithm for approximating the Max Weighted c-Matching problem, achieving a 2-approximation ratio in O(log^2 E) rounds in expectation and with high probability.
5bd73e4e-9511-5a40-adea-dcb7f91e5524|Frame-based Scheduling Algorithm|The authors propose a frame-based scheduling algorithm that constructs proper connection patterns for input packets in three stages: accumulating stage, scheduling stage, and switching stage. This algorithm is designed to optimize memory usage and reduce computational costs.
6858aba3-89d2-574e-9932-882a7343e72c|Graph Initialization Algorithm|The authors propose a graph initialization algorithm that initializes the consistently colored bipartite graph in a distributed manner. This algorithm is designed to reduce memory consumption and computational costs.
c4feceb1-9a73-5912-8b69-f9f1882cd45a|Parallel Complex Coloring|The authors propose a parallel complex coloring algorithm to optimize communication efficiency in distributed algorithms. This algorithm is designed to color the edges of a bipartite graph in parallel, reducing the number of communication rounds required.
532ec906-3561-5f2c-af91-e328aab0dec9|Frame-based Scheduling|The authors propose a frame-based scheduling algorithm to optimize communication efficiency in distributed algorithms. This algorithm divides time into frames and schedules packets within each frame to minimize communication rounds.
9bc913f0-e5e0-5258-8f64-efbfaa7a8c71|On-Line Scheduling Algorithm|The authors propose an on-line scheduling algorithm to optimize load balance in distributed systems. This algorithm uses a parallel complex coloring algorithm to eliminate variables in the bipartite graph and schedules packets in real-time to minimize contention and maximize throughput.
7a3bab86-bac3-5035-8d78-0b243447686d|Distributed Decomposition Algorithm|The Distributed Decomposition Algorithm is a solution proposed by the authors to address the challenge of memory-efficient scalable graph processing. This algorithm is designed to partition the graph into smaller components, allowing for more efficient processing and reducing memory consumption.
c29ab8c8-12f6-5681-8b3d-e7acb15b1bac|Coloring-Based Algorithm|The Coloring-Based Algorithm is another solution proposed by the authors to address the challenge of memory-efficient scalable graph processing. This algorithm uses a coloring approach to divide the vertices into independent sets, allowing for parallel processing and reducing memory consumption.
df91b938-3b9e-5115-a9dc-ca0a143b4e90|Fast Distributed Greedy Max Cut Algorithm|The Fast Distributed Greedy Max Cut Algorithm is a solution proposed by the authors to address the challenge of memory-efficient scalable graph processing. This algorithm uses a greedy approach to divide the vertices into two sets, allowing for efficient processing and reducing memory consumption.
f3e191a2-9141-5669-9161-4a73efc48297|Coloring-Based Technique|The Coloring-Based Technique is another solution proposed by the authors to address the challenge of optimizing communication efficiency in distributed algorithms. This technique involves using distributed coloring algorithms to find independent sets of vertices, which can then be used to make decisions in parallel.
470e255f-fedc-56b7-af5f-3871f32a6f96|Fast Distributed Greedy Max Dicut Algorithm|The Fast Distributed Greedy Max Dicut Algorithm is a solution proposed by the authors to address the challenge of optimizing communication efficiency in distributed algorithms. This algorithm is designed to find a 1/3-approximation for Max Dicut in O(log n) communication rounds in the LOCAL model.
b48ef5bb-2bca-5e17-86b8-1e8ad790e4a0|Fast Distributed Greedy Max-Cut Algorithm|The Fast Distributed Greedy Max-Cut Algorithm is a solution proposed by the authors to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm uses a greedy approach to find a maximum cut in the graph.
520ac64a-8bab-51ea-84c1-8794ffdce7fb|Clustering-Based Algorithm|The Clustering-Based Algorithm is a solution proposed by the authors to address the challenge of efficient graph dynamics processing. This algorithm involves clustering the graph into small-diameter clusters, allowing for efficient processing of dynamic updates. The algorithm uses a combination of the Distributed Decomposition Algorithm and the Coloring-Based Technique to cluster the graph. This approach ensures that the clusters have a small diameter, making it efficient to process updates within each cluster. The authors demonstrate that the Clustering-Based Algorithm can achieve a 1-approximation for Max Cut in O(log n) communication rounds in the LOCAL model.
ec8c3e32-1115-5d72-9a9c-117f1d87ab78|Degree-Ordered Directed Graph|The authors propose using a degree-ordered directed graph to mitigate computation and communication costs in graph processing. This approach involves ordering the vertices in the graph based on their degrees and converting undirected edges into directed edges that point from low-degree vertices to high-degree vertices. The degree-ordered directed graph is used to reduce the maximum out-degree of vertices, making it more efficient to process the graph. This approach is different from existing methods as it takes advantage of the degree ordering to reduce the number of edges that need to be processed. The authors demonstrate the effectiveness of this approach by showing that it can reduce the time required to count pr and qr squares in a graph.
45cc14d7-01e9-558b-8e7b-a7f646162c36|Merged pr and qr Square Counting|The authors propose merging the initial communication steps for counting pr and qr squares to reduce the overall communication cost. By merging the communication steps, the authors can reduce the number of messages that need to be sent between processors, resulting in lower communication overhead. The authors show that merging the communication steps can reduce the time required to count pr and qr squares, making the algorithm more efficient.
aa9b2ce0-e8e0-550f-9e27-0a2980804143|Caching of Wedges|The authors propose using a cache to store frequently accessed wedges (pairs of vertices) to reduce the number of messages that need to be sent between processors. The cache is used to store the counts of wedges, reducing the need to send messages to retrieve this information. The cache is implemented using a direct mapping between cached objects and cache slots with no associativity and a least recently used (LRU) eviction policy. The authors show that the cache can reduce the aggregate communication volume by approximately 29-30%.
3d452dcf-e147-5b03-b2f7-7b2ff617168a|Asynchronous Communication Framework|The authors propose using an asynchronous communication framework to handle the irregular communication patterns in graph processing. The framework uses message buffering and serialization to handle the large number of small messages that need to be sent between processors. The framework also provides a flexible capability for network scientists to count types of squares within massive graphs. The authors demonstrate the effectiveness of this approach by showing that it can handle the large number of small messages that need to be sent between processors, making the algorithm more efficient.
0c09e7f5-eb6b-5040-9ea4-9716c07bea00|Message Buffering|The authors propose a message buffering technique to optimize communication efficiency in distributed algorithms. This technique involves buffering small messages until they reach a predetermined size, reducing the overhead of sending individual messages immediately.
1a08c8bb-6808-53d8-b67c-e61115acb423|Degree-Ordered Directed Graphs|The authors propose using degree-ordered directed graphs to mitigate computation and communication costs in square counting. This approach involves ordering vertices by their degrees and directing edges from lower-degree vertices to higher-degree vertices. The degree-ordered directed graph approach reduces the complexity of identifying triangles and squares by exploiting the fact that lower-degree vertices are more likely to be part of a square. This approach also enables the use of a pull-based model for counting squares, which reduces communication costs. The authors demonstrate that this approach can count hundreds of trillions of squares in a massive distributed graph, although they acknowledge that their current implementation is not scalable.
482f80c9-84e6-56cc-a149-7348a0a404fb|Merging pr and qr Square Counting|The authors propose merging the initial communication steps for counting pr and qr squares to reduce communication costs. By merging the communication steps, the authors can reduce the number of messages sent between nodes, which helps to alleviate the communication imbalance and improve performance. The authors report a small but positive impact on performance when merging the pr and qr square counting steps.
ec2d4b7a-02f8-5a3f-a7f8-0b4ba3d46e27|Caching for pq Square Counting|The authors propose using caching to reduce the amount of communication required for counting pq squares. The authors use a simple caching mechanism that stores pairs of vertices seen frequently on each rank, which helps to reduce the number of messages sent between nodes. The authors report that the caching mechanism can reduce the aggregate communication volume by approximately 29-30%.
68990a29-22df-5729-8a4b-e88814c72891|Data Replication|The authors propose replicating high-degree vertices to reduce communication costs and improve load balancing. By replicating high-degree vertices, the authors can reduce the number of messages sent between nodes and improve load balancing, which can help to alleviate the communication imbalance and improve performance. The authors do not provide quantitative results for this solution, but they suggest that it has the potential to improve scalability.
78ad88aa-14f1-5095-a80d-9956421ca019|Pull-Based Model for Square Counting|The authors propose using a pull-based model for counting squares, which involves accumulating the 2-hop neighborhood at vertex p and then pulling vertex r's adjacency list to perform the intersection operation. The pull-based model reduces communication costs by avoiding the need to send adjacency information from vertex p to its 2-hop neighbors. The authors do not provide quantitative results for this solution, but they suggest that it has the potential to improve performance and scalability.
43bdb4f9-c5b7-5105-bddf-0365a4a61b3a|Data Replication for Load Balancing|The authors propose replicating high-degree vertices at the expense of a small amount of memory on each rank to avoid the communication associated with these problematic vertices. This solution involves identifying high-degree vertices that are repeatedly accessed and replicating them across ranks to reduce communication overhead. The paper does not provide explicit results for this solution, but it is mentioned as a potential mitigation for the communication imbalance.
cc5cbf46-3667-5ff1-8d5f-b7e0f37dde66|Merging Initial Communication Steps|The authors propose merging the initial communication steps for counting pr and qr squares to reduce the overall communication volume. This solution involves combining the initial communication steps for counting pr and qr squares, which reduces the number of messages sent and received. The paper reports a small but positive impact on performance when merging the initial communication steps, with a small decrease in overall communication.
330faadc-cb0e-59d6-9ab0-5076998117f5|Caching for Reduced Communication|The authors propose using caches to reduce the amount of total communication required for counting pq squares. This solution involves keeping a cache of pairs of vertices seen frequently on each rank and using a least recently used (LRU) eviction policy to manage the cache. The paper reports that the caches were able to reduce the aggregate communication volume by approximately 29-30%.
2ab0facb-bd10-50e5-b0c4-7f515f270e38|Degree-Ordered Directed Graph (G) Approach|The authors propose using a degree-ordered directed graph (G) to efficiently count squares in large graphs. This approach involves ordering the vertices of the graph based on their degrees and then using this ordering to reduce the number of wedge checks required to identify squares.
bdbd7291-dee8-5ee3-a0e8-da43f62a34f5|Asynchronous Communication Framework (YGM)|The authors propose using an asynchronous communication framework (YGM) to handle the irregular communication patterns seen in distributed memory graph processing.
1ec04f7c-5c16-546c-84fd-a5b5d07066dc|Degree-Based Neighbor Partitioning|The authors propose a degree-based neighbor partitioning scheme to reduce the number of set comparisons in set intersection and to reduce the amount of remote data to be transferred. This approach partitions neighbors based on the degree of the vertices, rather than using lexicographical comparison.
793591ee-090f-5585-92c0-0c99f325f1fb|Blocking Vertices in Sets|The authors propose blocking vertices in sets to reduce the number of messages relative to the approach discussed in Section 3.2. This approach reduces the number of messages by sending a subset of vertices, called a block, instead of sending a complete chunk of neighbors to the owner of another set.
11362676-e0fe-591a-9c7b-60457552d020|Block Aggregation|The authors propose block aggregation to reduce the latency overhead of sending large numbers of small messages. This approach maintains a buffer of bytes per each destination rank and stores blocks in the buffer instead of sending them directly to the destination rank.
ea124bed-4c7d-536c-86af-615af8c80177|Generalized Triangle Counting Algorithm|The authors propose a generalized triangle counting algorithm that can be used to derive different triangle counting algorithms. This algorithm uses two common abstractions to reduce in-node load imbalance and improve cache utilization.
abad83bf-a4cc-5204-85f2-1cd82bf9e1b3|Blocking Vertices|The authors propose blocking vertices in sets to reduce the number of messages relative to the approach discussed in Section 3.2. The blocking vertices technique reduces the number of messages by sending a subset of vertices, called a block, instead of a complete chunk of neighbors. The paper demonstrates that blocking vertices reduces the number of messages and achieves better cache utilization.
70af4186-5e4b-5df4-95cf-63e79d252dea|Cyclic Distribution|The authors propose using cyclic distribution to minimize the load imbalance between parallel threads. The cyclic distribution approach assigns vertex identifiers to nodes in a round-robin fashion, which minimizes the load imbalance between parallel threads. The paper demonstrates that cyclic distribution minimizes the load imbalance between parallel threads and achieves better performance.
4f5eb47b-110e-59b2-bce1-6425b13d19f6|Degree Partitioned Triangle Counting Algorithm|The authors propose a degree partitioned triangle counting algorithm that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by partitioning neighbors based on vertex degrees, rather than lexicographical comparison. This approach minimizes the number of predecessors, reducing load imbalance and enhancing memory locality.
4d58b325-5ce6-5948-96ac-fb5eaa915376|Vertex Blocking|The authors propose a vertex blocking technique to reduce the number of messages and enhance memory locality in distributed execution. This approach blocks vertices in predecessor sets and successor sets, reducing the number of messages and enhancing memory locality.
53cbfacb-33ef-5ac7-9f3d-114415a1046a|1D Cyclic Distribution|This solution involves distributing vertices among nodes in a round-robin fashion to minimize the load imbalance between nodes.
0756e06e-7734-5871-8f23-a72476fdedfa|Small Separator Decomposition|The authors propose a small separator decomposition technique to address the challenge of memory-efficient scalable graph processing. This technique involves partitioning the graph into smaller clusters and a small separator set, allowing for simultaneous reconfiguration of distant parts of the graph without interference.
51f71bf6-d353-541e-bc6f-847f7076bb76|Batch Compression|The authors propose a batch compression technique to transform a sequential schedule into a batch schedule, reducing the number of batches while maintaining a small approximation overhead.
d7bc945f-2481-550a-b666-7b5eac50f9be|Distributed Computation of Schedules|The authors propose a distributed computation of schedules technique to address the challenge of memory-efficient scalable graph processing. This technique involves computing batch reconfiguration schedules in a distributed setting, using a small separator decomposition and batch compression.
7686219e-ff0f-5ea7-996c-95a6f02c3875|Batch Reconfiguration Scheduling|The authors propose a batch reconfiguration scheduling technique to optimize communication efficiency in distributed algorithms. This method involves grouping nodes into batches and reconfiguring them simultaneously, reducing the number of communication rounds required.
e6825e8e-3ff7-5c36-b305-71039d55a588|Distributed Algorithm for Small Separator Decomposition|The authors propose a distributed algorithm for computing small separator decompositions to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm involves using a combination of graph partitioning and clustering algorithms to identify clusters with a small separator set.
71940a3f-1244-598d-95f9-0838896fa0c8|Distributed Vertex Cover Reconfiguration|The authors propose a distributed algorithm for vertex cover reconfiguration, which allows for the reconfiguration of multiple vertices concurrently while maintaining feasibility. This approach addresses the challenge of load balance optimization in distributed systems by enabling the efficient reconfiguration of vertex covers in a distributed setting.
6411d6d9-cce5-5893-bc5e-6f2774e137f3|Cactus Core Decomposition|The authors propose a cactus core decomposition algorithm that divides the graph into independent clusters, which can be reconfigured simultaneously. This approach addresses the challenge of load balance optimization in distributed systems by enabling the efficient decomposition of the graph into smaller subgraphs.
acdf9165-78a7-5e37-9094-050ef24165ce|Load-Aware RDF Partitioning|The authors propose a load-aware RDF partitioning technique that partitions the data into fragments based on a representative query load. This approach aims to minimize communication between hosts and reduce memory consumption by allocating fragments that are used together in a query to the same host. The technique involves extracting relevant information from the query load, partitioning the data into fragments, and allocating the fragments to hosts. The authors use a global fragment query graph to model the co-occurrence of triple patterns in queries and assign fragments to hosts based on their access frequency and size. The authors demonstrate the effectiveness of their approach through experiments, showing that their technique outperforms state-of-the-art approaches in terms of response time and throughput.
6ac8b5f6-8397-5f2b-b087-aaea7d239c3b|Distributed Query Processing|The authors propose a distributed query processing approach that optimizes query execution plans for a distributed setup. This approach aims to minimize communication between hosts and reduce memory consumption by identifying the hosts that are relevant to the leaf node scans and using a pipelined query plan involving hash joins and union operators. The approach involves using a coordinator to generate a suitable query plan for distributed query execution, identifying the hosts that are relevant to the leaf node scans, and using a pipelined query plan to minimize communication between hosts. The authors demonstrate the effectiveness of their approach through experiments, showing that their technique outperforms state-of-the-art approaches in terms of response time and throughput.
51697810-35ff-5760-9224-e6f885451204|COM MIN Algorithm|The COM MIN algorithm is a method for minimizing the number of communication rounds in distributed algorithms by iteratively constructing a set of simple predicates that ensure minimality and completeness.
c6b52eaa-29dd-57dc-b28d-ae30be370376|Pipelined Query Plan|The pipelined query plan is a method for optimizing communication efficiency in distributed query processing by executing parts of the query in parallel and minimizing the transfer of intermediate results between hosts.
5ad6ffb5-ebb6-5c8d-9a7e-e2e85753cb61|Distributed Join Optimization|The distributed join optimization is a method for optimizing communication efficiency in distributed query processing by introducing distributed joins that exploit pipelining.
d601f57c-1bb6-5ae8-9f4f-c6a5ddf453b5|Query Load Aware RDF Partitioning|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by proposing a query load aware RDF partitioning technique. The technique partitions the data into fragments based on a representative query load, which helps to reduce the load imbalance and communication overhead in distributed query processing.
8ed6543d-63c2-5916-929a-314bcd3eae4f|Distributed Query Processing with Remote Fetcher Operators|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by proposing a distributed query processing approach that uses remote fetcher operators to minimize communication overhead.
097c7436-8b76-5140-923b-53addab5b76a|Load-Aware Fragment Allocation|This solution specifically addresses the challenge of optimizing load balance in distributed systems by allocating fragments of data to hosts in a way that balances the load across all nodes. The authors propose a greedy algorithm that allocates fragments in descending order by their load, considering the current load of each host and the storage capacity of each host.
4a82f18d-45b8-5e82-a83c-660712e79597|Query Load-Aware Partitioning|This solution addresses the challenge of optimizing load balance in distributed systems by partitioning the data in a way that takes into account the query load. The authors propose a method that extracts simple predicates from the query load and uses them to partition the data into fragments.
570f05f7-b9cf-5439-b65e-fbb7fa0f8a22|Distributed Query Optimization|This solution addresses the challenge of optimizing load balance in distributed systems by optimizing query execution plans for distributed query processing. The authors propose a two-step approach that first optimizes the query plan with respect to cardinality and then optimizes the plan with respect to load balance.
98865815-a521-5dd3-85ba-bd18a9382cc0|Svelto Architectural Template|The Svelto architectural template is a solution proposed by the authors to address the challenge of memory-efficient scalable graph processing. This template is designed to support single-cycle context switching, hide external memory access latency, maximize memory utilization, and provide dynamic load balancing.
c8d04e3e-62f2-5b32-af63-eae659609b73|Context Switching|The authors propose context switching as a solution to address the challenge of memory-efficient scalable graph processing. This approach is designed to provide latency tolerance and maximize memory utilization.
fe2efe56-9ed1-50ef-a977-85e0cc0c6196|Hierarchical Memory Controller|The Hierarchical Memory Controller is a solution proposed by the authors to optimize communication efficiency in distributed algorithms. It is designed to manage the arbitration of requests from workers to the top memory controller, ensuring efficient communication and minimizing latency.
d174053a-9d09-5a9d-8535-614481a51b3a|Parallel Memory Controller|The Parallel Memory Controller is a solution proposed by the authors to optimize communication efficiency in distributed algorithms. It is designed to handle the arbitration of requests from workers to the top memory controller, ensuring efficient communication and minimizing latency.
02a02c30-8380-5612-bf4a-ab3fef3926fe|Dispatcher|The Dispatcher is a solution proposed by the authors to optimize communication efficiency in distributed algorithms. It is responsible for distributing tasks to workers and managing the execution of tasks.
dd1181cf-b816-517e-8466-0d851d606c3c|Scheduler|The Scheduler is a solution proposed by the authors to optimize communication efficiency in distributed algorithms. It is responsible for managing the execution of tasks within a worker and ensuring that tasks are executed efficiently.
36f8ea34-3be1-52a0-8a08-1b47b5d30482|High-Level Synthesis (HLS) Methodology|The HLS methodology is a solution proposed by the authors to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This methodology is designed to automatically generate custom accelerators for graph processing, which can be optimized for specific graph structures and algorithms.
d666f257-c70f-5fb0-b308-880a7193ba65|Graph Pattern Matching Routines|The graph pattern matching routines are a solution proposed by the authors to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. These routines are designed to efficiently process and analyze complex graph structures, and can be optimized for specific graph algorithms and structures.
7f7b9fb8-7028-52a0-83ca-53af8711a171|Dynamic Task Scheduling with Context Switching|This solution addresses the challenge of optimizing load balance in distributed systems by employing dynamic task scheduling with context switching. The approach involves dynamically scheduling tasks to workers based on their availability and workload, and using context switching to quickly switch between tasks when a worker is idle or waiting for memory access.
f60b22bd-bd69-547b-a0be-2b8033eeec29|Parallel Memory Controller with Round-Robin Arbitration|This solution addresses the challenge of optimizing load balance in distributed systems by designing a parallel memory controller with round-robin arbitration. The approach involves using a parallel memory controller that manages memory requests from multiple workers and uses round-robin arbitration to ensure fair access to memory resources.
1f523105-2856-5152-8a6b-9a3e74da0eb8|Svelto Architecture Template|The Svelto architecture template is a novel solution proposed by the authors to address the challenge of efficient graph dynamics processing. It is a high-level synthesis methodology that generates custom accelerators optimized for irregular graph kernels. The template exploits both instruction-level and task-level parallelism, providing dynamic load balancing and maximizing external memory utilization through latency tolerance.
82bcd3ab-ec5b-523b-9a3c-a26edae8bfd3|Hybrid Task Decomposition Strategy|The authors propose a hybrid task decomposition strategy to address the challenge of memory-efficient scalable graph processing. This strategy combines color-ignorant task decomposition with a new strategy that decomposes tasks based on the number of vertices in the subgraph.
ef2a7b38-d28d-53e6-81a5-e0ddda8c798f|Bounded Memory Consumption through Task Pooling|The authors propose a task pooling approach to address the challenge of memory-efficient scalable graph processing. This approach keeps a pool of active tasks for processing at any time, bounding memory cost and minimizing CPU idle time.
4fdfc8e5-9f96-50e5-8087-7b6e30a53cc0|Load Balancing through Global Task Queue|The authors propose a load balancing approach using a global task queue to address the challenge of memory-efficient scalable graph processing. This approach prioritizes the scheduling of big tasks to minimize CPU idle time and maximize parallelism.
5a763ea1-231d-5140-9eac-7fcf237ac4f6|Priority-based Task Scheduling with Global Task Queue|This solution addresses the challenge of optimizing load balance in distributed systems by introducing a priority-based task scheduling mechanism that utilizes a global task queue. The global task queue allows for the prioritization of tasks based on their computational requirements, ensuring that tasks with higher computational demands are executed first. This approach enables the system to effectively balance the workload across nodes, reducing the likelihood of straggler tasks and improving overall system performance.
67f15cf7-12fb-5769-bd36-30b2b6683d20|Dynamic Task Decomposition with Timeout Mechanism|This solution addresses the challenge of optimizing load balance in distributed systems by introducing a dynamic task decomposition mechanism that incorporates a timeout mechanism. The timeout mechanism allows the system to detect and decompose tasks that are taking too long to execute, preventing straggler tasks and improving overall system performance.
0923fb10-50e1-52d2-99e1-1c1b14eb3b14|Work Stealing with Load Balancing|This solution addresses the challenge of optimizing load balance in distributed systems by introducing a work stealing mechanism that incorporates load balancing. The work stealing mechanism allows idle nodes to steal tasks from busy nodes, reducing the likelihood of straggler tasks and improving overall system performance.
6dd0e97f-a1f5-5de8-a877-62cb6b38c7db|Column-wise Message Compression|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a column-wise message compression technique to reduce the memory consumption and communication overhead in distributed graph processing systems. The technique involves organizing messages in a column-wise manner, where messages sent from the same partition are stored in a single file, and then compressing the messages using a combination of Run-Length Encoding (RLE) and Gzip algorithms. This approach takes advantage of the column-wise organization to achieve a higher compression ratio, reducing the memory consumption and communication overhead. The paper reports that the column-wise message compression technique can reduce the communication cost by a factor of 1.7 to 2.4.
e615ea0f-5653-5266-a37e-f716b52ef5fd|Lazy Decompression|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a lazy decompression technique to reduce the memory consumption and communication overhead in distributed graph processing systems. The technique involves decompressing messages only when necessary, instead of decompressing all messages at once. This approach reduces the memory consumption and communication overhead by avoiding unnecessary decompression. The paper reports that the lazy decompression technique can reduce the communication cost by a factor of 1.68 to 2.23.
02650c3a-050b-566c-aedf-181570fa34ab|Partition-Based Recovery|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a partition-based recovery technique to reduce the memory consumption and communication overhead in distributed graph processing systems. The technique involves redistributing the subgraphs originally residing in the failed nodes over a subset of compute nodes to parallelize the recovery process. This approach reduces the memory consumption and communication overhead by minimizing the number of nodes involved in the recovery process. The paper reports that the partition-based recovery technique can reduce the communication cost by a factor of 6.5 to 37.9.
7db89e0b-169a-52a8-be27-6665c3b68097|Cost-Sensitive Reassignment|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a cost-sensitive reassignment technique to reduce the memory consumption and communication overhead in distributed graph processing systems. The technique involves generating a reassignment plan that takes into account the computation and communication costs of the recovery process. This approach reduces the memory consumption and communication overhead by minimizing the number of nodes involved in the recovery process. The paper reports that the cost-sensitive reassignment technique can reduce the total cost by a factor of 2.5.
8c93ff64-03b5-55f1-bcdb-d3a3c2f0bbcd|Partition-based Recovery|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by parallelizing the recovery process. The authors propose redistributing the subgraphs originally residing in the failed nodes over a subset of compute nodes to parallelize the recovery process.
581c27e2-c269-540e-bb28-093f22746e20|Cost-Sensitive Reassignment Algorithm|The Cost-Sensitive Reassignment Algorithm is a novel approach to reassigning failed partitions among healthy nodes in a distributed graph processing system, with the goal of minimizing the recovery time by considering both computation and communication costs. The algorithm uses a cost model that takes into account the computation and communication costs of reassigning failed partitions. It employs a heuristic approach to find a good reassignment that balances the computation and communication costs, rather than simply minimizing one or the other. The paper presents experimental results showing that the Cost-Sensitive Reassignment Algorithm outperforms existing methods by up to 30 times in terms of recovery time and reduces communication cost by a factor of 38.
7104b6e8-5382-5071-bc4b-0454d3e700f4|Column-Wise Message Compression|The Column-Wise Message Compression technique is a method for compressing messages in a distributed graph processing system, which reduces the logging overhead and communication cost during failure recovery. The technique organizes messages into a three-level hash table and compresses messages using a column-wise approach, which allows for efficient compression and decompression of messages. The paper presents experimental results showing that the Column-Wise Message Compression technique reduces the logging overhead and communication cost during failure recovery.
2a65c389-c901-59fa-b003-0833af8d77de|Cost-Sensitive Reassignment Algorithm (COSTSEN)|COSTSEN is a reassignment algorithm designed to optimize load balance in distributed systems by minimizing the total recovery time after a failure. It takes into account both computation and communication costs to reassign failed partitions to healthy nodes.
17c48b6c-520e-57fe-b7ab-effd7b57a57e|Lazy Decompression Technique|This solution involves decompressing messages lazily during the recovery process to reduce the overhead of message decompression.
8e21e599-83fc-5c6d-8148-c47d49ca557e|Partition-Based Recovery Algorithm|The authors propose a partition-based recovery algorithm to address the challenge of efficient graph dynamics processing. This algorithm aims to minimize the recovery time by redistributing the subgraphs originally residing in failed nodes over a subset of compute nodes, thereby parallelizing the recovery process.
2682a5d5-d794-575c-8b7a-0894e0ca7401|Column-Wise Message Compression Scheme|The authors propose a column-wise message compression scheme to reduce the logging overhead in graph dynamics processing. This scheme is used in conjunction with the partition-based recovery algorithm to address the challenge of efficient graph dynamics processing.
9352bb0f-538a-522e-89a1-69e4a20b4dbf|Storing Edges At Subvertices (SEAS)|SEAS is an optimization technique that stores the edges of supervertices in a distributed fashion among its subvertices. This approach avoids the cost of sending adjacency lists to the supervertex and reduces communication overhead. SEAS retains the adjacency lists of subvertices and keeps them active, rather than sending them to the supervertex and becoming inactive. SEAS can improve performance by reducing the number of messages sent between vertices and avoiding unnecessary communication.
24203fcf-8267-5d46-bde3-6ca1a5b35460|Distributed Kruskal's Algorithm|The authors propose a distributed Kruskal's algorithm to optimize communication efficiency in distributed algorithms. This solution specifically addresses the challenge by utilizing a greedy strategy for the MST problem, allowing for concurrent multi-edge choices. The algorithm constructs a BFS tree and then uses aggregate computation and broadcast to identify the minimum weight outgoing edge incident to each node.
407e500f-9dd5-5ca4-9ead-56f30fd82ad8|Constant Time Distributed Dominating Set Approximation|The authors propose a constant time distributed dominating set approximation algorithm to optimize communication efficiency in distributed algorithms. This solution specifically addresses the challenge by utilizing a probabilistic rounding technique to achieve a constant time approximation.
9c62ea5b-b352-542d-98d1-12ce6f2f6a33|Distributed Approximation of Minimum Cut|The authors propose a distributed approximation of minimum cut to optimize communication efficiency in distributed algorithms. This solution specifically addresses the challenge by utilizing a distributed algorithm that achieves a 2-approximation in O(log n) rounds.
b1bbb61d-bcf2-53b0-9786-778d76add613|Distributed All-Pair Shortest Paths|The authors propose a distributed all-pair shortest paths algorithm to optimize communication efficiency in distributed algorithms. This solution specifically addresses the challenge by utilizing a token circulation approach to construct n BFS trees for all n source nodes.
8375407f-ce69-57f3-b1e4-23c2c1b85423|Alternating Base Tree Construction|The authors propose a novel tree structure, called the alternating base tree, to address the challenge of memory-efficient scalable graph processing. This tree structure is designed to preserve the reachability between two nodes by alternating paths, which is essential for finding augmenting paths in the graph.
5f30ec41-6f88-53b7-860c-e624250f445a|Sparse Certificate Construction|The authors propose a novel technique for constructing a sparse certificate, which is a sparse subgraph of the input graph that preserves at least one augmenting path. This technique is designed to reduce memory consumption and optimize memory usage.
4de6c04d-375f-5f2a-8d48-ce3040bbf632|Distributed Implementation of Sparse Certificate Construction|The authors propose a distributed implementation of the sparse certificate construction technique, which is designed to minimize communication between processors and optimize memory usage.
af941fa0-79b8-51a0-b92d-f693417656b4|Augmenting Path Construction|The authors propose a novel technique for constructing an augmenting path in the graph, using the sparse certificate constructed earlier. This technique is designed to optimize memory usage and reduce computational costs.
dd84be37-81aa-5a1b-be8f-0bab3327ea38|Distributed Augmenting Path Construction|The authors propose a novel approach to constructing an augmenting path in a distributed setting. This approach involves the use of a sparse certificate and the identification of a subset of edges that satisfy certain conditions.
abce2fe5-a400-5dd3-a5ea-6c2be50aa82c|Maximum Matching Verification|The authors propose a novel approach to verifying a maximum matching in a distributed setting. This approach involves the use of a sparse certificate and the identification of a subset of edges that satisfy certain conditions.
7fe0c2f6-bdb4-5889-a5b4-9f952a2f2f6c|disHHK Algorithm|The disHHK algorithm is a distributed graph simulation algorithm designed to address the challenge of memory-efficient scalable graph processing. It specifically targets the evaluation of graph simulation queries in a distributed setting, aiming to minimize memory consumption and optimize memory usage.
c604b72b-8206-5c66-a4bb-07146e75f699|disHHK Algorithm with Query Minimization|The disHHK algorithm with query minimization is an extension of the disHHK algorithm that incorporates query minimization techniques to further reduce memory consumption and optimize memory usage.
a4df9a58-fdd8-5601-94e9-539337498922|disHHK Algorithm with Data Locality|The disHHK algorithm with data locality is an extension of the disHHK algorithm that incorporates data locality techniques to reduce computations and data shipments.
12ed8d40-d1d2-580a-a437-65399d46a0c1|disHHK Algorithm with Distributed Evaluation|The disHHK algorithm with distributed evaluation is an extension of the disHHK algorithm that incorporates distributed evaluation techniques to further reduce memory consumption and optimize memory usage.
551ec3ab-6d0a-54a0-904e-aefc5c68d7a4|dSchedule|dSchedule is an approximation algorithm designed to minimize the makespan of distributed algorithms while maintaining a balance between makespan and data shipment. It assigns connected components to machines based on their optimal data shipment and computation costs.
176c8b43-dbb5-507d-a678-20742a37a6c7|disHHK|disHHK is a distributed algorithm for graph simulation that aims to minimize data shipment and makespan. It consists of five stages, including local evaluation of partial matches, data shipment, and computation of maximum matches.
0c4676df-2877-50b1-a1d0-ef4af42b394e|localHHK|localHHK is a local algorithm for computing partial matches in fragments of the data graph. It is used in the disHHK algorithm to reduce data shipment and computation costs.
d25052ba-3799-538f-b6e9-6b921bc0ace1|reneHHK|reneHHK is a refinement of the HHK algorithm that uses partial matches evaluated at stage 1 as initial candidate matches. It processes the matches of boundary nodes first to reduce computation costs.
a5532012-d76f-519c-8095-9362ccf6d036|Lazy Evaluation|Lazy evaluation is a technique used to determine whether a match belongs to the maximum match in the entire data graph. It involves checking whether all nodes in the descendant subgraph of a pattern graph match no boundary nodes in a connected component.
edc743db-0d89-54c9-b9da-605a893e83d5|Heuristic Weighted Memory Based Algorithm (HWMA)|The HWMA is a distributed algorithm designed to address the challenge of memory-efficient scalable graph processing by providing better approximation for the minimum weighted vertex cover (MWVC) problem in multi-agent systems. The HWMA employs local rules of perturbation and weighted memory to facilitate the emergence of dominant Nash equilibria (DNEs), which are used to approximate the MWVC solution. The algorithm uses a memory-based stochastic learning process, where each player updates its action based on the actions of its neighbors and its own memory. The weighted memory rule assigns a higher probability to a players action being chosen if its neighbors can cover their adjacent edges with a lower average cost. The paper presents numerical experiments on typical complex networks, demonstrating the HWMAs superiority over the state of the art in terms of both solution efficiency and computation speed. The results show that the HWMA generates better results than the state of the art within less computation time, with an improvement on the worst-best case result achieved in almost all instances.
8220aa75-6725-58ff-8875-f0f8855ba34a|Restricted Greed and Memory Based Algorithm (RGMA)|The RGMA is a distributed algorithm designed to address the challenge of memory-efficient scalable graph processing by providing better approximation for the MWVC problem in multi-agent systems. The RGMA employs a restricted greed rule and a memory-based stochastic learning process to facilitate the emergence of Nash equilibria, which are used to approximate the MWVC solution. The algorithm uses a memory-based stochastic learning process, where each player updates its action based on the actions of its neighbors and its own memory. The paper presents numerical experiments on typical complex networks, demonstrating the RGMAs effectiveness in providing better approximation for the MWVC problem. However, the results show that the RGMA is outperformed by the HWMA in terms of both solution efficiency and computation speed.
eea828ff-27dc-529c-b81c-535010a0d4ef|FBR Algorithm|The FBR algorithm is a distributed algorithm designed to address the challenge of memory-efficient scalable graph processing by providing better approximation for the MWVC problem in multi-agent systems. The FBR algorithm employs an asymmetric game model and a payoff matrix to facilitate the emergence of Nash equilibria, which are used to approximate the MWVC solution. The algorithm uses a memory-based stochastic learning process, where each player updates its action based on the actions of its neighbors and its own memory. The paper presents numerical experiments on typical complex networks, demonstrating the FBR algorithms effectiveness in providing better approximation for the MWVC problem. However, the results show that the FBR algorithm is outperformed by the HWMA in terms of both solution efficiency and computation speed.
e01af385-777a-50cd-b137-6c6464f1c62d|Perturbation Rule|The perturbation rule is a technique used in the HWMA to destroy the stability of inferior Nash equilibrium (INE) solutions. It involves recording one's best restricted response and 0 simultaneously, allowing the algorithm to converge to higher-quality Nash equilibria.
c5536c76-f1fd-5086-8a46-d6c1c09b3cb6|Weighted Memory Rule|The weighted memory rule is a technique used in the HWMA to update the actions of players based on their memories. It assigns a higher probability to ai=0 when i's neighbors could cover their adjacent edges with a lower average cost.
7b7e82e2-7a35-57d9-a60c-dbeeea2a681d|Distributed 2-Approximation Algorithm (D2AA)|The D2AA is a distributed algorithm designed to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. It is designed to solve the MWVC problem and uses a step function to update the actions of players in the game.
3108c950-cc59-5819-b6c3-98b02c9001c9|Restricted Greed and Memory-Based Algorithm (RGMA)|The RGMA is a distributed algorithm designed to solve the MWVC problem by decomposing the system-level objective into local utilities. It employs a restricted greed rule and a memory-based rule to update player actions.
cdd06cae-684f-5db2-8725-fb1ada10f162|Fully Best Response (FBR) Algorithm|The FBR algorithm is designed to solve the MWVC problem by using an asymmetric game, where the utility of a player is co-determined by the actions and weights of neighboring nodes.
6bf1f6cc-5a0e-576e-bfeb-338feac6dbe0|Asymmetric Game-Based Algorithm (FBR)|The FBR is a distributed algorithm designed to address the challenge of efficient graph dynamics processing by providing better approximation for the MWVC problem in multi-agent systems. The algorithm focuses on learning in potential games, where each vertex makes decisions using local information of its own and the immediate neighbors only.
75681e84-6050-5f80-8742-fc4d30502a5d|Vertex Cuts|Vertex cuts are a technique used in the PowerGraph system to partition the graph into smaller subgraphs, reducing memory consumption and improving scalability. It works by assigning each edge to a machine and allowing vertices to span machines, reducing the number of edges that need to be stored on each machine.
0f0ad53b-60e7-5985-abee-af335e63280a|Greedy Vertex Cuts|Greedy vertex cuts are a technique used in the PowerGraph system to further improve the partitioning of the graph, reducing memory consumption and improving scalability. It works by using a greedy algorithm to assign edges to machines, taking into account the degree of each vertex and the number of edges already assigned to each machine.
dbd7fe18-82d2-5ff3-b019-60fb94889c5c|Coordinated Vertex Cuts|Coordinated vertex cuts are a technique used in the PowerGraph system to further improve the partitioning of the graph, reducing memory consumption and improving scalability. It works by using a coordinated algorithm to assign edges to machines, taking into account the degree of each vertex and the number of edges already assigned to each machine.
154b3082-7c9e-5e36-97fd-c112967e5fd0|Asynchronous Serializable Engine (Async S)|The Async S engine is a technique used to ensure serializability in asynchronous execution. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the number of conflicts and ensuring that the algorithm converges faster.
e781d58d-44aa-5642-991a-2795b370d0e0|PowerGraph Abstraction|The PowerGraph abstraction is a novel approach to distributed graph processing that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs. It exploits the internal structure of graph programs to factor vertex programs over edges, splitting high-degree vertices and exposing substantially greater parallelism.
1b637e2e-35fe-5b97-87bd-c511cd0a4bf1|Vertex-Cut Approach|The vertex-cut approach is a method for distributed graph placement that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs. It involves assigning edges to machines and allowing vertices to span machines, reducing communication and storage overhead.
b3caa24f-95c4-59be-95d7-0eccf2291871|Delta Caching Procedure|The delta caching procedure is a technique for dynamically maintaining computation state in graph processing, addressing the challenge of adaptive algorithms for heterogeneous and irregular graphs. It reduces the need for redundant computation and communication by caching intermediate results.
bd5f5c75-19ee-5373-a9b8-defeac8ff33e|Vertex-Cut-Based Load Balancing|The authors propose a vertex-cut-based approach to load balancing in distributed graph processing systems. This method involves partitioning the graph into subgraphs and assigning each subgraph to a different machine, ensuring that the workload is evenly distributed across machines.
38691116-7739-5c3d-8e52-effad72599db|Time-Variant Binary Log-Linear Learning Algorithm (TVBLLA)|The TVBLLA is a distributed optimization algorithm designed to solve the vertex cover problem in complex networks. It addresses the challenge of memory-efficient scalable graph processing by proposing a novel individual cost function with the cost value of each individual vertex bounded in [0,1], which bridges the optimal solutions of the vertex cover problem and the potential game. The algorithm guarantees that the covering states of all vertices converge to the minimum vertex cover (MVC) state of a general complex network.
c71e5efa-3e3d-5484-9714-b79dd760f4d9|Relaxed Greedy and Memory-Based Algorithm (RGMA)|The RGMA is a distributed optimization algorithm that addresses the challenge of optimizing communication efficiency by allowing each vertex to record the temporal strategy by the relaxed greedy rule of the latest step into its memory. This enables the algorithm to select a strategy from the memory with a certain probability, reducing the need for extensive communication.
8a7200d6-15eb-591f-a3bc-43b482701a83|Game-Based Memetic Algorithm (GMA)|The GMA is a hybrid algorithm that addresses the challenge of optimizing communication efficiency by combining a game-based approach with a memetic algorithm. The algorithm allows each vertex to evolve locally using the asynchronous updating rule and repeats each chromosome evolution for a certain number of generations.
334c8d8d-8379-5dbf-b2ef-a5044afe7e66|Binary Log-Linear Learning Algorithm (BLLA)|The BLLA is a distributed optimization algorithm that addresses the challenge of optimizing communication efficiency by allowing each vertex to update its strategy independently using a log-linear learning rule.
4f965afa-2c3b-5fa5-8105-c7b4d204140d|Genetic Algorithm (GA)|The GA is a centralized algorithm that addresses the challenge of optimizing communication efficiency by using a genetic approach to evolve the population of solutions.
967b5186-57d7-5a4b-966d-adab4f489cb6|Time-Variant Binary Log Linear Learning Algorithm (TVBLLA)|The TVBLLA is a distributed optimization algorithm designed to solve the vertex cover problem in complex networks. It employs a novel cost function and a time-variant learning rate to drive the covering states of all vertices to converge to the minimum vertex cover (MVC) state of a general complex network.
f3c6e797-6f4b-5df3-8792-f4539a8d66c0|Memory Cloud with Globally Addressable Distributed Memory Storage|The authors propose a memory cloud with globally addressable distributed memory storage to address the challenge of memory-efficient scalable graph processing. This solution involves partitioning a machine's local memory space into 2p memory trunks, each of which is stored on a machine, and using a hashing mechanism to address a key-value pair. The memory cloud is designed to provide fast random access and parallel computing, enabling the processing of large-scale graph data.
cdbce2dd-5775-5367-9fdf-58a236f537ca|TSL Trinity Specification Language for Data and Network Communication Modeling|The authors propose TSL, a high-level language for data and network communication modeling in Trinity, to address the challenge of memory-efficient scalable graph processing. TSL provides object-oriented data manipulation for graph data and facilitates data integration, system extension, and transparent query processing over memory cloud and RDBMSs.
e3bdea6c-8b79-5fe9-8255-873b98aed74c|Vertex-Based Computation Model with Message Passing Optimization|The authors propose a vertex-based computation model with message passing optimization to address the challenge of memory-efficient scalable graph processing. This solution involves using a vertex-based computation model to reduce communication overhead and optimize message passing between nodes.
0d455b01-4c87-5cc6-98b4-65c1cf6c48be|Blob-Based Data Storage with Compact Memory Layout|The authors propose a blob-based data storage approach with compact memory layout to address the challenge of memory-efficient scalable graph processing. This solution involves storing graph data as blobs of arbitrary length in the memory cloud, using a compact memory layout to reduce memory consumption.
e1c8a750-bf1f-52dd-bb90-287b382184ec|Predictable Communication Pattern Optimization|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by exploiting the predictable communication pattern in vertex-centric computation. By assuming that each vertex mainly communicates with a fixed set of vertices, such as its neighbors, the solution enables the preparation of most messages in advance, reducing the need for extensive communication during computation.
8564f4a2-2f0d-5f03-9c16-b9802c9cdd85|Action Script-based Message Ordering|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by introducing an action script-based approach to message ordering. By sending out action scripts to remote machines, which specify the order of message delivery, the solution ensures that messages are delivered in a predictable and efficient manner.
e2a3e802-8f45-5ea3-8819-449193719d68|Deterministic Blocker Set Sequence Algorithm|The authors propose a deterministic blocker set sequence algorithm to address the challenge of memory-efficient scalable graph processing. This algorithm is designed to compute a sequence of blocker sets that intersect all exact shortest paths from source nodes with high hop length, allowing for the computation of shortest cycles and paths without requiring an initial computation of all-pairs shortest paths (APSP).
c85bfe67-c798-5092-bcd5-82b11933bee3|Multi-Source Shortest Paths (MSSP) Algorithm|The authors propose a deterministic algorithm for computing multi-source shortest paths (MSSP) in a graph, which is designed to optimize communication efficiency in distributed algorithms. The algorithm uses a combination of blocker sets and h-hop shortest paths to compute the shortest paths from multiple source nodes to all other nodes in the graph.
25e994ef-1f5d-5287-a398-99ab09ad4a42|Undirected Minimum Weight Cycle (MWC) Algorithm|The authors propose a deterministic algorithm for computing the minimum weight cycle (MWC) in an undirected graph, which is designed to optimize communication efficiency in distributed algorithms. The algorithm uses a combination of blocker sets and h-hop shortest paths to compute the minimum weight cycle in the graph.
89c30474-1b49-5bfc-9604-2fdf4764aec6|Directed Shortest Cycles (DSC) Algorithm|The authors propose a deterministic algorithm for computing the shortest cycles in a directed graph, which is designed to optimize communication efficiency in distributed algorithms. The algorithm uses a combination of blocker sets and h-hop shortest paths to compute the shortest cycles in the graph.
da7db096-1c3c-54c4-b716-d28ddce7e61f|Critical Edge Property|The authors propose a critical edge property, which is used to reduce the number of candidate cycles in the computation of shortest cycles.
ec635eda-b067-5c15-9e9a-099a266eafaf|Minimum Weight Cycle (MWC) Algorithm|The authors propose a deterministic algorithm for computing the minimum weight cycle (MWC) in a graph, which is a fundamental problem in graph dynamics processing.
6fe57ae1-9c4c-5beb-acdb-c12c71ca2a82|All Nodes Shortest Cycle (ANSC) Algorithm|The authors propose a deterministic algorithm for computing the shortest cycle for each node in a graph, which is a crucial component of efficient graph dynamics processing.
de710c5e-1277-5a51-868e-757357325940|Flexible Vertex-Edge Based Input|The authors propose a flexible vertex-edge based input model that allows loading vertex data and edges from separate sources, enabling the reuse of datasets and reducing memory usage.
2b785261-9ee5-59da-9faf-a70e34f4ee85|Superstep Splitting|The authors propose superstep splitting, a technique that allows for the processing of large graphs by splitting the computation into smaller fragments, reducing memory usage and improving performance.
d675d626-b9b1-5b4a-9ee0-c43ee2c82c44|Memory Optimization|The authors propose memory optimization techniques, including reducing object memory overhead and improving message combiners, to reduce memory usage and improve performance.
87107c87-610c-5e44-bab7-63727a4a5a1d|Composable Computation|The authors propose a solution called Composable Computation to address the challenge of optimizing load balance in distributed systems. This solution involves breaking down complex computations into smaller, reusable components, which can be composed together to form more complex computations.
2b2a937e-56c1-5f1e-8b76-7505a977a86e|Multithreading|The authors propose a solution called Multithreading to address the challenge of optimizing load balance in distributed systems. This solution involves using multiple threads to process different parts of the graph in parallel, which can improve the overall performance of the system.
7b00b335-3e20-5d76-aea3-135cc1a6355d|BFS-DFS Adaptive Scheduling|This solution addresses the challenge of memory-efficient scalable graph processing by dynamically adapting the scheduling strategy between Breadth-First Search (BFS) and Depth-First Search (DFS) based on memory usage. The approach aims to balance parallelism and memory consumption by switching to DFS when memory usage exceeds a certain threshold.
422c7f43-5b72-5a55-ac4d-f4952809e45a|Least Recent Batch Used (LRBU) Cache|This solution addresses the challenge of memory-efficient scalable graph processing by introducing a novel cache structure called Least Recent Batch Used (LRBU) cache. The LRBU cache is designed to minimize memory copies and reduce cache overhead.
1c655638-806f-53f7-a57f-e50310ca46d9|Two-Stage Execution with Lock-Free Data Structure|This solution addresses the challenge of memory-efficient scalable graph processing by introducing a two-stage execution mode with a lock-free data structure. The approach aims to reduce memory contention and improve parallelism.
efb84128-db9a-5af4-bd70-732f5ce34f70|Work Stealing for Load Balancing|This solution addresses the challenge of memory-efficient scalable graph processing by introducing a work-stealing approach for load balancing. The approach aims to reduce memory consumption and improve parallelism by dynamically distributing the workload among processors.
11f6de5b-7833-507f-9914-7945bbbb7b5a|Hybrid Communication Layer|The authors propose a hybrid communication layer that supports both pushing and pulling communication modes. This layer allows the system to adaptively choose the most efficient communication mode based on the specific requirements of the algorithm and the characteristics of the data.
cb5f0d9c-aa93-513d-870f-d36b781b3feb|Two-Stage Execution Mode|The authors introduce a two-stage execution mode that combines a lock-free and zero-copy cache design with a novel execution strategy. This approach enables efficient processing of subgraph enumeration queries while minimizing communication overhead.
2f043271-465d-5200-b45a-d336df9ac136|Batching RPC Requests|The authors propose batching RPC requests to improve network utilization and reduce communication overhead. This approach enables the system to aggregate multiple requests into a single batch, reducing the number of communication rounds.
94446bb7-1765-5d2d-adcc-a94448ead3b1|Adaptive Scheduling|The authors propose an adaptive scheduling approach that dynamically adjusts the scheduling strategy based on the characteristics of the data and the algorithm. This approach enables the system to optimize communication efficiency and minimize memory usage.
1f5cb979-2482-528f-8985-2a8c6301cc75|Work-Stealing Load Balancing|The authors propose a work-stealing load balancing technique to address the challenge of load imbalance in heterogeneous and irregular graphs. This technique allows workers to steal tasks from other workers, reducing load imbalance and enhancing memory locality.
2360d357-4290-5cfa-9f72-c529a58084ca|PUSH JOIN Operator|The authors propose a PUSH JOIN operator that allows the system to handle complex queries with multiple joins. This operator reduces the memory usage and communication overhead by pushing the join operation to the data.
a78ff4a0-aec4-55dd-a19c-f9da353ad299|Work Stealing with Intra-Machine Deque|The authors propose a work stealing mechanism with an intra-machine deque to address load imbalance in distributed systems. This solution involves maintaining a deque in each worker, where partial results are injected and popped out for processing. When a worker completes its task, it sends its status to the first machine in the cluster, which then broadcasts the message to other machines. A list of finished machines is maintained, and their jobs are not stolen.
d7b56026-f25d-5062-8e6b-2594c2d7360f|Adaptive Scheduling with Output Queue|The authors propose an adaptive scheduling technique that uses a fixed-capacity output queue for each operator. The scheduler tends to let an operator consume as much input data as possible while preventing memory overflow by yielding when the output queue is full.
4e75f5c8-8bbf-5bd7-b97b-7e40026f280d|LRBU Cache Structure|The LRBU cache structure is designed to efficiently manage remote vertices and their neighbors in the context of subgraph enumeration. It consists of three members: a cache to store remote vertices and their neighbors, an ordered set to track the order of remote vertices that can be safely removed from the cache, and a set to store the IDs of remote vertices that need to be fetched. The LRBU cache structure uses a two-stage execution strategy, where the first stage fetches the required remote vertices and the second stage performs the intersection to obtain the results. This approach allows for zero-copy and lock-free cache access, reducing memory copies and locks. The paper demonstrates that the LRBU cache structure outperforms other cache designs, such as LRU and concurrent LRU, in terms of execution time and memory usage.
d7c32a76-f7f3-590d-aec8-334e5cbb18f8|DFS-BFS Adaptive Scheduler|The DFS-BFS adaptive scheduler is designed to dynamically control the memory usage of subgraph enumeration. It adapts between BFS and DFS scheduling strategies based on the memory usage, ensuring that the memory bound is not exceeded. The scheduler uses a threshold to determine when to switch between BFS and DFS scheduling. When the memory usage exceeds the threshold, the scheduler switches to DFS scheduling to reduce memory consumption. The paper demonstrates that the DFS-BFS adaptive scheduler achieves a tight memory bound of 2 for subgraph enumeration, where is the number of query vertices and is the maximum degree of the data graph.
b5431b97-9b28-5aca-8c77-83586940d568|Work-Stealing Technique|The work-stealing technique is designed to balance the load among machines in a distributed environment. It allows machines to steal work from other machines that are overloaded, ensuring that the load is evenly distributed. The work-stealing technique uses a decentralized approach, where each machine is responsible for stealing work from other machines. This approach allows for efficient load balancing and reduces the overhead of centralized load balancing. The paper demonstrates that the work-stealing technique achieves better load balancing and reduces the execution time compared to other load balancing techniques.
d95c4677-baea-5151-a120-4a7ac6595826|Optimal Execution Plan|The optimal execution plan is designed to compute the optimal join order and physical settings for subgraph enumeration. It uses a dynamic programming approach to minimize the communication cost and computation time. The optimal execution plan uses a combination of join algorithms and communication modes to minimize the communication cost and computation time. It also uses a cache structure to store remote vertices and their neighbors, reducing the need for communication. The paper demonstrates that the optimal execution plan achieves better performance and reduces communication overhead compared to other execution plans.
074f04b9-df4c-5d8f-ae53-d91737941e9a|Optimized DFS Clustering Algorithm|The Optimized DFS Clustering Algorithm is a solution that addresses the challenge of memory-efficient scalable graph processing by reducing redundant work and improving load balancing in the clustering approach. This algorithm is designed to minimize the overlap in work done by different subtasks and balance the load between different tasks, resulting in improved parallel performance.
c803450a-28f8-5dcf-a40c-7c6a97450d26|Load Balancing using Vertex Degree|Load Balancing using Vertex Degree is a solution that addresses the challenge of memory-efficient scalable graph processing by balancing the load between different tasks based on the degree of vertices. This approach is designed to reduce the imbalance in load between different reducers and improve parallel performance.
35656ee7-dfe0-5767-bd09-74f2c6f3d7b7|Load Balancing using Size of 2-Neighborhood|Load Balancing using Size of 2-Neighborhood is a solution that addresses the challenge of memory-efficient scalable graph processing by balancing the load between different tasks based on the size of the 2-neighborhood of vertices. This approach is designed to reduce the imbalance in load between different reducers and improve parallel performance.
e174e2c9-b060-5152-b804-db0251b07cfc|Consensus Algorithm|The Consensus Algorithm is a solution that addresses the challenge of memory-efficient scalable graph processing by using a consensus operation to generate new bicliques. This approach is designed to reduce the number of bicliques generated and improve parallel performance.
78c9cd39-94c6-5631-9755-4c28efb7c217|Load Balancing using Size of 2-neighborhood|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by load balancing the work among reducers based on the size of the 2-neighborhood of vertices. The authors propose to assign vertices to reducers based on the size of their 2-neighborhood, which helps to distribute the workload more evenly and reduce the communication overhead.
ecc78d1d-ea77-5637-b465-e911484e5240|Reducing Redundant Work|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing redundant work among reducers. The authors propose to modify the sequential DFS algorithm to reduce redundant work and minimize the communication overhead.
522d872b-e294-5f57-ad78-a2d8497b96e4|Load Balancing using Vertex Degree (CD1)|This solution addresses the challenge of optimizing load balance in distributed systems by adjusting the position of vertices in the total order based on their degree. The degree of a vertex is used as a proxy for the complexity of its cluster, with higher degree vertices being placed earlier in the order to reduce the burden on the reducer handling that cluster.
d4d04c75-c824-5b5a-ba51-32dea6059f6f|Load Balancing using Size of 2-Neighborhood (CD2)|This solution addresses the challenge of optimizing load balance in distributed systems by adjusting the position of vertices in the total order based on the size of their 2-neighborhood. The size of the 2-neighborhood is used as a proxy for the complexity of the cluster, with larger 2-neighborhoods indicating more complex clusters.
ba69460b-fcea-51f6-9cb1-966d56692322|Optimized DFS with Load Balancing|The authors propose an optimized depth-first search (DFS) algorithm with load balancing to efficiently process dynamic updates in large graphs. This solution specifically addresses the challenge of efficient graph dynamics processing by minimizing computational costs and iterations.
d006fbdc-1f6e-5eba-8e6e-5bbdf1b799c0|Relaxation Simulation|The authors propose a relaxation simulation approach to address the challenge of memory-efficient scalable graph processing. The relaxation simulation approach allows for the efficient processing of large graphs by relaxing the matching conditions between the pattern graph and the data graph.
41cc4a73-c8b6-56f6-a720-179d2d01e73f|Dual Relaxation Simulation (DRS)|The authors propose a DRS approach to address the challenge of memory-efficient scalable graph processing. The DRS approach is an extension of the relaxation simulation approach and allows for the efficient processing of large graphs by considering both the child and parent nodes.
0b2ceae4-6d0f-54c3-80a3-d8e15961aa68|Unidirectional Relaxation Simulation (URS)|The authors propose a URS approach to address the challenge of memory-efficient scalable graph processing. The URS approach is an extension of the relaxation simulation approach and allows for the efficient processing of large graphs by considering only the child nodes.
7a598385-43ef-5373-b08d-d030c48ef931|Bidirectional Communication in Dual Relaxation Simulation (DRS)|The authors propose a solution to optimize communication efficiency in distributed algorithms by introducing bidirectional communication in the Dual Relaxation Simulation (DRS) algorithm. This approach allows vertices to send messages to both their matched parents and children, reducing the need for additional communication rounds.
7032caf1-89ad-576e-9bfe-9027f6992746|Bulk Synchronous Parallel BSP Model|The authors propose using the Bulk Synchronous Parallel BSP model to design parallel algorithms for graph pattern matching. This solution specifically addresses the challenge of efficient graph dynamics processing by allowing for the concurrent computation of multiple vertices in the graph, reducing the overall computational cost and iterations required for graph updates.
d59fd9fa-c7ed-56b3-8db8-19e51c132e20|Unidirectional Relaxation Simulation URS|The authors propose the Unidirectional Relaxation Simulation URS algorithm, which extends graph simulation by allowing partially absent vertices. This solution specifically addresses the challenge of efficient graph dynamics processing by reducing the number of iterations required for graph updates and allowing for more flexible graph matching.
e311e5ed-e628-5785-8c33-7b988b28487d|Dual Relaxation Simulation DRS|The authors propose the Dual Relaxation Simulation DRS algorithm, which extends the URS algorithm by considering both child and parent nodes. This solution specifically addresses the challenge of efficient graph dynamics processing by further reducing the number of iterations required for graph updates and allowing for more flexible graph matching.
45575ba8-6ccd-5926-adf4-243adbd3c378|SCC-Centric Execution Model|The SCC-Centric Execution Model is a novel approach to memory-efficient scalable graph processing. It addresses the challenge by leveraging the strongly connected component (SCC) structure in directed graphs to reduce redundant disk I/O accesses and improve convergence speed.
917045ba-9ea2-5687-82c3-cbd2738e91e3|Level-Based Graph Processing Method|The Level-Based Graph Processing Method is a technique used in conjunction with the SCC-Centric Execution Model to further improve memory efficiency and scalability. It addresses the challenge by dividing the DAG sketch into levels and processing the SCCs at each level in parallel.
29f6f1c8-1dd0-568b-9a10-fe0b8517019d|Incremental Maintenance of DAG Sketch|The Incremental Maintenance of DAG Sketch is a technique used to support evolving graphs. It addresses the challenge by maintaining the DAG sketch in an incremental way, allowing for efficient updates to the graph structure.
7eb60330-2606-5199-a4f5-dc7570f02e11|Hierarchical Graph Partitioning Algorithm (HGPA)|HGPA is a distributed algorithm designed to efficiently process massive graphs within a distributed computing environment. It addresses the challenge of memory-efficient scalable graph processing by employing a hierarchical graph partitioning strategy, which reduces memory consumption and optimizes memory usage.
046e24e7-cb1d-5aa0-be49-976c23c34408|Distributed Partial Vectors Computation|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a distributed approach to computing partial vectors, which are used to construct the Personalized PageRank Vector (PPV).
864963bc-0d12-5f05-a37a-b5a9266d8544|Distributed Skeleton Vectors Computation|This solution involves computing skeleton vectors for all nodes in parallel on separate machines, utilizing a dynamic programming algorithm.
5aced49e-d7a9-567b-b734-ce71f06fa624|Distributed Pre-Computation|The distributed pre-computation solution is a technique used in conjunction with HGPA to pre-compute partial vectors and skeleton vectors for each node in the graph. This approach addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by reducing the computational cost and enhancing memory locality.
6539300d-a154-5c11-b5cb-50346698581e|Hub Node Selection|This solution involves selecting hub nodes to partition the graph into smaller subgraphs. It addresses the challenge of efficient graph dynamics processing by reducing the computational cost and iterations required for PPV computation.
e6e85d54-2e04-5286-a7cc-880c24d7a5c5|Load Centrality Computation|The authors propose a distributed algorithm for computing load centrality, which is a measure of the amount of traffic that passes through a given node in a network. The algorithm is designed to minimize the number of communication rounds while maintaining solution quality.
28cf9f44-2ad0-5a25-86a6-387402940378|Load Centrality Estimation|The authors propose a method for estimating load centrality, which is a related measure to betweenness centrality. The method is based on a distributed algorithm that exchanges messages with a bounded number of entries.
634168e7-37f1-56b8-b679-e380c28477a1|Load Centrality-based Optimization|The authors propose using load centrality, a related measure to betweenness centrality, to optimize load balance in distributed systems. Load centrality is defined as the amount of flow passing through a node, and the authors show that it can be used to determine the optimal frequency of sensing its neighbors.
eaba690c-83d1-5dd8-9f06-52b079f076e2|Distributed Betweenness Centrality Computation|The authors propose a distributed algorithm for computing betweenness centrality in dynamic graphs, which enables every node to compute its own centrality by exchanging messages with its neighbors. The algorithm is based on a distributed implementation of the dynamic programming algorithm by Brandes, which is well-suited for a distributed implementation. The algorithm uses a constant number of elementary operations per message, making it efficient for large graphs. The authors demonstrate the effectiveness of their algorithm through simulations, showing that it converges in a number of distance vector phases proportional to the diameter of the network.
3725b8c2-dbf3-5174-9208-0adee2908734|Adaptive Edge Sampling|The authors propose an adaptive edge sampling technique to reduce memory consumption while processing large graphs. This method involves sampling edges with different probabilities in different areas of the graph, with denser areas being sampled more aggressively.
7a32fec6-d77c-55b7-9617-c6de8fb9af89|Exclusive Core Labeling|The authors propose an exclusive core labeling algorithm to efficiently compute the coreness number of nodes in a graph. This algorithm works by iteratively removing nodes with low degree and updating the coreness number of remaining nodes.
5685b401-dff5-5ef9-af4c-4c5fd4d51681|MapReduce-based Algorithm|The authors propose a MapReduce-based algorithm to compute the k-core decomposition of a graph in a distributed computing environment. This algorithm works by partitioning the graph into smaller subgraphs and processing each subgraph in parallel.
6947a583-e93b-5c11-a4c7-8e18fd5e96d7|Hierarchical Core Labeling|The authors propose a hierarchical core labeling technique to handle irregular graph structures. This technique involves recursively applying the core labeling algorithm to subgraphs of the original graph.
f3927093-4273-5cff-b6a6-d93298ea43da|Streaming Algorithm for Core Labeling|The authors propose a streaming algorithm for core labeling that can handle large graphs with limited memory.
2e4ad35c-9222-5b16-9461-e08a2ef66c87|MapReduce Algorithm for Core Labeling|The authors propose a MapReduce algorithm for core labeling that can handle large graphs in a distributed setting.
541486a3-46a1-590b-ad65-a2744f43cf53|Adaptive Edge Sampling Strategy|The authors propose an adaptive edge sampling strategy to optimize load balance in distributed systems. This strategy involves sampling edges with different probabilities in different areas of the graph, with denser areas being sampled more aggressively.
d891fd75-a435-5297-80ab-03ced3491b13|Semi-Streaming Algorithm|The authors propose a semi-streaming algorithm to compute an approximate k-core decomposition of a graph in one pass using only O(n) space.
d55fb7e9-9ca9-587e-a5a9-ffd1052a0815|Adaptive Sampling for k-Core Decomposition|The authors propose an adaptive sampling technique for k-core decomposition, which efficiently processes dynamic updates in large graphs by iteratively estimating the coreness of all nodes in the graph through the analysis of sparse subgraphs. The solution involves adaptively sampling edges with different probabilities in denser areas of the graph and less aggressively in sparser areas. This approach allows for the efficient estimation of coreness numbers and the maintenance of the k-core decomposition under dynamic updates. The paper demonstrates the effectiveness of this solution through experiments on real-world graphs, showing that the adaptive sampling technique achieves a good approximation of the coreness numbers while using significantly less memory than the input graph.
3f353c34-a91e-512d-b6c1-43daf92230ec|Lower-Ranking Activation|This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the number of active vertices in each superstep, thereby minimizing memory consumption and communication overhead.
2117dd0e-2f19-593f-bee8-dc1a54c83003|Same-Status Activation|This solution addresses the challenge of memory-efficient scalable graph processing by further reducing the number of active vertices in each superstep, building upon the lower-ranking activation technique.
a6ece913-75ce-53b6-a21d-4f069473b02f|Synchronization-Based Computing Model (ScaleG)|This solution addresses the challenge of memory-efficient scalable graph processing by utilizing a synchronization-based computing model, ScaleG, which reduces communication costs by maintaining a copy of each vertex in each machine.
1e2e7105-79fc-5755-8f1c-cbaaae7c8389|Order-Independent MIS Computation (OIMIS)|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a novel distributed framework for MIS computation, OIMIS, which relaxes the order dependency in existing algorithms.
5a09b453-a00f-58c2-974c-9154a3abb264|Batch Update Processing|This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by processing batch update operations effectively. The mechanism involves activating all vertices in the affected area of the batch update and then running the algorithm.
b8990836-3b5a-51c5-a965-134966090aad|Batch Update Algorithm|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by introducing a batch update algorithm. This algorithm efficiently handles batch updates by activating all affected vertices and their neighbors, thereby reducing the overhead of frequent updates. The unique mechanism involved is the use of affected vertices and their neighbors to minimize the impact of irregular graph structures on performance.
a71fe99b-a990-5140-8431-be953b73c939|Order-Independent MIS Computation|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by introducing an order-independent MIS computation framework. This framework relaxes the order dependency in existing algorithms, making it more efficient and effective for handling irregular graph structures. The unique mechanism involved is the use of local property checks and selective activation of vertices, which helps to minimize the impact of irregular graph structures on performance.
a6e2a602-da77-5c0f-bfda-d7609edec26f|Lower Ranking Activation|This solution specifically addresses the challenge of optimizing load balance in distributed systems by reducing the number of active vertices in each superstep, thereby decreasing computation and communication costs. The unique mechanism involved is the selective activation strategy, which only activates lower-ranking neighbors for each vertex, significantly reducing the number of active vertices and the associated costs.
0f816ec2-e198-52f6-8f75-5066468afd7b|Same Status Activation|This solution addresses the challenge of optimizing load balance in distributed systems by reducing the number of active vertices in each superstep, thereby decreasing computation and communication costs. The unique mechanism involved is the selective activation strategy, which only activates vertices with the same status, reducing the number of active vertices and the associated costs.
c723f8c6-6c08-503b-8b73-737aca3dba2d|Dynamic OIMIS (DOIMIS)|DOIMIS is an extension of OIMIS for handling dynamic graph updates. It activates affected vertices and their neighbors to re-run the algorithm, ensuring the maintenance of a high-quality independent set.
285c54b6-1f03-51c8-8f99-016172232a36|2-Hop Local Neighborhood Information-Based Vertex Cover Algorithm|The authors propose a localized distributed algorithm that uses 2-hop local neighborhood information to construct a vertex cover of a given network. This algorithm is designed to be memory-efficient and scalable for processing massive graphs within distributed computing environments.
d95dd427-f3cb-5216-a597-3b4e9659c872|2-Hop Local Neighborhood Information-Based Distributed Algorithm|The authors propose a distributed algorithm that utilizes 2-hop local neighborhood information to construct the vertex cover of a given network. This approach enables nodes to make decisions based on their local subgraph, reducing the need for extensive communication.
7c624f9d-1673-5f2b-8c1d-f1e83f8c3302|Localized Distributed Algorithm for Vertex Cover|The authors propose a localized distributed algorithm for detecting vertex cover using 2-hop local neighborhood information in distributed systems. This algorithm is designed to handle heterogeneous graph structures with varying degrees, weights, and sparsity. The algorithm uses a scoring-based mechanism to minimize the number of nodes in the vertex cover by identifying nodes that cover more edges than others. Each node decides about its neighbors status in or out of the vertex cover based on the total edges and 1-hop neighbors in the subgraph. The algorithm generates smaller vertex covers than existing algorithms in all benchmarks, with an average difference of up to 11 in some evaluated graph instances.
55c9c4e2-f7c3-57e8-b06f-147ad70ed41a|Sub-graph Centric Programming Abstraction|The authors propose a sub-graph centric programming abstraction that targets the deficiencies of vertex-centric models for large-scale graph analytics on distributed systems. This model combines the scalability of vertex-centric programming with the flexibility of using shared memory algorithms at the sub-graph level.
777dc08c-391b-57cf-ac1a-421fea645928|GoFS Distributed Graph Store|The authors design a distributed graph storage system, GoFS, that stores partitioned graphs across hosts in a cluster. GoFS uses a sub-graph oriented model for mapping the partitions content to slice files, which form units of storage on the local file system.
da5cdd10-e6c1-5982-9917-f730e5b3ce5d|Gopher Sub-graph Centric Framework|The authors propose a sub-graph centric framework, Gopher, that executes sub-graph centric algorithms using the Floe data flow engine on a Cloud or cluster in conjunction with GoFS. Gopher allows users to implement their algorithm in Java within a Compute method where they get access to a local sub-graph object and data messages from the previous superstep.
9feda425-e1aa-58e1-86c9-8b150e3f6966|Sub-Graph Centric Programming Abstraction|The authors propose a sub-graph centric programming abstraction that targets the deficiencies of the vertex-centric model. This model combines the scalability of vertex-centric programming with the flexibility of using shared memory algorithms at the sub-graph level. The connected nature of sub-graphs provides stronger guarantees for such algorithms and allows exploiting degrees of parallelism across sub-graphs in a partition.
d12b24a3-df7c-5d52-affc-0124a6efb483|Co-Designed Distributed Persistent Graph Storage|The authors propose a co-designed distributed persistent graph storage, GoFS, which is optimized for sub-graph access patterns. GoFS stores partitioned graphs across hosts, one partition per machine, using the METIS tool to balance vertices per partition and minimize edge cuts.
0fa28615-c465-5c07-a10c-968ae8c181ea|BSP Execution with Sub-Graph Concurrency|The authors propose a BSP execution model that leverages sub-graph concurrency within a machine and distributed scaling using BSP. The Gopher framework has a compute worker running on each machine and a manager on one machine.
d577f0a3-6466-5cbb-b453-82807f0d7f63|GoFFish Framework|The authors introduce the GoFFish framework, which co-designs a distributed execution runtime (Gopher) with a distributed sub-graph aware storage (GoFS) to optimize sub-graph centric access patterns. The framework is designed to handle heterogeneous graph structures and irregular memory access patterns.
b9b2d7fd-0675-530b-9e4d-6c87b1e46b44|Sub-Graph Centric Load Balancing|The authors propose a sub-graph centric approach to load balancing in distributed systems. This approach involves partitioning the graph into sub-graphs and processing each sub-graph in parallel. The sub-graphs are processed in a way that minimizes the number of messages exchanged between them, reducing the communication overhead and improving the overall load balance.
0e99c221-7d77-592d-ab83-9e82bbe48bb3|Dynamic Partitioning|The authors propose a dynamic partitioning strategy that adapts to the changing workload and graph structure. This approach involves re-partitioning the graph at runtime to ensure that the workload is evenly distributed across the processors.
9154e634-06c0-54a5-a0e3-9b5d8177885e|Co-Designed Distributed Storage and Execution Model|The authors propose a co-designed distributed storage and execution model, where the storage is optimized for sub-graph access patterns, and the execution model is designed to leverage concurrency across sub-graphs.
7bd1ff14-4aeb-5667-83f0-219f47106916|Adaptive Sub-Graph Partitioning|The authors propose an adaptive sub-graph partitioning approach, which balances the number of sub-graphs per partition and minimizes edge cuts.
2954d7a9-0444-5f6b-b578-2bbf09b4d709|Distributed Improvement Algorithm|The Distributed Improvement Algorithm is a method for reducing the maximum degree of a spanning tree in a distributed graph processing environment. It iteratively applies a series of improvements to the tree, with each improvement reducing the degree of a high-degree node.
3c7b8bdc-6003-5dd1-a8df-83da83e12ff2|REHAB Algorithm|The REHAB Algorithm is a method for reducing the maximum degree of a spanning tree in a distributed graph processing environment. It uses a technique called
ee316e66-2007-5fd8-b100-fe140b1bae7b|Component Primitives for Efficient Communication|The authors propose the use of component primitives, specifically COMPONENT BROADCAST, COMPONENT MAX, and COMPONENT MERGE, to optimize communication efficiency in distributed algorithms. These primitives enable the dissemination of information within components, calculation of max functions, and updating of nodes within newly merged components.
342c04b2-5713-5f61-b73e-892af4ea1087|Matching-Based Minimum Degree Spanning Tree Algorithm|The authors propose a matching-based algorithm, MATCHING-MDST, which constructs a minimum degree spanning tree by iteratively merging components using component matchings. The algorithm uses the component primitives to efficiently disseminate information and merge components.
6b3f522a-f316-5059-a5c5-c1ea687925cc|Local Improvement Algorithm for Minimum Degree Spanning Tree|The authors propose a local improvement algorithm that iteratively improves the degree of a given spanning tree by applying a series of q, 0 q improvements. The algorithm uses the component primitives to efficiently disseminate information and apply the improvements.
718a8072-40d8-5df5-b706-dce683ca22ce|MATCHINGMDST Algorithm|The MATCHINGMDST algorithm is a distributed algorithm designed to construct a spanning tree with a maximum degree of d O d log n in O D n log2 n rounds, where D is the diameter of the graph. This algorithm addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by iteratively merging components using a combination of maximal component matchings and 1, d component matchings.
e87516c2-cabc-510d-b595-7eb235990f1f|CONSTRAINED MATCHING Algorithm|The CONSTRAINED MATCHING algorithm is a distributed algorithm designed to find a maximal constrained 1, q matching in a bipartite graph. This algorithm addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by finding a maximal matching in a graph with heterogeneous vertex and edge attributes.
d9567003-a2e9-5348-981b-451c33965c37|EPOCHS Algorithm|The EPOCHS algorithm is a distributed algorithm designed to efficiently process graph dynamics by iteratively improving the degree of nodes in a spanning tree. It works by running the REHAB algorithm with progressively smaller block sizes to achieve a better approximation of the minimum degree spanning tree.
7271b34e-9815-55cb-92a0-7c61ee97d2c2|IMPROVE Algorithm|The IMPROVE algorithm is a distributed algorithm designed to efficiently process graph dynamics by identifying valid improvements to the degree of nodes in a spanning tree. It works by identifying a subgraph of good oriented edges and applying a series of improvements to reduce the degree of nodes in the tree.
1fdc9d7b-70ee-5b5a-a14f-807b1e22de6b|COMPONENT MERGE Algorithm|The COMPONENT MERGE algorithm is a distributed algorithm designed to efficiently process graph dynamics by merging components in a graph. It works by selecting a new leader for the merged component and updating the component size.
c83cb7b9-3d2a-5780-b382-638e9e119667|COMPONENT BROADCAST Algorithm|The COMPONENT BROADCAST algorithm is a distributed algorithm designed to efficiently process graph dynamics by broadcasting a message to all nodes in a component. It works by using a convergecast strategy to disseminate the message to all nodes in the component.
220595d3-1971-5dae-8337-690b28e861be|COMPONENT MAX Algorithm|The COMPONENT MAX algorithm is a distributed algorithm designed to efficiently process graph dynamics by finding the maximum value in a component. It works by using a convergecast strategy to disseminate the maximum value to all nodes in the component.
07a701fc-e200-589d-92f4-511f1be598bd|D CM Algorithm|The D CM algorithm is a distributed algorithm designed to efficiently process graph dynamics by finding a 1, d component matching in a bipartite graph. It works by modifying a maximal matching algorithm to also work for components.
6d4a4b16-6914-5338-9b05-01820472473c|MATCHING MDST Algorithm|The MATCHING MDST algorithm is a distributed algorithm designed to efficiently process graph dynamics by finding a minimum degree spanning tree. It works by iteratively adding edges to the tree while maintaining the tree structure.
94c3187a-ea5f-59c2-a8c0-e880cd170852|COMPONENT MATCHING Algorithm|The COMPONENT MATCHING algorithm is a distributed algorithm designed to efficiently process graph dynamics by finding a component matching in a graph. It works by modifying a maximal matching algorithm to also work for components.
bdd432be-c6da-5a02-8c3b-9af50f72a87e|Graph Sketch-based Path Processing|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a novel graph sketch-based path processing approach. The approach represents the graph into paths and extracts a small graph sketch consisting of the paths between important vertices. This sketch serves as a fast bridge for most state propagations, enabling the system to efficiently process the graph along the paths in the sketch multiple times within each round of graph processing.
710f397c-43dd-5286-95f9-279277afede9|Forward-Backward Intra-Path Processing|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a forward-backward intra-path processing approach. The approach processes the graph along the paths in a forward-backward way, enabling the system to efficiently conduct vertex state propagations along the graph paths with high parallelism.
c99f5c43-b83b-5811-a57f-0aa3e6bddca5|Asynchronous Intra-Path Propagation|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing an asynchronous intra-path propagation method. This method enables the state propagations of most vertices to be effectively conducted on the GPUs in a concurrent way, thereby reducing the number of communication rounds.
851a4947-2464-551f-bb2c-251d8594830d|Structure-Aware Asynchronous Execution|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a structure-aware asynchronous execution approach. This approach enables the state propagations of most vertices to be effectively conducted on the GPUs in a concurrent way, thereby reducing the number of communication rounds.
61583086-ead8-5ba1-8031-f4aa2afd3901|Structure-Aware Asynchronous Execution Approach|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by proposing a structure-aware asynchronous execution approach. This approach enables the state propagations of most vertices to be effectively conducted on the GPUs in a concurrent way, thereby increasing the GPU utilization ratio.
762da3a4-4396-5d83-bfca-3787094474ec|Graph Sketch Extraction|This solution addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by proposing a graph sketch extraction method. This method identifies the paths between important vertices in the graph, which are used to create a graph sketch that serves as a fast bridge for most state propagations.
8c8e1535-58f8-5b3b-bdfb-289881a39785|Asynchronous Path-Based Processing|This solution addresses the challenge of load balance optimization by proposing an asynchronous path-based processing approach. The authors suggest dividing the graph into paths and processing them asynchronously to achieve better load balance. This approach enables the state propagations of most vertices to be effectively conducted on the GPUs in a concurrent way, leading to a higher GPU utilization ratio.
dedaece6-d5e4-5f44-913a-97fcc96564c9|Path-Based Memory Access Optimization|This solution involves reorganizing graph data into paths and storing them in a contiguous manner in GPU memory to optimize memory access patterns. By doing so, the solution enables coalesced memory access, reducing memory access latency and improving overall graph processing performance.
26813b1f-1811-5e5d-96e9-30111bff0190|Graph Sketch-Based Memory Access Optimization|This solution involves extracting a graph sketch from the original graph and processing it multiple times within each round of graph processing to optimize memory access patterns. By doing so, the solution enables efficient memory access and minimizes synchronization overhead.
b4fa839d-1198-5bf7-903f-6bf3c70e7618|AsynGraph|AsynGraph is a novel system designed to maximize data parallelism for efficient iterative graph processing on GPUs. It proposes an efficient structure-aware asynchronous execution approach to fully exploit the high parallelism of GPUs for faster convergence speed of iterative graph processing.
7893d3ba-6033-5c2d-9e44-8c50e45a3a41|Distributed Graph Realization|The authors propose a distributed graph realization algorithm that addresses the challenge of memory-efficient scalable graph processing by realizing a given degree sequence in a distributed manner. The algorithm works by ensuring that the highest degree node is satisfied first and once satisfied, its degree is set to 0. This process is repeated in parallel, allowing several nodes of the highest degree to be satisfied simultaneously.
8affcef0-44b8-50e4-8834-e3d8da1c56c0|Connectivity Threshold Realization|The authors propose a connectivity threshold realization algorithm that addresses the challenge of memory-efficient scalable graph processing by realizing a given connectivity threshold in a distributed manner. The algorithm works by ensuring that the number of edges in the overlay network is larger by at most twice that of the optimal realization.
cac1f9db-f17f-5628-adaf-9bd678b354e2|Non-Preassigned Degree Realization|The authors propose a non-preassigned degree realization algorithm that addresses the challenge of memory-efficient scalable graph processing by realizing a given degree sequence in a distributed manner, without assigning specific degrees to nodes.
dcc5fb6a-8455-55c2-9d4d-85f1ea16c131|Graph Equivalence|The authors propose a graph equivalence algorithm that addresses the challenge of memory-efficient scalable graph processing by transforming any low-degree graph into a directed path in O(polylog n) rounds.
d15ef1ac-f3d6-5026-bc93-d21464dcf687|Distributed Degree Realization Algorithm|"The authors propose a distributed degree realization algorithm that optimizes communication efficiency by minimizing the number of rounds required to realize a given degree sequence. The algorithm works by iteratively satisfying the highest degree nodes in parallel, ensuring that each node learns the IDs of its neighbors in a efficient manner. Mechanisms/Techniques: The algorithm uses a combination of sorting, aggregation, and selective broadcasting to minimize the number of rounds. The authors also employ a technique called ""prefix sums"" to efficiently compute the number of children each node requires, reducing the number of communication rounds. Results: The authors show that their algorithm achieves an optimal round complexity of O(min(m/p, D)) for implicit degree realization, where m is the number of edges, p is the number of processors, and D is the maximum degree."
af98518c-a4a5-5e73-93df-8b6e5d710dba|Distributed Tree Realization Algorithm|"The authors propose a distributed tree realization algorithm that optimizes communication efficiency by minimizing the number of rounds required to realize a given tree. The algorithm works by iteratively constructing a balanced binary tree, ensuring that each node learns the IDs of its neighbors in a efficient manner. Mechanisms/Techniques: The algorithm uses a combination of sorting, aggregation, and selective broadcasting to minimize the number of rounds. The authors also employ a technique called ""prefix sums"" to efficiently compute the number of children each node requires, reducing the number of communication rounds. Results: The authors show that their algorithm achieves an optimal round complexity of O(log n) for explicit tree realization, where n is the number of nodes."
a42bacd2-3c0e-5686-82ec-f8be26b92a67|Distributed Connectivity Threshold Realization Algorithm|"The authors propose a distributed connectivity threshold realization algorithm that optimizes communication efficiency by minimizing the number of rounds required to realize a given connectivity threshold. The algorithm works by iteratively satisfying the highest degree nodes in parallel, ensuring that each node learns the IDs of its neighbors in a efficient manner. Mechanisms/Techniques: The algorithm uses a combination of sorting, aggregation, and selective broadcasting to minimize the number of rounds. The authors also employ a technique called ""prefix sums"" to efficiently compute the number of children each node requires, reducing the number of communication rounds. Results: The authors show that their algorithm achieves an optimal round complexity of O(D) for explicit connectivity threshold realization, where D is the maximum degree."
2385f920-4827-5fec-8cfa-cf11a8d78917|Token Collection|The authors propose a token collection algorithm that can efficiently collect tokens from nodes in a distributed graph, which is useful for handling irregular graph structures. The algorithm uses a combination of aggregation and multicast to collect tokens from nodes. It also employs a unique group ID to ensure that tokens are correctly delivered to their destinations. The authors demonstrate the effectiveness of their algorithm through various experiments and provide a theoretical analysis of its time complexity.
f898b836-d117-5ee7-a7da-23d729783fa4|Distributed Degree Realization|The authors propose a distributed degree realization algorithm that can handle heterogeneous and irregular graphs by realizing degree sequences in a distributed setting. The algorithm uses a combination of sorting, aggregation, and token collection to realize degree sequences. It also employs a balanced binary search tree to efficiently collect and aggregate data. The authors demonstrate the effectiveness of their algorithm through various experiments and provide a theoretical analysis of its time complexity.
a3b6a4c7-cf34-5c22-9b61-7e0cd87e88a9|Message Aggregation Strategy|The authors propose a message aggregation strategy to reduce massive redundant messaging and avoid the memory over ow problem while processing large scale graphs. This strategy involves each vertex maintaining a message vector according to the priority of neighbors and sending the message vector to each machine instead of its neighbors.
c8206b45-276f-5f87-bbc2-185c55ce95d7|Task Split Strategy|The authors propose a task split strategy to maintain tip numbers when given bipartite graphs are updated. This strategy involves reconstructing a subgraph consisting of candidate vertices and peeling them according to their 2 hop neighbors tip numbers.
205f5eb4-bf95-5085-b020-c0f49539a036|Distributed Tip Decomposition Algorithm (DTDA)|The authors propose a distributed tip decomposition algorithm (DTDA) to compute the tip numbers of vertices in a given bipartite graph. This algorithm involves partitioning the vertices into multiple independent subsets that can be concurrently peeled.
e56e8da2-f3f9-5143-b710-e4694a9736d2|Distributed Tip Maintenance Algorithm (DTMA)|The authors propose a distributed tip maintenance algorithm (DTMA) to maintain the tip numbers of vertices when given bipartite graphs are updated. This algorithm involves applying a BFS based manner to nd candidate vertices and updating the support value of the vertex.
24fd445f-5693-5c0b-8b99-6de7952e8f42|Candidate Sharing Theory|The authors propose a candidate sharing theory to reduce redundant computation of sub-tasks with two same endpoints.
e88e6ba6-2cfe-5aa4-bcc1-35848d261924|Task-Split Strategy|The authors propose a task-split strategy to maintain tip numbers when given bipartite graphs are updated. This strategy involves breaking down the complex problem into several sub-tasks, each of which is processed in serial.
88448eef-c7c9-56af-a519-b64d546c9369|Distributed Butterfly Counting Algorithm (DBCA)|DBCA is a distributed algorithm designed to efficiently count the number of butterflies in large-scale bipartite graphs. It addresses the challenge of efficient graph dynamics processing by utilizing a priority-based message passing strategy to reduce the number of messages sent and received.
b60e1659-d2ad-5e29-95a8-797d5874be04|Update Combination Mechanism|The authors propose an update combination mechanism to combine the updates that have the same destination vertex before writing them into the external memory. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the number of external memory writes and minimizing the memory consumption.
e1c03b93-38d9-592d-845d-97d2c445efa8|Inter-Partition Parallelism|The authors propose to process distinct partitions in parallel using multiple processing engines. This solution specifically addresses the challenge of memory-efficient scalable graph processing by increasing the parallelism and reducing the processing time.
a45de90e-2541-57e9-918c-53f7a516955f|Intra-Partition Parallelism|The authors propose to employ parallel pipelines to concurrently process distinct edges or updates of each shard during the scatter-gather phase. This solution specifically addresses the challenge of memory-efficient scalable graph processing by increasing the parallelism and reducing the processing time.
d72e068e-e176-5b5e-a137-d06e67ce5088|Parallel Pipelined Processing|The authors propose parallel pipelined processing to concurrently process distinct edges or updates of each shard, improving performance and reducing the need for external memory accesses.
95363829-dc0a-5e6a-986d-c3cdead1187c|Hazard Detector|The authors propose a hazard detector to prevent race conditions when accessing vertex attributes in parallel.
f0c0fd65-be05-547a-a826-a0c5adca7f06|Per-Vertex Parallelization with Load Balancing|The authors propose a per-vertex parallelization approach to address the challenge of memory-efficient scalable graph processing. This solution involves creating a separate subproblem for each vertex and processing them in parallel using the ParTTT algorithm. To achieve load balancing, the authors consider different methods for ranking the vertices, such as degree-based, triangle count-based, and degeneracy-based ranking.
64158277-c8a4-51c7-bc98-946c6d2f6e10|Shared-Memory Parallel Algorithm for Maximal Clique Enumeration|The authors propose a shared-memory parallel algorithm, ParMCE, for maximal clique enumeration. This solution involves using a parallel algorithm, ParTTT, to enumerate maximal cliques from each subgraph induced by a vertex and its neighborhood.
0ce8f0c5-eebb-5da2-8c6d-fec43f41145f|Work-Efficient Parallel Algorithm for Maximal Clique Enumeration|The authors propose a work-efficient parallel algorithm, ParTTT, for maximal clique enumeration. This solution involves using a parallel algorithm to enumerate maximal cliques from a graph, with a focus on minimizing memory consumption and improving efficiency.
af67c9f2-aaa7-5cf8-9530-770cd0e14b08|ParMCE|ParMCE is a shared memory parallel algorithm for Maximal Clique Enumeration (MCE) in large graphs. It addresses the challenge of efficient graph dynamics processing by providing a work-efficient parallelization of a sequential algorithm due to Tomita et al. ParMCE uses a ranking function to order vertices and then processes each vertex in parallel, enumerating all maximal cliques containing that vertex. This approach allows for efficient processing of dynamic updates in large graphs.
316c0e19-baf5-53ad-98ff-b60d8ff2eb08|Triangle Indexing (TrIndexing)|TrIndexing is a technique that precomputes and indexes the triangles of the data graph to facilitate pruning infeasible results. This approach enables the algorithm to reduce memory consumption by avoiding the storage of unnecessary intermediate results.
57ed1b8a-4f00-590e-a01b-5dbd01f790fa|Batching|Batching is a technique that divides the whole computation into sub-tasks that can be evaluated independently, reducing memory consumption by avoiding the need to store large intermediate results.
0c6b5e81-19e2-5503-8328-e660afee3d2f|Compression|Compression is a technique that maintains the intermediate results in a compressed form to reduce maintaining and communication cost.
e501a51b-433b-5726-b3ab-d981f6392a49|FullRep|FullRep is a strategy that maintains the whole graph in each partition and parallelizes embarrassingly.
4986d569-152b-5f28-af74-84f8074a6c3e|WOptJoin|WOptJoin is a strategy that computes subgraph matching by growing the partial results in a worst-case optimal manner.
f484e206-a962-5e6c-b5fa-42e42fb96d18|BinJoin|BinJoin is a strategy that computes subgraph matching by solving a series of binary joins.
25f29f76-d784-5cb4-99c8-8c9721ed2bbf|Triangle Partition|Triangle Partition is a technique that allows BinJoin to use clique as the join unit, which can greatly shorten the execution especially when the query is dense.
c026657d-cc2f-59c1-9ccf-31544fbaf836|TrIndexing|TrIndexing is a technique used to precompute and index the triangles (3-cycles) of the graph to facilitate pruning and reduce communication cost. TrIndexing involves grouping vertices that are connected to the same set of vertices, allowing for local computation of intersections and reducing the need for communication. The paper shows that TrIndexing can greatly improve the performance of BinJoin and WOptJoin algorithms, especially when the graph is dense.
a78fd621-25c0-51b9-a779-03e5849fe53c|Proxy Edge-Based Partitioning Policy|The authors propose a novel application-agnostic graph partitioning strategy that duplicates edges to avoid communication during triangle counting. This approach is designed to distribute an undirected graph across multiple machines, ensuring that each host has a complete view of its local subgraph, thereby eliminating the need for inter-host communication during the computation phase.
019394b7-037c-5e47-a29a-0b07f4fb46eb|Proxy Edge-Based Partitioning|The authors propose a novel graph partitioning strategy that duplicates edges to avoid communication during triangle counting. This approach is designed to minimize communication overhead by ensuring that all necessary information for triangle counting is available locally on each host.
a916311f-2a02-5c56-bfa1-550ddca71eea|Binary Search-Based Intersection|The authors employ a binary search-based intersection method for finding triangles, which improves performance on GPUs due to better exploitation of memory bandwidth.
a1e82f20-d664-54db-9a70-00df87ae2901|Local Triangle Counting with Aggregation|The authors propose a local triangle counting approach that counts triangles independently on each host without communication, and then aggregates the local counts at the end to obtain the final triangle count.
4b6fa0f3-8005-5410-90f4-827a6a0f92f2|Binary Search-Based Intersection Method|The authors employ a binary search-based intersection method for finding triangles, which improves performance on GPUs due to improved exploitation of memory bandwidth. This method involves using binary search to find triangles instead of edgelist intersection, which reduces memory access patterns and improves coalesced memory accesses. The paper reports that this method achieves better performance on a single host compared to the merge-based intersection method used in IrGL, with an average speedup of 1.54.
3db49409-85d8-5b25-83ac-78772866c604|Proxy Edge Based Partitioning Policy|The authors propose a novel application-agnostic graph partitioning strategy that eliminates almost all inter-host communication during triangle counting. This approach is designed to optimize load balance in distributed systems by ensuring that each host has a balanced workload and can process its assigned tasks independently without requiring frequent communication with other hosts.
93b31f85-e036-5192-8434-fc7d2fd41ea2|Binary Search-based Triangle Counting|The authors propose a binary search-based triangle counting approach to optimize GPU memory access for graph processing. This method involves using binary search to find triangles in the graph, which improves memory locality and reduces memory access overhead.
4704bc88-8751-51f9-b77a-fc87f2bbbe90|Edge Proxy-based Partitioning|The authors propose an edge proxy-based partitioning approach to optimize GPU memory access for graph processing. This method involves creating edge proxies to reduce memory access overhead and improve memory locality.
1b5bbf4a-2412-5eb5-9daa-7979653a53a8|Hybrid Approach using Matrix-based Insights|This solution proposes a hybrid approach that combines matrix-based insights with existing parallelization techniques to optimize memory usage and scalability in graph processing.
d059ff32-ec30-5f78-9852-124cf3090521|Dynamic Thread Creation and Assignment|This solution involves dynamically creating and assigning threads to handle varying workloads in graph processing, reducing memory contention and improving scalability.
e08c98d8-d71e-56f5-9a5c-4153523c581f|Partitioned Global Address Space Computing|This solution proposes using partitioned global address space computing to optimize memory usage and scalability in graph processing.
cc530567-5d24-5aa6-a206-407aeccf461c|Multithreaded Graph Library (MTGL)|This solution proposes using the MTGL to optimize memory usage and scalability in graph processing.
2f53a35a-fead-581a-b9f4-ba5efab38a34|UPC-based Approach|This solution proposes using UPC-based programming to optimize memory usage and scalability in graph processing.
db2f221c-d20f-5241-96c6-a89312920551|Compressed Representation of Data Structures|This solution proposes using compressed representations of data structures to optimize memory usage in graph processing.
7425f455-3990-5680-bd3c-4f09616fd5d9|Hashing Scheme for Vertex Assignment|This solution proposes using a hashing scheme to assign vertices to processors, reducing memory consumption and improving scalability.
78a6948f-627f-50b8-a6a1-5f734e6b0f78|Ghost Cells for Optimizing Runtime|This solution proposes using ghost cells to optimize runtime and reduce memory consumption in graph processing.
515b56bb-d514-5813-886c-d799f8e93205|Load Balancing through Dynamic Thread Creation|This solution proposes using dynamic thread creation to balance loads and reduce memory contention in graph processing.
5d330d83-012a-52cd-8007-d9e9eee2edb3|Simultaneous Query Processing|This solution proposes using simultaneous query processing to optimize memory usage and scalability in graph processing.
9f7d8fee-394b-5d9a-b7d4-9369ba7761f4|One-Sided Communication|One-sided communication is proposed as a solution to optimize communication efficiency in distributed algorithms by reducing the latency and bandwidth challenges.
d69deeae-32ad-5284-8465-c171269bf459|Partitioned Global Address Space (PGAS) Computing|PGAS computing is proposed as a solution to optimize communication efficiency in distributed algorithms by providing a global address space without the complexity and performance cost of cache coherence.
04b086f7-2f55-5231-9a58-7f70a1ef0d1a|Partitioned Global Address Space Languages|The authors propose using partitioned global address space languages, such as UPC, to address the challenge of optimizing load balance in distributed systems. This approach provides a global address space computing model while running on inexpensive, distributed memory hardware.
91510459-d0ec-5458-88d9-4d5717e727f3|Hashing Scheme|The authors propose using a hashing scheme to assign vertices to processors, which can result in memory savings compared to ghost cells.
ddc3a18d-7d91-51ae-8b1a-31c4e58719a3|Ghost Cells|The authors propose using ghost cells to optimize runtime and improve load balancing in distributed systems.
6f3720e1-d214-5721-9d60-988631db407d|Hybrid Routing Table Design|The authors propose a hybrid routing table design that combines the benefits of both compute and lookup designs. This design uses a range-based routing table that maps dense ranges of virtual IDs to the respective partition, which has a low memory footprint and can fit into the cache of the multiprocessors. Additionally, a dictionary is used to map virtual vertex IDs to the original vertex IDs, which is generated after the locality-aware partitioning strategy is applied.
a8050f45-7a3f-59cc-b794-b3fba3014743|Redundancy in Terms of Partitioning|The authors propose using redundancy in terms of partitioning to reduce the need for broadcasts and improve query performance. By redundantly storing the graph partitioned by source vertex and partitioned by target vertex, the system can avoid broadcasts and improve scalability.
0f4debb6-68e2-5390-9c43-90b61ca8d822|Locality-Aware Partitioning Strategy|The authors propose using a locality-aware partitioning strategy to create well-balanced partitions that minimize the edge cut of the partitioning. This approach considers graph properties like locality or semantic relationships between vertices.
4bd90647-4652-51cd-af20-fbcf949759b3|Hybrid Design Routing Table|The authors propose a hybrid design routing table that combines the low memory footprint of the compute design and the locality awareness of the lookup design. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the number of remote memory accesses on NUMA systems.
17206cc8-b89d-5080-9d0b-8c079adc3901|Redundancy in Partitioning|The authors propose using redundancy in partitioning to reduce the need for broadcasts and improve communication efficiency. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by minimizing the number of communication rounds.
b8bda2de-0558-50a5-9776-ac5f1f9d7eac|Asynchronous Query Processing Model|The authors propose an asynchronous query processing model for graph pattern matching that allows for efficient communication and reduces the need for synchronization. This solution specifically addresses the challenge of optimizing communication efficiency in distributed algorithms by minimizing the number of communication rounds.
02f28eef-2ac7-579a-b087-d66eb56222c6|Hybrid Design for Routing Table|The authors propose a hybrid design for the routing table that combines the low memory footprint of the compute design and the locality awareness of the lookup design. This design uses a range-based routing table that maps dense ranges of virtual IDs to the respective partition and has a very low memory footprint, allowing it to fit into the cache of the multiprocessors. Additionally, a dictionary is used to map virtual vertex IDs to the original vertex IDs, which is generated after the locality-aware partitioning strategy is applied.
bd204278-c8a4-56da-a441-7ef603f66862|Hybrid Design for Routing Table and Partitioning Strategy|The authors propose a hybrid design that combines the low memory footprint of the compute design and the locality awareness of the lookup design. This solution specifically addresses the challenge of optimizing load balance in distributed systems by enabling the system to leverage both the advantages from a compute design and a lookup design.
a1d6692f-c7c6-5183-a132-9c42b5aae5aa|Data-Oriented Architecture|The authors propose a data-oriented architecture for graph processing, which preserves data locality and minimizes concurrency-related overhead. The architecture uses a combination of worker threads and a message-passing layer to enable efficient processing of queries.
810c965e-663f-5d72-8cbf-c0796dedd891|Defective Coloring Algorithm|The authors propose a defective coloring algorithm to address the challenge of memory-efficient scalable graph processing. This algorithm is designed to reduce memory consumption by minimizing the number of colors used in the coloring process, which in turn reduces the memory required to store the colored graph.
f9e4d093-9333-58cb-a2e5-8cf403f20ec6|Set Theoretic Method|The authors propose a set theoretic method to address the challenge of memory-efficient scalable graph processing. This method involves using a family of sets to assign colors to vertices, which reduces memory consumption by minimizing the number of colors used.
37755470-5994-5cba-81da-f876a412d19d|Union 1 Cover-Free Family|The authors propose using a union 1 cover-free family to address the challenge of memory-efficient scalable graph processing. This family of sets is designed to minimize memory consumption by ensuring that each vertex can be assigned a color that is used by at most a certain number of its neighbors.
05ac2adb-4b1f-5403-be46-fe6666455651|Iterative Procedure for 1-Coloring|The authors present an iterative procedure for 1-coloring that builds upon their defective coloring algorithm. This procedure iteratively applies the defective coloring algorithm to reduce the number of colors used, ultimately achieving a 1-coloring.
92168e9c-11f7-551f-a921-529e71422145|Union 1-Cover Free Family Construction|The authors propose a construction for a union 1-cover free family, which is used in their defective coloring algorithm. This construction enables the algorithm to efficiently select colors for vertices while minimizing conflicts.
94049a3a-f2ed-5784-95b4-b48a719a5a5d|Combinatorial Method for Defective Coloring|The authors propose a combinatorial method for defective coloring that employs a different approach than their set-theoretic method. This method is designed to achieve efficient coloring in linear time.
d7cccd2d-9c9b-5ad8-a1e3-d63d99b2b482|KW Iterative Procedure|The authors propose the KW iterative procedure to optimize load balance in distributed systems. This procedure uses a iterative approach to assign colors to nodes, ensuring that each node has a unique color that is different from its neighbors.
f19da02a-ac9d-57ee-8d67-4793f690aab8|Algorithm Ak|The authors propose Algorithm Ak, which is designed to compute a 1-coloring in O(log k) time. This algorithm uses a combination of the defective coloring algorithm and the KW iterative procedure.
5895ae19-ea0a-5961-a3da-278ce8dd1992|Algorithm J|The authors propose Algorithm J, which is designed to compute a 1-coloring in O(log log log n) time. This algorithm uses a combination of the defective coloring algorithm and the KW iterative procedure.
d73c8153-17ce-5e0c-9308-a3600537711f|Distributed Delegate Partitioning|This solution addresses the challenge of memory-efficient scalable graph processing by introducing a novel graph partitioning technique called distributed delegate partitioning. This approach partitions high-degree vertices across multiple processors, reducing memory consumption and improving scalability.
03ad88bc-f226-5a5a-b332-abc4c34c75b5|Asynchronous Visitor Queue Abstraction|This solution addresses the challenge of memory-efficient scalable graph processing by introducing an asynchronous visitor queue abstraction. This abstraction provides a framework for parallelizing graph algorithms and reducing memory consumption by allowing vertices to be processed independently.
41743196-12f4-5eb8-9922-8bd0de63e86f|Routed Point-to-Point Communication|This solution addresses the challenge of memory-efficient scalable graph processing by introducing a routed point-to-point communication technique. This technique reduces communication overhead by routing messages through a synthetic network, minimizing the number of communicating pairs.
7a375ced-5419-5557-b4db-b3cb8dbb9bc3|Distributed Delegates Partitioning|This solution addresses the challenge of optimizing load balance in distributed systems by distributing high-degree vertices (hubs) across multiple partitions, thereby reducing the storage, computation, and communication imbalances associated with these hubs. The solution involves identifying high-degree vertices, distributing their edges across multiple partitions, and using a delegate tree structure to coordinate communication and computation among the delegates. This approach differs from existing methods, such as 1D and 2D partitioning, by specifically targeting high-degree vertices and using a tree-based structure to manage communication and computation. The paper demonstrates that distributed delegates partitioning achieves better load balance and scalability than existing methods, with a 15% improvement in performance over the best-known Graph500 results on the IBM BG/P Intrepid supercomputer.
9a48e942-bdaa-5bbe-bcac-07650f4eb935|Asynchronous Visitor Queue|This solution addresses the challenge of optimizing load balance in distributed systems by providing a framework for asynchronous graph traversal that can effectively handle high-degree vertices and reduce communication overhead. The solution involves using an asynchronous visitor queue to manage graph traversal, which allows for efficient handling of high-degree vertices and reduces communication overhead by minimizing the number of messages required. The paper demonstrates that the asynchronous visitor queue framework can effectively handle high-degree vertices and achieve good scalability, with a weak scaling study showing excellent performance up to 131K cores.
89ae1b0f-44e1-5d7e-87c0-e9135f767466|Distributed Multimodal Path Query (DMP) Framework|The DMP framework is designed to process multimodal path queries over large transportation networks in a distributed environment. It addresses the challenge of memory-efficient scalable graph processing by distributing the graph into smaller fragments, processing queries in parallel, and minimizing communication between machines.
b2d3c5a7-8ac8-5a4e-b972-bfeaff3c52c7|Graph Partitioning Approach|The graph partitioning approach is designed to partition large multimodal graphs into smaller fragments that can be processed in parallel. It addresses the challenge of memory-efficient scalable graph processing by minimizing the number of edges that need to be processed and reducing the communication overhead between machines.
f331d478-2878-5ebb-b0a9-672891f28912|Parallel Shortest Path Algorithm|The parallel shortest path algorithm is designed to compute shortest paths in multimodal graphs in parallel. It addresses the challenge of memory-efficient scalable graph processing by minimizing the number of nodes that need to be processed and reducing the communication overhead between machines.
e028fc36-6b37-5263-a662-d7c3c9e64477|Assembly Procedure|The assembly procedure is designed to assemble the results of parallel query processing into a single result. It addresses the challenge of memory-efficient scalable graph processing by minimizing the number of nodes that need to be processed and reducing the communication overhead between machines.
22c66b03-ce33-5231-b853-38a2fa989e28|Distributed Multimodal Path Query Algorithm (ALGdmp)|ALGdmp is a distributed algorithm designed to process multimodal path queries over large transportation networks. It aims to minimize communication efficiency by reducing the number of visits to each machine and the total network traffic. ALGdmp employs a novel approach by incorporating an automata that describes the regular language used in the query. It also uses a graph partitioning approach to accelerate the algorithm. The algorithm works in parallel, and each machine is visited only once, reducing the need for extensive communication. The paper reports that ALGdmp is parallel scalable, with a running time of O(m/k * n/k * log(n/k)), where m is the number of edges, n is the number of nodes, and k is the number of machines. The algorithm also achieves a significant reduction in network traffic, with a total network overhead of O( Vf ^2).
fd72fc73-3cca-5e1a-ba8c-bc9d4c95f9b5|Shortest Path Algorithm with Automata|The shortest path algorithm with automata is designed to process multimodal path queries over large transportation networks. It aims to minimize communication efficiency by reducing the number of visits to each machine and the total network traffic. The algorithm incorporates an automata that describes the regular language used in the query. It uses a novel approach to compute the shortest path, by incorporating the automata into the shortest path computation. The paper reports that the algorithm achieves a significant reduction in network traffic, with a total network overhead of O( Vf ^2). The algorithm also achieves a good balance between query time and communication costs.
8ac51156-cccc-5921-a254-e74de2d16a9e|Parallel Scalable Algorithm (ALGdmp)|ALGdmp is a parallel scalable algorithm designed to process distributed multimodal path queries over large transportation networks. It addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by utilizing a distributed computing framework that can handle varying graph structures and irregular memory access patterns.
d2c1dc68-992b-5e30-98c8-11c194961c7c|Distributed Multimodal Graph Partitioning (ALGprt)|ALGprt is a graph partitioning approach designed to support the parallel scalable algorithm ALGdmp. It addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by partitioning the graph into fragments that can be efficiently processed in parallel.
9759e4e6-6244-5291-aa87-3d28dda0b8ad|Assembling at Machine Mc (Ambl_Dis)|Ambl_Dis is an algorithm designed to assemble the partial answers from different machines to obtain the final answer. It addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by efficiently combining the results from different machines.
db4d9b43-ab89-518a-beec-783f4fbccc1f|ALGdmp|ALGdmp is a distributed algorithm designed to process multimodal path queries over large transportation networks. It aims to optimize load balance by minimizing the number of visits to each machine, reducing network traffic, and achieving parallel scalability.
877eaaaa-597a-5564-a11a-69f77e46ce67|ALGprt|ALGprt is a graph partitioning strategy designed to divide the multimodal graph into balanced fragments, ensuring that each machine processes a similar amount of work.
7843ae2b-8221-5e96-9e40-84ceec2c5ef3|Function System|The function system is a mechanism used in ALGdmp to compute the least travel time from the source to the destination.
02cd8b54-4263-51cd-95d6-12bb40de0278|Hybrid Approach Using BFS and SV Algorithm|The authors propose a hybrid approach that combines parallel Breadth-First Search (BFS) and Shiloach-Vishkin (SV) algorithm to compute connected components in undirected graphs. This approach addresses the challenge of memory-efficient scalable graph processing by dynamically selecting the most suitable algorithm based on the graph topology.
46d10803-2911-558a-ba24-7cba00efd1e5|Edge-Based Parallel Algorithm for Distributed Memory Systems|The authors propose an edge-based parallel algorithm for distributed memory systems based on the Shiloach-Vishkin (SV) approach. This algorithm addresses the challenge of memory-efficient scalable graph processing by reducing communication volume and improving load balancing.
cade77e6-b09a-51fc-9e73-4ea5de67b44a|Dynamic Approach for Runtime Algorithm Selection|The authors propose a dynamic approach that analyzes the graph structure and selectively uses the parallel BFS and SV algorithms to compute connected components. This approach addresses the challenge of memory-efficient scalable graph processing by adapting to the graph topology and minimizing communication overhead.
17ab5eb4-0c91-511d-ad22-d118f6cf9499|Load Balancing through Excluding Completed Partitions|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by excluding completed partitions from further processing, thereby reducing the amount of data that needs to be communicated among processors.
de802c8a-44c9-5e78-aaac-53cdd15feb9c|Parallel Sorting with Custom Reduction Operators|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by using parallel sorting with custom reduction operators to minimize the number of communication rounds.
edefe9f6-c297-5073-b617-44b3efbccd04|Hybrid Approach using BFS and SV Algorithm|The authors propose a hybrid approach that combines parallel Breadth-First Search (BFS) and Shiloach-Vishkin (SV) algorithm to compute connected components in undirected graphs. This approach is designed to adapt to heterogeneous graph structures by dynamically selecting the most suitable algorithm based on the graphs characteristics. The hybrid approach uses a pre-processing phase to classify the graph as scale-free by estimating the goodness of fit of its degree distribution to a power-law curve. If the graph is determined to be scale-free, the algorithm executes a BFS iteration to find the largest connected component before switching to the SV algorithm to process the remaining graph. This approach allows the algorithm to adapt to different graph topologies and achieve better performance. The authors report that their hybrid approach achieves a speedup of up to 24.5 compared to the state-of-the-art algorithm, and it can compute connected components in a graph with over 50 billion edges and 300 million components in less than 4 minutes using 32,761 cores.
323ac833-1352-56ff-8c37-47603d7fb462|Load Balancing using Partition Joining|The authors propose a load balancing technique that uses partition joining to reduce the imbalance of active elements across iterations. This technique is designed to address the issue of load imbalance in distributed memory systems. The technique involves joining partitions from larger IDs to smaller IDs, which helps to reduce the imbalance of active elements. The authors also use a bucket update mechanism to update the partition IDs of the edges. The authors report that their load balancing technique achieves a significant improvement in the total runtime, with a maximum speedup of 2.5 compared to the naive approach.
9afc4f6c-e4fe-58e7-9523-0d115ccbfe2d|Dynamic Pre-processing using Degree Distribution Statistics|The authors propose a dynamic pre-processing phase that uses degree distribution statistics to classify the graph as scale-free. This phase is designed to guide the algorithm selection at runtime and adapt to different graph topologies. The pre-processing phase uses a statistical framework to fit a power-law curve to the discrete graph degree distribution and estimate the goodness of fit with a one-sample Kolmogorov-Smirnov test. If the graph is determined to be scale-free, the algorithm executes a BFS iteration before invoking the SV algorithm. The authors report that their dynamic pre-processing phase achieves a significant improvement in the overall performance, with a maximum speedup of 3.5 compared to the opposite choice of executing BFS iteration only for large diameter graphs.
a06410e0-7454-5b98-b898-cfc5a240ad36|Load Balancing through Redistributing Active Tuples|This solution addresses the challenge of optimizing load balance in distributed systems by redistributing active tuples after each iteration of the parallel SV algorithm. The goal is to evenly distribute the active tuples across processors, reducing the imbalance of data distribution and its effect on the overall run time.
326a2f5e-0975-505c-9cb1-78022656763e|Removing Completed Components|This solution addresses the challenge of optimizing load balance in distributed systems by removing completed components along the iterations of the parallel SV algorithm. The goal is to reduce the size of the working set per each iteration, leading to a more balanced workload across processors.
5d2ff702-8dba-567d-bda5-a76d15c0f86d|Hybrid Approach for Efficient Graph Dynamics Processing|The authors propose a hybrid approach that combines parallel Breadth-First Search (BFS) and Shiloach-Vishkin (SV) algorithms to efficiently process dynamic updates in large graphs. This approach dynamically selects the optimal algorithm based on the graph topology, using BFS for scale-free networks and SV for large diameter graphs.
b8b764eb-92ef-5858-a3f3-3cfe62710c13|Load Balancing for Efficient Graph Dynamics Processing|The authors propose a load balancing technique to optimize the performance of their parallel SV algorithm. This technique removes completed partitions along iterations, reducing the size of the working set and improving load balance among processes.
da20fd52-57fe-5e64-9eb6-69e5629d1882|Parallel SV Algorithm for Efficient Graph Dynamics Processing|The authors propose a parallel SV algorithm for efficient graph dynamics processing. This algorithm uses a novel edge-based approach to compute connected components in large graphs, reducing the number of iterations required for maintaining graph structures.
22fa055e-9426-5fc0-9b6b-042c095cfa00|Lovsz Local Lemma (LLL) Algorithm|The authors propose an LLL algorithm that can be used to construct a number of combinatorial objects, including defective coloring, frugal coloring, and vertex coloring. This algorithm is designed to be memory-efficient and scalable, addressing the challenge of processing massive graphs within distributed computing environments.
014f17df-a572-56e3-99a1-2408f735bfe6|Derandomization Algorithm|The authors propose a derandomization algorithm that can be used to transform any randomized distributed algorithm for a locally checkable problem into an efficient deterministic algorithm. This algorithm is designed to be memory-efficient and scalable, addressing the challenge of processing massive graphs within distributed computing environments.
ba1d2c5a-cd20-5b4f-99a3-d28b6c4dd71d|Shattering Algorithm|The authors propose a shattering algorithm that can be used to construct a number of combinatorial objects, including defective coloring, frugal coloring, and vertex coloring. This algorithm is designed to be memory-efficient and scalable, addressing the challenge of processing massive graphs within distributed computing environments.
7e910338-779f-52c7-8188-c2df43b5692c|Network Decomposition-based Derandomization|The authors propose a derandomization technique for distributed algorithms using network decompositions. This approach transforms randomized algorithms into deterministic ones by leveraging the properties of network decompositions to reduce the number of communication rounds.
b4347ab6-fa6f-53f2-b704-7fecf97aaf34|Shattering-based Distributed Algorithm|The authors propose a shattering-based distributed algorithm for solving the Lovsz Local Lemma (LLL) problem. This approach involves a two-phase process, where the first phase uses a randomized algorithm to find a partial solution, and the second phase uses a deterministic algorithm to refine the solution.
744bad80-749f-5506-8a9c-c9b6077379b7|Bootstrapping-based Distributed Algorithm|The authors propose a bootstrapping-based distributed algorithm for solving the Lovsz Local Lemma (LLL) problem. This approach involves a recursive process, where the algorithm is applied to a smaller instance of the problem, and the solution is then used to solve the original problem.
20d39bab-e098-59a4-8571-e0f937dc9d27|Memory-Aware Task Colocation|This solution addresses the challenge of optimizing load balance in distributed systems by using machine learning to create an extensive model for different types of tasks. The model is used during task execution to estimate its behavior and future resource requirements, allowing for more efficient task colocation and load balancing.
d03ec21d-2c24-51eb-bb80-a70a72aef7d4|Heterogeneity-Aware Task Scheduler|This solution addresses the challenge of optimizing load balance in distributed systems by proposing a heterogeneity-aware task scheduler for Spark. The scheduler takes into account the heterogeneity of tasks and nodes in the cluster, allowing for more efficient task allocation and load balancing.
0786d112-161c-51bc-ae5f-63c256602818|Defective Coloring Technique|The authors propose a defective coloring technique to address the challenge of memory-efficient scalable graph processing. This technique involves computing a defective coloring of the graph, which allows for the partitioning of the graph into smaller subgraphs that can be processed independently.
aabbfee7-4b60-5f9d-bc21-70d3197207ea|Procedure Refine|The authors propose Procedure Refine, a technique for refining the coloring of a graph to reduce the number of colors used.
5adab6ff-2b79-58d4-a99f-b4600ec671f6|Procedure Tradeo Delta Color|The authors propose Procedure Tradeo Delta Color, a technique for trading off between the number of colors used and the running time of the algorithm.
4869fcc9-572d-534e-8a70-9e88a8280337|Delta-Color Procedure|The authors propose the Delta-Color Procedure, a technique for computing a 1-coloring of a graph in a distributed setting.
32279ead-8d4d-53b2-82f7-32f47dff5dd7|Procedure Defective Color|The authors propose a procedure called Procedure Defective Color, which is used to compute a defective coloring of the graph. This procedure involves iteratively refining the coloring of the graph, using Procedure Refine, until a defective coloring is obtained.
5acb44a3-78cb-549f-8df6-cde78ac1a3ee|Trade-off between Solution Accuracy and Communication Rounds|The authors propose a trade-off between solution accuracy and communication rounds, which involves sacrificing some solution accuracy in order to reduce the number of communication rounds required.
e4560cb7-bbb8-5f87-8891-bcb1e48bea9a|Procedure Delta Color|The authors propose a procedure called Procedure Delta Color, which is used to compute a k-coloring of the graph. This procedure involves using a defective coloring technique to reduce the number of communication rounds required.
dfe6c9c8-108f-5e2a-9f37-0a0986589d0a|SV Algorithm|The authors propose the SV algorithm, which is an algorithm for computing a c2-coloring of a graph.
0075647a-7d73-50ef-87ee-2acb1ee1dcc6|Trade-off Delta Color Algorithm|The authors propose a trade-off delta color algorithm to efficiently process graph dynamics. This algorithm allows for the computation of a trade-off between the number of colors used and the running time of the algorithm.
704ecf0d-6f6e-5669-a8fc-e0a8fd923c5e|Delta Color Algorithm|The authors propose a delta color algorithm to efficiently process graph dynamics. This algorithm allows for the computation of a 1-coloring of the graph in a single round.
452addcc-64a2-547c-bdfc-72c1bd0e330e|Low-Congestion Shortcuts|The authors propose the use of low-congestion shortcuts to address the challenge of memory-efficient scalable graph processing. This solution involves constructing shortcuts in the graph that minimize congestion and dilation, allowing for efficient communication and computation.
8ef9f8c4-2c60-59f3-856b-f3ee7d671846|Part-Wise Aggregation|The authors propose the use of part-wise aggregation to address the challenge of memory-efficient scalable graph processing. This solution involves dividing the graph into smaller parts and performing aggregation operations within each part, reducing the need for global communication and memory usage.
f1fb6758-cb8a-5005-a815-6df8e776dd70|Disjointness Gadgets|The authors propose the use of disjointness gadgets to address the challenge of memory-efficient scalable graph processing. This solution involves constructing gadgets that can be used to prove lower bounds on the time complexity of algorithms, providing insights into the fundamental limits of memory-efficient graph processing.
0a96cf25-2bca-5681-a3fa-9b48d20f7592|Crown Constructions|The authors propose the use of crown constructions to address the challenge of memory-efficient scalable graph processing. This solution involves constructing crowns that can be used to prove lower bounds on the time complexity of algorithms, providing insights into the fundamental limits of memory-efficient graph processing.
e6cad19e-53df-59c4-86f2-fd17e69528e8|Moving Cuts|The authors introduce Moving Cuts as a tool to prove distributed information-theoretic lower bounds. Moving Cuts are used to lift strong unconditional lower bounds from the classic communication complexity setting into the distributed setting.
7b88055e-d0a8-58b1-a360-791c71bd827a|Oblivious Routing Schemes|The authors propose Oblivious Routing Schemes as a solution to optimize communication efficiency in distributed algorithms. Oblivious Routing Schemes are designed to reduce the number of communication rounds required for solving global network optimization problems.
b9df03dc-7fb1-546b-9570-b6f73434e377|Shortcut Quality|The authors propose Shortcut Quality as a graph parameter that captures the inherent complexity of distributed network optimization problems. Shortcut Quality is used to prove lower bounds on the number of communication rounds required for solving distributed problems.
0d089878-f21e-5794-85bd-e9b3ff4c4e42|Hop-Constrained Oblivious Routing|The authors propose the use of hop-constrained oblivious routing as a solution to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This approach involves constructing routing schemes that can efficiently route messages in a graph while minimizing congestion and dilation.
84fb518c-e838-52fa-afad-3a69d69415c3|Crown Construction|The authors propose Crown Construction as a solution to address the challenge of efficient graph dynamics processing. Crown Construction is a novel approach to constructing efficient algorithms for managing and processing dynamic updates in large graphs.
98cdd2c8-198b-5d10-9733-6b515366be01|Supported CONGEST Algorithm|The authors propose a Supported CONGEST Algorithm as a solution to address the challenge of efficient graph dynamics processing. This algorithm is designed to efficiently process dynamic updates in large graphs.
fad5c318-b4dd-508b-88a4-650e98702c04|Randomized Delay Technique for Multi-Source Bounded-Hop Shortest Path Algorithm|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by introducing a randomized delay technique to minimize congestion in the multi-source bounded-hop shortest path algorithm. The technique involves adding a small delay to the starting time of each execution, ensuring that at most log n messages are sent through each edge in every round, with high probability.
7fb59b88-10a4-5819-9676-ad4313d1964b|Light-Weight h-Hop SSSP Algorithm|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a light-weight h-hop SSSP algorithm that reduces the number of communication rounds. The algorithm solves h-hop SSSP in O(h + D) time, where h is the number of hops and D is the network diameter.
f53285be-3c90-54e5-bf0b-6a5addc954f9|Reduction to Single-Source Shortest Path on Overlay Networks|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by reducing the single-source shortest path problem on a general network to a single-source shortest path problem on an overlay network. The overlay network is constructed by selecting a subset of nodes and adding edges between them.
ddd7b66a-5722-5a69-96c5-c4701954a71b|(1 + o(1))-Approximate SSSP on Overlay Networks|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing a (1 + o(1))-approximate SSSP algorithm on overlay networks. The algorithm solves SSSP on the overlay network in O(D(G)/ + ) time, where D(G) is the diameter of the overlay network,  is the number of nodes in the overlay network, and  is a constant.
0d8e5f07-15fd-5a3e-a209-e3d8003ecd01|O(n) Time Exact Algorithm for SSSP on Fully Connected Networks|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing an O(n) time exact algorithm for SSSP on fully connected networks. The algorithm solves SSSP on the fully connected network in O(n) time.
c5481693-d8d6-5985-8f11-c5b6bcbf4a5b|O(n) Time 2 + o(1) Approximation Algorithm for APSP on Fully Connected Networks|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by proposing an O(n) time 2 + o(1) approximation algorithm for APSP on fully connected networks. The algorithm solves APSP on the fully connected network in O(n) time.
ac176cde-b098-53e0-9773-753191cec6e7|Distributed Verification of Networks|The authors propose a distributed verification approach to address the challenge of memory-efficient scalable graph processing. This approach focuses on verifying whether a given subgraph of a network has a specified property, such as connectivity or bipartiteness, in a decentralized manner.
382da2d8-755d-55aa-8516-071612489f38|Simulation Theorem|The Simulation Theorem is a solution that addresses the challenge of optimizing communication efficiency in distributed algorithms by establishing a connection between communication complexity and distributed computing. This theorem allows for the reduction of lower bounds of problems in the standard communication complexity model to the lower bounds of the equivalent problems in the distributed version of communication complexity.
2c78719b-b3cf-54bf-9cc3-4efd9a2742c9|Distributed Verification of Functions|Distributed verification of functions is a solution that addresses the challenge of optimizing communication efficiency in distributed algorithms by providing a framework for verifying the correctness of functions in a distributed setting. This framework allows for the reduction of communication complexity lower bounds to distributed computing lower bounds.
19ad1156-2bdf-5ec9-9983-37b4a2bed7a6|Reduction from Communication Complexity to Distributed Computing|The reduction from communication complexity to distributed computing is a solution that addresses the challenge of optimizing communication efficiency in distributed algorithms by providing a framework for reducing communication complexity lower bounds to distributed computing lower bounds.
40a46fb9-fcb1-55db-b65e-c9efc223cb32|Distributed Verification Algorithm|The authors propose a distributed verification algorithm to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm is designed to verify whether a given subgraph of a network has a specified property, such as being connected or containing a cycle.
eaa40a9c-331d-5f33-bc64-6af8eae38cac|Communication Complexity-based Lower Bound|The authors propose a communication complexity-based lower bound to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This lower bound is used to establish a limit on the time complexity of distributed algorithms for solving certain problems on graphs.
44ee94df-7953-548d-b5bf-fc656fe65a54|Reduction Technique|The authors propose a reduction technique to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This technique is used to convert a verification problem into an optimization problem, which allows the authors to apply existing optimization algorithms to solve the verification problem.
e7e71e96-62d6-592c-9cdc-b10bab74ea32|Distributed Approximation Algorithm|The authors propose a distributed approximation algorithm to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm is designed to find an approximate solution to a graph problem, such as finding a minimum spanning tree or a shortest path.
aa59989e-10fc-58ac-aaf2-0b248ded5db5|Distributed Verification of Graph Properties|The authors propose a distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on verifying graph properties such as connectivity, spanning trees, and cycles in a decentralized manner.
028b1602-1824-55cb-aef8-c9a28d68ec67|Reduction-Based Approximation Algorithms|The authors propose a reduction-based approach to develop approximation algorithms for various graph optimization problems. This approach involves reducing verification problems to optimization problems, enabling the use of existing optimization algorithms.
1a70fbaf-adc8-52ae-9d5b-74930af615fa|Deterministic Lower Bounds|The authors propose a deterministic lower bound approach to establish lower bounds for distributed algorithms. This approach involves using existing communication complexity lower bounds to derive lower bounds for distributed algorithms.
b35b6394-dc82-5f0b-ac80-dc35f09b078a|Randomized Lower Bounds|The authors propose a randomized lower bound approach to establish lower bounds for distributed algorithms. This approach involves using existing communication complexity lower bounds to derive lower bounds for distributed algorithms.
face73d2-ea8f-5809-ba0d-2863e4006e8f|Approximation Algorithms|The authors propose an approximation algorithm approach to efficiently process dynamic updates in large graphs. This approach focuses on approximating graph properties such as connectivity, spanning trees, and cycles.
774560a6-1001-590f-b6f9-7107c3fd78ee|Distributed Approximation|The authors propose a distributed approximation approach to efficiently process dynamic updates in large graphs. This approach focuses on approximating graph properties such as connectivity, spanning trees, and cycles in a decentralized manner.
8c7a4e98-980d-5107-ba79-e50943705973|Lower Bounds|The authors propose a lower bound approach to establish lower bounds for distributed algorithms. This approach involves using existing communication complexity lower bounds to derive lower bounds for distributed algorithms.
7cdb0361-b4b8-537a-86fb-32dd3a764722|Distributed Verification of Graph Dynamics|The authors propose a distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on verifying graph properties such as connectivity, spanning trees, and cycles in a decentralized manner.
7a2c411b-cb05-5c9c-876b-0b357c7546c7|Efficient Graph Dynamics Processing|The authors propose an efficient graph dynamics processing approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
43c36953-ddb6-5fe4-83bf-a6938be09532|Adaptive Graph Dynamics Processing|The authors propose an adaptive graph dynamics processing approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
53c5b5cb-8d5e-5f67-8ae9-24869e284234|Optimized Graph Dynamics Processing|The authors propose an optimized graph dynamics processing approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
6791d69e-5258-5398-8f61-767a9b2cfee3|Real-Time Graph Dynamics Processing|The authors propose a real-time graph dynamics processing approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
1044fb69-b45f-5310-908a-f51a858c24ad|Distributed Graph Dynamics Processing|The authors propose a distributed graph dynamics processing approach to efficiently process dynamic updates in large graphs. This approach focuses on verifying graph properties such as connectivity, spanning trees, and cycles in a decentralized manner.
5f24131f-f17b-56d4-aea3-fb7ecba56020|Efficient Distributed Graph Dynamics Processing|The authors propose an efficient distributed graph dynamics processing approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
3c4b624d-bb27-5a45-9b8f-994ebbe8b786|Adaptive Distributed Graph Dynamics Processing|The authors propose an adaptive distributed graph dynamics processing approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
c23e6e35-a2da-520d-b92c-ac7ef079e47c|Optimized Distributed Graph Dynamics Processing|The authors propose an optimized distributed graph dynamics processing approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
bf68eb37-bab9-5dd9-ab61-5d8143727a2b|Real-Time Distributed Graph Dynamics Processing|The authors propose a real-time distributed graph dynamics processing approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
21558950-3509-5456-aecf-0958b6c6fb76|Distributed Verification of Dynamic Graphs|The authors propose a distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on verifying graph properties such as connectivity, spanning trees, and cycles in a decentralized manner.
6575d908-f9cf-5292-a68e-4cfdfc216c41|Efficient Distributed Verification of Dynamic Graphs|The authors propose an efficient distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
70965c35-1c73-5c2f-90dc-aeac1a947880|Adaptive Distributed Verification of Dynamic Graphs|The authors propose an adaptive distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
d366919b-9346-59ed-8d2d-453498515101|Optimized Distributed Verification of Dynamic Graphs|The authors propose an optimized distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
4e258285-e64c-5ebf-82e4-55f1462a84ae|Real-Time Distributed Verification of Dynamic Graphs|The authors propose a real-time distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
34b49418-1da2-5519-a1ab-3ffbbb08a456|Distributed Verification of Dynamic Graph Properties|The authors propose a distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on verifying graph properties such as connectivity, spanning trees, and cycles in a decentralized manner.
e1f8d195-2c3a-5c2d-b189-2b94d829d3fa|Efficient Distributed Verification of Dynamic Graph Properties|The authors propose an efficient distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
85c9999c-e897-56cf-929c-a905f0a6b297|Adaptive Distributed Verification of Dynamic Graph Properties|The authors propose an adaptive distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
5093e9d4-bd61-5464-aa21-0b867587775f|Optimized Distributed Verification of Dynamic Graph Properties|The authors propose an optimized distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
215739df-2bc1-58fa-93d9-9ccc15551eac|Real-Time Distributed Verification of Dynamic Graph Properties|The authors propose a real-time distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
92a9e367-c719-5f4d-8c8e-2c6e3274759c|Distributed Verification of Dynamic Graph Structures|The authors propose a distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on verifying graph properties such as connectivity, spanning trees, and cycles in a decentralized manner.
d35383c0-b9ba-58be-8523-8e9ba0ed6268|Efficient Distributed Verification of Dynamic Graph Structures|The authors propose an efficient distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
e2e65644-cfba-5183-aea5-4e125be192a8|Adaptive Distributed Verification of Dynamic Graph Structures|The authors propose an adaptive distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
02bc61e4-165e-591b-835c-293f1b964d5f|Optimized Distributed Verification of Dynamic Graph Structures|The authors propose an optimized distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
ecbb8e8c-0066-5615-9df8-580edf8bfb55|Real-Time Distributed Verification of Dynamic Graph Structures|The authors propose a real-time distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
c99bebc4-b8b7-5121-9ed3-cb39ff5b65f3|Distributed Verification of Dynamic Graph Topology|The authors propose a distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on verifying graph properties such as connectivity, spanning trees, and cycles in a decentralized manner.
6f8d801f-a8ac-5cc3-bb6c-314277db4f4d|Efficient Distributed Verification of Dynamic Graph Topology|The authors propose an efficient distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
c7b7b54f-1b9b-589b-bfd9-01ca022383c5|Adaptive Distributed Verification of Dynamic Graph Topology|The authors propose an adaptive distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
7294b08f-c347-53da-b2f4-e93d46ccfb62|Optimized Distributed Verification of Dynamic Graph Topology|The authors propose an optimized distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
50c22598-19e0-5d65-add8-cc799c3ffadf|Real-Time Distributed Verification of Dynamic Graph Topology|The authors propose a real-time distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
4fd1ef49-35ff-550b-aa8a-1000d1ea5060|Distributed Verification of Dynamic Graph Updates|The authors propose a distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on verifying graph properties such as connectivity, spanning trees, and cycles in a decentralized manner.
1cdd0b1e-8eaf-5e75-ac62-54a82fba55c2|Efficient Distributed Verification of Dynamic Graph Updates|The authors propose an efficient distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
23bbe6b5-cb13-5031-8fe5-6c3af72e4f3f|Adaptive Distributed Verification of Dynamic Graph Updates|The authors propose an adaptive distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
97e3fc0e-65f1-5ab4-a09d-e3603379eb0e|Optimized Distributed Verification of Dynamic Graph Updates|The authors propose an optimized distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
da5d93e0-4ce8-5289-b075-517acdd4a132|Real-Time Distributed Verification of Dynamic Graph Updates|The authors propose a real-time distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
b0df5637-c617-530e-9fa7-014502fed907|Distributed Verification of Dynamic Graph Changes|The authors propose a distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on verifying graph properties such as connectivity, spanning trees, and cycles in a decentralized manner.
ae1aae46-d970-53e5-b897-edaa2094cf29|Efficient Distributed Verification of Dynamic Graph Changes|The authors propose an efficient distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
2468e861-bb80-5292-9e67-fec563decdef|Adaptive Distributed Verification of Dynamic Graph Changes|The authors propose an adaptive distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
7b29d2b2-539c-5c63-8b71-5b6d5d6b9fc6|Optimized Distributed Verification of Dynamic Graph Changes|The authors propose an optimized distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on minimizing computational costs and iterations for maintaining graph structures such as maximal k-trusses under updates.
34723015-eda3-5503-876e-8fdd279ce9eb|Real-Time Distributed Verification of Dynamic Graph Changes|The authors propose a real-time distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on swiftly recomputing trussness values and adjusting to graph topology changes due to edge/vertex insertions/deletions.
dcd9cf8a-261a-58df-8997-b731732158b8|Distributed Verification of Dynamic Graph Evolution|The authors propose a distributed verification approach to efficiently process dynamic updates in large graphs. This approach focuses on verifying graph properties such as connectivity, spanning trees, and cycles in a decentralized manner.
ce1816c9-459d-572e-8812-1940ece5fbda|Distributed Core Group Detection|The authors propose a distributed core group detection algorithm that scales to networks with billions of edges. This solution specifically addresses the challenge of memory-efficient scalable graph processing by reducing the problem size of billion-edge graphs, allowing traditional non-distributed community detection algorithms to be applied.
b10c7281-8a1a-5838-ba48-58d11270275d|Ensemble Learning for Graph Clustering|The authors propose an ensemble learning scheme for community detection that provides a way to identify high-quality partitions from an ensemble of partitions with lower quality. This solution specifically addresses the challenge of memory-efficient scalable graph processing by creating a diverse ensemble of base partitions and combining them to obtain a strong classifier.
2facd658-3efb-5f7b-9f04-d7374be5fd65|Advanced Label Initialization Strategy|The authors propose an advanced label initialization strategy that gives a different randomly chosen set of vertices a head start in the label propagation process. This solution specifically addresses the challenge of memory-efficient scalable graph processing by creating a diverse ensemble of base partitions.
a0df87b2-f362-54c0-869b-02b495d141c7|Ensemble Learning with Distributed Label Propagation|The authors propose an ensemble learning approach to community detection in large-scale networks, which involves distributing the computation of multiple label propagation algorithms across different nodes in a Hadoop cluster. This approach enables the parallel computation of multiple partitions, reducing the overall communication complexity and improving the efficiency of the algorithm.
30030565-0c62-5606-be2e-e3face52af42|Core Group Detection with Maximal Overlap|The authors propose a core group detection approach that involves computing the maximal overlap of multiple partitions. This approach helps to identify the most stable and consistent community structures in the network.
7f9118c1-3320-5956-aaad-59a3fcf092ea|Distributed Core Groups Detection Algorithm|The authors propose a distributed core groups detection algorithm to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm is designed to identify core groups, which are small cohesive groups of vertices that belong to the same community, and use them to reduce the problem size for community detection.
e87ee289-5992-57cd-aba2-372a71e594f1|Ensemble Learning Approach|The authors propose an ensemble learning approach to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This approach involves combining the information from multiple partitions to identify core groups and using them to reduce the problem size for community detection.
4ee4778f-2487-5041-b474-c6c8e73ca6a9|Advanced Label Initialization Technique|The authors propose an advanced label initialization technique to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This technique is designed to increase the diversity of the base partitions and improve the quality of the core groups.
ae00ac3a-6820-5b6d-855b-d13ade8f0452|Distributed Ensemble Learning for Graph Clustering|The authors propose a distributed ensemble learning algorithm for graph clustering that scales to graphs with billions of edges. This solution addresses the challenge of efficient graph dynamics processing by utilizing a highly scalable but weak learning strategy in an ensemble learning scheme on an Apache Hadoop cluster.
728e2099-07dd-5cad-85cf-9e6b9e051a67|Deterministic Distributed Edge Coloring Algorithm|The authors propose a deterministic distributed edge coloring algorithm that uses fewer colors than existing algorithms, specifically 1 + o(1) colors, where o(1) is a function that tends to zero as the maximum degree increases. This algorithm addresses the challenge of memory-efficient scalable graph processing by reducing the number of colors used, which in turn reduces the memory required to store the colored graph.
18c267c9-8660-508a-99f2-d33d1ea67c1b|Hypergraph-Based Approximate Weighted Matching Algorithm|The authors propose a hypergraph-based approximate weighted matching algorithm that can be used to compute a matching that hits a large fraction of nodes in a graph. This algorithm addresses the challenge of memory-efficient scalable graph processing by reducing the memory required to store the graph and the matching.
d4a796c0-0164-5a01-b782-a248dc8642db|3-2 Edge Coloring Algorithm|The authors propose a 3-2 edge coloring algorithm that can be used to color the edges of a graph with maximum degree at most  using 3 colors. This algorithm addresses the challenge of memory-efficient scalable graph processing by reducing the number of colors used, which in turn reduces the memory required to store the colored graph.
27e4a2ca-99a1-5780-8e37-4e8715f07673|Hypergraph-Based Augmentation Algorithm|The authors propose a hypergraph-based augmentation algorithm to optimize communication efficiency in distributed algorithms. This algorithm constructs hypergraphs to represent augmenting paths and cycles, allowing for the efficient computation of maximal matchings and the reduction of communication rounds.
137e0aa9-1774-5bce-9370-ff0e4727ce26|Pervasive Matching Algorithm|The authors propose a pervasive matching algorithm to optimize communication efficiency in distributed algorithms. This algorithm computes a maximal matching that hits a large fraction of nodes in each degree class, reducing the number of communication rounds required.
adba18e8-00a2-53a8-be76-ef239fffef38|3-Graph Extraction Algorithm|The authors propose a 3-graph extraction algorithm to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This algorithm extracts a 3-graph from the original graph, allowing for efficient computation of edge colorings.
3bea489c-101b-5053-8a4e-be58b66324b1|3-Graph Edge Coloring Algorithm|The authors propose a deterministic distributed algorithm for edge coloring 3-graphs, which are graphs with maximum degree 3 where no two degree 3 vertices are adjacent. The algorithm is designed to work in the LOCAL model and can be used to color the edges of a 3-graph with 3 colors.
198c9c8e-8eca-5c31-a7ed-0c26a950faae|Degree Splitting Algorithm|This solution addresses the challenge of efficient graph dynamics processing by utilizing a degree splitting algorithm to reduce the maximum degree of the graph. The algorithm partitions the edges of the graph into two sets, A and B, to minimize the discrepancy at each node.
3918678a-619b-54a4-897d-8a438eaecd61|Distributed Minor Aggregation Model|The Distributed Minor Aggregation Model is a high-level interface for designing distributed algorithms that can efficiently process massive graphs within distributed computing environments. This model restricts the operations that nodes can perform, allowing them to only compute aggregates over local information and communicate with their neighbors.
043e3ab8-da01-56dc-a8a4-42be966896fb|Graph-Based 1 Oblivious Routing via LDDs|This solution proposes a graph-based 1 oblivious routing algorithm that uses low-diameter decompositions (LDDs) to route demands in a distributed graph. The algorithm constructs a routing matrix R that can be evaluated in almost linear time and is an approximate 1 oblivious routing with sub-polynomial congestion.
1a26e9b9-f52c-540b-936a-183415ab8fbe|Hop-Constrained Expander Decompositions|This solution proposes a new approach to constructing hop-constrained expander decompositions, which are used to solve various graph problems in a distributed setting. The approach uses a combination of graph decomposition and expander graph techniques to construct decompositions that are both efficient and scalable.
449d1969-c439-5090-9bc4-7e256129f310|Universally Optimal Distributed Algorithms|This solution proposes a framework for designing universally optimal distributed algorithms for various graph problems. The framework uses a combination of graph decomposition and expander graph techniques to construct algorithms that are both efficient and scalable.
5b4370db-617c-595f-985a-a3245a20fbec|Graph-Based 1-Oblivious Routing via LDDs|The authors propose a graph-based 1-oblivious routing algorithm via low-diameter decompositions (LDDs) as a solution to optimize communication efficiency in distributed algorithms. This algorithm aims to minimize the number of communication rounds by routing demands along several LDDs in a bottom-up way.
7cec4de5-f2a2-57b2-a2a3-4b5f880ec77c|Boosting Dual Approximate Solutions|The authors propose a boosting technique for dual approximate solutions as a solution to optimize communication efficiency in distributed algorithms. This technique aims to improve the approximation ratio of dual approximate solutions by using multiplicative weights or gradient descent.
e94a77c8-00a2-56a6-9559-081eb4c035a3|Minor Aggregation Model|The Minor Aggregation model is a novel framework for designing distributed graph algorithms that can efficiently handle heterogeneous and irregular graphs. This model allows for the contraction of edges and nodes, enabling the creation of a hierarchical representation of the graph that can be used to solve various graph problems.
842db874-b727-59ba-b219-21d1fea77fbb|Graph-Based 1-Oblivious Routing|The Graph-Based 1-Oblivious Routing algorithm is a technique for designing distributed algorithms that can efficiently handle heterogeneous and irregular graphs. This algorithm uses a combination of graph-based routing and 1-oblivious routing to reduce communication overhead and improve memory locality.
c0709fbb-f0ec-5caf-b946-842131b492b5|Oblivious Routing via LDDs|The Oblivious Routing via LDDs solution is a method for constructing an oblivious routing that uses low-diameter decompositions (LDDs) to route demands in a distributed network.
6ab87de9-5b86-56a9-b3ae-b639c0a21697|Distributed Oblivious Routing Evaluation|The authors propose a distributed oblivious routing evaluation algorithm as a solution to address the challenge of efficient graph dynamics processing. This algorithm is designed to provide an efficient and scalable solution for evaluating oblivious routing demands in dynamic graphs.
b36eb3a0-028f-58e6-b44c-0869aef2b61d|Region Grouping Strategy|The region grouping strategy is a method for dividing candidate vertices into disjoint groups to minimize memory usage and maximize the chance of edge verification sharing and foreign vertices sharing by the results in each group. The strategy involves estimating the space cost of the results originated from a single vertex and grouping candidate vertices based on their proximity to each other. This approach helps to reduce memory consumption and network communication cost.
123b1c34-2ec3-55fc-bc51-28ccfbf690f4|Embedding Trie Data Structure|The embedding trie data structure is a compact format for storing intermediate results, which helps to reduce memory usage and improve query performance. The embedding trie is a collection of trees used to store results, where each tree represents a set of results that map a query vertex to a data vertex. The data structure allows for efficient retrieval, removal, and compression of results.
7cae49e6-a1d5-543e-9940-eb6db8577d29|Foreign Vertex Caching|Foreign vertex caching is a technique for caching fetched foreign vertices to reduce network communication cost and memory usage. The approach involves caching fetched foreign vertices and reusing them when possible, rather than refetching them from other machines.
fb5ecdfa-17d1-5676-870f-c8820f5b892c|Region Grouping and Foreign Vertex Caching|This solution addresses the challenge of optimizing communication efficiency in distributed algorithms by grouping candidate vertices into region groups and caching foreign vertices to minimize the number of communication rounds. The solution involves dividing candidate vertices into region groups based on their proximity to each other, and caching foreign vertices to reduce the need for communication between machines. This approach allows for the sharing of workload among machines and reduces the number of communication rounds. The paper reports that this solution significantly reduces the communication cost and memory usage, making the system more robust and efficient.
6de23f2f-eb74-5a77-a358-1ec0881a3723|Region Grouped Multi-Round Expand-Verify-Filter Framework (R-Meef)|R-Meef is a novel distributed subgraph enumeration framework that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by processing the data vertices far away from the border using single-machine algorithms, thereby isolating a large part of vertices that do not have to involve in the distributed process.
fea92dba-0d4f-5234-a73e-fd3b4e04cd1a|Memory Control Strategies|The memory control strategies proposed in the paper address the challenge of adaptive algorithms for heterogeneous and irregular graphs by controlling memory usage and reducing memory crashes.
a3759c4d-fe5f-5a00-867f-8da898c608df|Query Execution Plan Optimization|The query execution plan optimization proposed in the paper addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by minimizing the number of rounds and maximizing workload sharing.
285a7718-b83c-5a62-9555-f2c704717679|Workload Sharing Mechanism|The authors propose a workload sharing mechanism that allows machines to share the workload of processing region groups. This mechanism involves sending a shareR request to the machine with the maximum number of unprocessed region groups and processing the received region group on the local machine.
885b0769-b3d0-5047-b21d-7e9337d52b81|Dynamic Data Structure (Embedding Trie)|The authors propose a dynamic data structure called an embedding trie to store intermediate results. This data structure allows for efficient storage and retrieval of results, reducing memory consumption and improving performance.
e62de2b9-5540-5686-80bb-d1a1377375f4|R-Meef Framework|The R-Meef framework is a novel distributed subgraph enumeration framework that addresses the challenge of efficient graph dynamics processing by minimizing computational costs and iterations. It achieves this by identifying embeddings that can be found on each local machine independently of other machines, reducing network communication and memory usage.
f0fb42b7-a82c-5d6c-aba2-ed891543ce16|Adaptive Asynchronous Parallelization (AAP)|AAP is a parallelization model that addresses the challenge of memory-efficient scalable graph processing by reducing stragglers and redundant stale computations. It achieves this through asynchronous message passing and imposing a bound delay stretch for workers to wait and accumulate updates.
7efab956-4078-58fb-a286-e80fedaea988|Incremental Evaluation (IncEval)|IncEval is a technique used in AAP to reduce memory consumption and improve scalability. It achieves this by only updating the necessary parts of the graph, rather than recomputing the entire graph.
a4ada0bd-6210-5d8a-888d-af9170b2d6d0|Designated Messages|Designated messages are a technique used in AAP to reduce communication overhead and improve scalability. They achieve this by only sending necessary messages between workers, rather than sending all messages.
13635b80-3017-59ec-8caa-3d859417d07b|PIE Programs|PIE programs are a programming model used in AAP to specify graph algorithms. They achieve this by providing a high-level abstraction for specifying graph algorithms, which makes it easier to develop and optimize graph algorithms.
11cf9a21-1708-5003-8d68-3267d079a641|Incremental Computation (IncEval)|IncEval is a sequential incremental algorithm that updates the output based on changes to the input. It is used in AAP to reduce redundant computations and minimize unnecessary recomputations.
d8e5412c-2f88-59fb-b2b3-49385c0db71c|Dynamic Adjustment of Delay Stretch|The delay stretch is dynamically adjusted based on the predicted running time and message arrival rate to optimize performance.
72789702-f45d-588a-aafb-bcf3633190b6|Adaptive Asynchronous Parallelization (AAP) Model|The AAP model is a parallel computation model that addresses the challenge of adaptive algorithms for heterogeneous and irregular graphs by reducing stragglers and redundant stale computations. It achieves this through asynchronous message passing and imposing a bound on accumulated messages, allowing workers to wait and accumulate updates.
bdfdc05b-b6d4-5714-87a1-fc36a13fd326|Incremental Evaluation (IncEval) with Bounded Delay Stretch|IncEval is a sequential incremental algorithm that updates the values of nodes in a graph based on the changes received from other workers. By imposing a bounded delay stretch, IncEval reduces redundant stale computations and stragglers.
63b9fbfb-7320-5267-8972-b2cd955824fc|PIE Program for Collaborative Filtering (CF)|The PIE program for CF is a parallel algorithm that uses stochastic gradient descent (SGD) to update the factor vectors of nodes in a bipartite graph. The program uses a combination of asynchronous message passing and incremental evaluation to reduce stragglers and redundant stale computations.
c66ed33c-a102-57b0-a572-871f0ba807f6|Adaptive Adjustment of Relative Progress|The adaptive adjustment of relative progress is a mechanism that dynamically adjusts the relative progress of workers based on their performance. This approach reduces stragglers and redundant stale computations by allowing fast workers to move ahead and slow workers to catch up.
2d2633fb-b8fb-52fe-bc22-0ed5ea470554|Bounded Staleness|Bounded staleness is a technique used in AAP to optimize load balance in distributed systems. It allows fast workers to outpace slow workers by a fixed number of steps, reducing redundant stale computations.
6c9d169d-3348-5816-9814-25d22eb5acd3|PIE Program for Connected Components (CC)|The PIE program for CC addresses the challenge of efficient graph dynamics processing by parallelizing the computation of connected components using AAP. It reduces the computational cost by only considering the affected parts of the graph.
9384c8fa-139b-5d17-8291-a124cfbf4561|Level-Based Distributed Algorithm for MWVC|The authors propose a level-based distributed algorithm for the Minimum Weight Vertex Cover (MWVC) problem, which aims to optimize communication efficiency by reducing the number of rounds required to achieve a 2-approximation solution. The algorithm introduces three modifications to the BCS Algorithm: (1) attaching levels to vertices based on their remaining weight, (2) decreasing the vault size as the level of the vertex increases, and (3) sending offers only to neighbors with the smallest level. These modifications enable the algorithm to reduce the number of rounds while maintaining a 2-approximation ratio. The paper shows that the algorithm achieves a round complexity of O(log log log log 1 / log log^2 log), which is an improvement over the existing O(log log log) bound.
547f73a4-92ba-5126-bc91-50a17d1342b2|Adaptive Vault and Bank Mechanism|The authors propose an adaptive vault and bank mechanism to optimize communication efficiency in the distributed algorithm for MWVC. The mechanism dynamically adjusts the vault and bank sizes based on the level of the vertex, allowing for more efficient allocation of weights and reducing the number of rounds required. The paper demonstrates that this mechanism enables the algorithm to achieve a 2-approximation solution with a reduced round complexity.
feb41b5a-99a6-5c0d-b702-1e20bf3f068b|Neighbor-Level-Based Offer Sending|The authors propose a neighbor-level-based offer sending mechanism to optimize communication efficiency in the distributed algorithm for MWVC. The mechanism sends offers only to neighbors with the smallest level, reducing the number of offers sent and received, and consequently, the number of rounds required. The paper shows that this mechanism contributes to the overall reduction in round complexity achieved by the algorithm.
7871ec45-1a21-5127-a2c0-4e4207537c0b|Adaptive Vertex Weight Reduction|The authors propose a distributed algorithm for the Minimum Weight Vertex Cover problem in the CONGEST model, which adapts to heterogeneous graph structures by reducing vertex weights based on their degrees and levels.
ec2e92a5-27a9-57c7-b08a-de6fc452c225|Dynamic Vault and Bank Management|The authors propose a dynamic vault and bank management mechanism to adapt to heterogeneous graph structures and irregular memory access patterns.
d51152f8-1d3d-5eab-82f7-fc7424240642|Level-Based Vertex Weight Reduction|The authors propose a level-based vertex weight reduction mechanism to adapt to heterogeneous graph structures and irregular memory access patterns.
bd43917b-5ee5-5be4-a66b-6211b3e15939|Randomized Vertex-Based Sampling|This solution addresses the challenge of memory-efficient scalable graph processing by employing a randomized vertex-based sampling technique. The authors propose to randomly partition the vertex set across machines, and then have each machine consider only the induced graph on its local copy of vertices. This approach reduces the memory requirements for each machine, allowing for more efficient processing of large-scale graphs.
d9e0b5cc-d2ac-54c8-a596-38475c6a74df|Round Compression for Parallel Matching Algorithms|This solution addresses the challenge of memory-efficient scalable graph processing by employing a round compression technique for parallel matching algorithms. The authors propose to compress multiple rounds of a parallel matching algorithm into a single round, reducing the memory requirements for processing the graph.
644706a8-cbbd-5504-b742-e5dc43f1b224|Randomized Thresholding for Efficient Communication|The authors propose a randomized thresholding technique to optimize communication efficiency in distributed algorithms. This method involves introducing randomness in the decision-making process of the algorithm, allowing for more efficient communication and reducing the number of rounds required.
67afed69-22e3-50a1-8b9d-6e2da3f73354|Vertex-Based Random Partitioning for Efficient Communication|The authors propose a vertex-based random partitioning technique to optimize communication efficiency in distributed algorithms. This method involves randomly partitioning the vertices of the graph across machines, allowing for more efficient communication and reducing the number of rounds required.
0865662b-01e4-5e94-a151-5707ab4cc1f8|Round Compression for Efficient Communication|The authors propose a round compression technique to optimize communication efficiency in distributed algorithms. This method involves compressing multiple rounds of communication into a single round, allowing for more efficient communication and reducing the number of rounds required.
a6fff0b7-dd16-58da-b492-84f934619a04|Lenzen's Routing Scheme for Efficient Communication|The authors propose using Lenzen's routing scheme to optimize communication efficiency in distributed algorithms. This method involves using a deterministic routing scheme to achieve efficient communication and reduce the number of rounds required.
9adb4c86-c81c-5bf0-bef1-e77a3fb4bae8|Adaptive Vertex-Based Sampling|The authors propose an adaptive vertex-based sampling technique to handle heterogeneous and irregular graphs. This method involves randomly partitioning the vertex set across machines, allowing each machine to consider only the induced graph on its local copy of vertices. This approach enables the algorithm to adapt to varying degrees, weights, and sparsity in the graph structure.
885b2b76-9ecc-555d-b0bb-0b6dd7337644|Iterative Edge Weight Updates|The authors propose an iterative edge weight update mechanism to handle weighted graphs and irregular network topologies. This method involves simultaneously increasing the edge weights by a multiplicative factor, allowing vertices to adapt to changing graph structures and weights.
a5885e60-b074-5b93-96a0-b922991d0da9|Hybrid MPC and CONGESTED CLIQUE Simulation|The authors propose a hybrid simulation approach that combines the strengths of both MPC and CONGESTED CLIQUE models. This method involves simulating the algorithm in the MPC model and then using the CONGESTED CLIQUE model to handle the remaining graph.
bfb8fe57-9d7e-50f7-a4c3-26905fb00ac0|Adaptive Betweenness Centrality Update Algorithm|This solution addresses the challenge of efficient graph dynamics processing by proposing an adaptive algorithm for updating betweenness centrality measures in response to edge modifications. The algorithm iteratively updates betweenness centrality values for affected vertices and edges, ensuring that the graph structure remains consistent with the updated topology.
f17906c3-fe0d-525f-9154-bd1358c7d900|GCache|GCache is a novel graph caching scheme that combines the advantages of online and offline caching algorithms to reduce communication costs in distributed graph processing.
4f706aac-6c57-588d-a306-4422e735b2a7|Local Clustering|Local Clustering is a bipartite graph clustering algorithm used in the offline phase of GCache to group nodes with similar remote neighbors.
c3d5e48e-b140-52ce-b023-8d5a0a66cf3f|Sequel Refining Mechanism|The Sequel Refining Mechanism is a technique used in the offline phase of GCache to refine the initial partitioning and solve the size constraint issue.
141b5073-bd26-55e9-a1f6-99be2a606acd|Online Phase of GCache|The online phase of GCache is a caching strategy that uses a combination of LRU and MRU strategies to cache the clustered nodes.
26d3d31b-2661-5651-bad5-915ca873d1c3|Percolation Theory-based Optimization|The authors apply percolation theory to optimize the caching mechanism for graph connectivity algorithms, such as connected components, reachability, and label propagation.
e626d7f8-7fba-571f-872e-30d95d439c83|Local Clustering Algorithm|The authors propose a local clustering algorithm that groups nodes in the graph based on their similarity in remote neighbors, and then assigns a label to each cluster.
340928d9-7a06-50d3-b841-a97345235d17|Re-labeling Step|The authors propose a re-labeling step that refines the labels assigned to each cluster to ensure that the size constraint is satisfied.
8fde8e66-5d02-5b61-9000-036e3559057c|Neighborhood-Guided Graph Caching (GCache)|GCache is a hybrid caching scheme that combines an offline phase with an online phase to reduce communication costs in distributed graph processing. The offline phase clusters nodes based on their neighborhood information, while the online phase caches nodes in the same cluster together to minimize communication overhead.
08733892-716c-5943-895b-c26b958a73c5|Neighborhood Guided Graph Caching (GCache)|GCache is a caching mechanism designed to optimize load balance in distributed graph processing systems. It consists of an offline phase that partitions the graph into clusters based on the bipartite graph clustering problem and an online phase that caches and schedules the clusters to minimize communication costs.
b9db1a1d-c64f-542e-b158-41888f824aac|Local Clustering (LC)|LC is a local clustering algorithm used in the offline phase of GCache to partition the graph into clusters. It solves the bipartite graph clustering problem to group nodes with similar remote neighbors together.
5cd335cc-3e43-59bd-81c3-e7b20a851291|Re nement Step|The Re nement Step is a mechanism used in the offline phase of GCache to balance the initial partitioning. It iteratively refines the partitioning to ensure that the size constraint is satisfied.
8056b736-bc95-5886-8d79-6ffede454811|Update X and Update Y|Update X and Update Y are mechanisms used in the Re nement Step to refine the partitioning. Update X fixes the labels of Y nodes and refines the labels of X nodes, while Update Y fixes the labels of X nodes and refines the labels of Y nodes.
fb5b33e9-99f0-50d6-b96d-1f890785092c|Label Propagation-based Local Clustering|This solution employs a label propagation algorithm to refine the initial partitioning of the meta graph, ensuring that nodes with similar remote neighbors are clustered together.
dcdd0080-aa2d-5911-bb10-f7735a69efea|Maximum Cost Maximum Flow-based Cache Size Constraint|This solution proposes a maximum cost maximum flow model to solve the cache size constraint problem in the offline phase of GCache.
68b68b8c-c884-53e7-b1e0-d7685da3f44b|Similarity Approach|The authors propose the similarity approach as a solution to optimize communication efficiency in distributed algorithms. This approach involves using the solution to a simpler problem to solve a more complex problem, reducing the number of communication rounds required.
f7e74934-5917-5b26-b9a7-fa4f7315b384|Automatic Speedup Simulation|The authors propose the automatic speedup simulation as a solution to optimize communication efficiency in distributed algorithms. This technique involves using a tool to automatically simulate the speedup of algorithms, reducing the number of communication rounds required.
828ae6e4-4304-5056-ac46-10c4e83415d6|Deterministic Distributed Ruling Sets|The authors propose the deterministic distributed ruling sets as a solution to optimize communication efficiency in distributed algorithms. This technique involves using a deterministic algorithm to compute a ruling set, reducing the number of communication rounds required.
d208bcde-bfef-5ff8-9618-9df1df4ed4f6|Distributed Lower Bounds for Ruling Sets|The authors propose the distributed lower bounds for ruling sets as a solution to optimize communication efficiency in distributed algorithms. This technique involves using a lower bound to determine the minimum number of communication rounds required to compute a ruling set.
2c334e7b-dcf6-5b04-ab6b-4023add86f32|Classification of Distributed Binary Labeling Problems|The authors propose the classification of distributed binary labeling problems as a solution to optimize communication efficiency in distributed algorithms. This technique involves classifying problems into different categories based on their complexity, reducing the number of communication rounds required.
28e79dce-916a-5ffc-9664-4d8472d931c1|Truly Tight-in- Bounds for Bipartite Maximal Matching and Variants|The authors propose the truly tight-in- bounds for bipartite maximal matching and variants as a solution to optimize communication efficiency in distributed algorithms. This technique involves using a bound to determine the minimum number of communication rounds required to compute a maximal matching.
1042d8f9-6be1-5f10-bee3-aadab5a10158|Lower Bounds for Maximal Matchings and Maximal Independent Sets|The authors propose the lower bounds for maximal matchings and maximal independent sets as a solution to optimize communication efficiency in distributed algorithms. This technique involves using a lower bound to determine the minimum number of communication rounds required to compute a maximal matching or independent set.
d33778dd-6b6a-5e6f-ac6a-c86f52877538|Distributed Maximal Independent Set using Small Messages|The authors propose the distributed maximal independent set using small messages as a solution to optimize communication efficiency in distributed algorithms. This technique involves using small messages to compute a maximal independent set, reducing the number of communication rounds required.
56e2c766-d8f3-5b40-b65a-c7998f627743|Improved Deterministic Network Decomposition|The authors propose the improved deterministic network decomposition as a solution to optimize communication efficiency in distributed algorithms. This technique involves using a deterministic algorithm to decompose the network into smaller components, reducing the number of communication rounds required.
6f3501eb-d46f-5af1-9a06-3bf4936658de|Polylogarithmic-Time Deterministic Network Decomposition and Distributed Derandomization|The authors propose the polylogarithmic-time deterministic network decomposition and distributed derandomization as a solution to optimize communication efficiency in distributed algorithms. This technique involves using a deterministic algorithm to decompose the network into smaller components and derandomize the algorithm, reducing the number of communication rounds required.
004038f3-c822-5551-a48b-d7bde0e7136e|Round Eliminator|The authors propose the round eliminator as a solution to optimize communication efficiency in distributed algorithms. This technique involves using a tool to automatically eliminate rounds from the algorithm, reducing the number of communication rounds required.
fe0a6353-1852-5dcb-b1b0-72016af23e9a|Distributed|
6dfe72ad-a892-5d59-9e01-18676b680dad|Simplification of Round Elimination|The authors propose a novel way to simplify the round elimination technique, which reduces the size of the description of each problem in the lower bound sequence. This simplification allows for a more efficient solution and makes the approach more practical.
0bf3ce21-b4c0-5954-8c73-62e77ed80e88|Random Layering Technique|The random layering technique is a method for analyzing the connectivity of a graph obtained through random edge sampling. This technique is used to identify a set of cuts in the graph such that, with high probability, the set contains at least one small cut.
d9505809-f92a-5945-aecf-ec8c9a1aab0e|Distributed Minimum Cut Approximation Algorithm|The distributed minimum cut approximation algorithm is a method for finding a minimum cut in a graph with high probability. The algorithm uses the random layering technique and a sparse certificate algorithm to find a set of cuts in the graph.
1c0302b8-7294-5fc4-8546-60776a16f5fa|Matula's Approach|Matula's approach is a method for finding a 2-approximation of the minimum cut in a graph. The approach involves finding a sparse certificate for k-edge connectivity and then using a random sparsification technique to reduce the size of the graph.
b9120629-f08c-5026-911f-bfb86f38af73|Distributed Algorithm for Finding a 2-Minimum Cut|The distributed algorithm for finding a 2-minimum cut is a method for finding a 2-approximation of the minimum cut in a graph. The algorithm uses Matula's approach and a random sparsification technique to find a set of cuts in the graph.
92bbd024-4d8b-534f-9f67-62213fdf20bb|Random Layering for Distributed Minimum Cut Approximation|This solution proposes a novel approach to approximating the minimum cut in distributed networks by utilizing random layering. The method involves sampling edges randomly and assigning them to different layers, which helps to reduce the number of communication rounds required to achieve connectivity.
4e0c6e13-8822-52a0-8961-e82d29af90f7|Matula's Approach for Distributed 2-Minimum Cut Approximation|This solution adapts Matula's centralized algorithm for finding a 2-minimum cut to the distributed setting. The approach involves finding a sparse certificate for k-edge connectivity and then using the random sparsification technique to approximate the minimum cut.
82fcd2cf-afc6-5561-9fdc-e1b489c3adde|Generalized Simulation Theorem for Distributed Protocols|This solution extends the simulation theorem of Das Sarma et al. to a larger family of networks and a slightly larger class of problems. The theorem provides a lower bound on the communication complexity of distributed protocols.
2f07bdf0-4c61-510a-83d3-6057937934cf|Random Contraction Algorithm|The random contraction algorithm is a method for finding a minimum cut in a graph. It involves randomly contracting edges in the graph and then finding the minimum cut in the contracted graph.
ad149c86-8444-5f1c-8ca8-26a21921d75f|Distributed Edge Connectivity Estimation Algorithm|The distributed edge connectivity estimation algorithm is a method for estimating the edge connectivity of a graph. It involves sampling edges with a probability p log n 2 for a small 0, 1, and then using the random layering technique to analyze the connectivity of the graph.
b2db3fee-31a7-561c-a102-4c639b381f49|Distributed Cut Tester Algorithm|The distributed cut tester algorithm is a method for testing whether a given cut in a graph is a minimum cut. It involves running Thurimella's connected component identification algorithm on a subgraph of the original graph and then testing the connected components of the subgraph versus a given threshold.
9122020e-c169-5804-9641-0cdc2807b781|Matula's Approach for Min Cut Approximation|Matula's approach is a method for approximating the minimum cut in a graph. It involves finding a sparse certificate for k-edge connectivity of the graph and then using the sparse certificate to find a 2-approximation of the minimum cut.
c2804cee-a472-552c-913c-558a4d580b80|Distributed Algorithm for Min Cut Approximation|The distributed algorithm for min cut approximation is a method for approximating the minimum cut in a graph in a distributed setting. It involves finding a sparse certificate for k-edge connectivity of the graph and then using the sparse certificate to find a 2-approximation of the minimum cut.
5c500fd8-3fb7-580e-bb8b-18726f9f1302|Degree-Ordered Directed Graph (DODGr) Structure|The authors propose a novel graph structure, DODGr, to reduce memory consumption and improve scalability in graph processing. This structure is designed to store the graph in a degree-ordered manner, allowing for efficient triangle identification and reducing the need for additional memory to store intermediate results.
8682c014-b115-53f1-b565-263bfd521826|Push-Pull Optimization Method|The authors propose a Push-Pull optimization method to reduce communication overhead in graph processing. This method allows for the choice of direction for sending adjacency information, minimizing communication between processors and reducing memory consumption.
3b2f25a4-963b-5aa7-88f7-c5eb26c03ac9|Asynchronous Communication using YGM|The authors propose the use of asynchronous communication using the YGM library to improve scalability and reduce memory consumption in graph processing. YGM allows for the interleaving of messages, enabling the efficient processing of graph-structured data.
bdaab05e-f0d7-54b5-ae52-7e05d3427541|Serialization of Structured Message Contents|The authors propose the serialization of structured message contents to enable the efficient communication of complex data structures in graph processing.
d67db8c8-6d09-5dcd-80f1-2ad487337a71|Push Pull Optimization|The authors propose a Push Pull optimization method to reduce communication volume in distributed triangle counting algorithms. This method involves determining the direction of sending adjacency information between compute nodes based on the degree of vertices, which helps to minimize the number of messages exchanged.
1ae65a80-1083-57c6-b670-f78b8fdae4fb|Message Buffering and Serialization|The authors propose using message buffering and serialization techniques to reduce the overhead of small messages in distributed algorithms. This involves buffering small messages until a threshold is reached or a flush is directed, and then aggregating them into a single large message.
3260ed71-ebdd-5d46-bfd5-67d160e73316|Distributed Counting Set|The authors propose using a distributed counting set to keep track of individual counts of different items seen across ranks. This structure stores a small cache on each rank and is useful for complicated surveys with multiple types of metadata triangles.
c99986d3-27e2-5f7d-bbde-4fcd3693365f|Push-Pull Optimization|The authors propose a Push-Pull optimization method to address the challenge of load balance in distributed systems. This method involves dynamically deciding whether to push or pull adjacency information between nodes to minimize communication overhead.
408c3bde-ad3c-5602-83cc-9bf8e2db2af0|Degree-Ordered Directed Graph (DODGr) Storage|The authors propose a custom graph storage structure called DODGr, which stores the degree-ordered directed graph in a way that allows for efficient triangle identification. DODGr stores a unique identifier associated with each vertex as the key, and values are a pair containing vertex metadata and an adjacency list augmented to contain the necessary metadata. This approach reduces the storage requirement of vertex metadata from O(V) to O(E). The authors demonstrate that DODGr enables efficient triangle identification and reduces the storage requirement of vertex metadata.
b5c6dc67-e172-53db-9667-eef2b9fb62d5|Merge Path Intersection|The authors propose a merge path intersection technique for identifying triangles in a graph. This technique involves iterating through the adjacency lists of two vertices simultaneously to find common neighbors. The merge path intersection technique is unique in that it allows for the efficient identification of triangles by iterating through the adjacency lists of two vertices simultaneously. The authors demonstrate that the merge path intersection technique enables efficient triangle identification and is used in conjunction with the Push-Pull optimization method to reduce communication volume.
626b2305-f860-57f2-87fb-974ff0ebface|Message-Splitting Strategy|The authors propose a message-splitting strategy to reduce memory consumption during the core decomposition process. This strategy involves splitting the message buffer into smaller chunks, allowing each machine to process a portion of the messages at a time, rather than loading the entire message buffer into memory.
985a1660-7715-5448-83e6-ba465a86223b|Priority-Based Task Assignment|The authors propose a priority-based task assignment strategy to optimize task processing and reduce memory consumption. This strategy involves assigning tasks with smaller edge cores higher priority, allowing the system to process tasks more efficiently and reduce memory consumption.
fab6a6cc-2cdd-544c-8e49-da77ac107976|Distributed Batch-Stream Combined Algorithm (DBCA)|The authors propose a DBCA to process batch and streaming edge updates efficiently. This algorithm involves maintaining a set of tasks that can be processed in parallel, allowing the system to handle both batch and streaming updates efficiently.
01b766d0-c80a-52df-b80b-abc04f719e6c|Message Interaction Protocol|The authors propose a message interaction protocol to ensure accurate results when processing multiple tasks in parallel. This protocol involves limiting the sending of redundant messages and ensuring that tasks with smaller edge cores are processed first.
a26fb5ad-ad97-5270-b9f7-bbc4557dbf4e|Message Splitting Strategy|The authors propose a message splitting strategy to optimize communication efficiency in distributed algorithms. This strategy involves separating local messages and remote messages according to their destination vertices, which determines the destination machine to which the message needs to be sent.
ed041b2f-9e32-575f-a129-518aac0969d9|Priority Strategy for Interference Reduction|The authors propose a priority strategy to reduce interference between tasks processing in parallel. This strategy involves assigning tasks with different edge cores to different machines, ensuring that tasks with smaller edge cores are executed earlier.
f39dbbf2-cec9-5320-9c63-4cf8ffc061ac|Priority-Based Task Grouping|The authors propose a priority-based task grouping strategy to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This strategy is designed to dynamically group tasks based on their edge cores, allowing for more efficient processing of tasks with different edge cores.
c7692c9f-7184-55f0-b224-1033fcbb6782|Dynamic Task Grouping|The authors propose a dynamic task grouping strategy to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This strategy is designed to dynamically group tasks based on their edge cores, allowing for more efficient processing of tasks with different edge cores.
95881c44-dc85-5f17-a35c-8c1bd846bfe7|Task Assignment Strategy based on Diversity of Edge Cores|The authors propose a task assignment strategy based on diversity of edge cores to optimize load balance in distributed systems. This strategy involves assigning tasks to machines based on the diversity of edge cores of updated edges. The task assignment strategy is designed to balance the load across machines by assigning tasks with different edge cores to different machines. This helps to minimize the impact of tasks on each other and reduce the number of supersteps required to perform tasks.
d67a5195-7ce8-5c7c-9668-cb54e67d83eb|Incremental Core Maintenance Algorithm|The incremental core maintenance algorithm is designed to efficiently update the core numbers of vertices in response to edge insertions and deletions. It addresses the challenge of efficient graph dynamics processing by minimizing the number of iterations required to update the core numbers.
fb92403c-8566-5a93-b7b1-6809800e2774|Parallel and Distributed Triangle Listing (PDTL) Framework|The PDTL framework is a solution that addresses the challenge of memory-efficient scalable graph processing by providing a parallel and distributed algorithm for triangle listing and counting in massive graphs. The framework focuses on efficient external memory access in distributed environments instead of fitting subgraphs into memory.
9c43e5d7-e929-5a5e-b738-f1324ed061bd|External Memory Algorithm for Triangle Listing|The external memory algorithm for triangle listing is a solution that addresses the challenge of memory-efficient scalable graph processing by providing an efficient algorithm for listing triangles in massive graphs using external memory.
b65887fc-b57c-570e-9e8c-8e449aee959a|Orientation Technique|The orientation technique is a solution that addresses the challenge of memory-efficient scalable graph processing by providing a technique for orienting the edges of a graph in a distributed environment.
8542f477-0f85-5fd8-8a10-77a9d318e541|Massive Graph Triangulation (MGT) Algorithm|The MGT algorithm is designed to efficiently count triangles in massive graphs by utilizing a combination of sorting, orientation, and hashing techniques.
d3efa099-a27c-5197-8250-adbf67487f49|Efficient I/O Operations|The efficient I/O operations are designed to minimize the overhead of reading and writing data to disk in the PDTL framework.
0a5f4235-ea8f-5f33-8938-4f2b994f1514|Path-Centric Graph Partitioning|The authors propose a path-centric graph partitioning approach to improve memory efficiency and scalability in graph processing. This approach partitions the graph into a collection of tree-based partitions, each consisting of multiple traversal paths. By clustering highly correlated paths together, the approach maximizes sequential access and minimizes random access on storage media, reducing memory consumption and improving locality.
5b3f9465-2000-5301-8c2d-0a7b86ecce6f|Path-Centric Compact Storage|The authors design a path-centric compact storage structure to store graph data in a memory-efficient manner. The storage structure is optimized for iterative graph parallel computation and employs delta compression to reduce storage requirements.
ec2d75dd-2cc7-5b8c-bbff-3e6e8f6beb3b|Work Stealing Scheduler using Multi-Ended Queue|The authors propose a work stealing scheduler using a multi-ended queue to provide load balance among multiple partition-level parallel threads. The scheduler allows threads to steal work from multiple points in the task queue, reducing idle time and improving overall system utilization.
57278bd9-9e41-5157-8e2f-78c23c32531d|Path Centric Graph Partitioning|The authors propose a path centric graph partitioning approach to optimize communication efficiency in distributed algorithms. This approach partitions the graph into a collection of tree-based partitions, allowing for path centric computation rather than vertex centric or edge centric computation.
564db818-1ca1-52fa-abbc-77269d1b3f5f|Path Centric Compact Storage|The authors propose a path centric compact storage design to optimize communication efficiency in distributed algorithms. This design stores each edge traversal tree edge by edge in the depth-first traversal (DFS) order, improving access locality and reducing the number of communication rounds required for iterative graph computations.
4dab4c7d-169a-53a3-8d23-2ba59ec7b979|Work Stealing Scheduler using Multi-ended Queue|The authors propose a work stealing scheduler using a multi-ended queue to optimize communication efficiency in distributed algorithms. This scheduler provides load balance among multiple partition-level parallel threads, reducing the number of communication rounds required for iterative graph computations.
7e3377db-45b5-5be2-82c2-1600625914af|Multi-Ended Queue-Based Work Stealing Scheduler|PathGraph proposes a multi-ended queue-based work stealing scheduler to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This scheduler provides load balance among multiple partition-level parallel threads.
08e73fd6-27a0-5882-882c-75b79dc99e39|Multi-Ended Queue (Meque) Work Stealing Scheduler|The authors propose a novel work stealing scheduler that utilizes multi-ended queues (meques) to replace traditional double-ended queues (deques) in load balancing. This approach allows multiple thieves to steal work from different ends of the queue, reducing contention and improving load balance.
641d7655-7572-5047-b9f1-60d093429a57|Vertex Relabeling Scheme|The authors propose a vertex relabeling scheme to improve access locality in the graph. This approach re-labels the vertices in each traversal tree to improve the access locality.
32c021d9-c06f-5445-8548-089aa0a9cb85|Bounded Rational Behavioral (BRB) Update Rule|The BRB update rule is a solution proposed by the authors to address the challenge of memory-efficient scalable graph processing. This solution focuses on developing a distributed algorithm for the vertex cover problem, which is a fundamental problem in graph theory. The BRB update rule is designed to overcome the shortcoming of the existing memory-based best response (MBR) update rule, which cannot guarantee convergence to a stable state when the memory length is 1.
c100807e-4b08-57f3-a1a5-59ef739c2bf2|Memory-Based Best Response (MBR) Update Rule|The MBR update rule is an existing approach to optimize communication efficiency in distributed algorithms, specifically designed to address the challenge of minimizing round complexity while maintaining solution quality. This solution focuses on the vertex cover problem and proposes a memory-based best response update rule that enables vertices to make decisions based on local information, reducing the need for extensive communication.
c133aee5-3954-5ac9-82a6-3976941c38c5|Spatial Snowdrift Game Model|The spatial snowdrift game model is a novel approach to optimize communication efficiency in distributed algorithms, specifically designed to address the challenge of minimizing round complexity while maintaining solution quality. This solution focuses on the vertex cover problem and proposes a spatial snowdrift game model that enables vertices to make decisions based on local information, reducing the need for extensive communication.
0373fe5e-7f90-5e51-b9ab-477b6ec3da9d|Nash Equilibrium-based Approach|The Nash equilibrium-based approach is a novel algorithmic approach designed to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. It is specifically tailored to handle the vertex cover problem in complex networks, which is a fundamental challenge in graph processing. The approach is based on the concept of Nash equilibrium, which is a well-known game-theoretic concept that describes a stable state in which no vertex can improve its payoff by unilaterally changing its strategy.
129c4f29-b1fc-59a5-b11c-4a4e399e199f|Nash Equilibrium-based Vertex Cover Algorithm|The Nash equilibrium-based vertex cover algorithm is a game-theoretic approach to solving the vertex cover problem in dynamic graphs. It addresses the challenge of efficient graph dynamics processing by finding a Nash equilibrium in the spatial snowdrift game model, where each vertex makes decisions based on local information and adapts to changes in the graph structure.
8be20d67-74e3-5f89-aed8-822bbd98942c|Deterministic Distributed Algorithm for Fractional Coloring|The authors propose a deterministic distributed algorithm for fractional coloring, which aims to minimize the round complexity while maintaining a good approximation ratio. The algorithm is designed to work in the LOCAL model of computation, where each vertex has unbounded computational power and can communicate with its neighbors in synchronous rounds.
5e6c7a98-1195-5532-a680-b500a7f17d49|Distributed Algorithm for 2q-1:q Coloring in Trees|The authors propose a distributed algorithm for 2q-1:q coloring in trees, which aims to minimize the round complexity while maintaining a good approximation ratio. The algorithm is designed to work in the LOCAL model of computation, where each vertex has unbounded computational power and can communicate with its neighbors in synchronous rounds.
6ed895a7-2ae4-5eeb-9e39-2fff8affa01e|Distributed Algorithm for 2q-1:q Coloring in Grids|The authors propose a distributed algorithm for 2q-1:q coloring in grids, which aims to minimize the round complexity while maintaining a good approximation ratio. The algorithm is designed to work in the LOCAL model of computation, where each vertex has unbounded computational power and can communicate with its neighbors in synchronous rounds.
27edee48-def8-594d-b728-4767c1e23890|Distributed Algorithm for 2q-1:q Coloring in Sparse Graphs|The authors propose a distributed algorithm for 2q-1:q coloring in sparse graphs, which aims to minimize the round complexity while maintaining a good approximation ratio. The algorithm is designed to work in the LOCAL model of computation, where each vertex has unbounded computational power and can communicate with its neighbors in synchronous rounds.
0b0c7d79-cb5b-53f9-8594-bcea0bd899c6|Distributed Fractional Coloring Algorithm|The authors propose a distributed algorithm for fractional coloring in graphs, which can be applied to efficient graph dynamics processing. The algorithm is designed to work in the LOCAL model of computation, where each vertex has unbounded computational power and can communicate with its neighbors in synchronous rounds. The algorithm uses a combination of techniques, including the construction of a graph H1, whose nodes are all the induced paths of length 2q-1 in G, and the computation of a maximal independent set S1 in H1. The algorithm also uses a graph H2, whose nodes are the elements of S1, to extend the coloring of G to the paths of P. The authors show that the algorithm can compute a q-1:q coloring of G in O(q^3 * 2^q * q * log n) rounds deterministically in the LOCAL model.
56d023f2-f59e-546c-ac57-0fc9292b0814|Asynchronous Distributed Memory Algorithm for Triangle Counting and Local Clustering Coefficient (LCC) Computation|The authors propose a fully asynchronous distributed memory algorithm for both triangle counting and LCC computation, which removes synchronization overheads by using Remote Memory Access (RMA) one-sided operations to retrieve remote parts of the graph. The algorithm uses a 1D partitioning scheme to distribute the graph among processes, and each process accesses remote partitions via RMA operations. This approach enables asynchronous computation and reduces memory requirements. The authors report a speedup of up to 14 on distributed memory and a reduction in total running time by up to 73 compared to a non-cached version.
06d22c86-9641-5cf5-8b39-51f4647929d6|Caching Mechanism for Remote Memory Accesses|The authors propose a caching mechanism for RMA accesses to reduce communication time and improve performance. The caching mechanism uses a transparent caching solution for RMA, CLaMPI, which caches remote data and stores a dynamically defined sub-graph containing frequently accessed vertices. The authors report a reduction in communication time by up to 51.6 with caching only wadj.
27e26de8-f911-5b07-b37c-79d83a86ac08|Hybrid Strategy for Triangle Computation|The authors propose a hybrid strategy for triangle computation based on the frontiers, which combines binary search and sorted set intersection (SSI) methods. The hybrid strategy uses a combination of binary search and SSI to compute the intersection of adjacency lists, which reduces computation time and improves performance. The authors report that the hybrid strategy always performed better than using SSI or binary search exclusively.
93be38c5-0e2d-518f-91e6-dc2188593cbb|Application-Defined Scores for Cached Entries|The authors propose using application-defined scores for cached entries to improve caching efficiency. The authors modify CLaMPI to accept application-defined scores for cached entries, which are used to influence the victim selection process. The authors report that using degree centrality as the score for LCC reduces the total running time by up to 73.
282cd7da-2a6c-5ccb-8ee5-a3db28d947f8|Asynchronous Distributed Memory Triangle Counting and LCC with RMA Caching|The authors propose an asynchronous distributed memory algorithm for triangle counting and local clustering coefficient (LCC) computation, which utilizes Remote Memory Access (RMA) caching to optimize communication efficiency.
b827dae0-e016-5ef5-89ea-cf2aaf6df21a|Hybrid Triangle Computation|The authors propose a hybrid approach for triangle computation that combines binary search and sorted set intersection (SSI) methods to adapt to the heterogeneity of graph structures. The hybrid approach dynamically switches between binary search and SSI based on the length of the adjacency lists, allowing for efficient computation of triangles in graphs with varying degrees and weights. The paper reports that the hybrid approach achieves up to 8x performance improvement compared to using a single method exclusively.
231179e6-d50c-5484-ad7b-46ba171142f2|Application-Specific Caching|The authors propose an application-specific caching mechanism that takes into account the degree centrality of vertices to improve caching efficiency in distributed graph processing. The caching mechanism uses a least recently used (LRU) scheme weighted on a positional score to limit external fragmentation and assigns a score to cached entries based on their temporal locality and fragmentation. The paper reports that the application-specific caching mechanism reduces the total running time by up to 73% compared to a non-cached version.
ebd3cdb6-8e0e-57ec-9bd6-28783e91a94a|Asynchronous Distributed Algorithm|The authors propose a fully asynchronous distributed algorithm for triangle counting and local clustering coefficient computation that removes synchronization overheads and achieves vertex delegation. The algorithm uses Remote Memory Access (RMA) one-sided operations to read remote parts of the graph without involving target nodes, allowing for asynchronous computation and reducing communication overhead. The paper reports that the asynchronous algorithm achieves up to 14x speedup from 4 to 64 nodes for the LiveJournal1 graph on distributed memory.
224f6213-0e5c-5afe-99d5-bc7c866177d6|Parallel Intersection Computation|The authors propose a parallel intersection computation method that computes the intersection of adjacency lists in parallel using OpenMP. The method distributes work among threads by splitting the shorter keys array into equal-sized chunks for binary search and splitting the longer array for SSI. The paper reports that the parallel intersection computation method achieves up to 2.7x speedup using 16 threads compared to a sequential implementation.
8b3b134c-1d5c-55ee-9e75-d28c86945694|Hybrid Triangle Computation Method|The authors propose a hybrid method for triangle computation that combines binary search and sorted set intersection (SSI) to optimize GPU memory access for graph processing. This method is designed to minimize memory access inefficiencies by leveraging the strengths of both approaches.
d914366c-59af-5c3b-8613-fa18e29d1053|Caching RMA Accesses with CLaMPI|The authors propose using CLaMPI, a caching layer for RMA, to optimize GPU memory access for graph processing. CLaMPI caches remote data to reduce memory access latency and improve coalescing.
6dd21bf2-f1e8-5d27-9bcc-55d530614e15|Asynchronous Computation with RMA|The authors propose using asynchronous computation with RMA to optimize GPU memory access for graph processing. This approach allows for overlapping computation and communication, reducing memory access latency.
40b5394e-5ec9-5707-ba42-66ac8d317dfb|Parallel Computation of Intersections|The authors propose computing the intersection of adjacency lists in parallel using OpenMP, which improves the performance of the hybrid strategy for triangle computation. The authors distribute work among threads by splitting the shorter keys array into equal-sized chunks for binary search, and splitting the longer array for SSI. The authors report a speedup of up to 2.7 using 16 threads compared to a sequential implementation.
7793f6eb-6889-5a11-ac37-fe2a4328a149|Priority-based Memory Controller for Embedded Systems (PMSMC)|PMSMC is a novel memory controller that prioritizes concurrently running applications by assigning uneven quota for each requestor, accompanied by a timer to control the dispatch rate and prevent starvation.
08eb8859-ac56-5675-884e-ff385b731f78|TwinTwig Decomposition|TwinTwig decomposition is a pattern decomposition strategy that decomposes the pattern graph into a sequence of TwinTwigs, where each TwinTwig is an edge or two incident edges of a node. This decomposition strategy is designed to reduce the size of the partial results and improve the scalability of the subgraph enumeration algorithm.
0b897965-0ed7-521d-bd87-5d74f39bb494|Order-Aware Cost Reduction|Order-aware cost reduction is an optimization strategy that uses the partial order to further reduce the computational cost. This strategy is designed to reduce the number of partial results and improve the scalability of the subgraph enumeration algorithm.
2ce1e7fd-dc99-5d37-9fba-7ac48c2ecd0d|Workload Skew Reduction|Workload skew reduction is an optimization strategy that reduces the workload skew caused by high-degree nodes in the data graph. This strategy is designed to improve the scalability of the subgraph enumeration algorithm.
12a4de72-631b-5002-be1a-9c042f6dc8aa|Early Filtering|Early filtering is an optimization strategy that uses the remaining memory to further filter invalid partial results in early stages. This strategy is designed to reduce the memory consumption and improve the scalability of the subgraph enumeration algorithm.
ed4b6764-c99a-51da-9ce5-414587853ca1|TwinTwigJoin|TwinTwigJoin is a novel algorithm for subgraph enumeration in MapReduce, which follows a left deep join framework with a new pattern decomposition strategy, namely, TwinTwig decomposition. This approach addresses the challenge of efficient graph dynamics processing by minimizing the computational cost and iterations required for subgraph enumeration in large graphs.
bdf5f5e1-42c0-5bba-a6f5-7c4e130bba3c|Local Aggregation Algorithm|The authors propose a local aggregation algorithm that allows for the simulation of algorithms on the line graph in the CONGEST model without incurring additional overhead. This solution specifically addresses the challenge of memory-efficient scalable graph processing by enabling the execution of algorithms on the line graph while avoiding congestion.
d96e109d-3eec-5e0c-9c34-11f175c2ab26|Distributed MaxIS Approximation Algorithm|The authors propose a distributed MaxIS approximation algorithm that finishes within O MIS G logW rounds with probability at least 1 p logW in the CONGEST model. This solution specifically addresses the challenge of memory-efficient scalable graph processing by providing a fast and efficient algorithm for approximating the maximum independent set in a graph.
ff8170b6-0aee-55dd-910c-98be7d3c3af8|Distributed MaxIS Approximation via Local Ratio|The authors propose a distributed MaxIS approximation algorithm that uses the local ratio technique to achieve a 2-approximation in O(log n) rounds.
faa4dd1d-e73b-5ccd-bfdf-bc49db47d09b|Deterministic Coloring-Based Approximation Algorithm|The authors propose a deterministic coloring-based approximation algorithm for MaxIS that achieves a 2-approximation in O(log n) rounds.
233add5e-4f81-57f8-bf8c-764308fc37b6|Nearly Maximal Independent Set Algorithm|The authors propose a nearly maximal independent set algorithm that achieves a 2-approximation in O(log log log n) rounds.
a88fb92b-472e-5245-9676-9c0c5ae080b4|Modified Nearly Maximal Independent Set Algorithm|The authors propose a modified nearly maximal independent set algorithm that achieves a 2-approximation in O(log log log n) rounds.
fd68451d-623b-5add-bcce-56b50c3d5449|Nearly Maximal Independent Set Algorithm for Adaptive Graph Processing|The authors present a nearly maximal independent set algorithm that adapts to heterogeneous graph structures by iteratively finding independent sets and reducing the weights of nodes in the graph.
2c813604-9cc7-5a77-aafa-b14c2277dfad|Local Aggregation Algorithm for Efficient Graph Processing|The authors propose a local aggregation algorithm that enables efficient graph processing by aggregating local information from neighboring nodes.
7caafa18-7754-5a8e-932c-0fdf1229c4e0|Coloring-Based Distributed Approximation Algorithm for Adaptive Graph Processing|The authors present a coloring-based distributed approximation algorithm that adapts to heterogeneous graph structures by iteratively finding independent sets and reducing the weights of nodes in the graph.
3a0f5e69-275d-5c9c-b8a7-5bd6a60eb716|Local Ratio Technique for Distributed MaxIS Approximation|The authors propose a distributed approximation algorithm for the maximum independent set (MaxIS) problem using the local ratio technique. This solution specifically addresses the challenge of efficient graph dynamics processing by providing a novel, yet simple, manner of adapting the local ratio technique to the distributed setting. The algorithm iteratively finds independent sets and finishes after logW iterations, yielding a approximation.
18d7d3e3-5336-5c86-8f72-0110eca6b335|Coloring-Based Distributed Approximation for Weighted MaxIS|The authors present a deterministic coloring-based algorithm for weighted MaxIS, which runs in O(log n) rounds. This solution addresses the challenge of efficient graph dynamics processing by providing a fast and efficient algorithm for finding a maximum independent set in a weighted graph.
d21c29b1-cb50-5778-8382-285e73598328|Local Aggregation Algorithms for Distributed 2-Approximation of Maximum Weighted Matching|The authors propose a distributed 2-approximation algorithm for maximum weighted matching using local aggregation algorithms. This solution addresses the challenge of efficient graph dynamics processing by providing a fast and efficient algorithm for finding a maximum weighted matching in a graph.
0b311569-8ecb-5269-b570-5535bfca2d40|Distributed Nearly Maximal Independent Set Algorithm|The authors propose a distributed nearly maximal independent set algorithm, which runs in O(log log log) rounds. This solution addresses the challenge of efficient graph dynamics processing by providing a fast and efficient algorithm for finding a nearly maximal independent set in a graph.
7479ad75-174e-5372-96a9-b0a0c0850596|Lightweight Approximate Sorting|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a lightweight approximate sorting algorithm to alleviate load imbalance among GPU threads. The algorithm maps each element of the input vector into a bucket and then sorts each bucket in parallel, reducing the complexity of sorting and improving the overall performance.
cf1b1c60-4e3d-5aba-8e35-2ccadbe2e390|Data Layout Remapping|This solution addresses the challenge of memory-efficient scalable graph processing by proposing a data layout remapping algorithm to mitigate non-coalesced memory access patterns. The algorithm groups vertices at the granularity of a warp and rearranges the data layout to achieve coalesced memory access.
01a3ded0-ea48-590d-be58-0269aab61311|Edge-Vertex Model|This solution addresses the challenge of memory-efficient scalable graph processing by proposing an Edge-Vertex model to simplify the programming task of using GPUs for graph computation. The model partitions the computation into two methods, EdgeCompute and VertexCompute, to reduce the complexity of graph algorithms.
c6e5a1c1-29e1-52c7-8d85-8b7b04f75c04|Approximate Sorting Algorithm|The authors propose an approximate sorting algorithm to optimize communication efficiency in distributed algorithms. This algorithm is designed to reduce the number of communication rounds by sorting the message buffer according to the vertex in-degree on the CPU, resulting in a significant performance improvement.
01440d65-0053-5dc6-ad49-f21980de6250|Data Layout Remapping of the Message Buffer|The authors propose a data layout remapping approach to optimize communication efficiency in distributed algorithms. This approach involves grouping vertices at the granularity of a warp and rearranging the data layout within a group to be column-major, while maintaining a row-major layout among groups.
a0adb44f-caa6-5e6b-98a9-7aef618cf9d9|Approximate Sorting|The authors propose an approximate sorting algorithm to address the challenge of load imbalance in distributed systems. This algorithm is designed to mitigate the workload imbalance among GPU threads by sorting the vertices based on their degrees.
d8195392-b1fb-5cea-a9f9-d59e85b941ef|Vertex Elimination-based Algorithm|The authors propose a vertex elimination-based algorithm to address the challenge of memory-efficient scalable graph processing. This algorithm iteratively eliminates vertices that do not meet the constraints specified by the pattern, reducing the search space and memory consumption.
d48830d1-c92b-5eb9-87c9-877ab9b5486f|Cycle Checking Visitor|The authors propose a cycle checking visitor algorithm to address the challenge of memory-efficient scalable graph processing. This algorithm iterates over the set of cycle constraints to be checked and validates each cycle one at a time, reducing memory consumption by avoiding unnecessary computations.
b10461da-b527-5341-96c2-8cf6e6f76e0b|Distributed Quiescence Detection Algorithm|The authors propose a distributed quiescence detection algorithm to address the challenge of memory-efficient scalable graph processing. This algorithm detects quiescence in a distributed graph processing system, reducing memory consumption by avoiding unnecessary computations.
506d41fb-9f55-5d4d-b207-d1af87df7974|Metadata Store|The authors propose a metadata store solution to address the challenge of memory-efficient scalable graph processing. This solution stores metadata independently of the graph topology, reducing memory consumption by avoiding unnecessary data storage.
da6afaa0-74f1-5829-92a3-b3f376fd7ff6|Token-based Cycle Checking|The authors propose a token-based cycle checking algorithm to optimize communication efficiency in distributed algorithms. This solution specifically addresses the challenge by minimizing the number of communication rounds required to verify cycle constraints in the graph.
f27ecb37-d182-50e8-9ff0-32a18b92873e|Asynchronous Visitor Abstraction|The authors propose an asynchronous visitor abstraction to optimize communication efficiency in distributed algorithms. This solution specifically addresses the challenge by allowing vertices to process visitors asynchronously, reducing the need for synchronization and minimizing communication rounds.
64e58da0-68c3-509e-a1e0-fbee5d54618a|Distributed Quiescence Detection|The authors propose a distributed quiescence detection algorithm to optimize communication efficiency in distributed algorithms. This solution specifically addresses the challenge by minimizing the number of communication rounds required to detect quiescence in the graph.
e052d54d-1017-56e6-9fb1-ae9bdd054a52|Iterative Vertex Elimination|The authors propose an iterative vertex elimination approach to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This approach involves iteratively eliminating vertices that do not meet local constraints imposed by the pattern, reducing the search space and improving performance.
fc98e876-9ee9-57f9-9c04-f93a5c17b64b|Token-Based Cycle Checking|The authors propose a token-based cycle checking approach to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This approach involves using tokens to verify the existence of cycles in the graph, reducing the overhead of cycle checking.
725209ba-2d82-56aa-8c18-3448670e51be|Distributed Pattern Matching|The authors propose a distributed pattern matching approach to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This approach involves distributing the pattern matching task across multiple nodes, reducing the overhead of pattern matching.
be908840-0104-5ad8-9a05-6f23e1fab300|HavoqGT's Delegate Partitioned Graph|The authors propose using HavoqGT's delegate partitioned graph to achieve load balancing. This approach distributes the edges of each high-degree vertex across compute nodes, ensuring that the workload is evenly distributed.
a3aca90b-1cad-5da8-b784-c679bf64c3ef|Token Passing for Cycle Detection|The authors propose a token passing approach to detect cycles of appropriate length in large graphs. This approach involves passing tokens through edges in the graph to verify whether a vertex participates in a cycle of the correct length.
09cee210-bb09-54f1-8d4d-e26b09b7bf77|Low Congestion Shortcuts|The authors propose using low congestion shortcuts to optimize communication efficiency in distributed algorithms. This solution involves constructing subgraphs that allow for efficient communication between nodes while minimizing congestion.
5712e710-29f4-53e8-ae77-d9c5fb66a212|Distributed Implementation of Set Cover Algorithm|The authors propose a distributed implementation of a set cover algorithm to optimize communication efficiency in distributed algorithms. This solution involves simulating a centralized set cover algorithm in a distributed setting using a decomposition of the graph into segments.
f9424314-bd62-5264-bdc5-1a43492c48cc|Tree Augmentation via Heavy-Light Decomposition|The authors propose using a heavy-light decomposition of the tree to optimize communication efficiency in distributed algorithms. This solution involves decomposing the tree into heavy and light edges, which allows for efficient communication between nodes.
70907dad-b517-5a33-bf94-b038b11bc0d5|Heavy-Light Decomposition|The authors propose using heavy-light decomposition to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution involves decomposing the graph into heavy and light edges to reduce communication overhead and enhance memory locality.
33e0a544-878e-5dcf-bfd1-efa413c97497|Ancestors Sum Problem|The authors propose using the Ancestors Sum Problem to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution involves computing the summation of the values of all ancestors of a node in an arbitrary given tree.
c5da1e88-9ee0-5e54-9743-605d809cac4d|Descendants Sum Problem|The authors propose using the Descendants Sum Problem to address the challenge of adaptive algorithms for heterogeneous and irregular graphs. This solution involves computing the summation of the values of all descendants of a node in an arbitrary given tree.
8283103a-e271-5b63-8c7f-c81361e80dc3|Distributed Tree Augmentation Algorithm|The authors propose a distributed algorithm for tree augmentation, which is a key component in solving the 2-ECSS problem. The algorithm works by simulating a parallel algorithm for set cover on a virtual graph, where all non-tree edges are between ancestors and descendants. The algorithm consists of two phases: a forward phase that adds edges to the augmentation and a reverse delete phase that removes edges while ensuring that all tree edges are covered.
8f92550a-b134-502c-90df-396b79ddea0c|Low-Congestion Shortcuts Framework|The authors utilize the low-congestion shortcuts framework to develop an O(log n)-approximation algorithm for 2-ECSS that runs in SC(G) rounds. The framework allows for efficient computation of aggregate functions and simulation of the set cover algorithm.
940838c1-b0b6-5378-9206-fe9b8766115a|Improved Approximation Algorithm|The authors propose an improved approximation algorithm that achieves a 2-approximation for weighted TAP in the virtual graph and a 3-approximation for weighted 2-ECSS in the original graph. The algorithm works by modifying the reverse delete phase to remove edges that cover tree edges multiple times.
32fbd1a0-2136-5876-8711-df778fd15432|Distributed Aggregation Scheduling (DAS)|DAS is a distributed algorithm designed to generate collision-free schedules for data aggregation in wireless sensor networks, aiming to minimize time latency and energy consumption. DAS employs a two-phase approach, first constructing a distributed aggregation tree and then performing distributed aggregation scheduling. The algorithm utilizes a greedy strategy to minimize the time latency of the aggregation schedule. The latency bound of the schedule is 24D + 6 * 16, where D is the network diameter and is the maximum node degree. The paper presents simulation results showing that DAS outperforms existing algorithms with much lower latencies. The bigger the number of nodes or the transmission radius, the better the improvement of DAS is in comparison with existing algorithms.
da9ca2d9-471d-5fdb-9279-a72c6070a368|Adaptive Scheduling using DAS|This solution proposes an adaptive method for updating the schedule when the network topology changes, making the algorithm suitable for networks with dynamic topologies. The adaptive method involves maintaining the aggregation tree and updating the schedule when nodes join or fail. The algorithm uses a recursive process to find new parents for nodes that lose their parents due to node failures. The paper presents simulation results showing that the adaptive DAS scales well with the size of the network, with a small increase in latency and transmission when nodes join or fail.
5873dd6d-a76f-523e-911a-421078683ee0|Distributed Aggregation Scheduling (DAS) Algorithm|The DAS algorithm is a distributed algorithm designed to optimize communication efficiency in wireless sensor networks by minimizing the number of communication rounds required for data aggregation.
01d98177-4b7e-5912-ab5d-2d051c5919b1|Adaptive DAS Algorithm|The adaptive DAS algorithm is an extension of the DAS algorithm, designed to handle dynamic network changes, such as node failures and new node joinings.
2b09c6fb-1a6b-5784-af5f-9be53ec47657|Competitor Set Determination Method|The competitor set determination method is a novel technique used in the DAS algorithm to determine the competitor set of each node.
dfe2db17-9af6-55c2-9bca-b31d2c93570c|Tree Construction Algorithm|The tree construction algorithm is a novel method used in the DAS algorithm to create a distributed aggregation tree.
8f6096b0-ccf3-5547-b6d3-5d7e4779bbc5|Adaptive Scheduling Method|The adaptive scheduling method is designed to address the challenge of adaptive algorithms for heterogeneous and irregular graphs by providing a method for updating the schedule in response to changes in the network topology.
c14d8734-8f54-5475-a159-3c936b820cca|Trie-Based Data Structure|The authors propose a novel trie-based data structure to reduce the memory footprint of graph processing while maintaining good memory access efficiency. This data structure is designed to efficiently store and manage intermediate results during subgraph isomorphism computations.
2e4e5d86-1665-5803-9890-298e8942434b|Hierarchical Load Balancing|The authors propose a hierarchical load balancing approach to minimize synchronization requirements and achieve good load balancing between different nodes in a distributed computing environment.
132a936d-df17-5439-bc2c-a60862578a6c|Micro-Kernel Intersection Design|The authors propose an efficient micro-kernel design for fast intersections, which is a critical component of subgraph isomorphism computations.
b3c71f63-e28f-5194-a769-2757b8d17d0e|Asynchronous Protocol for Work Distribution|The authors propose an asynchronous protocol for work distribution in the distributed implementation of their subgraph isomorphism algorithm. This protocol allows nodes to share their workload with other nodes without requiring synchronization, thereby minimizing communication overhead.
28c8a811-45d0-5aed-be3a-6feb26e18d00|Hybrid BFS-DFS Strategy|The authors propose a hybrid BFS-DFS strategy to efficiently process subgraph isomorphism patterns in heterogeneous and irregular graphs. This strategy combines the benefits of breadth-first search (BFS) and depth-first search (DFS) to adapt to varying graph structures and densities.
846a90c3-090e-54e6-8670-76cff16502ab|Adaptive Intersection Mechanism|The authors propose an adaptive intersection mechanism to efficiently compute the intersection of subgraph patterns in heterogeneous and irregular graphs. This mechanism is designed to adapt to varying graph structures and densities, reducing communication overhead and improving memory locality.
a5e37d3b-ddbf-5381-8399-b02b08f5e4d6|Adaptive Work Distribution Strategy|The authors propose an adaptive work distribution strategy to optimize load balance in distributed systems. This strategy involves dividing each outer iteration into several chunks and processing them sequentially. At the end of each chunk, busy nodes check if any free nodes are available to share the workload. If a free node is found, the busy node sends a portion of its work to the free node along with the required data, and this process is repeated until the entire work is done.
0ba583af-620c-5072-a505-42d81093f585|Virtual Warp Strategy|The authors propose a virtual warp strategy to optimize load balance in distributed systems. This strategy involves grouping paths based on the work into bins and using virtual warps to process the bins. Each warp processes a set of paths, reducing thread idling and improving load balance.
cd1afb5d-04ad-588a-aae6-e1c5bdaf898b|Trie-Based Data Structure for Intermediate Storage|The authors propose a novel trie-based data structure to store intermediate results during subgraph isomorphism computation. This data structure is designed to reduce memory footprint while maintaining good memory access efficiency.
3c71b2f7-82bf-51de-983f-42edd2536db9|Hybrid Scanning Strategy for Subgraph Isomorphism|The authors propose a hybrid scanning strategy that combines the benefits of depth-first search (DFS) and breadth-first search (BFS) strategies. This approach is designed to reduce memory usage and improve parallelism.
a65587c1-13b0-5f68-9cfa-00a9fbdd3566|Efficient Micro-Kernel Design for Intersection Operations|The authors propose an efficient micro-kernel design for intersection operations, which is a critical component of subgraph isomorphism computation.
623f795b-099e-5f71-8370-28e4ea081fd0|Distributed GPU Implementation for Subgraph Isomorphism|The authors propose a distributed GPU implementation for subgraph isomorphism computation, which is designed to handle larger datasets and complex query graphs.
a9f2c73b-4e53-5fd2-953a-4c65eac34d88|Trie-Based Data Structure for Efficient Graph Dynamics Processing|The authors propose a novel trie-based data structure to efficiently process dynamic updates in large graphs. This data structure is designed to minimize computational costs and iterations by allowing for swift recomputation of trussness values and adaptive adjustments to graph topology changes.
964d6ca6-8844-5ff8-9059-06d5627a9ffa|Deterministic Distributed Rounding Method|The authors propose a deterministic distributed rounding method for fractional matchings to integral matchings, which is a key component in solving the challenge of memory-efficient scalable graph processing. This method allows for the efficient processing of large-scale graph data by reducing memory consumption and optimizing memory usage.
f1c14f0c-9cc0-5059-a188-4f0a7120768e|2-Decomposition Technique|The authors propose a 2-decomposition technique for graphs, which is a key component in solving the challenge of memory-efficient scalable graph processing. This technique allows for the efficient processing of large-scale graph data by reducing memory consumption and optimizing memory usage.
39c94e13-c6fe-5229-8d2c-64b990456520|Constant Approximation Algorithm|The authors propose a constant approximation algorithm for maximum matching, which is a key component in solving the challenge of memory-efficient scalable graph processing. This algorithm allows for the efficient processing of large-scale graph data by reducing memory consumption and optimizing memory usage.
a36b5955-6a9c-51ef-b243-534fd000fe99|Almost Maximal Matching Algorithm|The authors propose an almost maximal matching algorithm, which is a key component in solving the challenge of memory-efficient scalable graph processing. This algorithm allows for the efficient processing of large-scale graph data by reducing memory consumption and optimizing memory usage.
17c523c1-d37b-5eb4-beb4-ae38edb069ac|Constant Approximation Algorithm for Maximum Matching|The authors propose a constant approximation algorithm for maximum matching, which is a key component of their deterministic distributed rounding method. This algorithm involves iteratively updating the values of edges in a graph to obtain a constant approximate maximum matching.
327e6fe2-3f5c-5726-937d-904509f0e5fe|O(log^2 log n) Round Algorithm for Maximal Matching|The authors propose an O(log^2 log n) round algorithm for maximal matching, which is a key application of their deterministic distributed rounding method. This algorithm involves iteratively applying their constant approximation algorithm for maximum matching to obtain a maximal matching.
8d753a09-4a15-5fdb-93b7-8b4f0dbbe427|Deterministic Local Computation Algorithm for Maximum Matching|The authors propose a deterministic local computation algorithm for maximum matching, which is a key application of their deterministic distributed rounding method. This algorithm involves using their constant approximation algorithm for maximum matching to obtain a maximum matching in a local computation model.
8c228ef0-391e-5fe4-a2c9-3254d833754b|2-Decomposition Method|The authors propose a 2-decomposition method for transforming a general graph into a bipartite graph with the same edge set, which can be used to compute a constant approximate maximum matching.
5a1743d6-d6c4-5e91-8a39-b0494b26c93a|Weighted Matching Algorithm|The authors propose a weighted matching algorithm that can be used to compute a 2-approximate maximum weighted matching in O(log^2 log n) rounds.
94818881-57e3-5168-a3cf-cc986e5e0850|B-Matching Algorithm|The authors propose a b-matching algorithm that can be used to compute a 2-approximate maximum b-matching in O(log^2 log n) rounds.
67279715-9e5a-558a-9572-bd24c228abe7|Edge Dominating Set Algorithm|The authors propose an edge dominating set algorithm to efficiently process graph dynamics. This algorithm involves computing a minimum edge dominating set in a graph, which is a set of edges that covers all other edges.
daeef533-373b-559d-950d-729fe5eecc23|Partition Centric Processing Methodology (PCPM)|PCPM is a novel approach to graph processing that focuses on partitions rather than individual nodes or edges. It uses a 2-phased Gather-Apply-Scatter (GAS) model to reduce memory accesses and improve locality.
9697a2f1-9235-5588-8bf8-854746d623e8|Partition-Node Graph (PNG) Data Layout|PNG is a new data layout that enables streaming updates to one bin at a time, reducing memory traffic and improving locality.
2577c54b-6946-5918-8803-954b68d169a0|Branch Avoidance Mechanism|The branch avoidance mechanism is a technique used to reduce data-dependent branches in the PCPM gather phase.
1cc6cf3e-7997-51f3-bab9-38f088b02a2b|Intelligent Node Labeling|Intelligent node labeling is a technique used to improve locality in graph processing.
f32626b4-aab7-5ba8-af49-b96d8be98f3b|Partition-Centric Processing Methodology (PCPM)|PCPM is a novel approach that optimizes communication efficiency in distributed algorithms by partitioning the graph into disjoint sets of contiguously labeled nodes. This methodology reduces the number of communication rounds by allowing each thread to process one partition at a time, thereby minimizing the need for extensive communication.
94370dc5-cc86-53cf-be04-0c8dfc5355c0|Binning with Vertex-Centric GAS (BVGAS)|BVGAS is a methodology that allocates multiple bins to store incoming messages, reducing the number of communication rounds by inducing spatio-temporal locality in access patterns.
92223208-ca4e-526f-9eab-441b3e5ab683|Pull Direction PageRank (PDPR)|PDPR is a methodology that optimizes communication efficiency by using a pull direction approach, which enables all columns of the adjacency matrix to be traversed asynchronously in parallel without the need to store partial sums in memory.
740e2076-b317-5677-babd-a21b0c5feb62|Partition-wise Construction of Partition Neighborhood Graph (PNG)|PNG is a data structure that represents the graph as a set of bipartite graphs, each corresponding to a partition. This construction enables efficient processing of heterogeneous and irregular graphs by reducing the range of destination IDs and removing unused edges.
e8d6368a-54eb-5e44-8546-9b6e02700e2f|Binning with Vertex-Centric Processing|Binning is a technique used in conjunction with Vertex-Centric processing to reduce memory access inefficiencies. It stores updates in a semi-sorted manner, inducing spatio-temporal locality in access patterns.
bc637934-e226-595b-a5f3-24f32fe48964|Graph Reordering with GOrder Algorithm|The GOrder algorithm is used to reorder the nodes in the graph, improving spatial locality and reducing memory access inefficiencies.
e8b9fb21-c472-51ae-8c16-dd71b2522d46|Bipartite Partition-Node Graph (PNG) Data Layout|PNG is a new data layout that reduces communication and random memory accesses by compressing edges and storing them in a semi-sorted manner.
17f2e7e3-de14-5518-a6ec-817b37e000b1|External Neighbor Expansion|This solution addresses the challenge of memory-efficient scalable graph processing by ensuring that all edges needed for computing the embeddings are available locally, thereby minimizing the amount of communication required between partitions. The solution involves collecting external neighbor requests for any vertex in the embeddings of frequent patterns and then requesting the external neighbors from other partitions. This approach ensures that all relevant edges are available locally, reducing the need for communication between partitions. The paper demonstrates the effectiveness of this solution by showing that it can scale to billion-vertex graphs, with a speedup close to 3 for the lowest support value when comparing p=128 with p=32 compute nodes.
f9e21883-8172-515a-94e4-2b03cabded44|Support Bound Pruning|This solution addresses the challenge of memory-efficient scalable graph processing by reducing the number of patterns that need to be communicated between partitions, thereby minimizing memory consumption and communication overhead. The solution involves using a two-step support bounding technique to prune patterns that are not globally frequent, thereby reducing the number of patterns that need to be communicated between partitions. The paper demonstrates the effectiveness of this solution by showing that it can reduce the number of patterns that need to be communicated between partitions, thereby improving scalability and reducing memory consumption.
a66a1128-3bd4-54f4-9fd0-d949fcd1086f|Local Support Computation|This solution addresses the challenge of memory-efficient scalable graph processing by allowing local pruning of patterns that are not globally frequent, thereby reducing the amount of communication required between partitions. The solution involves computing the local support of patterns in each partition and then using this information to prune patterns that are not globally frequent. The paper demonstrates the effectiveness of this solution by showing that it can improve scalability and reduce memory consumption by allowing local pruning of patterns that are not globally frequent.
52dae157-9bbd-55cb-b9d1-7c1277c28fff|Embedding-Centric Approach|This solution addresses the challenge of memory-efficient scalable graph processing by using an embedding-centric approach that focuses on the embeddings of patterns rather than the patterns themselves. The solution involves using a set of optimizations and techniques, including external neighbor expansion, support bound pruning, and local support computation, to minimize memory consumption and communication overhead. The paper demonstrates the effectiveness of this solution by showing that it can scale to billion-vertex graphs and improve scalability and reduce memory consumption.
248ab4b0-24ca-5547-b0fd-f8117c1f19d4|Two-Step Support Bounding Technique|The authors propose a two-step support bounding technique to minimize communication efficiency in distributed algorithms. This technique involves determining patterns that are definitely globally frequent and infrequent, and then eliminating patterns that cannot possibly be globally frequent.
ed25e37c-5ac7-5dea-a0a4-5fb80e04b45d|Regeneration of Embeddings|The authors propose a regeneration of embeddings technique to minimize communication efficiency in distributed algorithms. This technique involves regenerating the embeddings for each pattern in a distributed manner.
67b33573-dd69-5e49-8f17-4467cee5c660|Collective Communication Primitives|The authors propose the use of collective communication primitives to minimize communication efficiency in distributed algorithms. This technique involves using primitives such as AllToAll and AllGather to minimize communication.
ffe32441-0ed3-5550-ad96-df7fc6377e0b|Hybrid Load Balancing via Partitioning and Multi-threading|The authors propose a hybrid approach that combines graph partitioning with multi-threading to optimize load balance in distributed systems. This solution involves dividing the input graph into multiple partitions and processing each partition using multiple threads within a compute node.
c5c8e40f-bbc2-55a4-8628-074af46cfe20|Support Bounding for Load Balancing|The authors propose a support bounding technique to prune patterns that are not globally frequent, which helps to reduce the amount of communication and computation required for load balancing.
c8b14ae7-388c-5976-8ccd-f5179fda4182|External Neighbor Expansion for Load Balancing|The authors propose an external neighbor expansion technique to reduce the amount of communication required for load balancing. This technique involves collecting external neighbor requests for each pattern and storing them in a set for later use.
9cfa557d-9cae-52ce-bbf2-e3bbf42bb0b8|Block Encoding|The authors propose a block encoding method to reduce memory consumption by grouping nearby edges in the adjacency matrix into blocks, allowing for more efficient storage and processing.
39fb262e-023c-58e6-97e6-9e26d8f30712|Clustered Edges|The authors propose a method to cluster edges in the graph, which can be combined with block encoding to further reduce memory consumption.
104b49a2-4369-50a8-9f1f-3d37c4be5fdd|Diagonal Block Iteration|The authors propose a method to reduce the number of iterations required for certain graph algorithms by multiplying diagonal matrix blocks and corresponding vector blocks as much as possible in one iteration.
cf416bc4-3ec7-5009-87e7-66e93d2d3d4d|Generalized Iterative Matrix-Vector Multiplication (GIM-V)|The authors propose a generalized framework for iterative matrix-vector multiplication, which can be used to implement various graph algorithms in a memory-efficient and scalable manner.
86943ac1-a74b-5679-983f-a97f32933001|Block Multiplication|The authors propose a block multiplication method to optimize communication efficiency in distributed algorithms. This method involves dividing the adjacency matrix into blocks and processing each block separately, reducing the number of communication rounds required.
231c84c0-ebd6-551f-926d-392dec00493b|GIM V (Generalized Iterative Matrix Vector multiplication)|GIM V is a generalized form of matrix vector multiplication that can be customized to support various graph mining operations, including PageRank, Random Walk with Restart, diameter estimation, and connected components. GIM V involves three customizable operations: combine2, combineAll, and assign. These operations can be tailored to specific graph mining tasks, allowing for efficient processing of heterogeneous and irregular graphs. The paper demonstrates the effectiveness of GIM V in handling large-scale graphs, including the YahooWeb graph with 6.7 billion edges. The authors report that GIM V achieves a good scale-up on the number of available machines and shows linear running time on the number of edges.
ff6fdc75-b616-5bcf-b97b-91da7fa3a149|Block Multiplication (GIM V BL)|Block Multiplication is a technique used in GIM V to group elements of the input matrix into blocks or submatrices of size b by b, and group elements of input vectors into blocks of length b. By grouping elements into blocks, Block Multiplication reduces the number of lines in the matrix and vector files, decreasing the sorting time and improving performance. The paper shows that GIM V BL is more than 5 times faster than GIM V BASE, and that the performance gain increases with the number of machines.
466c0b5f-7fb7-572e-80ae-9f0e0a430194|Clustered Edges (GIM V CL)|Clustered Edges is a technique used in GIM V to group edges in the input graph into clusters, reducing the number of blocks required for block multiplication. By clustering edges, GIM V CL reduces the number of blocks required for block multiplication, improving performance. The paper shows that GIM V CL can improve performance when combined with block encoding, but does not provide specific results for this technique.
d9c76b52-80cc-5832-be67-9cfe63ff86d9|Diagonal Block Iteration (GIM V DI)|Diagonal Block Iteration is a technique used in GIM V to multiply diagonal matrix blocks and corresponding vector blocks as much as possible in one iteration, reducing the number of iterations required. By multiplying diagonal blocks and vectors until the contents of the vectors do not change in one iteration, GIM V DI reduces the number of iterations required, improving performance. The paper shows that GIM V DI can reduce the number of iterations required, but does not provide specific results for this technique.
4aa91673-2a15-50db-8c94-8cef95f2ff1f|HCC (Hierarchical Connected Components)|HCC is a new algorithm for finding connected components in large graphs, using a hierarchical approach to reduce the number of iterations. HCC involves maintaining a component id for each node, which is updated iteratively until convergence. The paper demonstrates the effectiveness of HCC in finding connected components in large graphs.
f8981bc5-1277-55f6-9c59-d7209d98f410|HADI (Hadoop-based Diameter Estimation)|HADI is an algorithm for estimating the diameter of large graphs, using a bitstring update approach to reduce the number of iterations. HADI involves maintaining a bitstring for each node, which is updated iteratively until convergence. The paper demonstrates the effectiveness of HADI in estimating the diameter of large graphs.
3fa3b941-d249-5a7f-8b47-974748bc4788|Miss Optimized Memory System (MOMS)|MOMS is a memory system designed to handle the irregular read accesses typical of graph processing. It uses a combination of a cache and a non-blocking cache to minimize memory accesses and reduce memory consumption.
5b9194b5-2e6e-57fd-91b2-a569141234a4|Edge-Centric Model with Interval-Based Partitioning|This solution uses an edge-centric model that partitions edges into shards based on source and destination node intervals. This approach reduces memory accesses and improves locality.
31fd7ac2-f730-57a1-acb8-45c94dc8afb7|Node Reordering with Cache Line Reordering and DBG Reordering|This solution uses node reordering techniques to improve cache line reuse and reduce memory accesses. It uses cache line reordering and DBG reordering to group nodes with high outdegree together.
98de154e-b31e-514b-b502-e0ecddea8bd5|Graph Encoding and Memory Layout|This solution uses a graph encoding and memory layout that reduces memory consumption and improves locality. It uses a compressed edge format and organizes edges by shard.
a268c3cd-eb16-508e-8053-0ccd8eec5af8|Private and Two-Level MOMS|Private and two-level MOMS is an extension of the MOMS solution, designed to further optimize communication efficiency in distributed algorithms. It achieves this by reducing bank conflicts and providing reuse among multiple PEs without extra memory requests.
e0f684ae-9afa-5859-9536-32c3da3e0711|Node Reordering Techniques|Node reordering techniques are designed to optimize communication efficiency in distributed algorithms by minimizing the number of communication rounds. They achieve this by reordering nodes to improve cache line reuse and reduce the number of cache misses.
83114081-a95a-50e8-9d3b-0f7819941617|Multidie Aware MOMS|Multidie aware MOMS is an extension of the MOMS solution, designed to optimize communication efficiency in distributed algorithms on multidie FPGAs. It achieves this by using a combination of non-blocking caches and cuckoo hashing to store MSHRs in ordinary RAM.
a4e7b72d-01f8-5e49-986e-c64a04781d25|Interval-based Graph Partitioning|This solution involves partitioning the graph into intervals based on the node IDs, and then processing the intervals in parallel. This approach helps to reduce the memory access latency and improve memory locality.
affaece6-7c61-515b-b2bb-7a65a7f3b667|Node Reordering|This solution involves reordering the nodes in the graph to improve memory locality and reduce memory access latency.
c992f73f-cbcc-5541-94ca-86360500f1f1|Dynamic Burst Assembly|This solution involves dynamically assembling DRAM bursts over a multitude of random accesses to improve memory bandwidth utilization.
42e1815f-3f1a-594b-bf9f-16f2b191781c|Hash-based Relabeling|The authors propose a hash-based relabeling technique to optimize load balance in distributed systems. This technique involves reassigning node IDs to ensure that nodes with similar degrees are placed in the same interval, thereby reducing the skewness in the workload distribution.
9e55532c-6832-5e89-b181-e8bed07dc135|Dynamic Job Scheduling|The authors propose a dynamic job scheduling technique to optimize load balance in distributed systems. This technique involves dynamically scheduling jobs to processing elements (PEs) based on their availability, ensuring that each PE processes a similar number of jobs.
