<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description>graphscope blog</description>
    <link>https://graphscope.io/blog/</link>
    <atom:link href="https://graphscope.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 28 Apr 2025 07:05:28 +0000</pubDate>
    <lastBuildDate>Mon, 28 Apr 2025 07:05:28 +0000</lastBuildDate>
    <generator>Jekyll v4.4.1</generator>
    
      <item>
        <title>Why DuckDB Is Such A Good Database Product</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-10-title-picture.jpg&quot; alt=&quot;why-duckdb&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;tldr&quot;&gt;TL;DR&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Although databases are ancient, they still provide guidance for LLMs development.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DuckDB’s Success Formula&lt;/strong&gt;: Insight into trends + relentless focus on core competencies + extremely good product.&lt;/li&gt;
  &lt;li&gt;Everyone agrees on “big data,” but not everyone needs “big data.”&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://motherduck.com/blog/big-data-is-dead/&quot;&gt;“Big Data is Dead”&lt;/a&gt;, and large models are a better computational paradigm.&lt;/li&gt;
  &lt;li&gt;Be meticulous in initial technology selection, let data guide decisions, and avoid blind trust in authority.&lt;/li&gt;
  &lt;li&gt;Technology choices must always serve the product.&lt;/li&gt;
  &lt;li&gt;What appears “small and elegant” is actually the result of accumulated efforts in good products.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;It’s 2025, who still cares about databases? IT IS &lt;strong&gt;anyone who takes data seriously&lt;/strong&gt;. Since their inception in the 1970s, relational databases have remained the backbone of data management. Despite waves of NoSQL, NewSQL, and other trends, the dominance of traditional databases and SQL remains unshaken. At SIGMOD 2023, Don Chamberlin, co-creator of SQL, delivered a keynote titled &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3555041.3589336&quot;&gt;“49 Years of Queries”&lt;/a&gt;, reflecting on the evolution of relational databases and SQL over nearly half a century. Chamberlin emphasized that the longevity of database systems stems from E. F. Codd’s foundational theories, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Database_normalization&quot;&gt;Database Normalization&lt;/a&gt;&lt;/strong&gt;: Teaches structured data organization to eliminate redundancy.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Natural Language Queries&lt;/strong&gt;: A vision pursued since 1974 with Codd’s &lt;a href=&quot;https://dl.acm.org/doi/10.1145/1045283.1045298&quot;&gt;“Rendezvous”&lt;/a&gt; project, which led to SQL.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Relational_algebra&quot;&gt;Relational Algebra Closure&lt;/a&gt;&lt;/strong&gt;: Ensures uniform data formats for chained operations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fast forward 55 years: large models dominate the AI landscape, unstructured data processing explodes, yet database theory remains deeply influential. Normalization principles guide large language model (LLM) training, inspiring slogans like &lt;a href=&quot;https://arxiv.org/pdf/2307.09288&quot;&gt;“Data Quality is All You Need”&lt;/a&gt;. Natural language queries finally thrive with LLMs (though perhaps too enthusiastically). Developers of agent systems now recognize the importance of &lt;a href=&quot;https://medium.com/@kyeg/unlocking-structured-outputs-with-agents-8b5a564b5d44&quot;&gt;closure and standardization&lt;/a&gt; in ensuring stable, end-to-end execution.&lt;/p&gt;

&lt;p&gt;In the ever-shifting tech world, databases stand out as a rare bastion of stability—flashy trends come and go, but &lt;strong&gt;foundational principles endure&lt;/strong&gt;. For database products, “mastering the fundamentals” (correctness, reliability, efficiency) remains paramount. This doesn’t imply stagnation. Instead, strong foundations empower us to navigate trends confidently. Enter &lt;a href=&quot;https://duckdb.org/&quot;&gt;DuckDB&lt;/a&gt;, a database that marries trend awareness with technical depth. This article explores how DuckDB’s team built a competitive product by &lt;strong&gt;balancing innovation and foundation&lt;/strong&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;market-trends&quot;&gt;Market Trends&lt;/h1&gt;
&lt;p&gt;In 2023, MotherDuck founder Jordan Tigani’s blog &lt;a href=&quot;https://motherduck.com/blog/big-data-is-dead/&quot;&gt;“Big Data is Dead”&lt;/a&gt; sparked debate while crystallizing the rationale behind DuckDB’s embedded, lightweight, single-process design.&lt;/p&gt;

&lt;h2 id=&quot;debunking-big-data-mythsthree-truths&quot;&gt;Debunking “Big Data Myths”—Three Truths&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Most organizations don’t have “big” data&lt;/strong&gt;. Investor surveys reveal that even large B2B companies manage mere terabytes, with many operating on gigabytes. Internal data from SingleStore and others shows core datasets often fit in single-digit gigabytes. &lt;strong&gt;For most, data scale isn’t the bottleneck&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2025-04-27-duckdb/1745314131999-6d1a31f8-cce4-4d16-b464-5ba953aab6b7.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Source: &lt;a href=&quot;https://motherduck.com/blog/big-data-is-dead/&quot;&gt;Big Data is Dead&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Storage-compute imbalance&lt;/strong&gt;. Modern architectures decouple storage and compute, but storage grows linearly while compute demand lags. Most analytics focus on recent data, leaving historical datasets underutilized. This “storage obsession” incurs maintenance costs for rarely accessed data.&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2025-04-27-duckdb/1745314255837-38a43980-9230-431f-a704-c2f614d68426.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Source: &lt;a href=&quot;https://motherduck.com/blog/big-data-is-dead/&quot;&gt;Big Data is Dead&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Rapid data value decay&lt;/strong&gt;. Business data often loses relevance within weeks. Historical data serves audits or model training, not daily analytics. &lt;strong&gt;“Big data” is a game for the 1%&lt;/strong&gt;—most users operate within traditional single-machine capabilities.&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2025-04-27-duckdb/1745314228128-11b4dbc0-2b5d-48db-8f65-146e40570add.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Source: &lt;a href=&quot;https://motherduck.com/blog/big-data-is-dead/&quot;&gt;Big Data is Dead&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-scale-up-revolution&quot;&gt;The Scale-Up Revolution&lt;/h2&gt;
&lt;p&gt;The “big data” era emerged when scaling out (distributed clusters) was cheaper than scaling up (powerful single machines). Today, hardware advancements flip this equation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Memory/storage leaps&lt;/strong&gt;: DDR5 RAM hits 70GB/s bandwidth; NVMe SSDs approach DRAM latency.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unified architectures&lt;/strong&gt;: Apple’s M-series chips fuse CPU/GPU/NPU memory, slashing data movement overhead.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Result? Tasks once requiring Hadoop/Spark clusters now run efficiently on single machines. TPC-DS benchmarks show &lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU1NTg2ODQ5Nw==&amp;amp;mid=2247489796&amp;amp;idx=1&amp;amp;sn=f12a900681ff4d8ba5eb4aa2b38fc4db&amp;amp;chksm=fa8886daf63e08db5550fe6b4b5a46f42dfdf5fea6700cccb2d843844ce258bde47f4aa4904e#rd&quot;&gt;DuckDB outperforming Spark by 3-8x on 100GB datasets with 10x energy efficiency&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;large-models-vs-big-data&quot;&gt;Large Models vs. Big Data&lt;/h2&gt;
&lt;p&gt;Another significant reason for the decline of big data analytics lies in the gradual transition from “big data” to “large models.” As Large Language Models (LLMs) become the new technological cornerstone, the importance of traditional “big data” stacks is being re-evaluated. Large pre-trained models essentially act as &lt;a href=&quot;https://arxiv.org/pdf/2309.10668&quot;&gt;&lt;strong&gt;compression and distillation&lt;/strong&gt; of massive datasets&lt;/a&gt;. Insights that previously required labor-intensive analysis of terabytes of data are now embedded within these models. When people increasingly obtain information by querying models like GPT-4 instead of executing complex queries on logs or data warehouses, the underlying data processing and computational paradigms inevitably simplify.&lt;/p&gt;

&lt;p&gt;In this landscape, embedded databases like DuckDB thrive. Projects like DeepSeek’s &lt;a href=&quot;https://github.com/deepseek-ai/smallpond&quot;&gt;smallpond&lt;/a&gt; leverage DuckDB for lightweight, high-performance data processing.&lt;/p&gt;

&lt;h2 id=&quot;duckdbs-ascent&quot;&gt;DuckDB’s Ascent&lt;/h2&gt;
&lt;p&gt;Born in defiance of the ‘big data’ hype, DuckDB started with a simple insight: nobody was building an embedded analytical database. The chart below shows exactly what the creators saw missing:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2025-04-27-duckdb/1745315954205-ef8dd17e-184b-4314-8d82-cde9a774a1eb.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Source：https://dl.acm.org/doi/10.1145/3299869.3320212)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Born in 2018, gaining explosive traction by 2022, DuckDB has by 2025 positioned itself as the de facto leader in the analytical processing database arena - an unprecedented rise for an embedded database:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2025-04-27-duckdb/1745316024111-fd54646d-162c-40d2-8c13-90c017f519a4.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Source: &lt;a href=&quot;https://ossinsight.io/analyze/duckdb/duckdb#overview&quot;&gt;OSS Insight&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;product-excellence&quot;&gt;Product Excellence&lt;/h1&gt;
&lt;p&gt;DuckDB’s success isn’t just about riding the right trends - its killer product features deserve equal credit. While we could write volumes about its capabilities, let‘s highlight the most impressive ones.&lt;/p&gt;

&lt;h2 id=&quot;zero-copy-data-access&quot;&gt;Zero-Copy Data Access&lt;/h2&gt;
&lt;p&gt;DuckDB queries external files (CSV, Parquet) without importing data. Its vectorized engine and columnar format enable efficient scans, even with partial HTTP Range requests. For example:&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;data.parquet&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Queries run directly on Pandas DataFrames or Arrow data, eliminating data duplication. This “Swiss Army knife” versatility aligns perfectly with data scientists’ workflows.&lt;/p&gt;

&lt;p&gt;Queries run directly on Pandas DataFrames or Arrow data, eliminating data duplication. &lt;strong&gt;This “Swiss Army knife” versatility aligns perfectly with data scientists’ workflows&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;httpfs-extension&quot;&gt;HTTPFS Extension&lt;/h2&gt;
&lt;p&gt;Drawing inspiration from PostgreSQL’s design, DuckDB was architected with extensibility as a first-class citizen from day one. Among its extensions, ​HTTPFS stands out as a stroke of genius. This extension enables direct access to HTTP(S) endpoints and cloud object storage (like S3) - meaning DuckDB can query remote data as if it were local tables, using nothing more than a URL.&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;
&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;https://example.com/dataset/file.parquet&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;DuckDB automatically fetches only the required file segments via HTTP. For columnar formats like Parquet, it goes further by selectively retrieving just the needed columns, dramatically improving efficiency.&lt;/p&gt;

&lt;p&gt;This capability effectively complements the “big data” paradigm: users maintain large datasets in the cloud while pulling only relevant subsets for local analysis, even joining them with existing local datasets.&lt;/p&gt;

&lt;p&gt;In this quintessential &lt;strong&gt;“edge-cloud integration” scenario&lt;/strong&gt;, DuckDB delivers ad-hoc cloud data analysis &lt;strong&gt;without complex middleware&lt;/strong&gt; – streamlining data engineering workflows. Wait, isn’t this exactly how “datalakes” were supposed to work all along?&lt;/p&gt;

&lt;h2 id=&quot;the-ingenious-htap-strategy-of-seamlessly-integrating-oltpolap&quot;&gt;The Ingenious HTAP Strategy of Seamlessly Integrating OLTP/OLAP&lt;/h2&gt;

&lt;h3 id=&quot;the-pitfalls-of-htap&quot;&gt;The “Pitfalls” of HTAP&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Hybrid_transactional/analytical_processing&quot;&gt;HTAP, or Hybrid Transactional/Analytical Processing&lt;/a&gt;, aims to provide both TP and AP capabilities within a single database. This presents an opportunity for both TP-oriented and AP-oriented databases to expand their business scenarios, making HTAP a fiercely contested battleground in the database market. But is HTAP really that great? Whether extending from TP to HTAP or vice versa, one risks falling into the seemingly sweet “trap” of HTAP:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Storage-Compute Dissonance&lt;/strong&gt;: OLTP relies on row-based storage (Row Store) for high-concurrency transactions, while OLAP requires column-based storage (Column Store) to accelerate aggregate computations. Forced integration necessitates maintaining two storage engines (e.g., row-store replicas + column-store replicas), leading to skyrocketing data synchronization overhead and consistency maintenance costs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource Contention and Performance Degradation&lt;/strong&gt;: OLTP’s short transactions (millisecond-level responses) compete with OLAP’s long queries (minute-level computations) for CPU, memory, and I/O resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Technological Conflict&lt;/strong&gt;: OLTP emphasizes ACID transactions and row-level locks, whereas OLAP only requires relaxed eventual consistency. For instance, full-table scans in analytical queries may trigger row-store lock contention, causing transactional operations to stall.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;why-not-leverage-the-big-players&quot;&gt;Why Not Leverage the “Big Players”?&lt;/h3&gt;
&lt;p&gt;DuckDB’s philosophy is: Instead of forcing a single system to handle both, let systems excelling in transactions and those specializing in analysis each play to their strengths, then bridge them. Thus, PostgreSQL and DuckDB—the two most scalable systems in the TP and AP domains—naturally converge.&lt;/p&gt;

&lt;p&gt;On one hand, DuckDB offers the Postgres Scanner extension, which can directly connect to a PostgreSQL database and map its tables as virtual views within DuckDB for querying. This allows DuckDB to act as an analytical accelerator, reading PostgreSQL’s live data without additional replication. This approach avoids the hassles of dual writes and asynchronous synchronization. During queries, data stored in PostgreSQL is efficiently read via its binary protocol, while analytical logic is executed within DuckDB.&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;postgres_scan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;dbname=myshinydb&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;public&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;mytable&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On the other hand, the PostgreSQL ecosystem has seen a wave of extensions enhancing DuckDB’s AP capabilities, including the official pg_duckdb by DuckDB’s team.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2025-04-27-duckdb/1745323859852-27f3b8fe-8447-4247-9430-4a26ab8d3d7a.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://github.com/duckdb/pg_duckdb&quot;&gt;pg_duckdb&lt;/a&gt;’s implementation, DuckDB aligns with PostgreSQL’s logical timestamp synchronization (Logical Timestamp Alignment), ensuring analytical queries are based on the latest consistent snapshot of the transactional database, avoiding dirty reads and phantom reads. Additionally, DuckDB leverages its Postgres Scanner to directly access PostgreSQL’s row-store tables, then uses its vectorized engine to execute OLAP queries, boosting AP performance without data replication. For example, in TPC-H benchmarks, third-party tests showed pg_duckdb achieving over &lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzIzOTA2NjEzNQ==&amp;amp;mid=2454788670&amp;amp;idx=1&amp;amp;sn=24a3d2f17b8ad32dd7baed15598c4e28&amp;amp;chksm=ff377ebf7a2bfdf64db0980eddcdea4d586ee40f22ec68b865a5d3c464fcd7e745621b811583#rd&quot;&gt;1000x performance gains&lt;/a&gt; in complex analytical queries compared to native PostgreSQL.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PostgreSQL and DuckDB’s extensibility decouples the technical contradictions of OLTP and OLAP: PostgreSQL focuses on OLTP (e.g., order processing), DuckDB specializes in OLAP (e.g., real-time reporting), and the two achieve logical unity through in-process federated queries (rather than data copying).&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;graphscopes-attempts&quot;&gt;GraphScope’s Attempts&lt;/h3&gt;
&lt;p&gt;At this point, I can’t help but reflect on GraphScope’s attempt to extend graph analysis capabilities based on OLTP databases – the &lt;a href=&quot;https://github.com/GraphScope/GART&quot;&gt;GART project&lt;/a&gt; (unfortunately, GART’s latest commits are already six months old).&lt;/p&gt;

&lt;p&gt;GART’s original goal was to make OLTP databases the data source for graph analysis:&lt;/p&gt;

&lt;p&gt;OLTP databases offer mature data management solutions. In most enterprise applications, cleaned data first lands in OLTP databases (e.g., Ant Group’s OceanBase). Let GraphScope focus on graph analysis while leaving complex data management to mature OLTP databases. This vision aligns perfectly with DuckDB + PostgreSQL = HTAP. However, during GART’s development, GraphScope attempted to integrate graph analysis with transactional processing via real-time graph construction (Binlog parsing), but faced challenges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Overly Heavy Architecture&lt;/strong&gt;: Required maintaining an independent graph storage and compute cluster, with overly long synchronization chains (introducing Kafka + GraphEngine).&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Limited Scenarios&lt;/strong&gt;: Strong reliance on external data warehouses made lightweight embedded graph analysis difficult. What if we adopted DuckDB and PostgreSQL’s design philosophy? Here’s a potential approach:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Native Graph Operators&lt;/strong&gt;: Integrate libgrape-lite (or a lighter single-machine parallel version) via PostgreSQL’s extension interface to enable graph traversal, community detection, and other operations within the transactional database, avoiding cross-system data migration.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Federated Graph Computing&lt;/strong&gt;: Combine DuckDB’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;read_parquet&lt;/code&gt; function to directly query graph data (via &lt;a href=&quot;https://graphar.apache.org/&quot;&gt;GraphAr&lt;/a&gt;) from cloud storage.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;product-oriented-extreme-performance&quot;&gt;Product-Oriented Extreme Performance&lt;/h1&gt;

&lt;p&gt;The previous chapters showcased some of DuckDB’s impressive product capabilities, but behind them all lies one defining characteristic: &lt;strong&gt;speed&lt;/strong&gt; – faster than large-scale Spark clusters, faster than PostgreSQL in OLTP scenarios. This performance stems from deliberate system design and architectural choices, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Columnar Storage&lt;/strong&gt;: In analytical scenarios, computations typically only require a subset of columns. Traditional row storage would result in mostly inefficient scans, whereas columnar storage minimizes I/O by reading only relevant data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vectorized (Batch) Processing&lt;/strong&gt;: Operators process data in batches rather than row-by-row. This reduces function call overhead, improves cache utilization, and leverages modern hardware’s SIMD (Single Instruction Multiple Data) parallelism.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Zero-Copy Operations&lt;/strong&gt;: DuckDB meticulously avoids memory copies during data transformations. For example, when &lt;a href=&quot;https://duckdb.org/2021/12/03/duck-arrow.html&quot;&gt;reading Arrow data&lt;/a&gt;, DuckDB directly operates on Arrow memory buffers, and similarly outputs results in Arrow format. This enables seamless parsing of Arrow data within the query engine and direct Arrow-formatted result exports.&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While these topics are widely discussed, let’s delve deeper into some of the early design choices highlighted in DuckDB’s research papers.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;computational-models&quot;&gt;Computational Models&lt;/h2&gt;

&lt;p&gt;During DuckDB’s development, two dominant computational models existed in the database field — &lt;strong&gt;Vectorized Execution&lt;/strong&gt; and &lt;strong&gt;Data-Centric Code Generation (Compiled Execution)&lt;/strong&gt; — with significant differences in architecture, performance characteristics, and use cases. Both models are backed by numerous well-known systems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Vectorized Execution&lt;/strong&gt;:
&lt;a href=&quot;https://ir.cwi.nl/pub/19958/19958B.pdf&quot;&gt;VectorWise&lt;/a&gt;, &lt;a href=&quot;https://www.redbooks.ibm.com/abstracts/tips1204.html&quot;&gt;DB2 BLU&lt;/a&gt;, &lt;a href=&quot;https://learn.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview?view=sql-server-ver16&quot;&gt;Columnar SQL Server&lt;/a&gt;, &lt;a href=&quot;https://github.com/UWQuickstep/quickstep&quot;&gt;Quickstep&lt;/a&gt;, etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Compiled Execution&lt;/strong&gt;:
&lt;a href=&quot;https://tableau.github.io/hyper-db/&quot;&gt;HyPer&lt;/a&gt;, &lt;a href=&quot;https://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt;, &lt;a href=&quot;https://db.cs.cmu.edu/peloton/&quot;&gt;Peloton&lt;/a&gt;, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, comparing these models fairly is challenging. Directly contrasting real-world systems (e.g., HyPer vs. VectorWise) can lead to skewed conclusions due to differences in storage formats, parallelization strategies, etc. The DuckDB team adopted a &lt;strong&gt;unified experimental framework&lt;/strong&gt; for an apples-to-apples comparison:&lt;/p&gt;

&lt;h3 id=&quot;methodology&quot;&gt;Methodology&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Unified Framework&lt;/strong&gt;: Implemented both models in the same system — &lt;em&gt;Typer&lt;/em&gt; (compiled execution) and &lt;em&gt;Tectorwise&lt;/em&gt; (vectorized execution) — ensuring identical algorithms, data structures, and parallel frameworks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Isolated Variables&lt;/strong&gt;: Only the execution engine was altered, eliminating interference from storage compression or optimizer variations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cross-Scenario Validation&lt;/strong&gt;: Tested across TPC-H/SSB queries to analyze performance in compute-intensive, memory-bandwidth-sensitive, and other scenarios.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below is their comparative analysis from this &lt;a href=&quot;https://www.vldb.org/pvldb/vol11/p2209-kersten.pdf&quot;&gt;benchmark paper&lt;/a&gt;:&lt;/p&gt;

&lt;style&gt;
  .comparison-table {
    width: 100%;
    border-collapse: collapse;
    font-family: Arial, sans-serif;
    margin: 20px 0;
    box-shadow: 0 2px 3px rgba(0,0,0,0.1);
  }
  .comparison-table th {
    background-color: #f2f2f2;
    padding: 12px 15px;
    text-align: left;
    font-weight: bold;
    border-bottom: 2px solid #ddd;
  }
  .comparison-table td {
    padding: 10px 15px;
    border-bottom: 1px solid #ddd;
    vertical-align: top;
    min-width:150px;
  }
  .comparison-table tr:nth-child(even) {
    background-color: #f9f9f9;
  }
  .comparison-table tr:hover {
    background-color: #f1f1f1;
  }
  .comparison-table .dimension {
    font-weight: bold;
    color: #2c3e50;
  }
  .comparison-table .highlight {
    background-color: #e8f4f8;
  }
  @media screen and (max-width: 768px) {
    .comparison-table {
      display: block;
      overflow-x: auto;
    }
  }
&lt;/style&gt;

&lt;table class=&quot;comparison-table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Comparison Dimension&lt;/th&gt;
      &lt;th&gt;Vectorized&lt;/th&gt;
      &lt;th&gt;Codegen (Compiled)&lt;/th&gt;
      &lt;th&gt;Why DuckDB Chose Vectorized&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td colspan=&quot;4&quot; class=&quot;dimension&quot;&gt;Performance Traits&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compute-Intensive Workloads&lt;/td&gt;
      &lt;td&gt;Materializes intermediates; more instructions&lt;/td&gt;
      &lt;td&gt;Register-optimized; fewer instructions&lt;/td&gt;
      &lt;td class=&quot;highlight&quot;&gt;&lt;strong&gt;Memory access optimization&lt;/strong&gt;: OLAP workloads (e.g., hash joins/aggregations) benefit from lower memory stall cycles.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Memory-Bound Scenarios&lt;/td&gt;
      &lt;td&gt;Batch processing hides cache misses; optimized parallel memory access&lt;/td&gt;
      &lt;td&gt;Complex loops limit prefetching/out-of-order execution&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td colspan=&quot;4&quot; class=&quot;dimension&quot;&gt;Engineering Practicality&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compilation Overhead&lt;/td&gt;
      &lt;td&gt;No runtime compilation; primitives pre-compiled&lt;/td&gt;
      &lt;td&gt;LLVM dynamic compilation adds latency (seconds for complex queries)&lt;/td&gt;
      &lt;td class=&quot;highlight&quot;&gt;&lt;strong&gt;Embedded-friendly&lt;/strong&gt;: No wait for JIT compilation; reduces first-run latency.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Debugging &amp;amp; Profiling&lt;/td&gt;
      &lt;td&gt;Isolated performance stats per primitive (e.g., selection/hash functions)&lt;/td&gt;
      &lt;td&gt;Fused operators in generated code obscure performance attribution&lt;/td&gt;
      &lt;td class=&quot;highlight&quot;&gt;&lt;strong&gt;Developer-friendly&lt;/strong&gt;: Easier for contributors to pinpoint optimizations.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td colspan=&quot;4&quot; class=&quot;dimension&quot;&gt;Hardware Compatibility&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SIMD Optimization&lt;/td&gt;
      &lt;td&gt;Simple primitives enable manual vectorization (e.g., 8.4x speedup with AVX-512)&lt;/td&gt;
      &lt;td&gt;Complex loops rely on compiler auto-vectorization (limited to ICC)&lt;/td&gt;
      &lt;td class=&quot;highlight&quot;&gt;&lt;strong&gt;Future-proof&lt;/strong&gt;: Higher potential for SIMD utilization gains.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Multi-Core Parallelism&lt;/td&gt;
      &lt;td&gt;Balanced batch splitting in Morsel-Driven parallelism&lt;/td&gt;
      &lt;td&gt;Same framework, but complex code reduces HT efficiency (e.g., low gains on AMD)&lt;/td&gt;
      &lt;td class=&quot;highlight&quot;&gt;&lt;strong&gt;Thread scalability&lt;/strong&gt;: More balanced workload distribution.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td colspan=&quot;4&quot; class=&quot;dimension&quot;&gt;Functional Extensibility&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;OLTP Support&lt;/td&gt;
      &lt;td&gt;Inefficient for high-frequency single-row ops&lt;/td&gt;
      &lt;td&gt;Compiled stored procedures excel (ideal for HTAP)&lt;/td&gt;
      &lt;td class=&quot;highlight&quot;&gt;&lt;strong&gt;OLAP focus&lt;/strong&gt;: DuckDB targets analytics, not deep OLTP optimization.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Dynamic Optimization&lt;/td&gt;
      &lt;td&gt;Supports runtime adaptation (e.g., dynamic aggregation)&lt;/td&gt;
      &lt;td&gt;Logic hardcoded in generated code; requires recompilation&lt;/td&gt;
      &lt;td class=&quot;highlight&quot;&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;: Adapts to dynamic workloads.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td colspan=&quot;4&quot; class=&quot;dimension&quot;&gt;Storage Synergy&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Storage Synergy&lt;/td&gt;
      &lt;td&gt;Native fit for columnar formats (e.g., RLE/Dictionary)&lt;/td&gt;
      &lt;td&gt;Requires type conversion/decompression (added overhead)&lt;/td&gt;
      &lt;td class=&quot;highlight&quot;&gt;&lt;strong&gt;Columnar ecosystem&lt;/strong&gt;: Reduces intermediate data materialization.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Thus, DuckDB’s choice of vectorized execution was based on a &lt;strong&gt;systematic, comprehensive, textbook-grade evaluation&lt;/strong&gt; — even though &lt;a href=&quot;https://homepages.cwi.nl/~boncz/&quot;&gt;Peter Boncz&lt;/a&gt;, a pioneer of vectorized execution, was an early core advisor to DuckDB.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;high-efficiency-mvcc-implementation&quot;&gt;High-Efficiency MVCC Implementation&lt;/h2&gt;

&lt;p&gt;As an embedded database designed for analytical workloads, DuckDB’s concurrency control mechanism fundamentally differs from traditional OLTP databases. To address batch updates, columnar operations, high compression storage, and embedded lightweight requirements in analytical scenarios, DuckDB deeply optimized its &lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2019/papers/04-mvcc2/p677-neumann.pdf&quot;&gt;MVCC implementation strategy&lt;/a&gt; developed by the one and only &lt;a href=&quot;https://en.wikipedia.org/wiki/Thomas_Neumann&quot;&gt;Thomas Neumann&lt;/a&gt;, primarily solving:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Massive version management overhead&lt;/strong&gt;: Traditional row-level MVCC generates excessive redundant version data during batch updates&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Columnar compression vs. update conflicts&lt;/strong&gt;: Compressed column data cannot be updated in-place, requiring avoidance of frequent decompression/recompression&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource constraints in embedded scenarios&lt;/strong&gt;: Ensuring transaction efficiency under memory/storage limitations&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Analytical transaction characteristics&lt;/strong&gt;: Coexistence needs of long-running batch operations and high-concurrency queries&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below summarizes DuckDB’s innovative MVCC designs:&lt;/p&gt;

&lt;h3 id=&quot;optimizations-for-analytical-workloads&quot;&gt;&lt;strong&gt;Optimizations for Analytical Workloads&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Columnar Batch Granularity&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Row-level versions create massive Undo records during batch updates, consuming memory and incurring high scan costs&lt;/li&gt;
      &lt;li&gt;DuckDB uses “per 2048-row × per-column version metadata” to record entire batch modifications at once&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Storing Deltas Instead of Old Values&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Compressed pages cannot be modified in-place; Undo Buffer only records “change descriptions” while keeping old versions in original compressed blocks, minimizing write amplification&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;​&lt;strong&gt;Write Amplification-Minimized WAL/Checkpoint&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Bulk inserts (e.g., COPY 10GB CSV) first write directly to new data blocks, then log “block references” in WAL; commits atomically replace blocks, while rollbacks mark blocks as recyclable (avoiding “double writes”)&lt;/li&gt;
      &lt;li&gt;Default 16MB WAL threshold triggers auto-checkpoint (manual &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CHECKPOINT&lt;/code&gt; supported); &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsync&lt;/code&gt; enforced pre-write for durability&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lightweight--embedded-design&quot;&gt;&lt;strong&gt;Lightweight &amp;amp; Embedded Design&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Minimal Metadata&lt;/strong&gt;: 2048-row batch versions + columnar deltas ensure “zero cost when unmodified,” with memory usage scaling linearly only with modifications&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fast Startup&lt;/strong&gt;: Undo Buffer stored at data page tail; recovery only needs incremental WAL parsing. DuckDB survived 4000 TPCH refresh cycles in &lt;a href=&quot;https://github.com/dsrhaslab/lazyfs&quot;&gt;LazyFS&lt;/a&gt; power-failure/kill-process tests with zero data loss&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Compression-Friendly&lt;/strong&gt;: Version info is separated from data, keeping original column blocks compressed (no MVCC-triggered decompression/rewrites)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Single-Machine Concurrency&lt;/strong&gt;: Read transactions never block; conflicting batch writes auto-retry without lock contention&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This design enables DuckDB to deliver ​&lt;strong&gt;near-zero-cost concurrent transactions&lt;/strong&gt; and ​&lt;strong&gt;analytical-grade throughput&lt;/strong&gt; in single-machine memory environments, meeting modern data science and local embedded applications’ demands for “lightweight, compressed, ready-to-use” performance.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;compilation-optimization-focused-efficiency-where-it-matters-most&quot;&gt;Compilation Optimization: Focused Efficiency Where It Matters Most&lt;/h2&gt;

&lt;p&gt;Traditional database systems rely on “heavyweight” optimizers, while DuckDB implements lightweight yet &lt;strong&gt;most&lt;/strong&gt;-practical optimization rules specifically tailored for embedded scenarios. For comparison, we examined the compiled file sizes of DuckDB versus two well-known optimization frameworks in the industry: &lt;a href=&quot;https://github.com/apache/calcite&quot;&gt;Calcite&lt;/a&gt; and &lt;a href=&quot;https://github.com/greenplum-db/gporca-archive&quot;&gt;ORCA&lt;/a&gt;:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2025-04-27-duckdb/1745734878763-11ecef7d-9eb8-4620-9005-13fe7dcdbb5a.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Clearly, these optimization frameworks are overly bulky for DuckDB’s needs. Although DuckDB’s engineers and researchers fully understand &lt;a href=&quot;https://duckdb.org/2024/11/14/optimizers.html&quot;&gt;the importance of query optimization for data systems, especially in analytical scenarios&lt;/a&gt;, they didn’t blindly pursue optimization perfection in their implementation. Instead, they strategically allocated their limited resources to the most critical areas, aligning with their vision of a “lightweight embedded analytical engine.” They achieved an elegant balance between optimization capability and resource consumption through multi-layered architectural innovations. Here are two particularly intentional design choices:&lt;/p&gt;

&lt;h3 id=&quot;rule-first-heuristic-optimization-framework&quot;&gt;&lt;strong&gt;Rule-First Heuristic Optimization Framework&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;DuckDB employs a two-stage heuristic optimization architecture:
	- &lt;strong&gt;First Stage&lt;/strong&gt;: Applies deterministic rewrite rules (e.g., predicate pushdown, expression simplification) based on logical equivalence, ensuring execution plan safety with minimal overhead (O(1) time complexity).
 	- &lt;strong&gt;Second Stage&lt;/strong&gt;: Performs lightweight statistics-based cost optimization, focusing only on critical paths (e.g., join ordering) with limited search scope. Dynamic pruning strategies keep optimization time within milliseconds.&lt;/p&gt;

&lt;h3 id=&quot;statistics-independent-cardinality-estimation-model&quot;&gt;&lt;strong&gt;Statistics-Independent Cardinality Estimation Model&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;DuckDB addresses the challenge of computing/missing statistics in embedded scenarios (e.g., when querying Parquet/CSV files directly where histograms can’t be precomputed) by introducing:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Foreign Key Inference Engine&lt;/strong&gt;: Automatically detects implied PK-FK relationships in schemas to build dynamic equivalence sets.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Selectivity Propagation Model&lt;/strong&gt;: Enables multi-table join cardinality estimation via chain calculations like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;card(A⋈B) = card(A) * card(B) / tdom&lt;/code&gt;, achieving over 90% accuracy without stored statistics.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;DuckDB didn’t just ride the “big data backlash” —- it weaponized it using a good product. By combining:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;​&lt;strong&gt;Vectorized Execution&lt;/strong&gt; (2-8x faster than Spark on 100GB datasets)&lt;/li&gt;
  &lt;li&gt;​&lt;strong&gt;Zero-Copy Workflows&lt;/strong&gt; (Arrow/Parquet as first-class citizens)&lt;/li&gt;
  &lt;li&gt;​&lt;strong&gt;PostgreSQL Symbiosis&lt;/strong&gt; (1000x+ OLAP speedups via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;postgres_scan()&lt;/code&gt;)
…it redefined what’s possible in single-node analytics, achieving 90% of “big data” outcomes with 10% of the complexity.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DuckDB’s success signals a paradigm shift:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Era&lt;/th&gt;
      &lt;th&gt;Dogma&lt;/th&gt;
      &lt;th&gt;DuckDB’s Answer&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;​&lt;strong&gt;2000s&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;“Normalize all data”&lt;/td&gt;
      &lt;td&gt;“Query raw Parquet”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;​&lt;strong&gt;2010s&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;“Scale out or die”&lt;/td&gt;
      &lt;td&gt;“Scale up smarter”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;​&lt;strong&gt;2020s&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;“ETL everything”&lt;/td&gt;
      &lt;td&gt;“Embed everywhere”&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As LLMs democratize data access, DuckDB’s ​&lt;strong&gt;“small-data engine for a small-data world”&lt;/strong&gt; approach becomes prescient. The future belongs not to systems that demand data centralization, but to those that ​&lt;strong&gt;dissolve into the workflow&lt;/strong&gt; – invisible until needed, indispensable when used.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3555041.3589336&quot;&gt;49 Years of Query&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html&quot;&gt;Databases in 2024: A Year in Review&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://motherduck.com/blog/big-data-is-dead/&quot;&gt;Big data is dead&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.10668&quot;&gt;Language Modeling Is Compression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.vldb.org/pvldb/vol11/p2209-kersten.pdf&quot;&gt;Everything You Always Wanted to Know About Compiled and Vectorized Queries But Were Afraid to Ask&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://duckdb.org/2024/10/30/analytics-optimized-concurrent-transactions.html&quot;&gt;Analytics-Optimized Concurrent Transactions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://homepages.cwi.nl/~boncz/msc/2022-TomEbergen.pdf&quot;&gt;Join Order Optimization with (Almost) No Statistics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://duckdb.org/2024/11/14/optimizers.html#expression-rewriter&quot;&gt;Optimizers: The Low-Key MVP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
        <link>https://graphscope.io/blog/tech/2025/04/26/Why-DuckDB-Is-Such-A-Good-Database-Product.html</link>
        <guid isPermaLink="true">https://graphscope.io/blog/tech/2025/04/26/Why-DuckDB-Is-Such-A-Good-Database-Product.html</guid>
        
        
        <category>Tech</category>
        
      </item>
    
      <item>
        <title>GOpt Optimization Process: An In-depth Case Study Based on LDBC SNB Queries</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2024-02-22-title-picture.jpg&quot; alt=&quot;gopt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In today’s data analysis landscape, the optimization of complex graph queries has been a persistent challenge for the industry. GOpt is an optimizer framework designed for complex graph queries and was officially presented at &lt;a href=&quot;https://arxiv.org/abs/2401.17786&quot;&gt;SIGMOD Industry 2025&lt;/a&gt;. Its core objective is to provide an efficient and universal optimization solution that adapts to various graph query languages and execution engines. In this article, we will delve into the key innovations of GOpt in unified graph query optimization and analyze its optimization process through specific query cases from &lt;a href=&quot;https://ldbcouncil.org/benchmarks/snb&quot;&gt;LDBC SNB&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;GOpt achieves breakthroughs in the following areas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Seamless support for mainstream graph query languages: GOpt is compatible with current leading graph query languages, including &lt;a href=&quot;https://neo4j.com/docs/cypher-manual/current/introduction&quot;&gt;Cypher&lt;/a&gt; and &lt;a href=&quot;https://tinkerpop.apache.org&quot;&gt;Gremlin&lt;/a&gt;, with future plans to further support the graph query standard &lt;a href=&quot;https://www.gqlstandards.org&quot;&gt;GQL&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unified optimization of complex patterns: Deep optimization of various complex patterns such as Triangle, Path, Square, Clique, etc. Figure Fig.1 illustrates the various complex pattern forms supported by GOpt.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/complex_patterns.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Fig.1 Complex Patterns Supported by GOpt.&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Support for multi-pattern features in Cypher queries: GOpt first represents multi-patterns according to Cypher semantics as Inner/Left/Anti Joins, converting optimizations between multiple patterns into optimizations for Joins, thus directly reusing mature Join optimization rules from relational databases. Fig.2 showcases the various multi-pattern queries supported by GOpt and their internal representations within GOpt.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/multi_patterns.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Fig.2 Multi-Patterns Supported by GOpt.&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Integration with popular graph query execution engines: In a previous article &lt;a href=&quot;https://mp.weixin.qq.com/s/07w8YaH0VmhgJXJkDN0KUg&quot;&gt;Revolution: Enhancing Neo4j Efficiency with GOpt&lt;/a&gt;, we integrated GOpt into the Neo4j engine. Figure Fig.3 compares the execution effects of Neo4j and the GOpt optimization plans on the Neo4j engine. Within the &lt;a href=&quot;https://ldbcouncil.org/benchmarks/snb&quot;&gt;LDBC Social Network Benchmark (LDBC SNB)&lt;/a&gt; standard test suite (where IC and BI represent Interactive-Complex and Business Intelligence queries respectively), GOpt brings an average improvement of 15.8X for Neo4j. We have further integrated GOpt into the GraphScope engine, achieving an average query performance improvement of 243.4X compared to GraphScope’s built-in rule-based optimizer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/neo4_vs_gopt.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Fig.3 Time Cost of LDBC Queries on Neo4j.&lt;/div&gt;

&lt;p&gt;In this article, we will further explore points 1, 2, and 3, delving deeply into the optimization process of GOpt. We have chosen standard queries from LDBC SNB as a case study to intuitively demonstrate GOpt’s optimization process. Regarding point 4, GOpt has designed a dedicated Physical Converter layer. However, due to the excessive focus on underlying implementations, we will release subsequent articles on our official account to further decrypt this part, mainly covering: “How to Integrate GOpt with Neo4j?” and “How to Integrate GOpt with DuckDB?”. Stay tuned.&lt;/p&gt;

&lt;h2 id=&quot;optimization-process&quot;&gt;Optimization Process&lt;/h2&gt;

&lt;p&gt;Fig.4 illustrates the system framework of GOpt, which mainly consists of the following three layers:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Query Parser&lt;/strong&gt;: Cypher/Gremlin queries first undergo syntax checking via &lt;a href=&quot;https://www.antlr.org&quot;&gt;Antlr&lt;/a&gt;, followed by conversion of the Antlr AST into an initial GIR structure through the GIRBuilder Tool.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;GIR Optimizer&lt;/strong&gt;: Applies optimizations based on the GIR structure, performing transform operations on the input GIR structure and outputting an optimized GIR structure. Depending on the implementation of transforms, optimizations can be executed based on heuristic or top-down search approaches.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Physical Convertor&lt;/strong&gt;: Further converts the physical execution plan into code that can be executed by backend engines.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/gopt_overview.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Fig.4 System Overview of GOpt.&lt;/div&gt;

&lt;h3 id=&quot;gir-structure&quot;&gt;GIR Structure&lt;/h3&gt;

&lt;p&gt;Firstly, let’s introduce what the GIR structure is. GIR (Graph Intermediate Representation) is a graph query language-independent intermediate data structure for graph queries, encompassing definitions of data models and a series of operator combinations for manipulating these data models.&lt;/p&gt;

&lt;p&gt;The data model in GIR consists of graph data types and basic data types. Graph data types include Vertex (nodes), Edge (relationships), Path (paths), etc. Basic data types refer to Integer (integer), Float (floating point), String (character), Array/Map/Set (composite types), etc. GIR adopts data formats from relational databases: each data tuple contains multiple items, each with a name and a value, where the value type can be either a graph data type or a basic data type.&lt;/p&gt;

&lt;p&gt;Operator combinations in GIR include graph operators and relational operators.&lt;/p&gt;

&lt;p&gt;Graph operators define how queries retrieve graph data, including:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;GET_VERTEX: Represents fetching node data sources from a graph database or retrieving endpoints from input edge data.&lt;/li&gt;
  &lt;li&gt;EXPAND_EDGE: Represents fetching edge data sources from a graph database or expanding adjacent edges from input node data.&lt;/li&gt;
  &lt;li&gt;EXPAND_PATH: Represents expanding paths composed of multiple edges from input node data.&lt;/li&gt;
  &lt;li&gt;MATCH_PATTERN: Expresses the Match Clause in Cypher or Gremlin as a unified structure. In GIR, there are two ways to represent this: a. A composite structure consisting of the aforementioned graph operators, marked by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MATCH_START&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MATCH_END&lt;/code&gt; indicating the start and end of graph operators. b. Represented as a Graph structure using &lt;a href=&quot;https://jgrapht.org&quot;&gt;JGraphT&lt;/a&gt;. These two structures are equivalent and can be converted interchangeably, as shown in Fig.5(c), where the Match Clause in the left-side query is represented uniformly in both forms.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Relational operators include Filter, Sort, Limit, Unfold, Project, Group, Join, Union. The support for them in GOpt is consistent with traditional databases, hence we will not elaborate further here. As illustrated in Fig.5(c), both Cypher query Fig.5(a) and Gremlin query Fig.5(b) are uniformly represented as corresponding GIR structures.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/GIR.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Fig.5 GIR Representation of Query Example.&lt;/div&gt;

&lt;h2 id=&quot;optimization-strategies&quot;&gt;Optimization Strategies&lt;/h2&gt;

&lt;p&gt;Next, we introduce how GOpt optimizes queries based on the GIR structure. The entire optimization process in the GIR Optimizer can be represented as a Directed Acyclic Graph (DAG). Each node in the DAG represents a Strategy, which embodies the execution of one or more rules. The edges in the DAG represent the sequential order of rule execution. GOpt executes these rules according to the topological sort of the DAG graph. Figure 6 shows a series of Strategies implemented by GOpt and their corresponding DAG relationships.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/DAG.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Figure 6: DAG of Optimization Process in GOpt.&lt;/div&gt;

&lt;p&gt;The interface definition for Strategy is as follows:&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;interface&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Strategy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nc&quot;&gt;GIRPlan&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;GIRPlan&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Based on this interface, GOpt provides two types of Strategy implementations: RuleBasedStrategy and PatternStrategy.&lt;/p&gt;

&lt;p&gt;RuleBasedStrategy includes a series of heuristic rules, where each rule specifically implements the transform function to perform equivalent transformations on the input GIRPlan and output the transformed GIRPlan. These rules are partly reused from Calcite, including: FieldTrim, SortProjectTrans, AggJoinTrans. Additionally, to handle specific optimizations related to graph data and operations, GOpt has implemented specialized rules for graph data models, including: FilterIntoPattern, JoinToPattern, ComSubPattern, EVFusion, DegFusion, PKIndex, and LateProject.&lt;/p&gt;

&lt;p&gt;PatternStrategy primarily optimizes the sequence of graph operators within Patterns, referred to as PatternOrders. The transform method takes a Logical GIRPlan, composed of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MATCH_PATTERN&lt;/code&gt; and other relational operators, as shown in Figure 7(a); it outputs a Physical GIRPlan representing the PatternOrder, consisting of a series of physical operators, as shown in Figure 7(b). The transform method executes a top-down search algorithm as described in the &lt;a href=&quot;https://arxiv.org/abs/2401.17786&quot;&gt;GOpt paper&lt;/a&gt;, obtaining a series of PatternOrders along with their respective costs, and selects the PatternOrder with the lowest cost as the output Physical GIRPlan.&lt;/p&gt;

&lt;p&gt;Finally, through the Physical Convertor, the Physical GIRPlan is converted into an Execution Plan supported by various engines. As shown in Figure 7(c), the Physical GIRPlan is converted into an Execution Plan supported by the GraphScope engine, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Expand(v1-&amp;gt;v2, v3-&amp;gt;v2)&lt;/code&gt; is converted to the ExpandIntersect implementation. In the Neo4j engine, this operator is converted to the ExpandInto implementation, as shown in Figure 7(d).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/optimize.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Figure 7: Optimization and Physical Conversion of Query Example.&lt;/div&gt;

&lt;h1 id=&quot;case-study&quot;&gt;Case Study&lt;/h1&gt;

&lt;p&gt;We selected a query case from LDBC SNB that involves complex patterns, multiple patterns, and aggregate computations, which are common optimization requirements. We represent this query using Fig.8 and Cypher as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/query_case.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Fig.8 Query Case Description.&lt;/div&gt;

&lt;div class=&quot;language-cypher highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// M1&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;MATCH&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;person:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PERSON&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;:KNOWS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;otherP&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;),&lt;/span&gt;
      &lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;otherP&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;membership:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HASMEMBER&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forum&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// M2&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;MATCH&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;otherP&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;:HASCREATOR&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;:CONTAINEROF&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forum&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// M3&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;MATCH&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;otherP&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;:HASCREATOR&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;:POST&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;:HASTAG&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;tag:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tag&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Filter&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;person.id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;$personId&lt;/span&gt;
      &lt;span class=&quot;ow&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;otherP&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;$personId&lt;/span&gt;
      &lt;span class=&quot;ow&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;membership.joinDate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;$minDate&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Aggregate&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RETURN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;otherP&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;post_cnt&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;GOpt first converts the Cypher query into GIR (Graph Intermediate Representation) through the Query Parser, as shown in Fig.9. M1, M2, and M3 form the initial Join structure, followed by a series of relational operations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/case_gir.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Fig.9 Initial GIR of the Query Case.&lt;/div&gt;

&lt;p&gt;Next, the GIR Optimizer applies optimizations to the GIR based on strategies defined in the DAG (Directed Acyclic Graph). In the DAG diagram, we highlight the strategies that take effect for this query, as shown in Fig.10.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/case_dag.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Fig.10 DAG of Optimization Process in the Query Case.&lt;/div&gt;

&lt;h2 id=&quot;optimization-rules&quot;&gt;Optimization Rules&lt;/h2&gt;

&lt;p&gt;We further explain the optimization rules applicable to this query:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;FilterIntoPattern: An extended implementation based on Calcite’s FilterIntoJoin. It pushes filtering conditions further down to graph operation operators within patterns. For example, given the following Cypher query:&lt;/p&gt;

    &lt;div class=&quot;language-cypher highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   &lt;span class=&quot;k&quot;&gt;MATCH&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e1&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;),&lt;/span&gt;
       &lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e2&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v3&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;),&lt;/span&gt;
       &lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e3&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v3&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;c1&quot;&gt;// There is a &amp;lt;Join&amp;gt; between the two patterns in GIR&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;MATCH&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e4&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v4&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v3.name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;China&quot;&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;RETURN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v1.name&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;After applying this rule, the optimized query is equivalent to rewriting it as:&lt;/p&gt;

    &lt;div class=&quot;language-cypher highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   &lt;span class=&quot;k&quot;&gt;MATCH&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e1&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;),&lt;/span&gt;
       &lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e2&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v3&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;China&apos;&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;}),&lt;/span&gt;
       &lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e3&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v3&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;name:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;China&apos;&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;})&lt;/span&gt;
   &lt;span class=&quot;c1&quot;&gt;// There is a &amp;lt;Join&amp;gt; between the two patterns in GIR&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;MATCH&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e4&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v4&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;RETURN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v1.name&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AggJoinTrans: Pushes aggregate operations down to the Join operator. This optimization rule is inspired by &lt;a href=&quot;https://calcite.apache.org/javadocAggregate/org/apache/calcite/rel/rules/AggregateJoinTransposeRule.html&quot;&gt;relational databases&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;JoinToPattern: Removes the Join operator between patterns and merges the left and right sub-patterns into a unified pattern. Whether patterns can be merged depends on whether the semantics remain consistent after merging. Based on the semantic requirements of different query languages, we define three types of semantics within patterns:
    &lt;ul&gt;
      &lt;li&gt;Homomorphism Semantics: Allows repetition of vertices or edges within the pattern, which is adopted by Gremlin.&lt;/li&gt;
      &lt;li&gt;Edge-Distinct Semantics: Allows repetition of vertices but not edges within the pattern, which is adopted by Cypher.&lt;/li&gt;
      &lt;li&gt;Vertex-Distinct Semantics: Allows repetition of edges but not vertices.&lt;/li&gt;
      &lt;li&gt;Vertex-Edge-Distinct Semantics: Neither vertices nor edges can repeat within the pattern.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Since the Join between two patterns inherently represents Homomorphism semantics, this rule only applies under that premise.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;PatternStrategy: As described earlier, this strategy applies a Top-Down Search Algorithm to the input pattern structure and selects the optimal PatternOrder based on cost.&lt;/li&gt;
  &lt;li&gt;ComSubPattern: For the two sub-patterns involved in a Join, extracts their common parts as the original input data. If the common part consists of only a single vertex, the Join operation can be further optimized into an Expand operation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;optimization-process-1&quot;&gt;Optimization Process&lt;/h2&gt;

&lt;p&gt;The GIR Optimizer executes specific strategies on the GIR in the order specified by Fig.10:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;FilterIntoPattern is performed first: The filtering conditions &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{person.id = $personId, otherP &amp;lt;&amp;gt; $personId, membership.joinDate &amp;gt; $minDate}&lt;/code&gt; are pushed down to the graph operators that generate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person&lt;/code&gt; nodes, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;otherP&lt;/code&gt; nodes, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;membership&lt;/code&gt; edges.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/case_filter.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Fig.11 The Optimization of FilterIntoPattern.&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Next, AggJoinTrans is executed: This involves splitting the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{otherP, count(post) as post_cnt;}&lt;/code&gt; related group operation. Part of it is pushed down into the left branch of the Join, while the other part remains after the Join to sum up the previously computed group results to ensure semantic equivalence.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/case_agg.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Fig.12 The Optimization of AggJoinTrans.&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;JoinToPattern is then applied: The Join operation between M1 and M2 is removed, and M1 and M2 are merged into a unified pattern M4. In this example, we assume the patterns conform to Homomorphism semantics, which is a prerequisite for applying this rule.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/case_join_pat.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Fig.13 The Optimization of JoinToPattern.&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;PatternStrategy is executed next: The following figure represents the output Optimal Pattern Order, consisting of a series of physical operators.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/case_cbo.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Fig.14 The Optimization of PatternStrategies in M3 and M4.&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Finally, ComSubPattern is applied: This optimizes the reusable common results between patterns. As shown in Fig.14, in the left branch of the Join, M4 produces two columns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(p2, _cnt)&lt;/code&gt; after performing a Group operation, while the optimized PatternOrder of the right branch M3 starts with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Scan p2(PERSON, id &amp;lt;&amp;gt; $id)&lt;/code&gt;. Given that the common part only contains the single point p2, according to the ComSubPattern rule, the Join structure is further optimized into an Expand operation. Hence, M3 directly continues from the P2 point produced by the Group operation to execute &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Expand p2-&amp;gt;&quot;&quot; (HASCREATOR)&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/case_com.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align:center;&quot;&gt;Tab.1 The LDCB&lt;sub&gt;100&lt;/sub&gt; DataSet.&lt;/div&gt;

&lt;h2 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h2&gt;

&lt;p&gt;For the aforementioned case study, we conducted two main experiments: ablation tests and Pattern Orders. We used the LDBC&lt;sub&gt;100&lt;/sub&gt; dataset, as shown in Table Tab.1.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Graph&lt;/th&gt;
      &lt;th&gt;|V|&lt;/th&gt;
      &lt;th&gt;|E|&lt;/th&gt;
      &lt;th&gt;Size&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;sf100&lt;/td&gt;
      &lt;td&gt;283M&lt;/td&gt;
      &lt;td&gt;1754M&lt;/td&gt;
      &lt;td&gt;156GB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;Tab.1 The LDCB&lt;sub&gt;100&lt;/sub&gt; DataSet.&lt;/p&gt;

&lt;p&gt;The system configuration consists of a single node with the following specifications:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;8-core Intel Xeon E5-2620 v4 CPUs at 2.1GHz&lt;/li&gt;
  &lt;li&gt;512GB memory&lt;/li&gt;
  &lt;li&gt;10Gbps network.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We used the &lt;a href=&quot;https://github.com/alibaba/GraphScope/releases/tag/v0.29.0&quot;&gt;GraphScope v0.29.0&lt;/a&gt; system, powered by the underlying &lt;a href=&quot;https://github.com/alibaba/GraphScope/tree/main/interactive_engine/executor/engine/pegasus&quot;&gt;Gaia&lt;/a&gt; engine, with 32 threads.&lt;/p&gt;

&lt;p&gt;The ablation tests primarily compare the individual optimization effects of the rules: FilterIntoPattern, AggJoinTrans, JoinToPattern, and ComSubPattern. To avoid mutual interference between these rules, we sequentially added them in the order defined in Fig.10 and compared the execution time after each rule was added. The results are shown in Table Tab.2.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Rules&lt;/th&gt;
      &lt;th&gt;Time Cost (ms)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;770573&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+FilterIntoPattern&lt;/td&gt;
      &lt;td&gt;234313&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+AggJoinTrans&lt;/td&gt;
      &lt;td&gt;214955&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+JoinToPattern&lt;/td&gt;
      &lt;td&gt;64466&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;+ComSubPattern&lt;/td&gt;
      &lt;td&gt;7014&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div style=&quot;text-align:center;&quot;&gt;Tab.2 Time Cost of Ablation Tests in the Query Case.&lt;/div&gt;

&lt;p&gt;The Pattern Orders experiment compares the optimal order generated by PatternStrategy with two other randomly chosen orders. Figure Fig.16 illustrates the execution sequence of the three Pattern Orders and annotates the actual intermediate data volume produced at each step. Finally, we compare the execution times of the three orders in Table Tab.3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-25-gopt/case_orders.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;Fig.16 Pattern Orders of GOpt-plan, Alt-plan1, Alt-plan2.&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Rules&lt;/th&gt;
      &lt;th&gt;Time Cost (ms)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;GOpt-plan&lt;/td&gt;
      &lt;td&gt;7014&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Alt-plan1&lt;/td&gt;
      &lt;td&gt;22211&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Alt-plan2&lt;/td&gt;
      &lt;td&gt;194047&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div style=&quot;text-align:center;&quot;&gt;Tab.3 Time Cost of Pattern Orders in the Query Case.&lt;/div&gt;
</description>
        <pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate>
        <link>https://graphscope.io/blog/tech/2025/03/25/GOpt-Optimization-Process.html</link>
        <guid isPermaLink="true">https://graphscope.io/blog/tech/2025/03/25/GOpt-Optimization-Process.html</guid>
        
        
        <category>Tech</category>
        
      </item>
    
      <item>
        <title>RelGo: Optimizing Relational Databases with GOpt</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2025-03-10-title-picture.jpg&quot; alt=&quot;title&quot; /&gt;
The GraphScope team’s work on proposing RelGo for optimizing SQL/PGQ queries has been accepted by SIGMOD 2025. In this work, RelGo integrates &lt;a href=&quot;https://graphscope.io/blog/tech/2024/02/22/GOpt-A-Unified-Graph-Query-Optimization-Framework-in-GraphScope&quot;&gt;GOpt&lt;/a&gt; into relational databases (using DuckDB as an example), enhancing its ability to optimize SQL/PGQ queries and providing better optimization results than DuckDB’s own optimizer.
This article introduces the main content of RelGo.&lt;/p&gt;

&lt;h2 id=&quot;1-background-and-motivations&quot;&gt;1. Background and Motivations&lt;/h2&gt;
&lt;p&gt;In the realms of data management and analytics, relational databases have long been the bedrock of structured data storage and retrieval, empowering a plethora of applications, across domains ranging from finance to healthcare. To facilitate tasks like creating tables and retrieving information in a database, the Structured Query Language (SQL) was developed and has been widely adopted by various Relational Database Management Systems (RDBMS). Below is an example of querying on IMDB data with an SQL query:&lt;/p&gt;

&lt;style&gt;
    img.small_img {
        zoom: 20% !important;
        width: unset !important;
    }
    img.small_cap {
        zoom: 10% !important;
        width: unset !important;
    }
&lt;/style&gt;

&lt;pre&gt;&lt;code class=&quot;language-plain&quot;&gt;Query 1 (Q1)

SELECT n.name 
FROM NAME AS n1,
     CAST_INFO AS ci1,
     CAST_INFO AS ci2,
     TITLE AS t1,
     TITLE AS t2,
     MOVIE_COMPANIES AS mc1,
     MOVIE_COMPANIES AS mc2,
     COMPANY_NAME AS cn
WHERE n1.id = ci1.person_id
      AND ci1.movie_id = t1.id
      AND t1.id = mc1.movie_id
      AND mc1.company_id = cn.id
      AND n1.id = ci2.person_id
      AND ci2.movie_id = t2.id
      AND t2.id = mc2.movie_id
      AND mc2.company_id = cn.id
      AND t1.title &amp;lt;&amp;gt; t2.title;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The relationships between the tables relevant to Q1 in IMDB are illustrated in the following ER diagram&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/relgo/fig1.jpg&quot; style=&quot;zoom:10%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The NAME table and TITLE table store information about persons and movies, respectively. The CAST_INFO table holds the quaternary relationship between persons, movies, roles (characters played in the movies), and professions (such as actor, actress, director, writer, etc.). The COMPANY_NAME table contains the names of movie companies, while the MOVIE_COMPANIES table records the association between movies, movie companies, and company types. Therefore, the SQL query above aims to find individuals who have participated in the production of at least two movies for the same company.&lt;/p&gt;

&lt;p&gt;It’s easy to see that using SQL to describe such queries can sometimes result in very lengthy statements, especially when there’s a need to describe the relationships between tables.&lt;/p&gt;

&lt;p&gt;Another more intuitive example is finding n persons who all know each other. Each time we describe the mutual relationship between two persons, we need to add two join conditions. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-plain&quot;&gt;Person1.id = Knows.person1id AND Person2.id = Knows.person2id
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hence, describing n persons knowing each other requires writing at least $n \times (n-1)$ join conditions. Manually writing such query statements is evidently very inefficient.&lt;/p&gt;

&lt;p&gt;When a query requires describing the relationships between tables, using graph queries can be relatively more convenient. The advantage lies in being able to express the query in a graph-like manner. Using Cypher as an example, query Q1 can be expressed as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-plain&quot;&gt;MATCH (n1:NAME)-[ci1:CAST_INFO]-&amp;gt;(t1:TITLE)-[mc1:MOVIE_COMPANIES]-&amp;gt;(cn:COMPANY_NAME),
      (n1)-[ci2:CAST_INFO]-&amp;gt;(t2:TITLE)-[mc2:MOVIE_COMPANIES]-&amp;gt;(cn)
WHERE t1.title &amp;lt;&amp;gt; t2.title
RETURN n1.name;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This query intuitively describes the following graph pattern:&lt;br /&gt;
&lt;img src=&quot;/blog/assets/images/relgo/fig2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the scenario of finding n persons knowing each other, using a graph query only requires describing a complete graph with n vertices (i.e., an n-clique). The above example demonstrates that using graph queries can significantly reduce the time database users spend writing SQL, allowing them to express their intended queries more quickly and conveniently.&lt;/p&gt;

&lt;p&gt;Therefore, in ISO SQL:2023, an extension called SQL/PGQ (SQL/Property Graph Queries) has been introduced to support graph queries within SQL expressions. Assuming that each tuple in the NAME, TITLE, and COMPANY_NAME tables represents a vertex, and each tuple in the CAST_INFO and MOVIE_COMPANIES tables represents an edge, we can rewrite query Q1 as follows in SQL/PGQ:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-plain&quot;&gt;SELECT n_name FROM GRAPH_TABLE (G
    MATCH (n1:NAME)-[ci1:CAST_INFO]-&amp;gt;(t1:TITLE)-[mc1:MOVIE_COMPANIES]-&amp;gt;(cn:COMPANY_NAME),
      (n1)-[ci2:CAST_INFO]-&amp;gt;(t2:TITLE)-[mc2:MOVIE_COMPANIES]-&amp;gt;(cn)
    WHERE t1.title &amp;lt;&amp;gt; t2.title
    COLUMNS (n1.name AS n_name) 
) g;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The SQL/PGQ extension addresses how to write graph queries within SQL expressions. However, once we get the SQL/PGQ query statements, analyzing and optimizing them remains a new challenge brought by SQL/PGQ. Query optimization refers to finding an efficient query plan based on a user-given query to achieve better query performance. Research on relational database query optimizers is typically carried out for the SPJ (selection-projection-join) queries. However, SQL/PGQ queries cannot be expressed as SPJ queries, because the GRAPH_TABLE clause cannot be directly represented through traditional selection, projection, and join operators. Therefore, it is challenging to directly use traditional relational database optimizers to optimize SQL/PGQ statements.&lt;/p&gt;

&lt;p&gt;Addressing this issue, we introduce a new query skeleton called SPJM (selection-projection-join-matching) and subsequently designed and implemented a converged optimization framework named RelGo based on GOpt. Our main contributions can be summarized as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We define RGMapping to map relational tables to a property graph based on SQL/PGQ statements. Using RGMapping, we have defined a new query skeleton called SPJM, which can be used to express queries that contain both relational and graph queries.&lt;/li&gt;
  &lt;li&gt;We have developed a theory to transform SPJM queries into SPJ queries, allowing existing relational databases to directly handle SPJM queries. This approach is called the graph-agnostic approach. We have demonstrated that this graph-agnostic method has a significantly larger search space (exponentially larger) compared to our method in some scenarios.&lt;/li&gt;
  &lt;li&gt;We have designed and implemented RelGo to leverage relational and graph query optimization techniques to optimize SPJM queries. In detail, RelGo employs state-of-the-art graph optimization techniques and implements graph-related physical operations based on graph indexes.&lt;/li&gt;
  &lt;li&gt;We developed RelGo by integrating it with the industrial relational optimization framework Calcite, and employing DuckDB for execution runtime. We conducted extensive experiments to evaluate its performance. The results on the LDBC Social Network Benchmark indicate that RelGo significantly surpasses the performance of the graph-agnostic baseline, achieving an average speedup of 21.9x over the baseline, which remains 5.4x faster even when graph indexing is enabled.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This work has been accepted at SIGMOD 2025 (&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3698828&quot;&gt;https://dl.acm.org/doi/10.1145/3698828&lt;/a&gt;，&lt;a href=&quot;https://arxiv.org/pdf/2408.13480&quot;&gt;https://arxiv.org/pdf/2408.13480&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;2-concepts-and-spjm-skeleton&quot;&gt;2. Concepts and SPJM Skeleton&lt;/h2&gt;

&lt;p&gt;Let’s start by briefly explaining some of the concepts that will be used in this blog, including Property Graph and RGMapping. In this blog, for the sake of convenience, we will use the terms “table” and “relation” interchangeably.&lt;/p&gt;

&lt;h3 id=&quot;21-concepts&quot;&gt;2.1 Concepts&lt;/h3&gt;

&lt;p&gt;Given an expression that contains both relational queries and graph queries, in order to integrate the query plans of both, it’s necessary to define a method for translating between relational data and graph data. Therefore, we use the following concepts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Property Graph&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A property graph is composed of vertices and edges, where vertices and edges can have various attributes. In the upper right corner of Figure 3(a), an example of a property graph is provided. This property graph contains a total of 5 vertices, including 3 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt;s and 2 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Message&lt;/code&gt;s. Additionally, the property graph includes 8 edges, each tagged with either &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Knows&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;RGMapping&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;RGMapping maps tuples from relational tables to vertices or edges in a property graph. Specifically, RGMapping includes a vertex mapping and an edge mapping, which respectively map tuples from tables to different vertices and edges. To more conveniently describe the definition of RGMapping, we will illustrate this with the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/relgo/fig3.jpg&quot; /&gt;
&lt;img src=&quot;/blog/assets/images/relgo/fig3cap.jpg&quot; class=&quot;small_cap&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fig. 3(a) contains four relational tables: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Knows&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Message&lt;/code&gt;. The relationships between them can be described using the ER diagram in the bottom left corner of Fig. 3(a). In relational data modeling, an ER diagram includes entities and relationships. Consequently, vertices can be mapped from relations corresponding to entities, and edges can be mapped from relations corresponding to relationships. In this example, tuples in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Message&lt;/code&gt; tables are mapped to vertices, while tuples in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Knows&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt; tables are mapped to edges. The resulting property graph is shown in the upper right corner of Figure 3(a), labeled as property graph G.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In the property graph G, p1 represents the tuple with person_id = 1 in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt; table, and k1 represents the tuple with knows_id = 1 in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Knows&lt;/code&gt; table.&lt;/strong&gt; RGMapping is used to define the mapping relationships between relational tables and the property graph. For ease of explanation, we will refer to tables mapped to vertices as vertex tables and tables mapped to edges as edge tables. For edge tables, it is not sufficient to just know the mapping relationship between a tuple and an edge; it is also necessary to know the association between the edge’s source and target vertices and the vertex tables. Specifically, for the edge table &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt; in this example, we can provide the following two association relationships:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/relgo/formula1.jpg&quot; alt=&quot;image&quot; style=&quot;zoom:40%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These two association relationships respectively indicate that for a tuple in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt; table, which corresponds to an edge in the property graph, the source vertex of the edge corresponds to a tuple in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt; table, and the target vertex corresponds to a tuple in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Message&lt;/code&gt; table. The correspondence between tuples is determined based on the values of Likes.pid, Person.person_id, and Message.message_id. Specifically, a tuple in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt; table is associated with a tuple in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt; table if and only if the value of the pid attribute matches the value of the person_id attribute. For example, the first row in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt; table is associated with the first row in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt; table, because l1.pid = p1.person_id. It means that the source vertex of the edge mapped from the first row of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt; is mapped from the first row of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Generally, this association can be derived from the primary-foreign key relationships between relational tables. Specifically, because the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt; table has foreign keys pid and mid pointing to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Message&lt;/code&gt; tables, respectively, the vertex tables related to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt; are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Message&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;With this information, it is possible to convert between relational tables and vertices and edges in the property graph. Therefore, given a relational database, as long as the corresponding RGMapping is defined, part of the relational tables can be converted into a property graph, thereby leveraging the capabilities of a graph optimizer in query optimization.&lt;/p&gt;

&lt;h3 id=&quot;22-spjm-skeleton&quot;&gt;2.2 SPJM Skeleton&lt;/h3&gt;

&lt;p&gt;To represent graph queries, we introduce a new operator into the relational operators, namely the Matching Operator. This operator takes a graph relation and a pattern graph as input and produces a graph table as output. Here, a graph relation is a special type of table. Each tuple in this relation represents a property graph, with each attribute value of the tuple corresponding to a vertex or edge. The table shown at the bottom of Fig. 3(b) is an example of a graph relation.&lt;/p&gt;

&lt;p&gt;For each tuple (i.e., property graph) in the input graph relation, the Matching Operator searches for all subgraphs isomorphic to the input pattern graph. Each row of the graph relation returned by the Matching Operator corresponds to one such found isomorphic subgraph. In this paper, we consider queries that include selection, projection, join, and matching operators. Queries composed of these operators are referred to as SPJM queries.&lt;/p&gt;

&lt;h2 id=&quot;3-optimizing-matching-operator&quot;&gt;3. Optimizing Matching Operator&lt;/h2&gt;

&lt;p&gt;In this section, we focus on handling the matching operator, which plays a distinct role within the SPJM queries. We discuss two main perspectives of optimizing the matching operator: logical transformation and physical implementation. Logical transformation is responsible for transforming a matching operator into a logically equivalent representation, while physical implementation focuses on how the matching operator can be efficiently executed.&lt;/p&gt;

&lt;h3 id=&quot;31-logical-transformation&quot;&gt;3.1 Logical Transformation&lt;/h3&gt;

&lt;p&gt;Given a matching operator, suppose the input pattern graph is as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-plain&quot;&gt;MATCH (p1:PERSON)-[e1:KNOWS]-&amp;gt;(p2:PERSON)-[e3:LIKES]-&amp;gt;(m:MESSAGE),
      (p1)-[e2:LIKES]-&amp;gt;(m)
RETURN p1.name, p1.place_id, p2.name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This pattern graph, as shown in Figure 3(b), searches for two persons who like the same message. The table names in the pattern graph use uppercase letters, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PERSON&lt;/code&gt;. For this query, we propose two transformation methods: one is the graph-agnostic approach, and the other is the graph-aware approach.&lt;/p&gt;

&lt;h4 id=&quot;311-graph-agnostic-approach&quot;&gt;3.1.1 Graph-Agnostic Approach&lt;/h4&gt;

&lt;p&gt;The graph-agnostic approach directly converts the pattern graph into joins between tables based on the RGMapping, thus transforming the SPJM problem into an SPJ problem. Existing relational optimizers can then be used for query optimization. According to the RGMapping described in Section 2 and the ER diagram in Figure 3(a) that describes the relationships between the tables, it is easy to determine that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Message&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Knows&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt; tables are respectively mapped to vertices or edges labeled &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Message&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Knows&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt; in the graph.&lt;br /&gt;
Moreover, the following association relationships exist between the vertex tables and edge tables:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/relgo/formula2.jpg&quot; alt=&quot;image&quot; style=&quot;zoom:40%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, $Person_1$ and $Person_2$ represent two copies of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt; table, containing the same content as the original &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt; table. Subsequently, each edge in the pattern graph can be obtained through joins between the vertex tables and edge tables. Specifically, the edges in the aforementioned pattern graph can be converted into the following joins:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/relgo/formula3.jpg&quot; alt=&quot;image&quot; style=&quot;zoom:40%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Therefore, the matching operator can be converted to:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/relgo/formula4.jpg&quot; alt=&quot;image&quot; style=&quot;zoom:40%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then, relational optimizers can be used to optimize the query.&lt;/p&gt;

&lt;h4 id=&quot;312-graph-aware-approach&quot;&gt;3.1.2 Graph-Aware Approach&lt;/h4&gt;

&lt;p&gt;In this section, we introduce a graph-aware transformation that incorporates key ideas from the literature on graph optimization. Specifically, we first decompose the pattern graph in Figure 3(b) to obtain the following decomposition tree.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/relgo/fig4.jpg&quot; class=&quot;small_img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In detail, the tree has a root node that represents the pattern graph, and each non-leaf intermediate node is a sub-pattern (a subgraph of the pattern) , which has left and right child nodes. These two child nodes are both subgraphs of the intermediate node, and by joining the two child nodes, we obtain this intermediate node. Following state-of-the-art graph optimizers, to guarantee a worst-case optimal execution plan, all intermediate sub-patterns in the decomposition tree must be induced subgraphs of the pattern graph.&lt;/p&gt;

&lt;p&gt;The leaf nodes of the decomposition tree must be Minimum Matching Components (MMCs). An MMC can either be a single-vertex pattern or a complete-star pattern. A complete-star pattern must be located in the right subtree, with all the leaf nodes of the star appearing in the left subtree. It is easy to see that a single-edge pattern is a special type of complete-star pattern. This design is mainly to ensure that the execution plan is worst-case optimal.&lt;/p&gt;

&lt;p&gt;The decomposition tree in Fig. 4 contains three MMCs: $\mathcal{P}_2$ (complete-star pattern), $\mathcal{P}_3$ (single-vertex pattern subgraph), and $\mathcal{P}_4$ (single-edge pattern subgraph). After decomposing, for each node in the decomposition tree, we join its left and right child nodes to obtain the result of the current node. For example, when matching the property graph G in Fig. 3(a) with the pattern graph in Fig. 4, the matching result of $\mathcal{P}_1$ is (p1, k1, p2), (p2, k2, p1), (p2, k3, p3), (p3, k4, p2). The matching result of $\mathcal{P}_2$ is (p1, l1, m1, l2, p2), (p2, l2, m1, l1, p1), (p2, l3, m2, l4, p3), (p3, l4, m2, l3, p2).&lt;/p&gt;

&lt;p&gt;By joining these results on vertices $\mathcal{P}_1$ and $\mathcal{P}_2$ in the pattern, we obtain (p1, k1, p2, l1, m1, l2), (p2, k2, p1, l2, m1, l1),  (p2, k3, p3, l3, m2, l4), and (p3, k4, p2, l4, m2, l3). These results are exactly the results of matching the original pattern graph $\mathcal{P}$, as shown in the table below Fig. 3(b).&lt;/p&gt;

&lt;p&gt;We have demonstrated that in some scenarios, the search space of the graph-aware approach is exponentially smaller than that of the graph-agnostic approach. Taking RelGo (graph-aware approach) and Calcite (graph-agnostic approach) as examples, we conducted experiments on the LDBC SNB benchmark (on LDBC30 dataset) and JOB benchmark (on IMDB dataset) to compare their optimization times. The experimental results show that RelGo significantly improves optimization speed compared to Calcite.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/relgo/fig5a.jpg&quot; class=&quot;small_img&quot; /&gt;
&lt;img src=&quot;/blog/assets/images/relgo/fig5b.jpg&quot; class=&quot;small_img&quot; /&gt;
&lt;img src=&quot;/blog/assets/images/relgo/fig5cap.jpg&quot; class=&quot;small_img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;32-physical-implementation&quot;&gt;3.2 Physical Implementation&lt;/h3&gt;

&lt;p&gt;Physical implementation optimization refers to using efficient physical implementations when realizing the operators in the execution plan to make the actual execution more efficient. Referencing GRainDB, we implemented a graph index. The following illustration is an example of a graph index.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/relgo/fig6.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As shown in Fig. 6, the EV index is built on the edge table. For each tuple in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt; table (which is an edge table), two new attribute columns, pid_rowid and mid_rowid, are added to record the positions (i.e., row numbers) of the source and target vertices of the corresponding edge in the respective vertex tables. For example, l1 in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt; table corresponds to an edge whose source vertex is located at the 0-th row of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt; table and whose target vertex is at the 0-th row of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Message&lt;/code&gt; table.&lt;/p&gt;

&lt;p&gt;The VE index is built based on the vertex tables. For each tuple in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt; table, the VE index records the positions of its adjacent edges in the edge table (i.e., row numbers) and the positions of the other endpoints of these edges in the corresponding vertex tables. Specifically, for tuple p1 in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Person&lt;/code&gt; table, the VE index records its adjacent edge’s position at the 0-th row of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likes&lt;/code&gt; table and the other endpoint of that edge at the 0-th row of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Message&lt;/code&gt; table.&lt;/p&gt;

&lt;p&gt;Based on these two indexes, when actually joining the vertex and edge tables, it is possible to skip value comparisons and quickly obtain tuples that can be joined.&lt;/p&gt;

&lt;p&gt;Specifically, we implemented three main types of physical implementation optimizations for the decomposition tree:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Given an intermediate node, when its right subtree is an MMC containing only a single edge, the join of its left and right subtrees can be implemented as a combination of the EXPAND_EDGE and GET_VERTEX operators, without flattening the result during the computation of these two operators. For example, consider Figure 5. Suppose a decomposition tree has an intermediate node &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(:PERSON) JOIN (:PERSON)-[:LIKES]-&amp;gt;(:MESSAGE)&lt;/code&gt;. In this case, both the left and right subtrees are MMCs, i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(:PERSON)&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(:PERSON)-[:LIKES]-&amp;gt;(:MESSAGE)&lt;/code&gt;. Assume that a tuple from the matching results of the left subtree is (p2). When joined with the right subtree, the result is (p2, [l2, l3], [m1, m2]). After flattening, the resulting tuples are (p2, l2, m1) and (p2, l3, m2).&lt;/li&gt;
  &lt;li&gt;Given an intermediate node, when its right subtree is a complete-star pattern, we implemented the EXPAND_INTERSECT operator to avoid flattening the results during computation. Consider the intermediate node &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(:PERSON)-[:KNOWS]-&amp;gt;(:PERSON) JOIN (:PERSON)-[:LIKES]-&amp;gt;(:MESSAGE)&amp;lt;-[:LIKES]-(:PERSON)&lt;/code&gt;, which corresponds to the two child nodes of the root in the decomposition tree shown in Fig. 4. Suppose a tuple from the matching results of the left subtree is (p1, k1, p2), then the EXPAND_INTERSECT operator first applies the EXPAND_EDGE and GET_VERTEX operators on p1 and p2, yielding the results (p1, k1, p2, [l1], [m1]) and (p1, k1, p2, [l2, l3], [m1, m2]). Subsequently, it directly intersects these results to obtain (p1, k1, p2, [(l1, l2, m1)]). Finally, flattening this yields the result (p1, k1, p2, l1, l2, m1).&lt;/li&gt;
  &lt;li&gt;Whenever possible, use graph indices in the physical implementation to accelerate joins between vertex tables and edge tables.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-the-converged-optimization-framework-relgo&quot;&gt;4. The Converged Optimization Framework: RelGo&lt;/h2&gt;

&lt;p&gt;In this section, we specifically introduce the proposed converged optimization framework, RelGo, with its optimization workflow as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/relgo/fig7.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As illustrated in Fig. 7, the core workflow of the RelGo framework consists of two components: &lt;strong&gt;graph optimization&lt;/strong&gt; and &lt;strong&gt;relational optimization&lt;/strong&gt;. The graph optimization is responsible for handling the graph component in an SPJM query, leveraging graph optimization techniques to determine the optimal decomposition tree of the matching operator. On the other hand, the relational optimization takes over to optimize the relational component in the query. The order in which these two components are applied is not strictly defined. However, for the purpose of our discussion, we will first focus on the graph optimization and then proceed to the relational optimization.&lt;/p&gt;

&lt;p&gt;RelGo uses GOpt for graph optimization and applies techniques such as FilterIntoMatchRule TrimAndFuseRule during the optimization process. Specifically, the TrimAndFuseRule describes a scenario where the EXPAND_EDGE and GET_VERTEX operators, when executed consecutively without the need to retrieve edge properties, become equivalent to a direct neighbor retrieval operator. Therefore, if the corresponding graph index is constructed, the VE index can be utilized to merge the two operators into a single EXPAND operator, directly retrieving the neighboring vertices.&lt;/p&gt;

&lt;p&gt;Once the graph optimizer has computed the optimal execution plan for the matching operator, the next step is to integrate this plan with the remaining relational operators in the SPJM query. The relational optimization is responsible for optimizing these remaining operators, which are all relational operators.&lt;/p&gt;

&lt;p&gt;Specifically, to prevent the relational optimizer from delving into the internal details of the graph pattern matching process, we introduce a new physical operator called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SCAN_GRAPH_TABLE&lt;/code&gt;, as shown in Fig. 7(c), which encapsulates the optimal execution plan for the matching operator.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SCAN_GRAPH_TABLE&lt;/code&gt; operator acts as a bridge between the graph and relational components of the query. From the perspective of the relational optimizer, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SCAN_GRAPH_TABLE&lt;/code&gt; behaves like a standard SCAN operator, providing a relational interface to the matched results.&lt;/p&gt;

&lt;p&gt;We engineered the frontend of RelGo in Java and built it upon Apache Calcite to utilize its robust relational query optimization infrastructure. Firstly, we enhanced Calcite’s SQL parser to recognize SQL/PGQ extensions, specifically to parse the GRAPH_TABLE clause. We created a new ScanGraphTableRelNode that inherits from Calcite’s core RelNode class, translating the GRAPH_TABLE clause into this newly defined operator within the logical plan. Secondly, we incorporate heuristic rules such as FilterIntoMatchRule and TrimAndFuseRule into Calcite’s rule-based HepPlanner, by specifying the activation conditions and consequent transformations of each rule. For more nuanced optimization, we rely on the VolcanoPlanner, the cost-based planner in Calcite, to optimize the ScanGraphTableRelNode. We devised a top-down search algorithm that assesses the most efficient physical plan based on a cost model, combined with high-order statistics from GLogue for more accurate cost estimation. Lastly, the converged optimizer outputs an optimized and platform-independent plan formatted with Google Protocol Buffers (protobuf), ensuring the adaptability of \name’s output to various backend database systems&lt;/p&gt;

&lt;p&gt;We developed the RelGo framework’s backend in C++ using DuckDB as the relational execution engine to showcase its optimization capabilities. We integrated graph index support in GRainDB. Besides, we craft a new join on DuckDB called EI-Join for the support of EXPAND_INTERSECT. Without graph index, the HASH_JOIN operator is used throughout the entire plan.&lt;/p&gt;

&lt;h2 id=&quot;5-experiments&quot;&gt;5. Experiments&lt;/h2&gt;

&lt;p&gt;To evaluate the quality of the query plans generated by RelGo, we conducted performance testing using two datasets: LDBC and IMDB. The experiments were carried out on the LDBC SNB and JOB benchmarks. The LDBC dataset was generated by the official LDBC Data Generator with scale factors of 10, 30, and 100. The JOB benchmark queries were executed on the IMDB dataset. The baseline methods compared in the experiments include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DuckDB：Common relational databases that use graph-agnostic methods for optimization.&lt;/li&gt;
  &lt;li&gt;GRainDB：GRainDB implements graph indexes on DuckDB. After optimizing the queries using DuckDB’s optimizer, some join operators are replaced with predefined joins utilizing the graph indexes.&lt;/li&gt;
  &lt;li&gt;Umbra：Its query optimizer can generate query plans that include worst-case optimal joins.&lt;/li&gt;
  &lt;li&gt;Kùzu: Graph database management systems using the property graph model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The experimental results are as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/relgo/fig8a.jpg&quot; alt=&quot;LDBC10&quot; class=&quot;small_img&quot; /&gt;
LDBC10&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/relgo/fig8b.jpg&quot; alt=&quot;LDBC30&quot; class=&quot;small_img&quot; /&gt;
LDBC30&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/relgo/fig8c.jpg&quot; alt=&quot;LDBC100&quot; class=&quot;small_img&quot; /&gt;
LDBC100&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/relgo/fig8d.jpg&quot; alt=&quot;JOB&quot; class=&quot;small_img&quot; /&gt;
&lt;img src=&quot;/blog/assets/images/relgo/fig8e.jpg&quot; alt=&quot;JOB&quot; class=&quot;small_img&quot; /&gt;
JOB
&lt;img src=&quot;/blog/assets/images/relgo/fig8cap.jpg&quot; class=&quot;small_img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Experimental results show that on the LDBC100 dataset, the query plans generated by RelGo have average execution times that are 21.9x, 5.4x, 49.9x, and 188.7x faster than those generated by DuckDB, GRainDB, Umbra, and Kùzu, respectively. This indicates that RelGo has a significant advantage over baseline methods in optimizing graph-related queries.&lt;/p&gt;

&lt;p&gt;Another benchmark, i.e., the JOB benchmark that is established for assessing join optimizations in relational databases, lacks any cyclic-pattern queries. On the JOB benchmark, the query plans generated by RelGo have average execution times that are 8.2x, 4.0x, 1.7x, and 136.1x faster than those generated by DuckDB, GRainDB, Umbra, and Kùzu, respectively. This further demonstrates that RelGo can generate more efficient query plans.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3698828&quot;&gt;https://dl.acm.org/doi/10.1145/3698828&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2408.13480&quot;&gt;https://arxiv.org/pdf/2408.13480&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://graphscope.io/blog/tech/2024/02/22/GOpt-A-Unified-Graph-Query-Optimization-Framework-in-GraphScope&quot;&gt;GOpt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 10 Mar 2025 01:00:00 +0000</pubDate>
        <link>https://graphscope.io/blog/tech/2025/03/10/RelGo-Optimizing-Relational-Databases-with-GOpt.html</link>
        <guid isPermaLink="true">https://graphscope.io/blog/tech/2025/03/10/RelGo-Optimizing-Relational-Databases-with-GOpt.html</guid>
        
        
        <category>Tech</category>
        
      </item>
    
      <item>
        <title>GraphScope Flex: LEGO-like Graph Computing Stack</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/assets/images/flex-title.jpg&quot; alt=&quot;title&quot; /&gt;
From June 9th to June 15th, 2024, SIGMOD 2024 was held in Santiago, Chile. 
The GraphScope team presented their paper “&lt;a href=&quot;https://arxiv.org/abs/2312.12107&quot;&gt;GraphScope Flex: LEGO-like Graph Computing Stack&lt;/a&gt;” at the SIGMOD Industry Session. 
This article introduces the main content of that paper.&lt;/p&gt;

&lt;h3 id=&quot;diversified-graph-computing-demands&quot;&gt;Diversified Graph Computing Demands&lt;/h3&gt;
&lt;p&gt;Graph computing encompasses a wide range of types, commonly including graph analytics, graph interactive queries, and graph neural networks (GNNs). Previous graph computing systems typically cater to a specific type of graph computation. In a complex workflow, which may involve multiple types of graph computing tasks, users often need to employ multiple graph computing systems to complete this intricate process.&lt;/p&gt;

&lt;p&gt;To address these issues, we developed and open-sourced the industry’s first one-stop graph computing system, GraphScope, in 2020. As illustrated in the figure below, GraphScope integrates a Graph Analytics Engine (GAE), a Graph Interactive Engine (GIE), and a Graph Learning Engine (GLE) to support different types of graph computations. Additionally, GraphScope includes an immutable in-memory graph storage system called &lt;a href=&quot;https://github.com/v6d-io/v6d&quot;&gt;Vineyard&lt;/a&gt;, which allows various computing engines to share data through shared memory. To reduce the learning curve for users, GraphScope extends Gremlin as a unified query language and offers a simple and user-friendly Python interface.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/graphscope-arch.jpg&quot; alt=&quot;gs&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In real scenarios, we find that the demand for graph computing is highly diverse. The diagram below illustrates Alibaba’s real graph computing scenarios, where graph data may consist of immutable data stored in memory, may come from external data sources that are continuously updated, or may originate from files in a data lake. The workloads of graph computing are also quite varied. For example, Workload 1 represents running a ranking algorithm (e.g., PageRank) on the graph, which is a typical graph analytics task. Workload 2 represents running a fraud detection model based on graph neural networks on the graph. Workload 3 requires processing a large number of queries related to product recommendations in a short amount of time. Workload 4 represents the need to perform real-time online query operations on graph data through a WebUI, while Workload 5 represents data analysts needing to conduct BI analysis on graph data, aiming to obtain analytical results in the shortest time possible.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/diverse-graph.jpg&quot; alt=&quot;diverse&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the examples mentioned above, it is clear that the diversity of graph computing is reflected in various aspects such as storage, workloads, and performance metrics of interest. In light of this diversity, the one-stop design of GraphScope appears inadequate to handle such varied requirements.&lt;/p&gt;

&lt;h4 id=&quot;diversity-of-graph-types-and-storage&quot;&gt;Diversity of Graph Types and Storage&lt;/h4&gt;
&lt;p&gt;Firstly, there is a diversity in both graph types and graph storage. On one hand, there are multiple graph representation models currently available, as illustrated below. Common models include simple graphs, weighted graphs, sparse matrices/tensors, labeled property graphs, and RDF (Resource Description Framework) graphs. On the other hand, the storage characteristics of graph data also exhibit diversity, including in-memory vs. external storage, mutable vs. immutable data, and support for multi-versioning, among others. Therefore, the use of the property graph model with immutable in-memory storage, such as Vineyard in GraphScope, struggles to adapt to this diversity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/diverse-store.jpg&quot; alt=&quot;diverse-store&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;diversity-of-graph-computing-paradigms-and-query-interfaces&quot;&gt;Diversity of Graph Computing Paradigms and Query Interfaces&lt;/h4&gt;
&lt;p&gt;Secondly, the types of graph computing are highly diverse, encompassing various workloads such as graph analytics, graph querying, and graph neural networks. Moreover, different types of graph computing tasks typically employ different programming interfaces and query languages. Even within the same type of graph computation, there are multiple commonly used programming interfaces and query languages, making it difficult to expect users to express all workloads using a single, unified interface or language. For example, &lt;a href=&quot;https://graphscope.io/blog/tech/2021/03/25/a-review-of-programming-models-for-parallel-graph-processing&quot;&gt;common programming interfaces for graph analytics tasks&lt;/a&gt; include Pregel, GAS, and PIE, while frequently used query languages in the graph querying domain include Gremlin, Cypher, and GQL. Additionally, popular graph neural network systems like &lt;a href=&quot;https://www.dgl.ai/&quot;&gt;DGL&lt;/a&gt; and &lt;a href=&quot;https://www.pyg.org/&quot;&gt;PyG&lt;/a&gt; utilize different programming interfaces. Consequently, the use of a unified programming/query language in GraphScope faces challenges in addressing the diversity of computing paradigms and query interfaces.&lt;/p&gt;

&lt;h4 id=&quot;diversity-of-performance-requirements-in-graph-computing&quot;&gt;Diversity of Performance Requirements in Graph Computing&lt;/h4&gt;
&lt;p&gt;Finally, even with the same data and the same type of workload, the performance metrics we focus on may vary across different scenarios. For example, in tasks related to graph querying, when faced with a scenario that requires handling a large number of simple queries in a short time, our primary performance metric is system throughput. Conversely, in a scenario involving a small number of complex queries, the focus shifts to low latency, meaning that the system must return results for individual queries in the shortest time possible. Since any given system or component is typically optimized for a specific performance metric, a single component in GraphScope struggles to effectively meet multiple performance requirements simultaneously.&lt;/p&gt;

&lt;h3 id=&quot;graphscope-flex-modular-design-inspired-by-lego&quot;&gt;GraphScope Flex: Modular Design Inspired by LEGO&lt;/h3&gt;

&lt;p&gt;To better address the increasingly diverse needs of graph computing, we have designed the next-generation architecture of GraphScope, known as GraphScope Flex. This architecture adopts a modular design philosophy, allowing users to freely select appropriate components based on specific task requirements, much like building with LEGO bricks.&lt;/p&gt;

&lt;p&gt;The architecture of GraphScope Flex, as shown in the diagram below, is divided into three layers: the storage layer, the execution engine layer, and the frontend layer. Each layer contains a rich set of components that have been highly optimized for different objectives.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/flex-arch.jpg&quot; alt=&quot;flex-arch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When given a specific task, users simply need to select the components that meet their requirements from each layer. They can then use the building tool, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flexbuild&lt;/code&gt;, provided by GraphScope Flex to construct a graph computing system tailored to their needs. Specifically, each layer in GraphScope Flex includes the following components.&lt;/p&gt;

&lt;h4 id=&quot;storage-layer&quot;&gt;Storage Layer&lt;/h4&gt;
&lt;p&gt;The storage layer currently supports several types of storage: immutable in-memory storage (Vineyard), standard data file format for graph (&lt;a href=&quot;https://graphar.apache.org/&quot;&gt;GraphAr&lt;/a&gt;), multi-version mutable storage (&lt;a href=&quot;https://github.com/graphscope/gart&quot;&gt;GART&lt;/a&gt;), and mutable in-memory storage (mCSR). Each storage type has different access interfaces. To shield the execution engines from the interface discrepancies of various storage types, GraphScope Flex adopts a standardized graph access interface called &lt;a href=&quot;https://github.com/GraphScope/GRIN&quot;&gt;GRIN&lt;/a&gt;, requiring all storage implementations to comply with this interface. This way, all upper-layer computing engines can access all graph storage using a single, uniform interface. In the future, if new storage options are added to the storage layer, users won’t need to worry about compatibility issues resulting from these extensions.&lt;/p&gt;

&lt;h4 id=&quot;execution-engine-layer&quot;&gt;Execution Engine Layer&lt;/h4&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GRAPE&lt;/code&gt; is a high-performance distributed engine designed for graph analysis tasks, optimized for computation and communication on the CPU. Recently, it has introduced support for GPU, leveraging the high computing power of GPUs and high-speed interconnects like NVLink to accelerate graph analysis tasks. GRAPE provides external programming models such as Pregel, PIE, and FLASH, facilitating the development of customized graph analysis algorithms for users.&lt;/p&gt;

&lt;p&gt;For graph querying tasks, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GraphIR&lt;/code&gt; module translates user-written queries in Cypher or Gremlin into a query language-agnostic intermediate representation. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Graph Query Optimizer&lt;/code&gt; module employs both Rule-based Optimization (RBO) and Cost-based Optimization (CBO) techniques to optimize this intermediate representation. Depending on the performance metrics of interest, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Hiactor Codegen&lt;/code&gt; module generates physical execution plans that can be executed on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Hiactor&lt;/code&gt; component, a low-level parallel engine suitable for high-throughput scenarios. If the goal is to minimize the execution time of individual queries, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Gaia Codegen&lt;/code&gt; module will generate physical execution plans for execution on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Gaia&lt;/code&gt; component, a dataflow engine that automatically parallelizes queries to reduce execution time.&lt;/p&gt;

&lt;p&gt;To support graph neural networks, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Graph-Learn&lt;/code&gt; module handles graph sampling operations, offering support for both CPU and GPU. On the GPU platform, it incorporates an efficient caching mechanism to further accelerate the graph sampling speed. Additionally, the back-end tensor execution module supports both &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PyTorch&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TensorFlow&lt;/code&gt;, allowing users to choose according to their needs.&lt;/p&gt;

&lt;h4 id=&quot;frontend-layer&quot;&gt;Frontend Layer&lt;/h4&gt;
&lt;p&gt;The frontend layer includes a rich set of algorithm packages and provides various SDKs and APIs for external services. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Builtin Apps&lt;/code&gt; module includes common graph analysis algorithms, such as PageRank and shortest path. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GNN Models&lt;/code&gt; module encompasses popular graph neural network models like GCN, PinSAGE, and GraphSAGE. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Cypher&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Gremlin&lt;/code&gt; modules allow users to perform graph query operations directly using Cypher and Gremlin query languages. Furthermore, the application layer exposes RESTful/WebSocket APIs, making it easy for users to integrate GraphScope Flex with other systems, and provides C++, Python, and Java SDKs to facilitate the development of custom graph algorithm applications.&lt;/p&gt;

&lt;h3 id=&quot;real-world-use-cases-of-graphscope-flex&quot;&gt;Real-World Use Cases of GraphScope Flex&lt;/h3&gt;

&lt;h4 id=&quot;company-equity-analysis&quot;&gt;Company Equity Analysis&lt;/h4&gt;
&lt;p&gt;In the task of company equity analysis, we need to identify the ultimate controller of a company, i.e., the individual who controls more than 50% of the company’s equity. An individual may exert influence through multiple layers of companies, so we need to start from a particular individual and calculate both the equity they directly hold and the equity held through other companies. While this process can be expressed and executed using SQL on a relational database, it is often very inefficient. For instance, on a real dataset containing 300 million individuals/companies and 1.5 billion equity holding records, it can take over an hour in a relational database to compute the desired results without yielding any output.&lt;/p&gt;

&lt;p&gt;Given the interconnected nature of equity structures, we can transform this problem into a graph analytics task. First, we need to convert the equity relationships into a graph data structure, where the vertices represent individuals or companies, and the edges represent the equity holding relationships.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/equity-analysis.jpg&quot; alt=&quot;equity&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We then select the components illustrated in the diagram below to build a graph analytical system for completing the company equity analysis task.
To achieve higher performance, we choose the in-memory graph storage, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Vineyard&lt;/code&gt;, for the storage layer.
We select &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GRAPE&lt;/code&gt; as the execution engine for the graph analysis algorithms in the wxecution engine layer.
Users can write the equity analysis logic using the SDK, leveraging the Pregel/PIE interfaces exposed by GRAPE in the frontend layer.
With this configuration, GraphScope Flex can process the same dataset and obtain results in just 15 minutes, demonstrating a significant improvement in efficiency compared to traditional methods. This showcases the flexibility and power of GraphScope Flex in handling complex analysis tasks effectively.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/equity-arch.jpg&quot; alt=&quot;equity-arch&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;real-time-fraud-detection&quot;&gt;Real-Time Fraud Detection&lt;/h4&gt;

&lt;p&gt;E-commerce platforms need to perform real-time checks on each order to determine whether it involves fraudulent activities such as order brushing. As shown in the image below, the e-commerce platform has marked a portion of accounts as fraudulent accounts (fraud seeds). We can consider accounts that frequently have a “co-purchase” relationship with these fraudulent accounts as highly suspicious fraudulent accounts as well.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/fraud-graph.jpg&quot; alt=&quot;fraud-graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To express the “co-purchase” relationship mentioned above, we can use the following Cypher statement. Therefore, this fraud detection issue is well suited to be transformed into a graph computing problem.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;MATCH (v:Account{id:1})-[b1:BUY]-&amp;gt;(:Item)&amp;lt;-[b2:BUY]-(s:Account)
WHERE s.id IN SEEDS AND b1.date-b2.date &amp;lt; 5 /*within 5 days*/
WITH v, COUNT(s) AS cnt1
MATCH (v)-[:KNOWS]-(f:Account), (f)-[b1:BUY]-&amp;gt;(:Item)&amp;lt;-[b2:BUY]-(s:
          Account) WHERE s.id IN SEEDS WITH v, cnt1, COUNT(s) AS cnt2
WHERE w1 * cnt1 + w2 * cnt2 &amp;gt; threshold
RETURN v
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To achieve real-time fraud detection, we can select the components shown in the diagram to construct a graph computing system. Considering that order data is continuously arriving, we have chosen the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GART&lt;/code&gt; component, which supports multi-version variable memory graph storage.
At the execution engine layer, since we need to handle a large volume of orders in a short time, the core metric we need to focus on is system throughput. Therefore, we have selected the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Hiactor Codegen&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Hiactor&lt;/code&gt; components, along with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GraphIR&lt;/code&gt; component and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Graph Query Optimizer&lt;/code&gt; component, to receive queries written in Cypher from the frontend layer.
With this deployment plan on real datasets, we can achieve performance exceeding 350,000 QPS.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/fraud-arch.jpg&quot; alt=&quot;fraud-arch&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;friendship-relationship-prediction&quot;&gt;Friendship Relationship Prediction&lt;/h4&gt;

&lt;p&gt;When e-commerce platforms make product recommendations, a very important strategy is to recommend products liked by a user’s friends. However, the friendship relationship data among users on e-commerce platforms is often incomplete, so it is necessary to predict whether a friendship exists between two users. Since graphs can naturally depict the relationships among users, this issue can also be transformed into a graph computing problem. Currently, the industry commonly uses graph neural networks to handle graph-based friendship relationship prediction tasks.&lt;/p&gt;

&lt;p&gt;Based on e-commerce data, we can first construct a graph as shown in the diagram, where each vertex represents users, products, comments, etc., and edges represent friendships, purchases, comments, etc. In graph neural network models, such as those represented by the NCN algorithm, calculations like “common friends” need to be performed on the graph, as two users with many common friends are more likely to have a friendship.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/rec-graph.jpg&quot; alt=&quot;rec-graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To deploy a system for friendship relationship prediction based on GraphScope Flex, we can select the components shown in the diagram from GraphScope Flex. In this setup, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Vineyard&lt;/code&gt; component in the storage layer models the data as a property graph and stores it in memory. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Graph-Learn&lt;/code&gt; component is responsible for calculating common friends, neighbor sampling, and other operations, while the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PyTorch&lt;/code&gt; module is used for graph neural network inference. At the frontend layer, users can invoke the inference service through the Python SDK.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/rec-arch.jpg&quot; alt=&quot;rec-arch&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;high-performance-and-continuous-iteration&quot;&gt;High Performance and Continuous Iteration&lt;/h3&gt;

&lt;p&gt;Thanks to the modular design of GraphScope Flex, it has achieved efficient performance across a variety of workloads. Notably, in the internationally recognized benchmark for transactional online query scenarios, LDBC-SNB, GraphScope Flex achieved &lt;a href=&quot;https://graphscope.io/blog/tech/2024/06/27/GraphScope-refreshes-the-world-record-for-the-LDBC-benchmark&quot;&gt;a performance that surpasses the previous record holder by 2.6 times&lt;/a&gt;, securing the top position.
We will also publish articles in the future to provide detailed information about the design of each module, so stay tuned!&lt;/p&gt;

</description>
        <pubDate>Mon, 05 Aug 2024 01:00:00 +0000</pubDate>
        <link>https://graphscope.io/blog/tech/2024/08/05/GraphScope-Flex-LEGO-like-Graph-Computing-Stack.html</link>
        <guid isPermaLink="true">https://graphscope.io/blog/tech/2024/08/05/GraphScope-Flex-LEGO-like-Graph-Computing-Stack.html</guid>
        
        
        <category>Tech</category>
        
      </item>
    
      <item>
        <title>GraphScope&apos;s Perspective Sharing at the SIGMOD 2024 Panel: The Future of Graph Analytics</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/assets/images/sigmod-panel.png&quot; alt=&quot;benchmark&quot; /&gt;
From June 9th to June 15th, 2024, SIGMOD 2024 was held in Santiago, Chile. 
Graph computing remains a hot topic at this conference, and it is also the area that received the most paper submissions. 
SIGMOD 2024 organized a panel discussion on graph computation titled “The Future of Graph Analytics”.&lt;/p&gt;

&lt;p&gt;This panel invited six well-known individuals from the academic and industrial sectors of graph computing, including Professor &lt;a href=&quot;https://perso.liris.cnrs.fr/angela.bonifati/&quot;&gt;Angela Bonifati&lt;/a&gt; from Lyon 1 University, Professor &lt;a href=&quot;https://cs.uwaterloo.ca/~tozsu/&quot;&gt;M. Tamer Özsu&lt;/a&gt; from the University of Waterloo, &lt;a href=&quot;https://humming80.github.io/&quot;&gt;Yuanyuan Tian&lt;/a&gt;, a Principal Scientist Manager from Microsoft, &lt;a href=&quot;https://de.linkedin.com/in/hannesvoigt&quot;&gt;Hannes Voigt&lt;/a&gt;, a Staff Engineer from Neo4j, Professor &lt;a href=&quot;https://research.unsw.edu.au/people/professor-wenjie-zhang&quot;&gt;Wenjie Zhang&lt;/a&gt; from the University of New South Wales, and Dr. &lt;a href=&quot;https://wyu.io/&quot;&gt;Wenyuan Yu&lt;/a&gt;, the person in charge of GraphScope.&lt;/p&gt;

&lt;p&gt;Graph computing plays an important role in real-world business scenarios, and various graph computing systems are emerging one after another. Nowadays, the types of graph computing tasks are becoming more diverse, and they form complex workflows together with other task types, such as machine learning and Large Language Model (LLM) tasks. This also leads to new discussions and considerations regarding graph computing. 
The panel discussed the following six issues, and Dr. Wenyuan Yu shared GraphScope’s perspectives on these matters.&lt;/p&gt;

&lt;h5 id=&quot;q-given-the-varied-types-of-graph-computations-and-the-widespread-use-of-graph-query-languages-eg-cypher-gql-do-we-need-new-and-more-expressive-query-languages&quot;&gt;Q: Given the varied types of graph computations and the widespread use of graph query languages (e.g., Cypher, GQL), do we need new and more expressive query languages?&lt;/h5&gt;

&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: Considering the diverse and extensive applications of graphs, there is still a need for new query languages. However, languages with strong expressive power often come with increased complexity, so finding a balance between complexity and expressiveness is essential. The computing patterns for different types of graph computing tasks, such as pattern matching, graph analysis, and graph neural networks, exhibit both significant differences and some overlaps, making the balance challenging to achieve.&lt;/p&gt;

&lt;p&gt;Over the past two years, the development of Large Language Models (LLMs) has inspired the idea that natural language processing enhanced by LLMs could play a vital role in simplifying complex queries and reducing the learning curve for users.&lt;/p&gt;

&lt;h5 id=&quot;q-in-many-cases-graph-data-originates-from-oltp-systems-while-graph-analysis-is-a-typical-olap-operation-do-we-therefore-need-to-build-an-htap-system-for-graph-analysis&quot;&gt;Q: In many cases, graph data originates from OLTP systems, while graph analysis is a typical OLAP operation. Do we, therefore, need to build an HTAP system for graph analysis?&lt;/h5&gt;

&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: Regarding the necessity of building an HTAP system for graph analysis, we believe it is essential. However, it is not necessary to process OLTP and OLAP tasks within the same system simultaneously. On the one hand, users’ core businesses often rely on very mature OLTP systems, such as relational databases, and it may be challenging for users to migrate their core businesses to a new system simply to add OLAP capabilities for graph analysis. On the other hand, performing OLAP tasks directly on the storage systems designed for OLTP can be highly inefficient.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/gart.jpg&quot; alt=&quot;GART&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To address this, we developed and open-sourced a system called &lt;a href=&quot;https://github.com/GraphScope/GART&quot;&gt;GART&lt;/a&gt;. Its core architecture, as depicted in the diagram above, captures changes from OLTP systems (such as binlog from MySQL) and synchronizes these changes in real-time. The changes to table data in OLTP systems correspond to operations of adding, deleting, or modifying vertices and edges in graph data. GART has also been designed with efficient mutable graph storage and is integrated with the GraphScope’s graph analytical engine GAE and the graph interactive engine GIE. This allows users to intuitively express queries with graph semantics and achieve efficient execution on GraphScope’s engine. Consequently, graph HTAP capabilities are achieved without any modifications to the existing OLTP system.&lt;/p&gt;

&lt;h5 id=&quot;q-currently-benchmarks-for-graph-analysis-focus-more-on-system-performance-metrics-such-as-the-execution-of-algorithms-should-we-also-focus-on-other-metrics&quot;&gt;Q: Currently, benchmarks for graph analysis focus more on system performance metrics such as the execution of algorithms. Should we also focus on other metrics?&lt;/h5&gt;

&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: In addition to performance metrics, we believe that benchmarks for graph analysis should consider two additional aspects. Firstly, concerning the graphs themselves, it is important to increase the diversity of graph algorithms and datasets. The range of graph algorithms should extend beyond well-known examples like PageRank and Shortest Path. New algorithms need to exhibit a variety of graph access patterns and computational characteristics. In addition, benchmarks should include varied types of datasets beyond just social network datasets.
Secondly, when evaluating a system, we should assess the ease with which an algorithm can be implemented within that system, as this reflects the expressive power of the system’s interface. In this context, we have collaborated with external universities to propose &lt;a href=&quot;https://arxiv.org/abs/2404.06037&quot;&gt;a new benchmark for graph analysis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Moreover, graph computations may represent only a small component of a complex workflow. Therefore, when testing a system, it is insufficient to just consider the execution time of a particular algorithm; we should also evaluate the time required for the system to load the graph from common file formats and export the results in other formats.&lt;/p&gt;

&lt;h5 id=&quot;q-in-many-cases-a-complete-graph-computation-task-includes-multiple-types-of-sub-tasks-so-is-one-graph-storage-sufficient-in-this-case-do-we-need-multiple-graph-storages&quot;&gt;Q: In many cases, a complete graph computation task includes multiple types of sub-tasks. So, is one graph storage sufficient? In this case, do we need multiple graph storages?&lt;/h5&gt;

&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: Regarding whether multiple graph storages are needed, we believe they are necessary. Currently, various graph storages are designed for different scenarios and optimization objectives, and it is difficult to have a “one size fits all” solution. However, the existence of multiple storages means that when a task requires the use of multiple graph computing systems, data exchanges must be made between different systems to enable interfacing. To address this issue, GraphScope has taken the following two steps.&lt;/p&gt;

&lt;p&gt;First, we designed a standardized graph storage file format, &lt;a href=&quot;https://graphscope.io/blog/tech/2023/08/29/GraphAr-A-Standard-Data-File-Format-for-Graph-Data-Storage-and-Retrieval&quot;&gt;GraphAr&lt;/a&gt;, which defines a set of computing/storage system-independent file formats for graph data and provides a series of interfaces to generate, access, and convert these formatted files. As illustrated in the diagram below, &lt;a href=&quot;https://github.com/apache/incubator-graphar&quot;&gt;GraphAr&lt;/a&gt; aims to solve the problem of data import/export and mutual access between various graph computing systems. GraphAr is now an Apache Incubator project and is actively promoting integration with mainstream graph systems in the industry.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2023-08-29-graphar-overview.jpg&quot; alt=&quot;graphar-overview&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the other hand, we designed &lt;a href=&quot;https://github.com/GraphScope/GRIN&quot;&gt;GRIN&lt;/a&gt;, a unified graph access interface, which defines a series of standardized interfaces for accessing graph data. Storages must implement these interface methods, and computing engines only need to use these interfaces to execute operators, thereby facilitating easy docking between different engines and storages.&lt;/p&gt;

&lt;h5 id=&quot;q-in-a-complex-workflow-graph-analysis-may-be-just-one-component-alongside-data-analysis-machine-learning-llms-and-other-types-of-tasks-in-this-scenario-do-we-need-new-apisdsls-to-facilitate-interaction-between-graph-related-tasks-and-other-tasks&quot;&gt;Q: In a complex workflow, graph analysis may be just one component, alongside data analysis, machine learning, LLMs, and other types of tasks. In this scenario, do we need new APIs/DSLs to facilitate interaction between graph-related tasks and other tasks?&lt;/h5&gt;

&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: This question should be considered from two perspectives. On one hand, graphs are adept at intuitively expressing relationships between entities, which allows graph data to be utilized as part of knowledge graphs within RAG (Retrieval-Augmented Generation), interacting seamlessly with LLMs. Additionally, graph neural networks, which represent a fundamental aspect of machine learning, naturally integrate graph tasks with machine learning tasks.&lt;/p&gt;

&lt;p&gt;On the other hand, the input and output formats of tasks involving graphs differ from those of tasks involving LLMs. To enable seamless interoperability between graph computation tasks and LLM tasks, we may need to consider the development of an ETL (Extract, Transform, Load) DSL in the future. Such a DSL would assist in resolving data alignment challenges that arise when integrating different types of tasks.&lt;/p&gt;

&lt;h5 id=&quot;q-in-some-scenarios-graph-data-is-not-static-but-constantly-changing-what-are-the-expectations-for-dynamic-incremental-and-stream-graph-analysis-operations&quot;&gt;Q: In some scenarios, graph data is not static but constantly changing. What are the expectations for dynamic, incremental, and stream graph analysis operations?&lt;/h5&gt;

&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: For dynamic graph scenarios, we believe that compared to static graphs, the applications may be more diverse. The current hot topics in dynamic graph research include dynamic, incremental, and stream graph processing, with complex application scenarios requiring the integration of these technologies or even the advancement beyond current capabilities. To apply dynamic graphs effectively to real-world scenarios, we need to work on the following aspects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In terms of storage, efficient dynamic/stream graph storage capabilities are essential. These capabilities should not only support graph update operations efficiently but also provide efficient graph data access interfaces.&lt;/li&gt;
  &lt;li&gt;Concerning graph algorithms, it is necessary to research incremental graph algorithms and explore the use of previously computed results to efficiently calculate the required outcomes on newly updated graphs.&lt;/li&gt;
  &lt;li&gt;The time dimension is an essential aspect of dynamic graphs, and we need to enhance support for time-series graphs in our analysis operations.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 16 Jul 2024 01:00:00 +0000</pubDate>
        <link>https://graphscope.io/blog/tech/2024/07/16/GraphScope-Perspective-Sharing-at-SIGMOD-2024-Panel.html</link>
        <guid isPermaLink="true">https://graphscope.io/blog/tech/2024/07/16/GraphScope-Perspective-Sharing-at-SIGMOD-2024-Panel.html</guid>
        
        
        <category>Tech</category>
        
      </item>
    
      <item>
        <title>GraphScope Refreshes the World Record for the LDBC SNB Benchmark in both Performance and Data Scale</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/assets/images/ldbc-news-title.jpg&quot; alt=&quot;benchmark&quot; /&gt;
Recently, &lt;a href=&quot;https://ldbcouncil.org&quot;&gt;LDBC&lt;/a&gt; released the &lt;a href=&quot;https://ldbcouncil.org/benchmarks/snb/&quot;&gt;latest results for the LDBC SNB Interactive benchmark test&lt;/a&gt;, where GraphScope Flex leads the pack once again with a score exceeding 127,000 QPS (Queries Per Second), representing a more than 2.6 times improvement over the second place, which was the previous record holder!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2024-06-27-benchmark.jpg&quot; alt=&quot;Benchmark screenshot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In addition, GraphScope Flex has updated the maximum data scale for the benchmark test, being the only system among all the participants that used the SF1000 dataset for the test. The SF1000 dataset, generated with the LDBC’s official data generator, has a Scale Factor (SF) of 1000. This dataset contains approximately 2.9 billion vertices and 208 billion edges, with the original disk data file size around 1TB, making it an extremely large-scale graph. Previously, the maximum any system participating in the LDBC SNB Interactive had been tested was SF300, which is about a third of the scale of SF1000.&lt;/p&gt;

&lt;p&gt;LDBC is an internationally recognized authority in the field of graph computing, developing and leading a series of graph benchmarks, including the Social Network Benchmark (SNB), the Graphalytics Benchmark, and the Financial Benchmark (FinBench), to measure the capabilities and performance of various types of graph systems. The SNB Interactive benchmark test that GraphScope Flex participated in simulates a social network graph of Facebook at different scales, executing operations including basic Create, Read, Update, Delete (CRUD) as well as complex tasks like shortest paths and multi-hop queries, covering commonly used operations and queries in transactional online query scenarios and is recognized as an authoritative benchmark in the field of graph data.
During the test process, a driver continuously sends a variety of query requests to the system being tested at high speed, and the system’s performance is measured by its throughput (QPS) in handling queries. The testing procedure is professional and rigorous, with systems submitting their codes to be tested and their scores verified by LDBC’s professional staff.&lt;/p&gt;

&lt;p&gt;In recent years, with the collective efforts of R&amp;amp;D personnel in the graph database industry, the performance of graph computing has continuously improved. GraphScope Flex participated in the LDBC SNB Interactive benchmark test for the first time in July 2023, achieving more than a 2.45 times performance improvement over the then world record holder, &lt;a href=&quot;https://www.tugraph.org/&quot;&gt;TuGraph&lt;/a&gt; (developed by Ant Group). Then in December 2023, &lt;a href=&quot;https://atlasgraph.io/en/&quot;&gt;AtlasGraph&lt;/a&gt; took the lead with a performance 45% higher than that of GraphScope Flex. One year later, GraphScope Flex has once again won the top spot, achieving approximately 2.6 times the performance of AtlasGraph and testing on the larger data scale SF1000 for the first time. This performance is due to a series of technical upgrades in GraphScope Flex over the year:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A more compact graph structure and more efficient graph storage, allowing larger graphs to be loaded into limited memory with higher access efficiency;&lt;/li&gt;
  &lt;li&gt;Further optimized graph queries, with the help of auxiliary data, online cost analysis is utilized to choose the optimal execution sequence, reducing the overhead of queries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will provide a detailed analysis of the technical optimizations used in the version of GraphScope Flex for this benchmark test. Stay tuned!&lt;/p&gt;
</description>
        <pubDate>Thu, 27 Jun 2024 00:00:00 +0000</pubDate>
        <link>https://graphscope.io/blog/tech/2024/06/27/GraphScope-refreshes-the-world-record-for-the-LDBC-benchmark.html</link>
        <guid isPermaLink="true">https://graphscope.io/blog/tech/2024/06/27/GraphScope-refreshes-the-world-record-for-the-LDBC-benchmark.html</guid>
        
        
        <category>Tech</category>
        
      </item>
    
      <item>
        <title>GOpt: A Unified Graph Query Optimization Framework in GraphScope</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2024-02-22-title-picture.jpg&quot; alt=&quot;gopt&quot; /&gt;
In this blog, we introduce &lt;a href=&quot;https://arxiv.org/pdf/2401.17786.pdf&quot;&gt;&lt;strong&gt;GOpt&lt;/strong&gt;&lt;/a&gt;, which is a unified graph query optimization framework in GraphScope.
GOpt enables the system to support multiple graph query languages while providing consistent and efficient query optimization. We also present two practical cases to demonstrate the effectiveness of our optimizer.&lt;/p&gt;

&lt;h3 id=&quot;background-and-challenges&quot;&gt;Background and Challenges&lt;/h3&gt;

&lt;p&gt;In real applications, there has been a growing interest in integrating graph query semantics with relational query semantics to support more complex queries. A typical querying paradigm, termed as PatRelQuery, involves a two-stage process: first, identifying subgraphs of interest through pattern matching, and subsequently, performing relational operations, such as projection, selection and ordering, on the matched results for further analysis.
Below, we present an example of PatRelQuery, composed using Cypher, which is one of the most widely adopted graph query languages.
&lt;img src=&quot;/blog/assets/images/2024-02-22-example.jpg&quot; alt=&quot;example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As depicted in Figure 1(a), we consider a graph database comprising vertex types of “Person”, “Product”, and “Place”, alongside edge types of “Purchases”, “LocatedIn”, “ProducedIn”. The PatRelQuery shown in Figure 1(b) begins by identifying a triangle pattern. This pattern imposes a type constraint on vertex $v_3$, designating it as a “Place” as specified in the MATCH clause. Subsequent relational operations are then applied to the results of this pattern match, including filtering based on a specific place name and aggregating the results to return the top 10 entries. These steps are articulated through the WHERE, COUNT, ORDERBY, and LIMIT clauses. Notice that in this query only $v_3$ is explicitly assigned a type constraint, while the other vertices and edges are not. Consequently, these untyped vertices and edges are treated as with arbitrary types. From this example, we distill two primary features for PatRelQuery:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Hybrid Semantics&lt;/strong&gt;: PatRelQuery melds graph pattern matching with relational operations, showcasing a high level of expressiveness.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Arbitrary Types&lt;/strong&gt;: PatRelQuery allows arbitrary type specification for a greater degree of flexibility in pattern descriptions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, such a flexible query representation poses significant challenges for the graph optimizer:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The optimizer needs to support both graph pattern matching and relational operators within the same framework and perform unified optimization on it.&lt;/li&gt;
  &lt;li&gt;The optimizer must identify implicit type constraints within the graph patterns. For instance, in the above example, only the type constraints shown in Figure 1(c) are valid, whereas the combination of type constraints in Figure 1(d) is actually invalid.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To address the above challenges, we propose &lt;strong&gt;GOpt&lt;/strong&gt;, a graph-native optimization framework for PatRelQuery. We summarize the main contributions as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We have introduced GOpt, which, as far as we know, is the first graph-native optimization framework specifically designed for industrial-scale graph database systems. This framework integrates both graph and relational operations, offering a unified approach to query optimization in complex PatRelQueries across various graph query languages.&lt;/li&gt;
  &lt;li&gt;We have designed algorithms for efficient type inference in the context of arbitrary patterns. Our approach also includes a comprehensive set of heuristic optimization rules. Moreover, we have proposed a novel cardinality estimation technique that takes into account the arbitrary types. Building upon this, a cost-based optimizer has been developed, operating within a top-down framework with branch-and-bounding strategies.&lt;/li&gt;
  &lt;li&gt;We have implemented GOpt atop the open-source optimization framework &lt;a href=&quot;https://calcite.apache.org/&quot;&gt;Apache Calcite&lt;/a&gt;, inheriting its capabilities for optimizing relational operations. This integration has been smoothly executed to include our specialized graph-optimization techniques. The effectiveness of our proposed techniques has been validated through comprehensive experimental evaluations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;system-overview&quot;&gt;System Overview&lt;/h3&gt;

&lt;p&gt;First, we provide an overview of the system architecture of GOpt, as illustrated in Figure 2.
&lt;img src=&quot;/blog/assets/images/2024-02-22-arch.jpg&quot; alt=&quot;arch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The overall GOpt system is composed of four principal components:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parser&lt;/strong&gt; plays a pivotal role in transforming user queries into a &lt;em&gt;language-independent&lt;/em&gt; query plan based on a unified intermediate representation (IR). It accommodates various clients for different graph query languages, such as Gremlin and Cypher. The parser enables the decouple of the query language and the query optimization framework, which allows the system to support multiple query languages with the extensive reuse of techniques developed within GOpt.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Type Checker&lt;/strong&gt; is responsible for inferring and validating the type constraints in the query pattern against the graph schema. We allow users to provide arbitrary type constraints for vertices and edges in their query patterns, and even allow them not to specify any type constraints at all. The type checker is responsible for inferring implicit type constraints within the user’s query based on the graph schema. At the same time, the type checker will also promptly report INVALID errors for queries that are not valid.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Optimizer&lt;/strong&gt; aims to optimize the logical plan to derive the most efficient physical plan. It employs a combination of Rule-based Optimization (RBO) and Cost-based Optimization (CBO). The RBO consists of a set of rules heuristically applied to the logical plan to produce a more efficient equivalent. The CBO consists of two phases: First, to optimize pattern matching, we devise a top-down search algorithm with branch-and-bounding strategies, to identify the optimal plan for arbitrary pattern. Secondly, to refine the relational part, we integrate the proposed graph optimizer with the optimization framework Apache Calcite, leveraging its advanced optimization capabilities for relational queries.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Metadata Provider&lt;/strong&gt; consists of two parts:  the graph schema which defines the vertex and edge types within the data graph and assists the Type Checker in deducing and affirming type constraints within the query plan, and the statistics provider known as GLogue, which precomputed the frequencies of certain small patterns in the data graph to serve as high-order statistical information and provides this high-order statistics for more accurate cost estimation in the optimization phase.&lt;/p&gt;

&lt;h3 id=&quot;query-processing-and-optimization&quot;&gt;Query Processing and Optimization&lt;/h3&gt;

&lt;p&gt;Next, we present the workflow of query processing and optimization within GOpt as depicted in Figure 3, and we will delve into the details in the following.
&lt;img src=&quot;/blog/assets/images/2024-02-22-workflow.jpg&quot; alt=&quot;arch&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;query-processing&quot;&gt;Query Processing&lt;/h4&gt;

&lt;p&gt;First, we proposed a unified &lt;em&gt;Intermediate Representation&lt;/em&gt; (IR) framework as a foundation to process PatRelQueries. The IR abstraction defines a data model $\mathcal{D}$ that describes the structure of the intermediate results during query execution,and a set of operators $\Omega$. The data model $\mathcal{D}$ presents a schema-like structure in which each data field possesses a name, denoted as a String type, accompanied by a designated datatype. The supported datatypes includes graph-specific datatypes such as &lt;em&gt;Vertex&lt;/em&gt;, &lt;em&gt;Edge&lt;/em&gt;, and &lt;em&gt;Path&lt;/em&gt;, and general datatypes including &lt;em&gt;Primitives&lt;/em&gt; and &lt;em&gt;Collections&lt;/em&gt;. The operators in $\Omega$ operate on data tuples extracted from $\mathcal{D}$, and produce a new set of data tuples as a result. The set $\Omega$ is composed of graph operators such as &lt;em&gt;GetV&lt;/em&gt;, &lt;em&gt;EdgeExpand&lt;/em&gt; and &lt;em&gt;PathExpand&lt;/em&gt;, and relational operators such as &lt;em&gt;Project&lt;/em&gt;, &lt;em&gt;Select&lt;/em&gt;, &lt;em&gt;Join&lt;/em&gt;, etc.&lt;/p&gt;

&lt;p&gt;The IR abstraction enables the opportunity to convert various query languages into a unified form. Currently, GOpt supports two of the most widely used graph query languages, &lt;a href=&quot;https://neo4j.com/developer/cypher/&quot;&gt;Cypher&lt;/a&gt; and &lt;a href=&quot;https://tinkerpop.apache.org/&quot;&gt;Gremlin&lt;/a&gt;. We employs the parser tool provided by &lt;a href=&quot;http://www.antlr.org/&quot;&gt;Antlr&lt;/a&gt; to interpret the queries into an Abstract Syntax Tree (AST), based on which we further build a logical DAG that each node corresponds to an operator defined in IR. As shown in Figure 3, the queries written in Cypher and Gremlin respectively can be parsed in to a unified DAG shown in Figure 3(c).&lt;/p&gt;

&lt;p&gt;Notice that in the Match part in Figure 3(c), the user has not provided explicit type constraints for vertices $v_1$ and $v_2$. However, based on the graph schema, given that $v_3$ is a “Place”, we can infer that $v_1$ and $v_2$ could be either “Person” Or “Product”. Similarly, considering the edge ($v_1$, $v_2$), the type of $v_1$ can be further refined as “Person”. Through type inference, we can obtain a graph pattern that only includes valid types as shown in Figure 3(d).&lt;/p&gt;

&lt;p&gt;Building on the DAG with valid types, we further optimize it to obtain the optimal physical plan shown in Figure 3(e), which is then submitted to the backend for execution. Next, we will introduce the process of query optimization.&lt;/p&gt;

&lt;h4 id=&quot;query-optimization&quot;&gt;Query Optimization&lt;/h4&gt;

&lt;p&gt;The optimizer within GOpt comprises two parts: Rule-Based Optimization (RBO) and Cost-Based Optimization (CBO). The optimization process is illustrated in Figure 4.
&lt;img src=&quot;/blog/assets/images/2024-02-22-optimize.jpg&quot; alt=&quot;optimize&quot; /&gt;
To optimize PatRelQueries, we devised a comprehensive set of rules, taking into account potential optimization opportunities among graph operators, relational operators, and between the two. For example, as shown in Figure 4, since only the name attribute of $v_3$ is needed for filtering, we use the &lt;strong&gt;FieldTrimRule&lt;/strong&gt; (a relational optimization) to eliminate other unnecessary attributes, avoiding the retention of irrelevant data during computation. Besides, pattern matching often requires matching adjacent edges, and then further matching neighboring vertices through those edges. Under certain conditions, this can be optimized by the &lt;strong&gt;ExpandGetVFusionRule&lt;/strong&gt; (a graph operator optimization), which combines the two operations into a direct neighboring vertex match. In this query, we also see that after performing pattern matching, the user further selects results that meet certain conditions through a Select operation. We can apply the &lt;strong&gt;FilterIntoMatchRule&lt;/strong&gt; (a graph-relational operator optimization) to push the filter conditions directly into the graph operators, ensuring that only results that meet the filter conditions are matched during the pattern matching process.&lt;/p&gt;

&lt;p&gt;Next, we will demonstrate the cost-based optimization techniques within GOpt, focusing mainly on the optimization of graph patterns, which is often the most crucial part of the optimization process. Drawing from the capabilities provided by our previous work &lt;a href=&quot;https://www.usenix.org/conference/atc23/presentation/lai&quot;&gt;GLogS&lt;/a&gt;, we have designed effective graph pattern matching optimization techniques:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hybrid implementation strategies for pattern matching. For the physical implementation of graph pattern matching, we consider both the Worst-Case Optimal Join (WCOJ) and the traditional BinaryJoin methods, where the recent research has shown that such hybrid implementation strategies can execute pattern matching queries more efficiently.&lt;/li&gt;
  &lt;li&gt;Cost estimation based on high-order statistics. The cost estimation in our optimization uses high-order statistics (i.e., the frequencies of occurrences of small patterns, also known as motifs, in the data graph) as a basis, providing a more accurate estimate of the cost of the queried pattern. However, it should be noticed that the precomputed statistics are only for the motifs that contains basic types (i.e., the types defined in schema).&lt;/li&gt;
  &lt;li&gt;Support for arbitrary type constraints during the optimization process. For graph patterns provided by users, arbitrary type constraints may appear, and in such cases, we cannot directly query statistical information to estimate their costs. Therefore, we propose a new estimation method that considers only the change in the number of matches brought about by expanding one vertex at a time. Thus, we estimate the frequency of a pattern based on its subpatterns’ frequencies iteratively, until the subpattern can be queried from GLogue directly, or it is a single vertex or edge.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Based on the three key techniques, we propose a top-down, cost-based framework for optimizing arbitrary graph pattern matching, aiming to find the optimal execution plan for the arbitrary query patterns.&lt;/p&gt;

&lt;h3 id=&quot;case-study&quot;&gt;Case Study&lt;/h3&gt;

&lt;p&gt;We validated the effectiveness of GOpt through various experiments. Here, we demonstrate the role of GOpt in the entire query optimization process using two real-world cases.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Money Mule Detection.&lt;/strong&gt; In fraud detection, one of the most common fraud patterns is the &lt;em&gt;money mule&lt;/em&gt; pattern, where a fraudster transfers money to a money mule, who then transfers the money to another money mule, and so on, until the money is withdrawn by another fraudster. This can be formulated as a s-t path problem in PatRelQuery: Given two sets of fraudsters $S_1$ and $S_2$ and the hop number $k$, we aim to find all the money transformation paths between the fraudsters in $S_1$ and $S_2$ with the specified hop number. The query written in Cypher is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-Cypher&quot;&gt;  MATCH (p1:PERSON)-[p:*$k]-(p2:PERSON) 
  WHERE p1.id IN $S1 and p2.id IN $S2
  RETURN p
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Searching for such paths in a large-scale graph is challenging, as it may involve a large number of intermediate results. Two most commonly used approaches is as follows: single-direction expansion, which starts the traversal from vertices in $S_1$, expanding $k$ hops, and then applying a filter to ensure that the end vertices are contained within the set $S_2$; and bidirectional search, which begins traversal from $S_1$ and $S_2$ simultaneously, and when the sub-paths meet in the middle, join them to make result paths. In most cases, the second approach should be more efficient as it can reduce the number of intermediate results.
However, the question is that, is the middle vertex in the path always the best choice for the join? Our case study shows that it is not always true. We have conducted experiments on a real-world graph with $3.6$ billion vertices and $21.8$ billion edges, with the hop number $k$ set to $6$. We obtain five different settings randomly for the source fraudster sets $(S_1,S_2)$ from the real application, each corresponds to different fraudster groups, and the five queries are denoted as $ST_1\ldots ST_5$. For the pattern shown in Figure 5(a), GOpt applies the cost-based optimization to derive the optimal execution plans for $ST_1\ldots ST_5$, as shown in Figure 5(b-d). We notice that for queries with different settings, GOpt may generate different optimal execution plans. This is because the cost model takes into account of the number of intermediate results, which is not only affected by the expanding hops, but also affected by the number of matched vertices in the source fraudster sets.
&lt;img src=&quot;/blog/assets/images/2024-02-22-case-study.jpg&quot; alt=&quot;case-study&quot; /&gt;
To verify the efficiency, we further generate three alternative plans with different search order for each query as a comparison. The execution time cost of different plans are shown in Figure 6. Here, the tuples above the bars indicate position of the join vertex, e.g., for $ST_1$, the Alt-Plan1 has join vertex in the middle. From the figure, we can see that the plan generated by GOpt outperforms all the alternative plans from $3\times$ to two orders of magnitude, where all the single-direction expansion plans fails to complete the query in 1 hour. Notice that for $ST_1$ and $ST_2$, the plan generated by GOpt outperforms the alternatives that have the join vertex in the middle. This demonstrate the effectiveness of GOpt, that it is able to find the optimal plan adaptively according to the query and the data distribution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LDBC Interactive Complex Queries.&lt;/strong&gt; We conducted extensive experiments on the &lt;a href=&quot;https://ldbcouncil.org/&quot;&gt;LDBC SNB Interactive Workload&lt;/a&gt; to verify the effectiveness of GOpt. Here, we use the IC3 query as an example to compare the execution efficiency of GOpt and Neo4j, with the results shown in the table below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Execution Plan&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Avg. Runtime(s)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Intermediate Result Num.&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;GOpt Optimized Plan&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.085&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1,784,536&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Neo4j Optimized Plan&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;156.845&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;176,547,616&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The result shows that the execution time of GOpt’s plan is approximately 26 times faster than that of Neo4j’s plan. Additionally, GOpt’s plan produces a mere $1\%$ of the intermediate results generated by Neo4j’s plan, demonstrating a significant reduction in processing overhead. It is evident that, compared to Neo4j, GOpt is able to obtain more efficient execution plans.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;This article showcases the optimization framework GOpt in GraphScope, which integrates fragmented graph query optimizations into a unified framework. It offers a cohesive approach to optimizing complex PatRelQueries in real-world applications. For detailed techniques, readers can refer to our &lt;a href=&quot;https://arxiv.org/abs/2401.17786&quot;&gt;original paper&lt;/a&gt;. The unified optimization framework GOpt is also gradually being integrated into the main repository of GraphScope, and we welcome interested readers to stay tuned for updates.&lt;/p&gt;
</description>
        <pubDate>Thu, 22 Feb 2024 23:00:00 +0000</pubDate>
        <link>https://graphscope.io/blog/tech/2024/02/22/GOpt-A-Unified-Graph-Query-Optimization-Framework-in-GraphScope.html</link>
        <guid isPermaLink="true">https://graphscope.io/blog/tech/2024/02/22/GOpt-A-Unified-Graph-Query-Optimization-Framework-in-GraphScope.html</guid>
        
        
        <category>Tech</category>
        
      </item>
    
      <item>
        <title>Release Notes: v0.26.0</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/assets/images/release_note_t.png&quot; alt=&quot;release-note&quot; /&gt;
We are pleased to announce an array of improvements in the GraphScope 0.26.0 release. In this release, under the original GraphScope framework, the persistent storage Groot of the Graph Interactive Query Engine (GIE) allows users to launch a Secondary Instance in read-only mode, thereby enhancing the performance of reading graph data. Under the GraphScope Flex architecture, the graph query engine GraphScope Interactive, which is designed for high-concurrency scenarios, now supports running on macOS and has introduced compaction operations for graph data.&lt;/p&gt;

&lt;p&gt;We highlight the following improvements included in this release:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Optimization of the Graph Interactive Engine GIE&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Persistent storage Groot leverages RocksDB’s read-only mode to support the creation of a &lt;a href=&quot;https://graphscope.io/docs/latest/storage_engine/groot#secondary-instance&quot;&gt;Secondary Instance&lt;/a&gt; to improve the performance of reading graph data. Meanwhile, new graph update operations will be periodically synchronized to the Secondary Instance;&lt;/li&gt;
  &lt;li&gt;Added support for automatically inferring the types of vertices/edges in queries, providing more friendly error messages and some performance improvements.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. Improvements to GraphScope Interactive&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;GraphScope Interactive is committed to providing users with exceptional query processing capabilities in high-concurrency scenarios. In this update, we have made the following improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Added support for macOS;&lt;/li&gt;
  &lt;li&gt;Introduced operations for graph data compaction to optimize the reading performance of graph data;&lt;/li&gt;
  &lt;li&gt;Added support for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STRING&lt;/code&gt; type edge properties.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3. Fixes for other stability issues&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fixed a memory leak issue in GIE;&lt;/li&gt;
  &lt;li&gt;Resolved data race conditions in Groot.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more detailed improvements that have been made in this release, please refer to the complete &lt;a href=&quot;https://github.com/alibaba/GraphScope/releases/tag/v0.26.0&quot;&gt;changelog&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Wed, 31 Jan 2024 00:33:20 +0000</pubDate>
        <link>https://graphscope.io/blog/releasenotes/2024/01/31/release-notes-0.26.0.html</link>
        <guid isPermaLink="true">https://graphscope.io/blog/releasenotes/2024/01/31/release-notes-0.26.0.html</guid>
        
        
        <category>ReleaseNotes</category>
        
      </item>
    
      <item>
        <title>Release Notes: v0.25.0</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/assets/images/release_note_t.png&quot; alt=&quot;release-note&quot; /&gt;
We are glad to announce a suite of upgrades in the latest GraphScope 0.25.0 release, bringing significant improvements to the platform. Starting with this version, our updates will be divided into two parts: one is the updates introduced under the original GraphScope framework (including the graph analytics engine GAE, graph interactive engine GIE, and graph learning engine GLE); the other is the latest product features built for the new GraphScope Flex architecture.&lt;/p&gt;

&lt;p&gt;In this release, under the original GraphScope framework, the graph interactive engine (GIE) allows users to express queries in natural language, which are then automatically translated into Cypher; at the same time, the performance of GIE has been improved for certain queries through the optimization of the persistent storage Groot. For graph learning tasks, we have integrated the latest &lt;a href=&quot;https://github.com/alibaba/graphlearn-for-pytorch&quot;&gt;GraphLearn-for-Pytorch (GLTorch) engine&lt;/a&gt;, which supports GPU acceleration for graph sampling and feature extraction, thereby enhancing the training and inference performance of graph neural networks. Under the GraphScope Flex architecture, the graph query engine GraphScope Interactive, designed for high-concurrency scenarios, has also made a series of improvements in functionality and user-friendliness.&lt;/p&gt;

&lt;p&gt;We highlight the following improvements included in this release:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Integration of the Graph Interactive Query Engine GIE with LLMs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To express graph queries, users typically need to use the Cypher or Gremlin languages, which creates a certain usage barrier. With the rapid application of Large Language Models (LLMs) in various industries, we have also tried to leverage the powerful capabilities of LLMs to allow users to express queries in natural language, which are then automatically translated into Cypher and executed on GIE.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;graphscope.langchain_prompt.query&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query_to_cypher&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;question&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Who is the son of Jia Baoyu?&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cypher_sentence&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query_to_cypher&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;question&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endpoint&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cypher_sentence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# MATCH (p:Person)-[:son_of]-&amp;gt;(q:Person)
# WHERE p.name = &apos;Jia Baoyu&apos;
# RETURN q.name
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;2. Integration with GraphLearn-for-Pytorch (GLTorch)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;GLTorch is a PyTorch-based graph neural network framework optimized for scenarios with single-machine multi-GPUs. It uses GPUs to accelerate graph sampling and feature extraction operations in graph neural networks; in addition, its API is compatible with PyG, allowing users to run their graph neural network models originally written with PyG API on GraphScope with minimal code changes.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# load the ogbn_arxiv graph as an example.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_ogbn_arxiv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# specify the learning engine.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glt_graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graphlearn_torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;paper&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;citation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;paper&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;node_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;paper&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;feat_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]},&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;node_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;paper&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;edge_dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;out&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;random_node_split&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;num_val&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;num_test&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;3. Improvements to GraphScope Interactive&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;GraphScope Interactive is dedicated to providing users with outstanding query processing capabilities in high-concurrency scenarios. In this update, we have made the following improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Added a cache mechanism to avoid repeated compilation of the same query during multiple executions;&lt;/li&gt;
  &lt;li&gt;Supported the use of string-type attributes as the primary key for vertices;&lt;/li&gt;
  &lt;li&gt;Added &lt;a href=&quot;https://graphscope.io/docs/latest/flex/interactive_intro&quot;&gt;user documentation&lt;/a&gt; for GraphScope Interactive.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;4. Performance Optimization of Persistent Storage Groot&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Optimized the performance of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;count()&lt;/code&gt; operator for vertices/edges. The performance has been improved when executing queries like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g.V().count()&lt;/code&gt; in GIE;&lt;/li&gt;
  &lt;li&gt;Added APIs related to disk usage in Groot, including used disk capacity and remaining available disk capacity.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more detailed improvements that have been made in this release, please refer to the complete &lt;a href=&quot;https://github.com/alibaba/GraphScope/releases/tag/v0.25.0&quot;&gt;changelog&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Wed, 15 Nov 2023 01:33:20 +0000</pubDate>
        <link>https://graphscope.io/blog/releasenotes/2023/11/15/release-notes-0.25.0.html</link>
        <guid isPermaLink="true">https://graphscope.io/blog/releasenotes/2023/11/15/release-notes-0.25.0.html</guid>
        
        
        <category>ReleaseNotes</category>
        
      </item>
    
      <item>
        <title>Import and Export Graph Data of Neo4j with GraphAr</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2023-09-14-title-picture.jpg&quot; alt=&quot;title-picture&quot; /&gt;
&lt;a href=&quot;https://graphscope.io/blog/tech/2023/08/29/GraphAr-A-Standard-Data-File-Format-for-Graph-Data-Storage-and-Retrieval&quot;&gt;GraphAr&lt;/a&gt; is an open source, standard data file format for graph data storage and retrieval. It defines a standardized file format for graph data, and provides a set of interfaces for generating, accessing, and transforming these formatted files.
This post is a quick guide that shows how to import and export graph data of &lt;a href=&quot;https://neo4j.com/product/neo4j-graph-database/&quot;&gt;Neo4j&lt;/a&gt; with GraphAr.&lt;/p&gt;

&lt;h3 id=&quot;what-is-graphar&quot;&gt;What is GraphAr?&lt;/h3&gt;

&lt;p&gt;GraphAr (Graph Archive, abbreviated as GAR) defines a standardized, system-independent file format for graph data and provides a set of interfaces for generating, accessing, and converting these formatted files. GraphAr can help various graph computing applications or existing systems to conveniently build and access graph data. It can be used as a direct data source for graph computing applications, as well as for importing/exporting and persistently storing graph data, reducing the overhead of collaboration between various graph systems. The following figure shows the scenario of using GraphAr as a graph data archiving format and data source for graph computing applications: with GraphAr, users can quickly and easily import/export graph data from/to graph databases, such as Neo4j, and use GraphAr as a data source for graph computing applications, such as GraphScope.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2023-09-14-graphar-exchange.jpg&quot; alt=&quot;graphar-exchange&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;graphar-spark-sdk&quot;&gt;GraphAr Spark SDK&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://alibaba.github.io/GraphAr/user-guide/spark-lib.html&quot;&gt;GraphAr Spark SDK&lt;/a&gt; uses maven as a package build system and requires Java 8 or higher. We provide a script to build GraphAr Spark SDK. To build GraphAr Spark SDK, run the following command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# download GraphAr Spark SDK source code&lt;/span&gt;
git clone https://github.com/alibaba/GraphAr.git
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;GraphAr
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;spark

&lt;span class=&quot;c&quot;&gt;# build GraphAr Spark SDK&lt;/span&gt;
./scripts/build.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;exportimport-graph-data-of-neo4j-with-graphar&quot;&gt;Export/Import Graph Data of Neo4j with GraphAr&lt;/h3&gt;

&lt;p&gt;Neo4j is a popular graph database system and it provide &lt;a href=&quot;https://neo4j.com/docs/spark/current/overview/&quot;&gt;Neo4j Spark Connector&lt;/a&gt; tool to import/export graph data between Neo4j and Spark. GraphAr Spark SDK can be used as a data source for Neo4j Spark Connector to import/export graph data between Neo4j and GraphAr.&lt;/p&gt;

&lt;p&gt;To demonstrate how to export graph data of Neo4j to GraphAr, we use &lt;a href=&quot;https://github.com/neo4j-graph-examples/movies&quot;&gt;movie graph data&lt;/a&gt; of Neo4j as an example to show how to export graph data of Neo4j with GraphAr. The following figure shows the movie graph data of Neo4j:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2023-09-14-movie-example.jpg&quot; alt=&quot;movie-example&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;deploy-neo4j&quot;&gt;Deploy Neo4j&lt;/h4&gt;

&lt;p&gt;Before exporting graph data of Neo4j with GraphAr, we need to deploy Neo4j. Here we provide a script to deploy Neo4j to HOME directory. But If you already have a Neo4j instance, you can skip this step. To deploy Neo4j, run the following command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./scripts/get-neo4j-to-home.sh
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;NEO4J_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/neo4j-community-4.4.23&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;NEO4J_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/bin:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Set the initial password of Neo4j:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;neo4j-admin set-initial-password xxxx &lt;span class=&quot;c&quot;&gt;# set your password here&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;load-movie-graph-data-to-neo4j&quot;&gt;Load Movie Graph Data to Neo4j&lt;/h4&gt;

&lt;p&gt;Neo4j provides a movie graph data example in their dateset. We can load this movie graph data to Neo4j. To load movie graph data to Neo4j, run the following command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./scripts/deploy-neo4j-movie-data.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;After loading movie graph data to Neo4j, we can use Neo4j Browser to check the movie graph data. The username is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;neo4j&lt;/code&gt; and the password is the one you set in the previous step. Open the &lt;a href=&quot;http://localhost:7474/browser&quot;&gt;Neo4j browser&lt;/a&gt; to check the movie graph data. The following figure shows the movie graph data of Neo4j:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/images/2023-09-14-neo4j-browser.jpg&quot; alt=&quot;neo4j-browser&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;export-graph-data-of-neo4j-with-graphar&quot;&gt;Export Graph Data of Neo4j with GraphAr&lt;/h4&gt;

&lt;p&gt;GraphAr provides a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Neo4j2GraphAr&lt;/code&gt; example class to export movie graph data of Neo4j to GraphAr. The following code shows how to export graph data of Neo4j to GraphAr:&lt;/p&gt;

&lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Neo4j2GraphAr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Unit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// connect to the Neo4j instance&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;spark&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SparkSession&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;builder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;appName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Neo4j to GraphAr for Movie Graph&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neo4j.url&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;bolt://localhost:7687&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neo4j.authentication.type&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;basic&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;neo4j.authentication.basic.username&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;nv&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;NEO4J_USR&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;get&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;neo4j.authentication.basic.password&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;nv&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;NEO4J_PWD&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;get&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;spark.master&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;local&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;getOrCreate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// initialize a graph writer&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;GraphWriter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;GraphWriter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// put movie graph data into writer&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;readAndPutDataIntoWriter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// output directory&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;outputPath&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// vertex chunk size&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;vertexChunkSize&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Long&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toLong&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// edge chunk size&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;edgeChunkSize&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Long&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toLong&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// file type&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;fileType&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// write in graphar format&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;outputPath&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;MovieGraph&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;vertexChunkSize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;edgeChunkSize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;fileType&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The code above shows how to export graph data of Neo4j to GraphAr. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;readAndPutDataIntoWriter&lt;/code&gt; method is used to read graph data of Neo4j and put the graph data into a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GraphWriter&lt;/code&gt; instance. The detail of the implementation of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;readAndPutDataIntoWriter&lt;/code&gt; method can be found in the &lt;a href=&quot;https://github.com/alibaba/GraphAr/blob/main/spark/src/main/scala/com/alibaba/graphar/example/Neo4j2GraphAr.scala&quot;&gt;Neo4j2GraphAr.scala&lt;/a&gt; file.&lt;/p&gt;

&lt;p&gt;To run the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Neo4j2GraphAr&lt;/code&gt; example, just run the following command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;NEO4J_USR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;neo4j&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;NEO4J_PWD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;xxxx&quot;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# the password you set in the previous step&lt;/span&gt;
./scripts/run-neo4j2graphar.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The example will convert the movie data in Neo4j to GraphAr data and save it to the directory &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tmp/graphar/neo4j2graphar&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;import-graph-data-of-neo4j-with-graphar&quot;&gt;Import Graph Data of Neo4j with GraphAr&lt;/h4&gt;

&lt;p&gt;In the same way, GraphAr provides a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GraphAr2Neo4j&lt;/code&gt; example class to import movie graph data of GraphAr to Neo4j. The following code shows how to import graph data of GraphAr to Neo4j:&lt;/p&gt;

&lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;GraphAr2Neo4j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Unit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// connect to the Neo4j instance&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;spark&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SparkSession&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;builder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;appName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;GraphAr to Neo4j for Movie Graph&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neo4j.url&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;bolt://localhost:7687&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neo4j.authentication.type&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;basic&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;neo4j.authentication.basic.username&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;nv&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;NEO4J_USR&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;get&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;neo4j.authentication.basic.password&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;nv&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;NEO4J_PWD&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;get&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;spark.master&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;local&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;getOrCreate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// path to the graph information file&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;graphInfoPath&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;graphInfo&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;GraphInfo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;loadGraphInfo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graphInfoPath&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// The edge data need to convert src and dst to the vertex id , so we need to read&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// the vertex data with index column.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;graphData&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;GraphReader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graphInfoPath&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;vertexData&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;graphData&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;_1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;edgeData&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;graphData&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;_2&lt;/span&gt;

    &lt;span class=&quot;nf&quot;&gt;putVertexDataIntoNeo4j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graphInfo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vertexData&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;putEdgeDataIntoNeo4j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graphInfo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vertexData&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edgeData&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The code above shows how to import graph data of GraphAr to Neo4j. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;putVertexDataIntoNeo4j&lt;/code&gt; method is used to put vertex data of GraphAr into Neo4j. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;putEdgeDataIntoNeo4j&lt;/code&gt; method is used to put edge data of GraphAr into Neo4j. The detail of the implementation of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;putVertexDataIntoNeo4j&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;putEdgeDataIntoNeo4j&lt;/code&gt; methods can be found in the &lt;a href=&quot;https://github.com/alibaba/GraphAr/blob/main/spark/src/main/scala/com/alibaba/graphar/example/GraphAr2Neo4j.scala&quot;&gt;GraphAr2Neo4j.scala&lt;/a&gt; file.&lt;/p&gt;

&lt;p&gt;To run the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GraphAr2Neo4j&lt;/code&gt; example, just run the following command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./scripts/run-graphar2neo4j.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we have successfully imported the movie graph data of GraphAr to Neo4j. We can use Neo4j Browser to check the movie graph data like the previous step.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;GraphAr define a simple and standard data file format for graph data storage and retrieval. It provides a set of interfaces for generating, accessing, and converting these formatted files. GraphAr can help various graph computing applications or existing systems to conveniently build and access graph data. It can be used as a direct data source for graph computing applications, as well as for importing/exporting and persistently storing graph data, reducing the overhead of collaboration between various graph systems. In this post, we show how to import and export graph data of Neo4j with GraphAr. There are many other examples in GraphAr Spark SDK. Please refer to &lt;a href=&quot;https://alibaba.github.io/GraphAr/applications/out-of-core.html&quot;&gt;more examples&lt;/a&gt; to learn about the other available case studies utilizing GraphAr Spark SDK.&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Sep 2023 06:00:42 +0000</pubDate>
        <link>https://graphscope.io/blog/tech/2023/09/14/Import-and-Export-Graph-Data-of-Neo4j-with-GraphAr.html</link>
        <guid isPermaLink="true">https://graphscope.io/blog/tech/2023/09/14/Import-and-Export-Graph-Data-of-Neo4j-with-GraphAr.html</guid>
        
        
        <category>Tech</category>
        
      </item>
    
  </channel>
</rss>
